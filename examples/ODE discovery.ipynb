{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a67455",
   "metadata": {},
   "source": [
    "# Examples of ordinary differential equation discovery with EPDE framework\n",
    "\n",
    "This notebook presents an overview of the evolutionary discovery of ODE on examples... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b9842d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from typing import Union, Callable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141bc1de",
   "metadata": {},
   "source": [
    "ADD SOME LINK TO ARTICLE ON EPDE PRINCIPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0539da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19ff1073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import epde\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "mpl.rc('font', size=SMALL_SIZE)\n",
    "mpl.rc('axes', titlesize=SMALL_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ecb2b",
   "metadata": {},
   "source": [
    "## First order ODE with trigonometric functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2644fba",
   "metadata": {},
   "source": [
    "Principles of equation discovery can be illustrated by a simple example of reconstruction of first-order equation:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:ODE1}\n",
    "x \\sin{t} + \\frac{d x}{d t} \\cos{t} = 1,\n",
    "\\end{equation}\n",
    "\n",
    "where the general solution for an arbitrary constant $C$ is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:ODE1_solution}\n",
    "x = \\sin{t} + C \\cos{t}.\n",
    "\\end{equation}\n",
    "\n",
    "To generate the data we will use an analytical particular solution, matching initial condition of $x(0) = 1.3$ (thus, $C = 1.3$) on the interval of $(0, 4 \\pi)$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:ODE1_part_solution}\n",
    "x = \\sin{t} + 1.3 \\cos{t}.\n",
    "\\end{equation}\n",
    "\n",
    "Furthermore, we will provide an example of using a priori known derivatvies, that can be easily calculated by differentiating the solution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\label{eq:ODE1_deriv_solution}\n",
    "x' = \\cos{t} - 1.3 \\sin{t}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a517dd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "C = 1.3\n",
    "t = np.linspace(0, 4*np.pi, 200)\n",
    "x = np.sin(t) + C * np.cos(t)\n",
    "x_dot = np.cos(t) - C * np.sin(t)\n",
    "\n",
    "max_axis_idx = x.ndim - 1\n",
    "\n",
    "plt.plot(t, x, color = 'k', label = 'x(t)')\n",
    "plt.plot(t, x_dot, color = 'r', label = \"x'(t)\")\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf7912d",
   "metadata": {},
   "source": [
    "Next, we shall initialize the equation search object. Here we are using multiobjective optimization (here, it i denoted explicitly, despite it being the default option). Non-defalut parameter is the size of the boundary: $s_{bnd} = 20$.\n",
    "\n",
    "Additional parameters will be as follows  population size number of optimziation epochs: ``n_epochs = 30``.\n",
    "\n",
    "To be able to discovery the correct governing equation, the set of elementary functions (i.e. tokens) (in addition to the derivatives, that are always used) has to include the trigonometric functions. Furthermore, we will include grid as elementary block to provide diversity of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0250493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7fc1b510b610>\n",
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7fc1b510b610>\n",
      "trig_token_params: VALUES = (0, 0)\n",
      "OrderedDict([('power', (1, 1)), ('dim', (0, 0))])\n"
     ]
    }
   ],
   "source": [
    "bnd = 20\n",
    "n_epochs = 50\n",
    "popsize = 8\n",
    "\n",
    "epde_search_obj = epde.EpdeSearch(multiobjective_mode = True, boundary = bnd, \n",
    "                                  dimensionality = max_axis_idx, coordinate_tensors = [t,])\n",
    "\n",
    "trig_tokens = epde.TrigonometricTokens(freq = (0.95, 1.05), dimensionality=max_axis_idx)\n",
    "grid_tokens = epde.GridTokens(['x_0',], dimensionality = max_axis_idx)\n",
    "\n",
    "epde_search_obj.set_moeadd_params(population_size = popsize, training_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9640381",
   "metadata": {},
   "source": [
    "The detection of differential equations is initiated with ``EpdeSearch.fit()`` method. Data is passed in the ``data`` argument, where the elements of the argument list are numpy.ndarrays, describing individual variables. The derivatives can be passed explicity in the argument ``derivs`` with the same logic: the argument has to be a list with numpy.ndarrays of derivatives for each variable. The dimensionality of such arrays has to be $n\\_points \\; \\times \\; deriv\\_ords$, where $n\\_points$ is the total number of points in the domain, and $deriv\\_ords$ is the number of passed derivatives. With ``derivs = None``, the derivatives are calculated numerically.\n",
    "\n",
    "Additional elementary functions, that we expect to have in the resulting equations, are passed in the ``additional_tokens`` argument. \n",
    "\n",
    "We define equation structure properties with the following settings:\n",
    "\n",
    "- Argument ``max_deriv_order`` sets the highest order of derivatives, that can be used in equation search. \n",
    "- Number of terms, accepted into the structure before regularization is regulated by argument ``equation_terms_max_number``.\n",
    "- Argument ``equation_factors_max_number`` defined number of terms in the term can be set as integer (in this case, the number of factors for the particular term is selected from a uniform distribution $U(1, equation\\_factors\\_max\\_number)$), or by dictionary, as in our example.\n",
    "- Argument ``data_fun_pow`` indicates the highest power of derivative-like token in the equation. With ``data_fun_pow = 2``, supported by sufficiently high ``equation_factors_max_number`` and ``equation_terms_max_number``, the terms like $u^2 u'$ and $(u')^2 \\cdot u^2$ can appear in the equation, while terms like $u^3$ won't be allowed in structures.\n",
    "- Sparsity constant range ``eq_sparsity_interval`` controls the complexities of the initially created candidate equations: the argument has to be a tuple of $(a_{left}, a_{right})$. Each candidate during the population initialization is given a value $\\lambda = \\exp{l}; \\; l \\sim U(\\ln a_{left}, \\ln a_{right})$. With the boundaries shifted to 0, the equation is regularized less, while with high values of $ a_{left} \\; \\& \\; a_{right}$ the majority of terms will be filtered out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cb9d921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deriv orders after definition [[None], [0]]\n",
      "initial_shape (200,) derivs_tensor.shape (200, 1)\n",
      "self.tokens is ['u', 'du/dx0']\n",
      "Here, derivs order is {'u': [None], 'du/dx0': [0]}\n",
      "The cardinality of defined token pool is [2 2 1]\n",
      "Among them, the pool contains [2 1]\n",
      "Creating new equation, sparsity value [0.00013173]\n",
      "New solution accepted, confirmed 1/8 solutions.\n",
      "Creating new equation, sparsity value [0.00041759]\n",
      "New solution accepted, confirmed 2/8 solutions.\n",
      "Creating new equation, sparsity value [0.00025425]\n",
      "Creating new equation, sparsity value [0.00017177]\n",
      "New solution accepted, confirmed 3/8 solutions.\n",
      "Creating new equation, sparsity value [2.06723287e-06]\n",
      "New solution accepted, confirmed 4/8 solutions.\n",
      "Creating new equation, sparsity value [0.00086434]\n",
      "New solution accepted, confirmed 5/8 solutions.\n",
      "Creating new equation, sparsity value [4.96260272e-05]\n",
      "New solution accepted, confirmed 6/8 solutions.\n",
      "Creating new equation, sparsity value [3.79898092e-05]\n",
      "New solution accepted, confirmed 7/8 solutions.\n",
      "Creating new equation, sparsity value [1.04795544e-05]\n",
      "New solution accepted, confirmed 8/8 solutions.\n",
      "best_obj 2\n",
      "Multiobjective optimization : 0-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 1-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 2-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 3-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 4-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 5-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 6-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 7-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 8-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 9-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 10-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 11-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 12-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 13-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 14-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 15-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 16-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 17-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 18-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 19-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 20-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 21-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 22-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 23-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 24-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 25-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 26-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 27-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 28-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 29-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 30-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 31-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 32-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 33-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 34-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 35-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 36-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 37-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 38-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 39-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 40-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 41-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 42-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 43-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 44-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 45-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 46-th epoch.\n",
      "During MO : processing 0-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 47-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 48-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 49-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "The optimization has been conducted.\n"
     ]
    }
   ],
   "source": [
    "factors_max_number = {'factors_num' : [1, 2], 'probas' : [0.65, 0.35]} # 1 factor with P = 0.65, 2 with P = 0.35\n",
    "\n",
    "epde_search_obj.fit(data=[x,], variable_names=['u',], max_deriv_order=(1,), derivs=[x_dot.reshape((-1, 1)),],\n",
    "                    equation_terms_max_number=4, data_fun_pow = 1,\n",
    "                    additional_tokens=[trig_tokens, grid_tokens],\n",
    "                    equation_factors_max_number=factors_max_number,\n",
    "                    eq_sparsity_interval=(1e-6, 1e-2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7998ee",
   "metadata": {},
   "source": [
    "The discovered equations can be accessed with ``EpdeSearch.equations()`` method. If the ``only_print = True``, than the equations will be printed in their text forms. \n",
    "\n",
    "Otherwise, they will be return: if the followup argument ``only_str`` is ``True``, than the equations are returned only in their symbolic string-form. If ``only_str = False``, than the equation in its program implementation ``epde.structure.main_structures.SoEq`` are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e30fe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0-th non-dominated level\n",
      "\n",
      "\n",
      "0.0 * x_0{power: 1.0, dim: 0.0} + 0.025217005432275823 * u{power: 1.0} + 0.0 * x_0{power: 1.0, dim: 0.0} * u{power: 1.0} + 0.6669677405245148 = du/dx0{power: 1.0} * cos{power: 1.0, freq: 1.0499000873630582, dim: 0.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 4}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.005657086033085299}} , with objective function values of [5.89152992 2.        ] \n",
      "\n",
      "0.00888228979551941 * x_0{power: 1.0, dim: 0.0} + 0.006798153402721441 * u{power: 1.0} * cos{power: 1.0, freq: 0.9974444910634812, dim: 0.0} + -1.0004120960660567 * u{power: 1.0} * sin{power: 1.0, freq: 1.0076995322921416, dim: 0.0} + 0.9973172059164307 = du/dx0{power: 1.0} * cos{power: 1.0, freq: 1.006623183604285, dim: 0.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 4}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.005319516674076823}} , with objective function values of [0.0147906 4.       ] \n",
      "\n",
      "0.0 * u{power: 1.0} + -0.999037871228366 * u{power: 1.0} * sin{power: 1.0, freq: 1.000001467098697, dim: 0.0} + 0.0 * x_0{power: 1.0, dim: 0.0} * du/dx0{power: 1.0} + 1.0076499665982352 = du/dx0{power: 1.0} * cos{power: 1.0, freq: 1.0020254178539623, dim: 0.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 4}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.001500514159705181}} , with objective function values of [0.08473055 2.5       ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epde_search_obj.equations(only_print = True)\n",
    "res = epde_search_obj.equations(False, only_str = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72e4164",
   "metadata": {},
   "source": [
    "The particular solution of the ODE in addition to the governing equation $x \\sin{(t)} + x' \\cos{(t)} = 1,$ also matches the solution of equation $x' \\sin{(t)}  - x \\cos{(t)} = - 1.3$. With them having (in ideal case) the same fitness functions values, we can expect presence of at least one type of such results in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ca958",
   "metadata": {},
   "source": [
    "We can use visual analysis of the Pareto-optimal set of candidate equations, placed in the complexity-quality criteria space, to select the optimal equation. Method ``EpdeSearch.visualize_solutions()`` illustrates the first non-dominated set of solutions (i.e. Pareto frontier). With its help we can see, that equations like $x' \\cdot \\sin{(1.001 \\; t)} = 9.997\\cdot 10^{-1} x \\cdot cos{(1.006 \\; t)} + -1.274$ (due to stochastic nature of evolutionary optimization) matches the knee point of a Pareto frontier curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "010064f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\\begin{eqnarray*} \\frac{\\partial u}{\\partial x_0} \\cdot cos^{1.0}(1.05 x_{0.0}) = 2.522\\cdot 10^{-2} u + 6.67\\cdot 10^{-1}  \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial u}{\\partial x_0} \\cdot cos^{1.0}(1.007 x_{0.0}) = 8.882\\cdot 10^{-3} x_0 + 6.798\\cdot 10^{-3} u \\cdot cos^{1.0}(9.974\\cdot 10^{-1}  x_{0.0}) + -1.0u \\cdot sin^{1.0}(1.008 x_{0.0}) + 9.973\\cdot 10^{-1}  \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial u}{\\partial x_0} \\cdot cos^{1.0}(1.002 x_{0.0}) = -9.99\\cdot 10^{-1} u \\cdot sin^{1.0}(1.0 x_{0.0}) + 1.008 \\end{eqnarray*}$\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epde_search_obj.visualize_solutions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f355e42",
   "metadata": {},
   "source": [
    "## Second order non-linear ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789ce03f",
   "metadata": {},
   "source": [
    "A more complex example of the problem can be posed by a linear non-homogeneous equation $x'' + \\sin{(2t)} x' + 4 x = 1.5 t$. To get a synthetic dataset we can solve it with Runge-Kutta fourth order method. The particular solution matches the initial problem of $x|_{t = 0} = 0.8$, $x''|_{t = 0} = 2.0$ for domain $t \\in [0, 16)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4ec062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_order_lODE_by_RK(initial : tuple, timestep : float, steps : int, g1 : Callable, \n",
    "                           g2 : Callable, g3 : Callable, g4 : Callable):\n",
    "    res = np.full(shape = (steps, 2), fill_value = initial, dtype=np.float64)\n",
    "    for step in range(steps-1):\n",
    "        t = step*timestep\n",
    "        k1 = res[step, 1] ; x1 = res[step, 0] + timestep/2. * k1\n",
    "        l1 = (g4(t) - g3(t)*res[step, 0] - g2(t)*res[step, 1]) / g1(t); y1 = res[step, 1] + timestep/2. * l1\n",
    "\n",
    "        k2 = y1; x2 = res[step, 0] + timestep/2. * k2\n",
    "        l2 = (g4(t) - g3(t)*x1 - g2(t)*y1) / g1(t); y2 = res[step, 1] + timestep/2. * l2\n",
    "\n",
    "        k3 = y2\n",
    "        l3 = (g4(t) - g3(t)*x2 - g2(t)*y2) / g1(t);\n",
    "        \n",
    "        x3 = res[step, 0] + timestep * k1 - 2 * timestep * k2 + 2 * timestep * k3\n",
    "        y3 = res[step, 1] + timestep * l1 - 2 * timestep * l2 + 2 * timestep * l3\n",
    "        k4 = y3\n",
    "        l4 = (g4(t) - g3(t)*x3 - g2(t)*y3) / g1(t)\n",
    "        \n",
    "        res[step+1, 0] = res[step, 0] + timestep / 6. * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "        res[step+1, 1] = res[step, 1] + timestep / 6. * (l1 + 2 * l2 + 2 * l3 + l4)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91321ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = lambda x: 1.\n",
    "g2 = lambda x: np.sin(2*x)\n",
    "g3 = lambda x: 4.\n",
    "g4 = lambda x: 1.5*x\n",
    "\n",
    "step = 0.05; steps_num = 320\n",
    "t = np.arange(start = 0., stop = step * steps_num, step = step)\n",
    "solution = second_order_lODE_by_RK(initial=(0.8, 2.), timestep=step, steps=steps_num, \n",
    "                                  g1=g1, g2=g2, g3=g3, g4=g4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b50e0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(t, solution[:, 0], label = 'x(t)', color = 'limegreen')\n",
    "plt.plot(t, solution[:, 1], label = \"x'(t)\", color = 'darkmagenta')\n",
    "plt.grid(); plt.legend()\n",
    "plt.xlabel('T'); plt.ylabel('variable value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e2177",
   "metadata": {},
   "source": [
    "While it can be insightful to analyze the process with the data-driven differential equation, occasionally it will be needed to predict the future state of the system, based on the equation. In this experiment we will split the input dataset into train ($t \\in [0, 8)$) and test ($t \\in [8, 16)$) parts. To validate the predicitive ability, the best equation will be integrated over the domain, using internal automaitic differential equation solution methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d42f3288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epde_discovery(t, x, use_ann = False):\n",
    "    dimensionality = x.ndim - 1\n",
    "    \n",
    "\n",
    "    epde_search_obj = epde.EpdeSearch(use_solver = False, dimensionality = dimensionality, boundary = 30,\n",
    "                                      coordinate_tensors = [t,])\n",
    "    if use_ann:\n",
    "        epde_search_obj.set_preprocessor(default_preprocessor_type='ANN', # use_smoothing = True poly\n",
    "                                         preprocessor_kwargs={'epochs_max' : 50000})# \n",
    "    else:\n",
    "        epde_search_obj.set_preprocessor(default_preprocessor_type='poly', # use_smoothing = True poly\n",
    "                                         preprocessor_kwargs={'use_smoothing' : False, 'sigma' : 1, \n",
    "                                                              'polynomial_window' : 3, 'poly_order' : 3}) \n",
    "                                                            # 'epochs_max' : 10000})\n",
    "    eps = 5e-7\n",
    "    popsize = 8\n",
    "    epde_search_obj.set_moeadd_params(population_size = popsize, training_epochs=55)\n",
    "    trig_tokens = epde.TrigonometricTokens(freq = (2 - eps, 2 + eps), \n",
    "                                      dimensionality = dimensionality)\n",
    "    factors_max_number = {'factors_num' : [1, 2], 'probas' : [0.65, 0.35]}\n",
    "\n",
    "    custom_grid_tokens = epde.GridTokens(dimensionality = dimensionality)\n",
    "    \n",
    "    epde_search_obj.fit(data=[x,], variable_names=['u',], max_deriv_order=(2,),\n",
    "                        equation_terms_max_number=6, data_fun_pow = 1, \n",
    "                        additional_tokens=[trig_tokens, custom_grid_tokens], \n",
    "                        equation_factors_max_number=factors_max_number,\n",
    "                        eq_sparsity_interval=(1e-6, 1e-2))\n",
    "\n",
    "    epde_search_obj.equations(only_print = True, num = 1)\n",
    "    \n",
    "    '''\n",
    "    Having insight about the initial ODE structure, we are extracting the equation with complexity of 5\n",
    "    \n",
    "    In other cases, you should call epde_search_obj.equations(only_print = True), or epde_search_obj.\n",
    "    where the algorithm presents Pareto frontier of optimal equations.\n",
    "    '''\n",
    "\n",
    "    return epde_search_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c734869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7f2788616e20>\n",
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7f2788616e20>\n",
      "trig_token_params: VALUES = (0, 0)\n",
      "OrderedDict([('power', (1, 1)), ('dim', (0, 0))])\n",
      "Deriv orders after definition [[None], [0], [0, 0]]\n",
      "initial_shape (160,) derivs_tensor.shape (160, 2)\n",
      "self.tokens is ['u', 'du/dx0', 'd^2u/dx0^2']\n",
      "Here, derivs order is {'u': [None], 'du/dx0': [0], 'd^2u/dx0^2': [0, 0]}\n",
      "The cardinality of defined token pool is [3 2 1]\n",
      "Among them, the pool contains [3 1]\n",
      "Creating new equation, sparsity value [4.04371548e-06]\n",
      "New solution accepted, confirmed 1/8 solutions.\n",
      "Creating new equation, sparsity value [0.00581916]\n",
      "New solution accepted, confirmed 2/8 solutions.\n",
      "Creating new equation, sparsity value [6.99509856e-05]\n",
      "New solution accepted, confirmed 3/8 solutions.\n",
      "Creating new equation, sparsity value [5.32778731e-06]\n",
      "New solution accepted, confirmed 4/8 solutions.\n",
      "Creating new equation, sparsity value [1.10277863e-05]\n",
      "New solution accepted, confirmed 5/8 solutions.\n",
      "Creating new equation, sparsity value [0.00670686]\n",
      "New solution accepted, confirmed 6/8 solutions.\n",
      "Creating new equation, sparsity value [1.87153502e-06]\n",
      "New solution accepted, confirmed 7/8 solutions.\n",
      "Creating new equation, sparsity value [0.00113239]\n",
      "New solution accepted, confirmed 8/8 solutions.\n",
      "[0.54, 0.45999999999999996] [[0.16, 0.84], [0.0, 1.0], [0.64, 0.36], [0.46, 0.54], [0.98, 0.020000000000000018], [0.54, 0.45999999999999996]]\n",
      "best_obj 2\n",
      "Multiobjective optimization : 0-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 1-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 2-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 3-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 4-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 5-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 6-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 7-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 8-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 9-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 10-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 11-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 12-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 13-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 14-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 15-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 16-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 17-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 18-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 19-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 20-th epoch.\n",
      "During MO : processing 0-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 21-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 22-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 23-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 24-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 25-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 26-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 27-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 28-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 29-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 30-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 31-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 32-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 33-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 34-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 35-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 36-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 37-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 38-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 39-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 40-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 41-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 42-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 43-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 44-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiobjective optimization : 45-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 46-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 47-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 48-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 49-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 50-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 51-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 52-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 53-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "Multiobjective optimization : 54-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "The optimization has been conducted.\n",
      "\n",
      "\n",
      "0-th non-dominated level\n",
      "\n",
      "\n",
      "0.0 * d^2u/dx0^2{power: 1.0} + -0.11852093259823644 * t{power: 1.0, dim: 0.0} + 0.36577252189514686 * t{power: 1.0, dim: 0.0} * cos{power: 1.0, freq: 1.9999997461454138, dim: 0.0} + -0.980316863474351 * u{power: 1.0} * sin{power: 1.0, freq: 2.000000141169946, dim: 0.0} + 0.0 * t{power: 1.0, dim: 0.0} * du/dx0{power: 1.0} + 1.2254264146899703 = du/dx0{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.0044595328173458}} , with objective function values of [2.91933544 3.5       ] \n",
      "\n",
      "-0.2055064934951428 * du/dx0{power: 1.0} + -0.0035309324905232475 * d^2u/dx0^2{power: 1.0} * t{power: 1.0, dim: 0.0} + -0.779367282745194 * t{power: 1.0, dim: 0.0} * cos{power: 1.0, freq: 1.999999554309517, dim: 0.0} + -0.2876572777322552 * u{power: 1.0} + 2.0086219620617323 * u{power: 1.0} * cos{power: 1.0, freq: 2.0000004333155865, dim: 0.0} + -1.202721068333836 = du/dx0{power: 1.0} * sin{power: 1.0, freq: 1.999999518103412, dim: 0.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.0020695345959820125}} , with objective function values of [0.65309385 7.5       ] \n",
      "\n",
      "-3.951218144289313 * u{power: 1.0} + 0.0 * t{power: 1.0, dim: 0.0} * sin{power: 1.0, freq: 1.9999995807630981, dim: 0.0} + 0.0 * u{power: 1.0} * cos{power: 1.0, freq: 2.000000378681952, dim: 0.0} + -0.9900591677672463 * d^2u/dx0^2{power: 1.0} + 1.4766602719611732 * t{power: 1.0, dim: 0.0} + 0.014961528346664596 = du/dx0{power: 1.0} * sin{power: 1.0, freq: 1.9999995886960809, dim: 0.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.004122473023559842}} , with objective function values of [0.80876299 4.5       ] \n",
      "\n",
      "-3.973456847439746 * u{power: 1.0} + 1.4807802528320022 * t{power: 1.0, dim: 0.0} + 0.0 * d^2u/dx0^2{power: 1.0} * sin{power: 1.0, freq: 1.999999729438855, dim: 0.0} + -0.9922888819896745 * du/dx0{power: 1.0} * sin{power: 1.0, freq: 2.0000003247686093, dim: 0.0} + -0.02489648279447621 * u{power: 1.0} * sin{power: 1.0, freq: 1.9999998567712671, dim: 0.0} + 0.04836827440130382 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.0053721087732711585}} , with objective function values of [0.78069956 5.5       ] \n",
      "\n",
      "-3.9674622737518246 * u{power: 1.0} + 1.4800804249030497 * t{power: 1.0, dim: 0.0} + -0.9950430233227409 * d^2u/dx0^2{power: 1.0} + 0.02943784296981477 * t{power: 1.0, dim: 0.0} * sin{power: 1.0, freq: 1.9999995807630981, dim: 0.0} + -0.08104293233348653 * u{power: 1.0} * sin{power: 1.0, freq: 1.9999998567712671, dim: 0.0} + 0.05682269585623731 = du/dx0{power: 1.0} * sin{power: 1.0, freq: 1.9999997453694336, dim: 0.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.003101475166935847}} , with objective function values of [0.74033353 6.5       ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_max = 160\n",
    "t_train = t[:t_max]; t_test = t[t_max:] \n",
    "x_train = solution[:t_max, 0]; x_test = solution[t_max:, 0]\n",
    "\n",
    "epde_search_obj = epde_discovery(t_train, x_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d363bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\\begin{eqnarray*} \\frac{\\partial u}{\\partial x_0} = -1.185\\cdot 10^{-1} t + 3.658\\cdot 10^{-1} t \\cdot cos^{1.0}(2.0 x_{0.0}) + -9.803\\cdot 10^{-1} u \\cdot sin^{1.0}(2.0 x_{0.0}) + 1.225 \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial u}{\\partial x_0} \\cdot sin^{1.0}(2.0 x_{0.0}) = -2.055\\cdot 10^{-1} \\frac{\\partial u}{\\partial x_0} + -3.531\\cdot 10^{-3} \\frac{\\partial ^2u}{\\partial x_0^2} \\cdot t + -7.794\\cdot 10^{-1} t \\cdot cos^{1.0}(2.0 x_{0.0}) + -2.877\\cdot 10^{-1} u + 2.009u \\cdot cos^{1.0}(2.0 x_{0.0}) + -1.203 \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial u}{\\partial x_0} \\cdot sin^{1.0}(2.0 x_{0.0}) = -3.951u + -9.901\\cdot 10^{-1} \\frac{\\partial ^2u}{\\partial x_0^2} + 1.477t + 1.496\\cdot 10^{-2}  \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial ^2u}{\\partial x_0^2} = -3.973u + 1.481t + -9.923\\cdot 10^{-1} \\frac{\\partial u}{\\partial x_0} \\cdot sin^{1.0}(2.0 x_{0.0}) + -2.49\\cdot 10^{-2} u \\cdot sin^{1.0}(2.0 x_{0.0}) + 4.837\\cdot 10^{-2}  \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial u}{\\partial x_0} \\cdot sin^{1.0}(2.0 x_{0.0}) = -3.967u + 1.48t + -9.95\\cdot 10^{-1} \\frac{\\partial ^2u}{\\partial x_0^2} + 2.944\\cdot 10^{-2} t \\cdot sin^{1.0}(2.0 x_{0.0}) + -8.104\\cdot 10^{-2} u \\cdot sin^{1.0}(2.0 x_{0.0}) + 5.682\\cdot 10^{-2}  \\end{eqnarray*}$\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epde_search_obj.visualize_solutions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925ef946",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = epde_search_obj.get_equations_by_complexity(4.5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9580909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"-3.951218144289313 * u{power: 1.0} + 0.0 * t{power: 1.0, dim: 0.0} * sin{power: 1.0, freq: 1.9999995807630981, dim: 0.0} + 0.0 * u{power: 1.0} * cos{power: 1.0, freq: 2.000000378681952, dim: 0.0} + -0.9900591677672463 * d^2u/dx0^2{power: 1.0} + 1.4766602719611732 * t{power: 1.0, dim: 0.0} + 0.014961528346664596 = du/dx0{power: 1.0} * sin{power: 1.0, freq: 1.9999995886960809, dim: 0.0}\\n{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.004122473023559842}}\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system.text_form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab632c",
   "metadata": {},
   "source": [
    "While there are multiple equations on the Pareto frontier, we can establish two candidates for the knee-points (for brevity they are denoted by their complexities: 4 and 4.5). The further increase in complexity does not provide significant error decrease, thus is not necessary. We shall analyze the process representation quality on the test dataset by solving initial value problem.\n",
    "\n",
    "The equation with complexity of 4.5 is second-order, thus to solve it we need the function value and the solution derivatve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3da228c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.951218144289313 * u{power: 1.0} + 0.0 * t{power: 1.0, dim: 0.0} * sin{power: 1.0, freq: 1.9999995807630981, dim: 0.0} + 0.0 * u{power: 1.0} * cos{power: 1.0, freq: 2.000000378681952, dim: 0.0} + -0.9900591677672463 * d^2u/dx0^2{power: 1.0} + 1.4766602719611732 * t{power: 1.0, dim: 0.0} + 0.014961528346664596 = du/dx0{power: 1.0} * sin{power: 1.0, freq: 1.9999995886960809, dim: 0.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.004122473023559842}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from epde.interface.solver_integration import BoundaryConditions, BOPElement\n",
    "\n",
    "eq_1 = epde_search_obj.get_equations_by_complexity(4.5)[0]\n",
    "print(eq_1.text_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ee77966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ode_bop(key, var, term, grid_loc, value):\n",
    "    bop = BOPElement(axis = 0, key = key, term = term, power = 1, var = var)\n",
    "    bop_grd_np = np.array([[grid_loc,]])\n",
    "    bop.set_grid(torch.from_numpy(bop_grd_np).type(torch.FloatTensor))\n",
    "    bop.values = torch.from_numpy(np.array([[value,]])).float()\n",
    "    return bop\n",
    "\n",
    "bop_u = get_ode_bop('u', 0, [None], t_test[0], x_test[0])\n",
    "# Set derivative with central finite difference \n",
    "bop_du = get_ode_bop('dudt', 0, [0,], t_test[0], (x_test[1] - x_train[-1])/(2*(t_test[1] - t_test[0])))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e831e78d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using explicitly sent system of equations.\n",
      "dimensionality is 1\n",
      "grid.shape is (160,)\n",
      "Shape of the grid for solver torch.Size([160, 1])\n",
      "Grid is  <class 'torch.Tensor'> torch.Size([160, 1])\n",
      "torch.Size([1])\n",
      "[2023-11-17 14:22:25.884183] initial (min) loss is 158.1117401123047\n",
      "[2023-11-17 14:22:26.021060] Print every 5000 step\n",
      "Step = 0 loss = 158.111740 normalized loss line= -0.000000x+1.000000. There was 1 stop dings already.\n",
      "[2023-11-17 14:23:14.018034] No improvement in 100 steps\n",
      "Step = 1003 loss = 0.000193 normalized loss line= -1.568164x+154.301719. There was 1 stop dings already.\n",
      "[2023-11-17 14:23:18.626122] No improvement in 100 steps\n",
      "Step = 1105 loss = 0.001660 normalized loss line= -0.034247x+7.008235. There was 2 stop dings already.\n",
      "[2023-11-17 14:23:23.117335] No improvement in 100 steps\n",
      "Step = 1205 loss = 0.035738 normalized loss line= -0.001361x+0.462532. There was 3 stop dings already.\n",
      "[2023-11-17 14:23:29.748140] No improvement in 100 steps\n",
      "Step = 1349 loss = 0.000676 normalized loss line= 0.113782x+16.080034. There was 4 stop dings already.\n",
      "[2023-11-17 14:23:38.369849] No improvement in 100 steps\n",
      "Step = 1533 loss = 0.012405 normalized loss line= 0.013265x+0.129202. There was 5 stop dings already.\n",
      "[2023-11-17 14:23:42.897361] No improvement in 100 steps\n",
      "Step = 1633 loss = 0.000953 normalized loss line= -0.059929x+12.522327. There was 6 stop dings already.\n",
      "[2023-11-17 14:23:47.428764] No improvement in 100 steps\n",
      "Step = 1733 loss = 0.000575 normalized loss line= -0.029961x+31.163667. There was 7 stop dings already.\n",
      "[2023-11-17 14:23:55.428386] No improvement in 100 steps\n",
      "Step = 1908 loss = 0.015817 normalized loss line= -0.008667x+1.119618. There was 8 stop dings already.\n",
      "[2023-11-17 14:23:59.988453] No improvement in 100 steps\n",
      "Step = 2008 loss = 0.002209 normalized loss line= 0.027394x+6.372215. There was 9 stop dings already.\n",
      "[2023-11-17 14:24:04.566692] No improvement in 100 steps\n",
      "Step = 2108 loss = 0.013859 normalized loss line= 0.002540x+0.507256. There was 10 stop dings already.\n",
      "[2023-11-17 14:24:09.110130] No improvement in 100 steps\n",
      "Step = 2208 loss = 0.007771 normalized loss line= 0.013770x+1.697541. There was 11 stop dings already.\n"
     ]
    }
   ],
   "source": [
    "pred_u = epde_search_obj.predict(system=eq_1, boundary_conditions = [bop_u(), bop_du()],\n",
    "                                 grid = [t_test,], strategy='autograd', solver_kwargs = {'use_cache':False, 'step_plot_save':False})\n",
    "pred_u = pred_u.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa94fc40",
   "metadata": {},
   "source": [
    "To evaluate the quality of obtained equation, we will check the MAPE error and visualize the solution oftest domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01bcd998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on the test dataset is 0.006158279444752504\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'MAPE on the test dataset is {np.mean(np.abs((pred_u - x_test)/x_test))}')\n",
    "\n",
    "plt.plot(t_test, pred_u, color = 'b', label = 'Automatic solution of the equation')\n",
    "plt.plot(t_test[::3], x_test[::3], '*', color = 'r', label = 'Referential data')\n",
    "plt.xlabel('Time'); plt.ylabel('Predicted value') \n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f095cf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0.0 * d^2u/dx0^2{power: 1.0} + 0.6334141886464264 * u{power: 1.0} + -0.4036802767762263 * t{power: 1.0, dim: 0.0} + -0.5265121291057969 * t{power: 1.0, dim: 0.0} * sin{power: 1.0, freq: 2.000000359965463, dim: 0.0} + 0.2596251993636941 * t{power: 1.0, dim: 0.0} * cos{power: 1.0, freq: 1.9999999936069808, dim: 0.0} + 0.9225210424019804 = du/dx0{power: 1.0}\\n{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2], 'probas': [0.65, 0.35]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.005910228093653957}}\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eq2 = epde_search_obj.get_equations_by_complexity(4)[0]\n",
    "eq2.text_form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1a1f02",
   "metadata": {},
   "source": [
    "This equation has the first order, thus only a single initial condition $u|_{t = t_{val, 0}}$ is needed, and we get it from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4e621cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using explicitly sent system of equations.\n",
      "dimensionality is 1\n",
      "grid.shape is (160,)\n",
      "Shape of the grid for solver torch.Size([160, 1])\n",
      "Grid is  <class 'torch.Tensor'> torch.Size([160, 1])\n",
      "torch.Size([1])\n",
      "[2023-10-27 13:26:06.629574] initial (min) loss is 314.85504150390625\n",
      "[2023-10-27 13:26:06.673411] Print every 1000 step\n",
      "Step = 0 loss = 314.855042 normalized loss line= -0.000000x+1.000000. There was 1 stop dings already.\n",
      "[2023-10-27 13:26:13.090162] No improvement in 100 steps\n",
      "Step = 175 loss = 0.681533 normalized loss line= -0.670408x+46.723266. There was 1 stop dings already.\n",
      "[2023-10-27 13:26:16.941645] No improvement in 100 steps\n",
      "Step = 275 loss = 0.661911 normalized loss line= 0.000372x+1.001404. There was 2 stop dings already.\n",
      "[2023-10-27 13:26:21.296751] No improvement in 100 steps\n",
      "Step = 389 loss = 0.673448 normalized loss line= -0.000640x+1.035446. There was 3 stop dings already.\n",
      "[2023-10-27 13:26:35.747094] No improvement in 100 steps\n",
      "Step = 799 loss = 0.036245 normalized loss line= -0.000170x+1.011662. There was 4 stop dings already.\n",
      "[2023-10-27 13:26:42.428287] Print every 1000 step\n",
      "Step = 1000 loss = 0.035423 normalized loss line= -0.000115x+1.022673. There was 5 stop dings already.\n",
      "[2023-10-27 13:26:51.195499] No improvement in 100 steps\n",
      "Step = 1254 loss = 0.035086 normalized loss line= 0.020060x+0.660700. There was 5 stop dings already.\n",
      "[2023-10-27 13:27:04.285616] No improvement in 100 steps\n",
      "Step = 1624 loss = 0.034638 normalized loss line= -0.001039x+2.199179. There was 6 stop dings already.\n",
      "[2023-10-27 13:27:17.496981] Print every 1000 step\n",
      "Step = 2000 loss = 0.033968 normalized loss line= -0.000079x+1.001089. There was 7 stop dings already.\n",
      "[2023-10-27 13:27:18.136004] No improvement in 100 steps\n",
      "Step = 2015 loss = 0.033847 normalized loss line= -0.009918x+2.230062. There was 7 stop dings already.\n",
      "[2023-10-27 13:27:27.216756] No improvement in 100 steps\n",
      "Step = 2245 loss = 0.033715 normalized loss line= 0.034518x+0.585910. There was 8 stop dings already.\n",
      "[2023-10-27 13:27:38.043199] No improvement in 100 steps\n",
      "Step = 2560 loss = 0.033166 normalized loss line= 0.025307x+0.268369. There was 9 stop dings already.\n",
      "[2023-10-27 13:27:44.689097] No improvement in 100 steps\n",
      "Step = 2756 loss = 0.033120 normalized loss line= 0.041654x+0.022290. There was 10 stop dings already.\n",
      "[2023-10-27 13:27:52.201592] No improvement in 100 steps\n",
      "Step = 2970 loss = 0.097098 normalized loss line= 0.003246x+0.240966. There was 11 stop dings already.\n"
     ]
    }
   ],
   "source": [
    "bop_u = get_ode_bop('u', 0, [None], t_test[0], x_test[0])\n",
    "pred_u = epde_search_obj.predict(system=eq2, \n",
    "                                 boundary_conditions = [bop_u(),],# bop_du()],\n",
    "                                 grid = [t_test,], strategy='autograd')\n",
    "pred_u = pred_u.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "565e755c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on the test dataset is 0.40879904876947004\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'MAPE on the test dataset is {np.mean(np.abs((pred_u - x_test)/x_test))}')\n",
    "\n",
    "plt.plot(t_test, pred_u, color = 'b', label = 'Automatic solution of the equation')\n",
    "plt.plot(t_test[::3], x_test[::3], '*', color = 'r', label = 'Referential data')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbeda78f",
   "metadata": {},
   "source": [
    "In the result we can see, that the equation $1.42 t - 3.81 u -0.96 u'' + 0.007 = u' \\sin{(2.0 t)}$ can decently describe the process, while the alternative equation is not representative enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd718f89",
   "metadata": {},
   "source": [
    "## Van der Pol oscillator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea2c359",
   "metadata": {},
   "source": [
    "The problem of non-linear ordinary differential equations discovery can be demonstrated with the Van der Pol oscillator. Initially introduced to describe the relaxation-oscillation cycle produced by the electromagnetic field, the model has found applications in other spheres of science, such as biology or seismology. Its state is governed by equation $u'' + \\mathcal{E}(u^2 - 1)u' + u = 0$, where $\\mathcal{E}$ is a positive constant (in the example we will use $\\mathcal{E} = 0.2$). To prepare synthetic dataset we will again use Runge-Kutta method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ad06186e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VdP_by_RK(initial: tuple, timestep: float, steps: int, epsilon: float):\n",
    "    res = np.full(shape = (steps, 2), fill_value = initial, dtype=np.float64)\n",
    "    for step in range(steps-1):\n",
    "        t = step*timestep\n",
    "        k1 = res[step, 1] ; x1 = res[step, 0] + timestep/2. * k1\n",
    "        l1 =  - epsilon*(res[step, 0]**2 - 1)*res[step, 1] - res[step, 0]; y1 = res[step, 1] + timestep/2. * l1\n",
    "\n",
    "        k2 = y1; x2 = res[step, 0] + timestep/2. * k2\n",
    "        l2 = - epsilon*(x1**2 - 1)*y1 - x1; y2 = res[step, 1] + timestep/2. * l2\n",
    "\n",
    "        k3 = y2\n",
    "        l3 = - epsilon*(x2**2 - 1)*y2 - x2\n",
    "        \n",
    "        x3 = res[step, 0] + timestep * k1 - 2 * timestep * k2 + 2 * timestep * k3\n",
    "        y3 = res[step, 1] + timestep * l1 - 2 * timestep * l2 + 2 * timestep * l3\n",
    "        k4 = y3\n",
    "        l4 = - epsilon*(x3**2 - 1)*y3 - x3\n",
    "        \n",
    "        res[step+1, 0] = res[step, 0] + timestep / 6. * (k1 + 2 * k2 + 2 * k3 + k4)\n",
    "        res[step+1, 1] = res[step, 1] + timestep / 6. * (l1 + 2 * l2 + 2 * l3 + l4)\n",
    "    return res\n",
    "\n",
    "def prepare_VdP_data(initial = (np.sqrt(3)/2., 1./2.), step = 0.05, steps_num = 640, epsilon = 0.2):\n",
    "    t = np.arange(start = 0., stop = step * steps_num, step = step)\n",
    "    solution = VdP_by_RK(initial=initial, timestep=step, steps=steps_num, \n",
    "                                      epsilon=epsilon)\n",
    "    return t, solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e0a8ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epde_discovery_as_ode(t, x, use_ann:bool = False):\n",
    "    dimensionality = x.ndim - 1\n",
    "    epde_search_obj = epde.EpdeSearch(use_solver = False, dimensionality = dimensionality, boundary = 50,\n",
    "                                           coordinate_tensors = [t,])\n",
    "    if use_ann:\n",
    "        epde_search_obj.set_preprocessor(default_preprocessor_type='ANN',\n",
    "                                         preprocessor_kwargs={'epochs_max' : 35000})\n",
    "    else:\n",
    "        epde_search_obj.set_preprocessor(default_preprocessor_type='poly',\n",
    "                                         preprocessor_kwargs={'use_smoothing' : True, 'sigma' : 1, \n",
    "                                                              'polynomial_window' : 3, 'poly_order' : 3})\n",
    "    popsize = 12\n",
    "    epde_search_obj.set_moeadd_params(population_size = popsize, training_epochs=100)\n",
    "    \n",
    "    factors_max_number = {'factors_num' : [1, 2, 3], 'probas' : [0.4, 0.3, 0.3]}\n",
    "    \n",
    "    epde_search_obj.fit(data=[x,], variable_names=['u',], max_deriv_order=(2,),\n",
    "                        equation_terms_max_number=6, data_fun_pow = 2,\n",
    "                        equation_factors_max_number=factors_max_number,\n",
    "                        eq_sparsity_interval=(1e-12, 1e1))\n",
    "    \n",
    "    epde_search_obj.equations(only_print = True, num = 1)\n",
    "    \n",
    "    return epde_search_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f0f50bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t, x_stacked = prepare_VdP_data()\n",
    "t_max = 320\n",
    "x, y = x_stacked[:, 0], x_stacked[:, 1]\n",
    "t_train, t_test = t[:t_max], t[:t_max]\n",
    "x_train, x_test = x[:t_max], x[t_max:]\n",
    "y_train, y_test = y[:t_max], y[t_max:]\n",
    "plt.plot(x_train, y_train, color = 'midnightblue')\n",
    "plt.plot(x_test[::3], y_test[::3], color = 'orangered')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ea55038c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7fc1da55e7f0>\n",
      "setting builder with <epde.optimizers.builder.StrategyBuilder object at 0x7fc1da55e7f0>\n",
      "Deriv orders after definition [[None], [0], [0, 0]]\n",
      "Surface training t=0, loss=0.9886471033096313\n",
      "Surface training t=1, loss=0.9427707195281982\n",
      "Surface training t=2, loss=0.9672328531742096\n",
      "Surface training t=3, loss=0.9703816771507263\n",
      "Surface training t=4, loss=0.9534877836704254\n",
      "Surface training t=5, loss=0.8831912577152252\n",
      "Surface training t=6, loss=0.9205136001110077\n",
      "Surface training t=7, loss=0.9216731786727905\n",
      "Surface training t=8, loss=0.8929721117019653\n",
      "Surface training t=9, loss=0.9314711689949036\n",
      "Surface training t=10, loss=0.9022430181503296\n",
      "Surface training t=11, loss=0.9095550775527954\n",
      "Surface training t=12, loss=0.9485109448432922\n",
      "Surface training t=13, loss=0.8985200524330139\n",
      "Surface training t=14, loss=0.8988443613052368\n",
      "Surface training t=15, loss=0.9079181849956512\n",
      "Surface training t=16, loss=0.856785386800766\n",
      "Surface training t=17, loss=0.8943530321121216\n",
      "Surface training t=18, loss=0.9146427512168884\n",
      "Surface training t=19, loss=0.8674331605434418\n",
      "Surface training t=20, loss=0.9292161464691162\n",
      "Surface training t=21, loss=0.8778998255729675\n",
      "Surface training t=22, loss=0.8907906413078308\n",
      "Surface training t=23, loss=0.8701808154582977\n",
      "Surface training t=24, loss=0.8806784152984619\n",
      "Surface training t=25, loss=0.8846317827701569\n",
      "Surface training t=26, loss=0.8942444920539856\n",
      "Surface training t=27, loss=0.8487709760665894\n",
      "Surface training t=28, loss=0.8552230298519135\n",
      "Surface training t=29, loss=0.8501128554344177\n",
      "Surface training t=30, loss=0.886328250169754\n",
      "Surface training t=31, loss=0.89305579662323\n",
      "Surface training t=32, loss=0.8885660767555237\n",
      "Surface training t=33, loss=0.8705417215824127\n",
      "Surface training t=34, loss=0.9101085662841797\n",
      "Surface training t=35, loss=0.883236438035965\n",
      "Surface training t=36, loss=0.8755265474319458\n",
      "Surface training t=37, loss=0.8897087275981903\n",
      "Surface training t=38, loss=0.8987145125865936\n",
      "Surface training t=39, loss=0.8515855967998505\n",
      "Surface training t=40, loss=0.8273382484912872\n",
      "Surface training t=41, loss=0.8501545190811157\n",
      "Surface training t=42, loss=0.8157375752925873\n",
      "Surface training t=43, loss=0.8891990780830383\n",
      "Surface training t=44, loss=0.863417387008667\n",
      "Surface training t=45, loss=0.8449902534484863\n",
      "Surface training t=46, loss=0.8651705682277679\n",
      "Surface training t=47, loss=0.8387624621391296\n",
      "Surface training t=48, loss=0.8899108171463013\n",
      "Surface training t=49, loss=0.8949314057826996\n",
      "Surface training t=50, loss=0.8846881985664368\n",
      "Surface training t=51, loss=0.8558028638362885\n",
      "Surface training t=52, loss=0.8631694614887238\n",
      "Surface training t=53, loss=0.83486208319664\n",
      "Surface training t=54, loss=0.843884140253067\n",
      "Surface training t=55, loss=0.8584978580474854\n",
      "Surface training t=56, loss=0.8652488887310028\n",
      "Surface training t=57, loss=0.8313710391521454\n",
      "Surface training t=58, loss=0.7993413507938385\n",
      "Surface training t=59, loss=0.8742042779922485\n",
      "Surface training t=60, loss=0.870885968208313\n",
      "Surface training t=61, loss=0.8537605702877045\n",
      "Surface training t=62, loss=0.8842488527297974\n",
      "Surface training t=63, loss=0.8636505901813507\n",
      "Surface training t=64, loss=0.819481611251831\n",
      "Surface training t=65, loss=0.8114693462848663\n",
      "Surface training t=66, loss=0.8084243535995483\n",
      "Surface training t=67, loss=0.8779691755771637\n",
      "Surface training t=68, loss=0.8470955193042755\n",
      "Surface training t=69, loss=0.8639155626296997\n",
      "Surface training t=70, loss=0.8799643218517303\n",
      "Surface training t=71, loss=0.8338180184364319\n",
      "Surface training t=72, loss=0.8804574012756348\n",
      "Surface training t=73, loss=0.8388114273548126\n",
      "Surface training t=74, loss=0.893090695142746\n",
      "Surface training t=75, loss=0.8820487856864929\n",
      "Surface training t=76, loss=0.8558200895786285\n",
      "Surface training t=77, loss=0.8313979208469391\n",
      "Surface training t=78, loss=0.8422020971775055\n",
      "Surface training t=79, loss=0.8160859942436218\n",
      "Surface training t=80, loss=0.8119702637195587\n",
      "Surface training t=81, loss=0.850176602602005\n",
      "Surface training t=82, loss=0.8378373980522156\n",
      "Surface training t=83, loss=0.8290828764438629\n",
      "Surface training t=84, loss=0.8308189213275909\n",
      "Surface training t=85, loss=0.8508228659629822\n",
      "Surface training t=86, loss=0.8458747267723083\n",
      "Surface training t=87, loss=0.8200902938842773\n",
      "Surface training t=88, loss=0.8573931753635406\n",
      "Surface training t=89, loss=0.8189922571182251\n",
      "Surface training t=90, loss=0.8291053771972656\n",
      "Surface training t=91, loss=0.8653814196586609\n",
      "Surface training t=92, loss=0.8588663339614868\n",
      "Surface training t=93, loss=0.8343665599822998\n",
      "Surface training t=94, loss=0.8733040988445282\n",
      "Surface training t=95, loss=0.8483160138130188\n",
      "Surface training t=96, loss=0.8673844933509827\n",
      "Surface training t=97, loss=0.8467027246952057\n",
      "Surface training t=98, loss=0.8449001312255859\n",
      "Surface training t=99, loss=0.8182161748409271\n",
      "Surface training t=100, loss=0.8737571835517883\n",
      "Surface training t=101, loss=0.832281082868576\n",
      "Surface training t=102, loss=0.8507178723812103\n",
      "Surface training t=103, loss=0.8678694665431976\n",
      "Surface training t=104, loss=0.8542511463165283\n",
      "Surface training t=105, loss=0.819840282201767\n",
      "Surface training t=106, loss=0.855309009552002\n",
      "Surface training t=107, loss=0.8498559892177582\n",
      "Surface training t=108, loss=0.8562309145927429\n",
      "Surface training t=109, loss=0.8126132488250732\n",
      "Surface training t=110, loss=0.8217630684375763\n",
      "Surface training t=111, loss=0.8392975628376007\n",
      "Surface training t=112, loss=0.7749800086021423\n",
      "Surface training t=113, loss=0.8707666993141174\n",
      "Surface training t=114, loss=0.8442619144916534\n",
      "Surface training t=115, loss=0.8434422612190247\n",
      "Surface training t=116, loss=0.7835531532764435\n",
      "Surface training t=117, loss=0.8579587936401367\n",
      "Surface training t=118, loss=0.8538073897361755\n",
      "Surface training t=119, loss=0.8450124859809875\n",
      "Surface training t=120, loss=0.8199772834777832\n",
      "Surface training t=121, loss=0.8885166049003601\n",
      "Surface training t=122, loss=0.8371411561965942\n",
      "Surface training t=123, loss=0.843525230884552\n",
      "Surface training t=124, loss=0.8029419481754303\n",
      "Surface training t=125, loss=0.837071567773819\n",
      "Surface training t=126, loss=0.8534260392189026\n",
      "Surface training t=127, loss=0.832086980342865\n",
      "Surface training t=128, loss=0.8671885132789612\n",
      "Surface training t=129, loss=0.86307093501091\n",
      "Surface training t=130, loss=0.8132600784301758\n",
      "Surface training t=131, loss=0.7904165685176849\n",
      "Surface training t=132, loss=0.834281861782074\n",
      "Surface training t=133, loss=0.8295194208621979\n",
      "Surface training t=134, loss=0.8491339385509491\n",
      "Surface training t=135, loss=0.817213237285614\n",
      "Surface training t=136, loss=0.7950406074523926\n",
      "Surface training t=137, loss=0.822572261095047\n",
      "Surface training t=138, loss=0.8129115700721741\n",
      "Surface training t=139, loss=0.8043290078639984\n",
      "Surface training t=140, loss=0.7844990193843842\n",
      "Surface training t=141, loss=0.8011768758296967\n",
      "Surface training t=142, loss=0.8651831746101379\n",
      "Surface training t=143, loss=0.8040529787540436\n",
      "Surface training t=144, loss=0.8305290043354034\n",
      "Surface training t=145, loss=0.8372505307197571\n",
      "Surface training t=146, loss=0.8267313838005066\n",
      "Surface training t=147, loss=0.8250131011009216\n",
      "Surface training t=148, loss=0.8231572806835175\n",
      "Surface training t=149, loss=0.8346372246742249\n",
      "Surface training t=150, loss=0.7842526137828827\n",
      "Surface training t=151, loss=0.7906504273414612\n",
      "Surface training t=152, loss=0.8000355362892151\n",
      "Surface training t=153, loss=0.8082972764968872\n",
      "Surface training t=154, loss=0.8385337591171265\n",
      "Surface training t=155, loss=0.823697417974472\n",
      "Surface training t=156, loss=0.79609015583992\n",
      "Surface training t=157, loss=0.8089002072811127\n",
      "Surface training t=158, loss=0.77027228474617\n",
      "Surface training t=159, loss=0.8267933130264282\n",
      "Surface training t=160, loss=0.7704010605812073\n",
      "Surface training t=161, loss=0.8284273743629456\n",
      "Surface training t=162, loss=0.7760556936264038\n",
      "Surface training t=163, loss=0.7785756587982178\n",
      "Surface training t=164, loss=0.8614969551563263\n",
      "Surface training t=165, loss=0.9172786772251129\n",
      "Surface training t=166, loss=0.8526241779327393\n",
      "Surface training t=167, loss=0.8204451203346252\n",
      "Surface training t=168, loss=0.8336445689201355\n",
      "Surface training t=169, loss=0.7550641894340515\n",
      "Surface training t=170, loss=0.8167165815830231\n",
      "Surface training t=171, loss=0.8214873373508453\n",
      "Surface training t=172, loss=0.8498299419879913\n",
      "Surface training t=173, loss=0.8043765425682068\n",
      "Surface training t=174, loss=0.726538211107254\n",
      "Surface training t=175, loss=0.7382505834102631\n",
      "Surface training t=176, loss=0.8104452788829803\n",
      "Surface training t=177, loss=0.7555638253688812\n",
      "Surface training t=178, loss=0.830658495426178\n",
      "Surface training t=179, loss=0.7829939723014832\n",
      "Surface training t=180, loss=0.8563087284564972\n",
      "Surface training t=181, loss=0.8139315247535706\n",
      "Surface training t=182, loss=0.8359466195106506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=183, loss=0.8311417400836945\n",
      "Surface training t=184, loss=0.8251111507415771\n",
      "Surface training t=185, loss=0.8179400861263275\n",
      "Surface training t=186, loss=0.750582754611969\n",
      "Surface training t=187, loss=0.822142630815506\n",
      "Surface training t=188, loss=0.8134550452232361\n",
      "Surface training t=189, loss=0.7724684178829193\n",
      "Surface training t=190, loss=0.7809622287750244\n",
      "Surface training t=191, loss=0.775101512670517\n",
      "Surface training t=192, loss=0.8266279101371765\n",
      "Surface training t=193, loss=0.825160562992096\n",
      "Surface training t=194, loss=0.7346524298191071\n",
      "Surface training t=195, loss=0.805948406457901\n",
      "Surface training t=196, loss=0.8062301874160767\n",
      "Surface training t=197, loss=0.8098877668380737\n",
      "Surface training t=198, loss=0.7856581807136536\n",
      "Surface training t=199, loss=0.8060660362243652\n",
      "Surface training t=200, loss=0.831815093755722\n",
      "Surface training t=201, loss=0.817896157503128\n",
      "Surface training t=202, loss=0.8262318670749664\n",
      "Surface training t=203, loss=0.8007483184337616\n",
      "Surface training t=204, loss=0.783202588558197\n",
      "Surface training t=205, loss=0.7860988676548004\n",
      "Surface training t=206, loss=0.81536865234375\n",
      "Surface training t=207, loss=0.837171882390976\n",
      "Surface training t=208, loss=0.8292079567909241\n",
      "Surface training t=209, loss=0.7927564382553101\n",
      "Surface training t=210, loss=0.7929188013076782\n",
      "Surface training t=211, loss=0.8041452169418335\n",
      "Surface training t=212, loss=0.7681794762611389\n",
      "Surface training t=213, loss=0.8326329588890076\n",
      "Surface training t=214, loss=0.7487793862819672\n",
      "Surface training t=215, loss=0.7815586626529694\n",
      "Surface training t=216, loss=0.8123393654823303\n",
      "Surface training t=217, loss=0.76442551612854\n",
      "Surface training t=218, loss=0.784917414188385\n",
      "Surface training t=219, loss=0.7649313807487488\n",
      "Surface training t=220, loss=0.7694796919822693\n",
      "Surface training t=221, loss=0.8200274705886841\n",
      "Surface training t=222, loss=0.7867081165313721\n",
      "Surface training t=223, loss=0.7618357539176941\n",
      "Surface training t=224, loss=0.803013414144516\n",
      "Surface training t=225, loss=0.8156739175319672\n",
      "Surface training t=226, loss=0.7858262360095978\n",
      "Surface training t=227, loss=0.7860020995140076\n",
      "Surface training t=228, loss=0.7613597512245178\n",
      "Surface training t=229, loss=0.8027818202972412\n",
      "Surface training t=230, loss=0.7970919907093048\n",
      "Surface training t=231, loss=0.7968169450759888\n",
      "Surface training t=232, loss=0.8696111142635345\n",
      "Surface training t=233, loss=0.8197253346443176\n",
      "Surface training t=234, loss=0.767425924539566\n",
      "Surface training t=235, loss=0.7932302355766296\n",
      "Surface training t=236, loss=0.7588902115821838\n",
      "Surface training t=237, loss=0.7590893805027008\n",
      "Surface training t=238, loss=0.7907216250896454\n",
      "Surface training t=239, loss=0.7456654608249664\n",
      "Surface training t=240, loss=0.7929400205612183\n",
      "Surface training t=241, loss=0.7888955771923065\n",
      "Surface training t=242, loss=0.8303270637989044\n",
      "Surface training t=243, loss=0.757095605134964\n",
      "Surface training t=244, loss=0.754798948764801\n",
      "Surface training t=245, loss=0.7680453360080719\n",
      "Surface training t=246, loss=0.7991014719009399\n",
      "Surface training t=247, loss=0.7833987474441528\n",
      "Surface training t=248, loss=0.7984714210033417\n",
      "Surface training t=249, loss=0.7506426870822906\n",
      "Surface training t=250, loss=0.7900361716747284\n",
      "Surface training t=251, loss=0.7672570645809174\n",
      "Surface training t=252, loss=0.8189331293106079\n",
      "Surface training t=253, loss=0.7619256377220154\n",
      "Surface training t=254, loss=0.7758532762527466\n",
      "Surface training t=255, loss=0.78867968916893\n",
      "Surface training t=256, loss=0.7440716922283173\n",
      "Surface training t=257, loss=0.8046784698963165\n",
      "Surface training t=258, loss=0.7999950051307678\n",
      "Surface training t=259, loss=0.7786743342876434\n",
      "Surface training t=260, loss=0.7695385217666626\n",
      "Surface training t=261, loss=0.7454710006713867\n",
      "Surface training t=262, loss=0.7794701755046844\n",
      "Surface training t=263, loss=0.7954500913619995\n",
      "Surface training t=264, loss=0.7459602952003479\n",
      "Surface training t=265, loss=0.7910334169864655\n",
      "Surface training t=266, loss=0.7953811585903168\n",
      "Surface training t=267, loss=0.7273563742637634\n",
      "Surface training t=268, loss=0.7831596732139587\n",
      "Surface training t=269, loss=0.7274343073368073\n",
      "Surface training t=270, loss=0.751128077507019\n",
      "Surface training t=271, loss=0.763201504945755\n",
      "Surface training t=272, loss=0.7401313781738281\n",
      "Surface training t=273, loss=0.7804107069969177\n",
      "Surface training t=274, loss=0.7662611603736877\n",
      "Surface training t=275, loss=0.7668147683143616\n",
      "Surface training t=276, loss=0.7861534953117371\n",
      "Surface training t=277, loss=0.7928951978683472\n",
      "Surface training t=278, loss=0.7713221311569214\n",
      "Surface training t=279, loss=0.7488473355770111\n",
      "Surface training t=280, loss=0.7516737878322601\n",
      "Surface training t=281, loss=0.8057864904403687\n",
      "Surface training t=282, loss=0.746375173330307\n",
      "Surface training t=283, loss=0.7459585070610046\n",
      "Surface training t=284, loss=0.7397898733615875\n",
      "Surface training t=285, loss=0.7832269072532654\n",
      "Surface training t=286, loss=0.7710034251213074\n",
      "Surface training t=287, loss=0.7459835112094879\n",
      "Surface training t=288, loss=0.7609102725982666\n",
      "Surface training t=289, loss=0.7478221952915192\n",
      "Surface training t=290, loss=0.734485387802124\n",
      "Surface training t=291, loss=0.7935411334037781\n",
      "Surface training t=292, loss=0.7580453157424927\n",
      "Surface training t=293, loss=0.7685894668102264\n",
      "Surface training t=294, loss=0.8143926858901978\n",
      "Surface training t=295, loss=0.7661538124084473\n",
      "Surface training t=296, loss=0.7276478111743927\n",
      "Surface training t=297, loss=0.735186368227005\n",
      "Surface training t=298, loss=0.76703080534935\n",
      "Surface training t=299, loss=0.7294259071350098\n",
      "Surface training t=300, loss=0.7569030523300171\n",
      "Surface training t=301, loss=0.7563686668872833\n",
      "Surface training t=302, loss=0.795261561870575\n",
      "Surface training t=303, loss=0.736057460308075\n",
      "Surface training t=304, loss=0.7346121966838837\n",
      "Surface training t=305, loss=0.7478389739990234\n",
      "Surface training t=306, loss=0.7731813192367554\n",
      "Surface training t=307, loss=0.7585662305355072\n",
      "Surface training t=308, loss=0.7120082676410675\n",
      "Surface training t=309, loss=0.7812053859233856\n",
      "Surface training t=310, loss=0.7421368658542633\n",
      "Surface training t=311, loss=0.7565788328647614\n",
      "Surface training t=312, loss=0.752046674489975\n",
      "Surface training t=313, loss=0.7535577416419983\n",
      "Surface training t=314, loss=0.7659083306789398\n",
      "Surface training t=315, loss=0.7225235402584076\n",
      "Surface training t=316, loss=0.7078520059585571\n",
      "Surface training t=317, loss=0.7557200193405151\n",
      "Surface training t=318, loss=0.7756243944168091\n",
      "Surface training t=319, loss=0.7446266412734985\n",
      "Surface training t=320, loss=0.7558716833591461\n",
      "Surface training t=321, loss=0.8115290403366089\n",
      "Surface training t=322, loss=0.7421313524246216\n",
      "Surface training t=323, loss=0.7563617825508118\n",
      "Surface training t=324, loss=0.7665748000144958\n",
      "Surface training t=325, loss=0.7081491053104401\n",
      "Surface training t=326, loss=0.7442633807659149\n",
      "Surface training t=327, loss=0.7398814260959625\n",
      "Surface training t=328, loss=0.7718417346477509\n",
      "Surface training t=329, loss=0.7302681803703308\n",
      "Surface training t=330, loss=0.7807044386863708\n",
      "Surface training t=331, loss=0.7558240294456482\n",
      "Surface training t=332, loss=0.7502057254314423\n",
      "Surface training t=333, loss=0.7383986115455627\n",
      "Surface training t=334, loss=0.7327306568622589\n",
      "Surface training t=335, loss=0.7323121428489685\n",
      "Surface training t=336, loss=0.8141681849956512\n",
      "Surface training t=337, loss=0.7443369925022125\n",
      "Surface training t=338, loss=0.7796941697597504\n",
      "Surface training t=339, loss=0.7635084688663483\n",
      "Surface training t=340, loss=0.7697791457176208\n",
      "Surface training t=341, loss=0.7231041491031647\n",
      "Surface training t=342, loss=0.7566798031330109\n",
      "Surface training t=343, loss=0.7781175076961517\n",
      "Surface training t=344, loss=0.7420321106910706\n",
      "Surface training t=345, loss=0.7474707663059235\n",
      "Surface training t=346, loss=0.7616835534572601\n",
      "Surface training t=347, loss=0.7406007945537567\n",
      "Surface training t=348, loss=0.7859188914299011\n",
      "Surface training t=349, loss=0.7662819921970367\n",
      "Surface training t=350, loss=0.7279219031333923\n",
      "Surface training t=351, loss=0.7471508979797363\n",
      "Surface training t=352, loss=0.8032934367656708\n",
      "Surface training t=353, loss=0.7229422032833099\n",
      "Surface training t=354, loss=0.7381096482276917\n",
      "Surface training t=355, loss=0.7781494557857513\n",
      "Surface training t=356, loss=0.7953217923641205\n",
      "Surface training t=357, loss=0.7036938369274139\n",
      "Surface training t=358, loss=0.758266419172287\n",
      "Surface training t=359, loss=0.7811751067638397\n",
      "Surface training t=360, loss=0.7796443998813629\n",
      "Surface training t=361, loss=0.7833282947540283\n",
      "Surface training t=362, loss=0.7819331884384155\n",
      "Surface training t=363, loss=0.7594474256038666\n",
      "Surface training t=364, loss=0.7860160171985626\n",
      "Surface training t=365, loss=0.8089755475521088\n",
      "Surface training t=366, loss=0.7493117153644562\n",
      "Surface training t=367, loss=0.7841696441173553\n",
      "Surface training t=368, loss=0.7919690907001495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=369, loss=0.7493490278720856\n",
      "Surface training t=370, loss=0.728399395942688\n",
      "Surface training t=371, loss=0.7533872723579407\n",
      "Surface training t=372, loss=0.7769631445407867\n",
      "Surface training t=373, loss=0.7208417057991028\n",
      "Surface training t=374, loss=0.7503263652324677\n",
      "Surface training t=375, loss=0.773250162601471\n",
      "Surface training t=376, loss=0.6970821917057037\n",
      "Surface training t=377, loss=0.7650207579135895\n",
      "Surface training t=378, loss=0.7724440693855286\n",
      "Surface training t=379, loss=0.7594838440418243\n",
      "Surface training t=380, loss=0.7592717409133911\n",
      "Surface training t=381, loss=0.7685916125774384\n",
      "Surface training t=382, loss=0.7049773037433624\n",
      "Surface training t=383, loss=0.721996009349823\n",
      "Surface training t=384, loss=0.7028826177120209\n",
      "Surface training t=385, loss=0.7391636669635773\n",
      "Surface training t=386, loss=0.7510725557804108\n",
      "Surface training t=387, loss=0.7712835669517517\n",
      "Surface training t=388, loss=0.7432951629161835\n",
      "Surface training t=389, loss=0.7504519820213318\n",
      "Surface training t=390, loss=0.7527242004871368\n",
      "Surface training t=391, loss=0.7665772438049316\n",
      "Surface training t=392, loss=0.7643280029296875\n",
      "Surface training t=393, loss=0.7131465375423431\n",
      "Surface training t=394, loss=0.7564645409584045\n",
      "Surface training t=395, loss=0.7130173742771149\n",
      "Surface training t=396, loss=0.7397458851337433\n",
      "Surface training t=397, loss=0.6724823713302612\n",
      "Surface training t=398, loss=0.7076232135295868\n",
      "Surface training t=399, loss=0.6776107251644135\n",
      "Surface training t=400, loss=0.7664172351360321\n",
      "Surface training t=401, loss=0.7433988153934479\n",
      "Surface training t=402, loss=0.7445217669010162\n",
      "Surface training t=403, loss=0.7104253172874451\n",
      "Surface training t=404, loss=0.7556849718093872\n",
      "Surface training t=405, loss=0.7634987235069275\n",
      "Surface training t=406, loss=0.7595275938510895\n",
      "Surface training t=407, loss=0.7288810014724731\n",
      "Surface training t=408, loss=0.743641197681427\n",
      "Surface training t=409, loss=0.7720043361186981\n",
      "Surface training t=410, loss=0.7557145059108734\n",
      "Surface training t=411, loss=0.6778195202350616\n",
      "Surface training t=412, loss=0.731704443693161\n",
      "Surface training t=413, loss=0.7612960934638977\n",
      "Surface training t=414, loss=0.7778408527374268\n",
      "Surface training t=415, loss=0.6806983053684235\n",
      "Surface training t=416, loss=0.7315202951431274\n",
      "Surface training t=417, loss=0.7587389945983887\n",
      "Surface training t=418, loss=0.740537166595459\n",
      "Surface training t=419, loss=0.7830256223678589\n",
      "Surface training t=420, loss=0.7410067617893219\n",
      "Surface training t=421, loss=0.701801210641861\n",
      "Surface training t=422, loss=0.7950819730758667\n",
      "Surface training t=423, loss=0.7037926912307739\n",
      "Surface training t=424, loss=0.7361719608306885\n",
      "Surface training t=425, loss=0.7449906170368195\n",
      "Surface training t=426, loss=0.7571040987968445\n",
      "Surface training t=427, loss=0.7441794574260712\n",
      "Surface training t=428, loss=0.7765335142612457\n",
      "Surface training t=429, loss=0.7477844059467316\n",
      "Surface training t=430, loss=0.746534138917923\n",
      "Surface training t=431, loss=0.7632835209369659\n",
      "Surface training t=432, loss=0.7551207542419434\n",
      "Surface training t=433, loss=0.7352462112903595\n",
      "Surface training t=434, loss=0.7787033915519714\n",
      "Surface training t=435, loss=0.740862101316452\n",
      "Surface training t=436, loss=0.6915798485279083\n",
      "Surface training t=437, loss=0.7670897245407104\n",
      "Surface training t=438, loss=0.6802357733249664\n",
      "Surface training t=439, loss=0.7218988835811615\n",
      "Surface training t=440, loss=0.7605783641338348\n",
      "Surface training t=441, loss=0.7339531481266022\n",
      "Surface training t=442, loss=0.7462826073169708\n",
      "Surface training t=443, loss=0.7196294665336609\n",
      "Surface training t=444, loss=0.7065421044826508\n",
      "Surface training t=445, loss=0.7406232059001923\n",
      "Surface training t=446, loss=0.7209380567073822\n",
      "Surface training t=447, loss=0.7605304718017578\n",
      "Surface training t=448, loss=0.8049536943435669\n",
      "Surface training t=449, loss=0.6899971961975098\n",
      "Surface training t=450, loss=0.7651414573192596\n",
      "Surface training t=451, loss=0.6831399202346802\n",
      "Surface training t=452, loss=0.7278355658054352\n",
      "Surface training t=453, loss=0.763862669467926\n",
      "Surface training t=454, loss=0.7131136059761047\n",
      "Surface training t=455, loss=0.6652188301086426\n",
      "Surface training t=456, loss=0.7265937030315399\n",
      "Surface training t=457, loss=0.7163914144039154\n",
      "Surface training t=458, loss=0.7363368272781372\n",
      "Surface training t=459, loss=0.7619440853595734\n",
      "Surface training t=460, loss=0.7849406003952026\n",
      "Surface training t=461, loss=0.761025071144104\n",
      "Surface training t=462, loss=0.768541008234024\n",
      "Surface training t=463, loss=0.7745031118392944\n",
      "Surface training t=464, loss=0.7648779153823853\n",
      "Surface training t=465, loss=0.7353124618530273\n",
      "Surface training t=466, loss=0.7386506199836731\n",
      "Surface training t=467, loss=0.7756676077842712\n",
      "Surface training t=468, loss=0.7675487101078033\n",
      "Surface training t=469, loss=0.7427074611186981\n",
      "Surface training t=470, loss=0.733263373374939\n",
      "Surface training t=471, loss=0.7315665185451508\n",
      "Surface training t=472, loss=0.7460019886493683\n",
      "Surface training t=473, loss=0.7664823234081268\n",
      "Surface training t=474, loss=0.7148588001728058\n",
      "Surface training t=475, loss=0.6961595118045807\n",
      "Surface training t=476, loss=0.7603394389152527\n",
      "Surface training t=477, loss=0.735195517539978\n",
      "Surface training t=478, loss=0.687769889831543\n",
      "Surface training t=479, loss=0.7450501620769501\n",
      "Surface training t=480, loss=0.7445606291294098\n",
      "Surface training t=481, loss=0.6638004183769226\n",
      "Surface training t=482, loss=0.7673302888870239\n",
      "Surface training t=483, loss=0.7024849355220795\n",
      "Surface training t=484, loss=0.7528132498264313\n",
      "Surface training t=485, loss=0.7616735696792603\n",
      "Surface training t=486, loss=0.7683420181274414\n",
      "Surface training t=487, loss=0.7430952191352844\n",
      "Surface training t=488, loss=0.7528255879878998\n",
      "Surface training t=489, loss=0.8291309773921967\n",
      "Surface training t=490, loss=0.7286237478256226\n",
      "Surface training t=491, loss=0.696842759847641\n",
      "Surface training t=492, loss=0.744761735200882\n",
      "Surface training t=493, loss=0.8391381204128265\n",
      "Surface training t=494, loss=0.7562071979045868\n",
      "Surface training t=495, loss=0.7820682525634766\n",
      "Surface training t=496, loss=0.7160950303077698\n",
      "Surface training t=497, loss=0.7489967942237854\n",
      "Surface training t=498, loss=0.7511096298694611\n",
      "Surface training t=499, loss=0.7161718904972076\n",
      "Surface training t=500, loss=0.7435327470302582\n",
      "Surface training t=501, loss=0.6752430200576782\n",
      "Surface training t=502, loss=0.7289377152919769\n",
      "Surface training t=503, loss=0.7398143708705902\n",
      "Surface training t=504, loss=0.7572869062423706\n",
      "Surface training t=505, loss=0.7390636503696442\n",
      "Surface training t=506, loss=0.7293594479560852\n",
      "Surface training t=507, loss=0.7396262288093567\n",
      "Surface training t=508, loss=0.6592755019664764\n",
      "Surface training t=509, loss=0.7389075458049774\n",
      "Surface training t=510, loss=0.7587370872497559\n",
      "Surface training t=511, loss=0.7715282142162323\n",
      "Surface training t=512, loss=0.7712216973304749\n",
      "Surface training t=513, loss=0.7448973059654236\n",
      "Surface training t=514, loss=0.7488845586776733\n",
      "Surface training t=515, loss=0.7625684440135956\n",
      "Surface training t=516, loss=0.6990600228309631\n",
      "Surface training t=517, loss=0.7258197665214539\n",
      "Surface training t=518, loss=0.7379526793956757\n",
      "Surface training t=519, loss=0.7275170683860779\n",
      "Surface training t=520, loss=0.7592888474464417\n",
      "Surface training t=521, loss=0.7214208245277405\n",
      "Surface training t=522, loss=0.7130157947540283\n",
      "Surface training t=523, loss=0.7186299562454224\n",
      "Surface training t=524, loss=0.7217163443565369\n",
      "Surface training t=525, loss=0.7225414216518402\n",
      "Surface training t=526, loss=0.7205814123153687\n",
      "Surface training t=527, loss=0.7652427554130554\n",
      "Surface training t=528, loss=0.6987330913543701\n",
      "Surface training t=529, loss=0.7290058135986328\n",
      "Surface training t=530, loss=0.735487550497055\n",
      "Surface training t=531, loss=0.7330231666564941\n",
      "Surface training t=532, loss=0.7243134379386902\n",
      "Surface training t=533, loss=0.7332590818405151\n",
      "Surface training t=534, loss=0.748604953289032\n",
      "Surface training t=535, loss=0.6924480199813843\n",
      "Surface training t=536, loss=0.7261049449443817\n",
      "Surface training t=537, loss=0.725191056728363\n",
      "Surface training t=538, loss=0.7118787169456482\n",
      "Surface training t=539, loss=0.7153495848178864\n",
      "Surface training t=540, loss=0.6991517841815948\n",
      "Surface training t=541, loss=0.7354009747505188\n",
      "Surface training t=542, loss=0.7307234704494476\n",
      "Surface training t=543, loss=0.7050522267818451\n",
      "Surface training t=544, loss=0.7018854916095734\n",
      "Surface training t=545, loss=0.7836989462375641\n",
      "Surface training t=546, loss=0.7793920338153839\n",
      "Surface training t=547, loss=0.7337549030780792\n",
      "Surface training t=548, loss=0.7533702552318573\n",
      "Surface training t=549, loss=0.7374051213264465\n",
      "Surface training t=550, loss=0.824949324131012\n",
      "Surface training t=551, loss=0.7381605803966522\n",
      "Surface training t=552, loss=0.7142050266265869\n",
      "Surface training t=553, loss=0.7268541753292084\n",
      "Surface training t=554, loss=0.7443929016590118\n",
      "Surface training t=555, loss=0.7250387966632843\n",
      "Surface training t=556, loss=0.730885922908783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=557, loss=0.7335785627365112\n",
      "Surface training t=558, loss=0.7996465563774109\n",
      "Surface training t=559, loss=0.7331822216510773\n",
      "Surface training t=560, loss=0.7303139567375183\n",
      "Surface training t=561, loss=0.753900557756424\n",
      "Surface training t=562, loss=0.7240837812423706\n",
      "Surface training t=563, loss=0.7379560172557831\n",
      "Surface training t=564, loss=0.7360164523124695\n",
      "Surface training t=565, loss=0.706916332244873\n",
      "Surface training t=566, loss=0.7648134827613831\n",
      "Surface training t=567, loss=0.6997851133346558\n",
      "Surface training t=568, loss=0.6866654753684998\n",
      "Surface training t=569, loss=0.7547954320907593\n",
      "Surface training t=570, loss=0.6889091730117798\n",
      "Surface training t=571, loss=0.7452556788921356\n",
      "Surface training t=572, loss=0.7430039644241333\n",
      "Surface training t=573, loss=0.7243649363517761\n",
      "Surface training t=574, loss=0.7281731069087982\n",
      "Surface training t=575, loss=0.6830784380435944\n",
      "Surface training t=576, loss=0.7264013886451721\n",
      "Surface training t=577, loss=0.7192461490631104\n",
      "Surface training t=578, loss=0.7580182254314423\n",
      "Surface training t=579, loss=0.7615674138069153\n",
      "Surface training t=580, loss=0.7164229154586792\n",
      "Surface training t=581, loss=0.6958622634410858\n",
      "Surface training t=582, loss=0.7621951401233673\n",
      "Surface training t=583, loss=0.7826796174049377\n",
      "Surface training t=584, loss=0.7374458611011505\n",
      "Surface training t=585, loss=0.7170924544334412\n",
      "Surface training t=586, loss=0.7665317356586456\n",
      "Surface training t=587, loss=0.7644174098968506\n",
      "Surface training t=588, loss=0.717094749212265\n",
      "Surface training t=589, loss=0.698008120059967\n",
      "Surface training t=590, loss=0.7201486527919769\n",
      "Surface training t=591, loss=0.7125809192657471\n",
      "Surface training t=592, loss=0.6788913607597351\n",
      "Surface training t=593, loss=0.7416555881500244\n",
      "Surface training t=594, loss=0.7244059443473816\n",
      "Surface training t=595, loss=0.8055439591407776\n",
      "Surface training t=596, loss=0.7110392451286316\n",
      "Surface training t=597, loss=0.7275465130805969\n",
      "Surface training t=598, loss=0.7316097319126129\n",
      "Surface training t=599, loss=0.6738485395908356\n",
      "Surface training t=600, loss=0.7033663988113403\n",
      "Surface training t=601, loss=0.6726416945457458\n",
      "Surface training t=602, loss=0.6742091774940491\n",
      "Surface training t=603, loss=0.7290011346340179\n",
      "Surface training t=604, loss=0.73021200299263\n",
      "Surface training t=605, loss=0.7320062816143036\n",
      "Surface training t=606, loss=0.7162272334098816\n",
      "Surface training t=607, loss=0.6814646124839783\n",
      "Surface training t=608, loss=0.7287317514419556\n",
      "Surface training t=609, loss=0.7109494805335999\n",
      "Surface training t=610, loss=0.7013536989688873\n",
      "Surface training t=611, loss=0.7124360799789429\n",
      "Surface training t=612, loss=0.7302539646625519\n",
      "Surface training t=613, loss=0.690718024969101\n",
      "Surface training t=614, loss=0.7210116982460022\n",
      "Surface training t=615, loss=0.7361367046833038\n",
      "Surface training t=616, loss=0.7358748912811279\n",
      "Surface training t=617, loss=0.7187643945217133\n",
      "Surface training t=618, loss=0.7515898644924164\n",
      "Surface training t=619, loss=0.7570104897022247\n",
      "Surface training t=620, loss=0.7090416550636292\n",
      "Surface training t=621, loss=0.7249105870723724\n",
      "Surface training t=622, loss=0.7645576298236847\n",
      "Surface training t=623, loss=0.7221427261829376\n",
      "Surface training t=624, loss=0.7131045162677765\n",
      "Surface training t=625, loss=0.7128370702266693\n",
      "Surface training t=626, loss=0.7223873734474182\n",
      "Surface training t=627, loss=0.6938134729862213\n",
      "Surface training t=628, loss=0.7064409852027893\n",
      "Surface training t=629, loss=0.6835471987724304\n",
      "Surface training t=630, loss=0.6951909065246582\n",
      "Surface training t=631, loss=0.7335845232009888\n",
      "Surface training t=632, loss=0.7384417951107025\n",
      "Surface training t=633, loss=0.7113054990768433\n",
      "Surface training t=634, loss=0.7492238581180573\n",
      "Surface training t=635, loss=0.7437246739864349\n",
      "Surface training t=636, loss=0.7230762541294098\n",
      "Surface training t=637, loss=0.6603690385818481\n",
      "Surface training t=638, loss=0.7428089380264282\n",
      "Surface training t=639, loss=0.6941562294960022\n",
      "Surface training t=640, loss=0.7322643995285034\n",
      "Surface training t=641, loss=0.7312476933002472\n",
      "Surface training t=642, loss=0.6845727264881134\n",
      "Surface training t=643, loss=0.7321513891220093\n",
      "Surface training t=644, loss=0.7038001716136932\n",
      "Surface training t=645, loss=0.7047080099582672\n",
      "Surface training t=646, loss=0.6961954534053802\n",
      "Surface training t=647, loss=0.6955245137214661\n",
      "Surface training t=648, loss=0.706539511680603\n",
      "Surface training t=649, loss=0.7325050532817841\n",
      "Surface training t=650, loss=0.6885095536708832\n",
      "Surface training t=651, loss=0.7163238823413849\n",
      "Surface training t=652, loss=0.7262755334377289\n",
      "Surface training t=653, loss=0.7317382991313934\n",
      "Surface training t=654, loss=0.7233975827693939\n",
      "Surface training t=655, loss=0.7682432532310486\n",
      "Surface training t=656, loss=0.7688924968242645\n",
      "Surface training t=657, loss=0.7178004682064056\n",
      "Surface training t=658, loss=0.767298012971878\n",
      "Surface training t=659, loss=0.7156640589237213\n",
      "Surface training t=660, loss=0.7258177101612091\n",
      "Surface training t=661, loss=0.7761835157871246\n",
      "Surface training t=662, loss=0.7172451317310333\n",
      "Surface training t=663, loss=0.6390997469425201\n",
      "Surface training t=664, loss=0.6588055193424225\n",
      "Surface training t=665, loss=0.7361533045768738\n",
      "Surface training t=666, loss=0.6896987855434418\n",
      "Surface training t=667, loss=0.709961324930191\n",
      "Surface training t=668, loss=0.737312912940979\n",
      "Surface training t=669, loss=0.6744971871376038\n",
      "Surface training t=670, loss=0.716353565454483\n",
      "Surface training t=671, loss=0.7372320592403412\n",
      "Surface training t=672, loss=0.7279776334762573\n",
      "Surface training t=673, loss=0.7383294403553009\n",
      "Surface training t=674, loss=0.7921074628829956\n",
      "Surface training t=675, loss=0.730728268623352\n",
      "Surface training t=676, loss=0.7511984407901764\n",
      "Surface training t=677, loss=0.7362821698188782\n",
      "Surface training t=678, loss=0.7010192573070526\n",
      "Surface training t=679, loss=0.7068208158016205\n",
      "Surface training t=680, loss=0.7257193028926849\n",
      "Surface training t=681, loss=0.7409135401248932\n",
      "Surface training t=682, loss=0.6866344213485718\n",
      "Surface training t=683, loss=0.6568992733955383\n",
      "Surface training t=684, loss=0.7377621531486511\n",
      "Surface training t=685, loss=0.7308855950832367\n",
      "Surface training t=686, loss=0.7028296291828156\n",
      "Surface training t=687, loss=0.7057842314243317\n",
      "Surface training t=688, loss=0.7171322405338287\n",
      "Surface training t=689, loss=0.7264114320278168\n",
      "Surface training t=690, loss=0.7198080122470856\n",
      "Surface training t=691, loss=0.7055399715900421\n",
      "Surface training t=692, loss=0.6699729263782501\n",
      "Surface training t=693, loss=0.7051178216934204\n",
      "Surface training t=694, loss=0.7171212732791901\n",
      "Surface training t=695, loss=0.7367691397666931\n",
      "Surface training t=696, loss=0.697495847940445\n",
      "Surface training t=697, loss=0.7199646234512329\n",
      "Surface training t=698, loss=0.7132418751716614\n",
      "Surface training t=699, loss=0.697679728269577\n",
      "Surface training t=700, loss=0.7474850416183472\n",
      "Surface training t=701, loss=0.7306180894374847\n",
      "Surface training t=702, loss=0.7097681760787964\n",
      "Surface training t=703, loss=0.6825599670410156\n",
      "Surface training t=704, loss=0.717701643705368\n",
      "Surface training t=705, loss=0.6790167093276978\n",
      "Surface training t=706, loss=0.7094304859638214\n",
      "Surface training t=707, loss=0.7313178181648254\n",
      "Surface training t=708, loss=0.6945205330848694\n",
      "Surface training t=709, loss=0.7151888608932495\n",
      "Surface training t=710, loss=0.6610653698444366\n",
      "Surface training t=711, loss=0.7039419114589691\n",
      "Surface training t=712, loss=0.7051483988761902\n",
      "Surface training t=713, loss=0.7137382924556732\n",
      "Surface training t=714, loss=0.6875665485858917\n",
      "Surface training t=715, loss=0.6943468451499939\n",
      "Surface training t=716, loss=0.7512934803962708\n",
      "Surface training t=717, loss=0.6737077534198761\n",
      "Surface training t=718, loss=0.6954838633537292\n",
      "Surface training t=719, loss=0.7048223316669464\n",
      "Surface training t=720, loss=0.7260208427906036\n",
      "Surface training t=721, loss=0.7660005390644073\n",
      "Surface training t=722, loss=0.6920788288116455\n",
      "Surface training t=723, loss=0.7140858471393585\n",
      "Surface training t=724, loss=0.6916328966617584\n",
      "Surface training t=725, loss=0.730443149805069\n",
      "Surface training t=726, loss=0.7407736480236053\n",
      "Surface training t=727, loss=0.7278604507446289\n",
      "Surface training t=728, loss=0.748895913362503\n",
      "Surface training t=729, loss=0.7268277108669281\n",
      "Surface training t=730, loss=0.7223813831806183\n",
      "Surface training t=731, loss=0.7281583845615387\n",
      "Surface training t=732, loss=0.7131029367446899\n",
      "Surface training t=733, loss=0.7125158607959747\n",
      "Surface training t=734, loss=0.7400671243667603\n",
      "Surface training t=735, loss=0.639182448387146\n",
      "Surface training t=736, loss=0.7192652523517609\n",
      "Surface training t=737, loss=0.7149066627025604\n",
      "Surface training t=738, loss=0.7221704423427582\n",
      "Surface training t=739, loss=0.7272633016109467\n",
      "Surface training t=740, loss=0.7220409214496613\n",
      "Surface training t=741, loss=0.7429201602935791\n",
      "Surface training t=742, loss=0.7199559807777405\n",
      "Surface training t=743, loss=0.7195783257484436\n",
      "Surface training t=744, loss=0.6840713620185852\n",
      "Surface training t=745, loss=0.6771870255470276\n",
      "Surface training t=746, loss=0.6628480851650238\n",
      "Surface training t=747, loss=0.7060926258563995\n",
      "Surface training t=748, loss=0.7025270760059357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=749, loss=0.7543063461780548\n",
      "Surface training t=750, loss=0.7194603085517883\n",
      "Surface training t=751, loss=0.7463642358779907\n",
      "Surface training t=752, loss=0.7197608053684235\n",
      "Surface training t=753, loss=0.6490496098995209\n",
      "Surface training t=754, loss=0.718960702419281\n",
      "Surface training t=755, loss=0.6767506003379822\n",
      "Surface training t=756, loss=0.7567952573299408\n",
      "Surface training t=757, loss=0.6940947473049164\n",
      "Surface training t=758, loss=0.7004585564136505\n",
      "Surface training t=759, loss=0.731511265039444\n",
      "Surface training t=760, loss=0.719746470451355\n",
      "Surface training t=761, loss=0.7286590337753296\n",
      "Surface training t=762, loss=0.6941852271556854\n",
      "Surface training t=763, loss=0.713170737028122\n",
      "Surface training t=764, loss=0.6698143482208252\n",
      "Surface training t=765, loss=0.7449012100696564\n",
      "Surface training t=766, loss=0.6817027926445007\n",
      "Surface training t=767, loss=0.7364096641540527\n",
      "Surface training t=768, loss=0.7369137108325958\n",
      "Surface training t=769, loss=0.745562732219696\n",
      "Surface training t=770, loss=0.6996444463729858\n",
      "Surface training t=771, loss=0.7023485600948334\n",
      "Surface training t=772, loss=0.7252152562141418\n",
      "Surface training t=773, loss=0.6606780588626862\n",
      "Surface training t=774, loss=0.681865394115448\n",
      "Surface training t=775, loss=0.7005645930767059\n",
      "Surface training t=776, loss=0.7160609662532806\n",
      "Surface training t=777, loss=0.71548131108284\n",
      "Surface training t=778, loss=0.7027323544025421\n",
      "Surface training t=779, loss=0.7379911243915558\n",
      "Surface training t=780, loss=0.6856720447540283\n",
      "Surface training t=781, loss=0.7205274105072021\n",
      "Surface training t=782, loss=0.6930259168148041\n",
      "Surface training t=783, loss=0.6658027768135071\n",
      "Surface training t=784, loss=0.7316795885562897\n",
      "Surface training t=785, loss=0.7181362509727478\n",
      "Surface training t=786, loss=0.7581804096698761\n",
      "Surface training t=787, loss=0.7099959850311279\n",
      "Surface training t=788, loss=0.6916402280330658\n",
      "Surface training t=789, loss=0.7271737158298492\n",
      "Surface training t=790, loss=0.6979239583015442\n",
      "Surface training t=791, loss=0.7355188131332397\n",
      "Surface training t=792, loss=0.7156620025634766\n",
      "Surface training t=793, loss=0.6925897002220154\n",
      "Surface training t=794, loss=0.7156149446964264\n",
      "Surface training t=795, loss=0.7433619797229767\n",
      "Surface training t=796, loss=0.6343139111995697\n",
      "Surface training t=797, loss=0.6958023607730865\n",
      "Surface training t=798, loss=0.6969950497150421\n",
      "Surface training t=799, loss=0.7590940594673157\n",
      "Surface training t=800, loss=0.6678725481033325\n",
      "Surface training t=801, loss=0.7231174111366272\n",
      "Surface training t=802, loss=0.7144701182842255\n",
      "Surface training t=803, loss=0.7173844277858734\n",
      "Surface training t=804, loss=0.7063211798667908\n",
      "Surface training t=805, loss=0.6428735554218292\n",
      "Surface training t=806, loss=0.7223107516765594\n",
      "Surface training t=807, loss=0.721981018781662\n",
      "Surface training t=808, loss=0.7050890326499939\n",
      "Surface training t=809, loss=0.6994339823722839\n",
      "Surface training t=810, loss=0.6763952672481537\n",
      "Surface training t=811, loss=0.6902655363082886\n",
      "Surface training t=812, loss=0.6973614692687988\n",
      "Surface training t=813, loss=0.6612869501113892\n",
      "Surface training t=814, loss=0.6915771663188934\n",
      "Surface training t=815, loss=0.7157719433307648\n",
      "Surface training t=816, loss=0.6702070832252502\n",
      "Surface training t=817, loss=0.7414868175983429\n",
      "Surface training t=818, loss=0.6885150969028473\n",
      "Surface training t=819, loss=0.698404461145401\n",
      "Surface training t=820, loss=0.7460609674453735\n",
      "Surface training t=821, loss=0.7328645884990692\n",
      "Surface training t=822, loss=0.7464238405227661\n",
      "Surface training t=823, loss=0.6737407147884369\n",
      "Surface training t=824, loss=0.7445998787879944\n",
      "Surface training t=825, loss=0.6772380769252777\n",
      "Surface training t=826, loss=0.7257062196731567\n",
      "Surface training t=827, loss=0.6895685195922852\n",
      "Surface training t=828, loss=0.7211990654468536\n",
      "Surface training t=829, loss=0.7248699069023132\n",
      "Surface training t=830, loss=0.6858735978603363\n",
      "Surface training t=831, loss=0.6456812024116516\n",
      "Surface training t=832, loss=0.6741281747817993\n",
      "Surface training t=833, loss=0.7278043329715729\n",
      "Surface training t=834, loss=0.6989783644676208\n",
      "Surface training t=835, loss=0.6860463619232178\n",
      "Surface training t=836, loss=0.722197949886322\n",
      "Surface training t=837, loss=0.6716084480285645\n",
      "Surface training t=838, loss=0.709700733423233\n",
      "Surface training t=839, loss=0.6583209931850433\n",
      "Surface training t=840, loss=0.7345349490642548\n",
      "Surface training t=841, loss=0.7042964100837708\n",
      "Surface training t=842, loss=0.7051421403884888\n",
      "Surface training t=843, loss=0.7610217928886414\n",
      "Surface training t=844, loss=0.7213274538516998\n",
      "Surface training t=845, loss=0.6770359873771667\n",
      "Surface training t=846, loss=0.6838371455669403\n",
      "Surface training t=847, loss=0.699505478143692\n",
      "Surface training t=848, loss=0.6936376094818115\n",
      "Surface training t=849, loss=0.7039848566055298\n",
      "Surface training t=850, loss=0.6554263234138489\n",
      "Surface training t=851, loss=0.6247846633195877\n",
      "Surface training t=852, loss=0.7072729766368866\n",
      "Surface training t=853, loss=0.7030320465564728\n",
      "Surface training t=854, loss=0.7297440767288208\n",
      "Surface training t=855, loss=0.6974423825740814\n",
      "Surface training t=856, loss=0.7115783393383026\n",
      "Surface training t=857, loss=0.7018674314022064\n",
      "Surface training t=858, loss=0.7260897755622864\n",
      "Surface training t=859, loss=0.6732245981693268\n",
      "Surface training t=860, loss=0.6242143362760544\n",
      "Surface training t=861, loss=0.685344398021698\n",
      "Surface training t=862, loss=0.6662025153636932\n",
      "Surface training t=863, loss=0.7217663526535034\n",
      "Surface training t=864, loss=0.6722208559513092\n",
      "Surface training t=865, loss=0.7082835137844086\n",
      "Surface training t=866, loss=0.6739883124828339\n",
      "Surface training t=867, loss=0.6657216846942902\n",
      "Surface training t=868, loss=0.6877162754535675\n",
      "Surface training t=869, loss=0.661388099193573\n",
      "Surface training t=870, loss=0.6886064410209656\n",
      "Surface training t=871, loss=0.6927618980407715\n",
      "Surface training t=872, loss=0.7046584486961365\n",
      "Surface training t=873, loss=0.7365764379501343\n",
      "Surface training t=874, loss=0.6816737055778503\n",
      "Surface training t=875, loss=0.714830070734024\n",
      "Surface training t=876, loss=0.7367116212844849\n",
      "Surface training t=877, loss=0.7114833295345306\n",
      "Surface training t=878, loss=0.7035405933856964\n",
      "Surface training t=879, loss=0.7049341201782227\n",
      "Surface training t=880, loss=0.7019051313400269\n",
      "Surface training t=881, loss=0.7082690000534058\n",
      "Surface training t=882, loss=0.7504141926765442\n",
      "Surface training t=883, loss=0.7663953006267548\n",
      "Surface training t=884, loss=0.6959900856018066\n",
      "Surface training t=885, loss=0.6613894999027252\n",
      "Surface training t=886, loss=0.7334234714508057\n",
      "Surface training t=887, loss=0.7444504499435425\n",
      "Surface training t=888, loss=0.7145305573940277\n",
      "Surface training t=889, loss=0.724106639623642\n",
      "Surface training t=890, loss=0.6573009788990021\n",
      "Surface training t=891, loss=0.7044869065284729\n",
      "Surface training t=892, loss=0.6710876226425171\n",
      "Surface training t=893, loss=0.7252714335918427\n",
      "Surface training t=894, loss=0.7536189556121826\n",
      "Surface training t=895, loss=0.717133104801178\n",
      "Surface training t=896, loss=0.7106867134571075\n",
      "Surface training t=897, loss=0.6434835195541382\n",
      "Surface training t=898, loss=0.6709438264369965\n",
      "Surface training t=899, loss=0.7122803926467896\n",
      "Surface training t=900, loss=0.7295309007167816\n",
      "Surface training t=901, loss=0.6276375651359558\n",
      "Surface training t=902, loss=0.6939356923103333\n",
      "Surface training t=903, loss=0.6621975302696228\n",
      "Surface training t=904, loss=0.726634681224823\n",
      "Surface training t=905, loss=0.7003622949123383\n",
      "Surface training t=906, loss=0.6597552597522736\n",
      "Surface training t=907, loss=0.6949501931667328\n",
      "Surface training t=908, loss=0.7264885008335114\n",
      "Surface training t=909, loss=0.6257642507553101\n",
      "Surface training t=910, loss=0.6359628140926361\n",
      "Surface training t=911, loss=0.7042997181415558\n",
      "Surface training t=912, loss=0.7134723663330078\n",
      "Surface training t=913, loss=0.7309727966785431\n",
      "Surface training t=914, loss=0.6972938179969788\n",
      "Surface training t=915, loss=0.6806232631206512\n",
      "Surface training t=916, loss=0.6505230367183685\n",
      "Surface training t=917, loss=0.7306593954563141\n",
      "Surface training t=918, loss=0.6778813898563385\n",
      "Surface training t=919, loss=0.6735821068286896\n",
      "Surface training t=920, loss=0.7467707097530365\n",
      "Surface training t=921, loss=0.6429785490036011\n",
      "Surface training t=922, loss=0.6966996192932129\n",
      "Surface training t=923, loss=0.6880089938640594\n",
      "Surface training t=924, loss=0.6997828483581543\n",
      "Surface training t=925, loss=0.6953350603580475\n",
      "Surface training t=926, loss=0.6392444670200348\n",
      "Surface training t=927, loss=0.6705161929130554\n",
      "Surface training t=928, loss=0.7205768525600433\n",
      "Surface training t=929, loss=0.7035701274871826\n",
      "Surface training t=930, loss=0.6975596845149994\n",
      "Surface training t=931, loss=0.7066766917705536\n",
      "Surface training t=932, loss=0.6730194091796875\n",
      "Surface training t=933, loss=0.7085219323635101\n",
      "Surface training t=934, loss=0.7065645754337311\n",
      "Surface training t=935, loss=0.7064596116542816\n",
      "Surface training t=936, loss=0.7196711599826813\n",
      "Surface training t=937, loss=0.7117655575275421\n",
      "Surface training t=938, loss=0.6817173361778259\n",
      "Surface training t=939, loss=0.6831760704517365\n",
      "Surface training t=940, loss=0.7369321882724762\n",
      "Surface training t=941, loss=0.733904242515564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=942, loss=0.6883070468902588\n",
      "Surface training t=943, loss=0.7097288966178894\n",
      "Surface training t=944, loss=0.6895450353622437\n",
      "Surface training t=945, loss=0.6643058657646179\n",
      "Surface training t=946, loss=0.7045563459396362\n",
      "Surface training t=947, loss=0.7344096302986145\n",
      "Surface training t=948, loss=0.7063700556755066\n",
      "Surface training t=949, loss=0.7125669717788696\n",
      "Surface training t=950, loss=0.6978897750377655\n",
      "Surface training t=951, loss=0.7154608368873596\n",
      "Surface training t=952, loss=0.6837054193019867\n",
      "Surface training t=953, loss=0.736769437789917\n",
      "Surface training t=954, loss=0.702972948551178\n",
      "Surface training t=955, loss=0.6433483064174652\n",
      "Surface training t=956, loss=0.6924918293952942\n",
      "Surface training t=957, loss=0.7022729218006134\n",
      "Surface training t=958, loss=0.6861046850681305\n",
      "Surface training t=959, loss=0.6925418674945831\n",
      "Surface training t=960, loss=0.7304800748825073\n",
      "Surface training t=961, loss=0.6652023792266846\n",
      "Surface training t=962, loss=0.6501529514789581\n",
      "Surface training t=963, loss=0.7224452793598175\n",
      "Surface training t=964, loss=0.7229047119617462\n",
      "Surface training t=965, loss=0.7302038967609406\n",
      "Surface training t=966, loss=0.6698238253593445\n",
      "Surface training t=967, loss=0.7042911350727081\n",
      "Surface training t=968, loss=0.6913245618343353\n",
      "Surface training t=969, loss=0.6765964925289154\n",
      "Surface training t=970, loss=0.7049328684806824\n",
      "Surface training t=971, loss=0.6676728129386902\n",
      "Surface training t=972, loss=0.6922657191753387\n",
      "Surface training t=973, loss=0.6894004344940186\n",
      "Surface training t=974, loss=0.7228500545024872\n",
      "Surface training t=975, loss=0.7497037351131439\n",
      "Surface training t=976, loss=0.7136208415031433\n",
      "Surface training t=977, loss=0.6763185262680054\n",
      "Surface training t=978, loss=0.726411908864975\n",
      "Surface training t=979, loss=0.7041929364204407\n",
      "Surface training t=980, loss=0.6936042904853821\n",
      "Surface training t=981, loss=0.7127459645271301\n",
      "Surface training t=982, loss=0.6977021396160126\n",
      "Surface training t=983, loss=0.7019914090633392\n",
      "Surface training t=984, loss=0.7036579549312592\n",
      "Surface training t=985, loss=0.6820262968540192\n",
      "Surface training t=986, loss=0.6903435587882996\n",
      "Surface training t=987, loss=0.6731123626232147\n",
      "Surface training t=988, loss=0.6699405908584595\n",
      "Surface training t=989, loss=0.6927416026592255\n",
      "Surface training t=990, loss=0.6640583574771881\n",
      "Surface training t=991, loss=0.7109065651893616\n",
      "Surface training t=992, loss=0.7418050765991211\n",
      "Surface training t=993, loss=0.6721242666244507\n",
      "Surface training t=994, loss=0.6950087547302246\n",
      "Surface training t=995, loss=0.6750801503658295\n",
      "Surface training t=996, loss=0.6377302408218384\n",
      "Surface training t=997, loss=0.7425239384174347\n",
      "Surface training t=998, loss=0.7111711800098419\n",
      "Surface training t=999, loss=0.6869493424892426\n",
      "Surface training t=1000, loss=0.7180209755897522\n",
      "Surface training t=1001, loss=0.7055557072162628\n",
      "Surface training t=1002, loss=0.6950848400592804\n",
      "Surface training t=1003, loss=0.6584821939468384\n",
      "Surface training t=1004, loss=0.7264292240142822\n",
      "Surface training t=1005, loss=0.6788455545902252\n",
      "Surface training t=1006, loss=0.6448472738265991\n",
      "Surface training t=1007, loss=0.6812686324119568\n",
      "Surface training t=1008, loss=0.6958890855312347\n",
      "Surface training t=1009, loss=0.678543895483017\n",
      "Surface training t=1010, loss=0.718345046043396\n",
      "Surface training t=1011, loss=0.6247154176235199\n",
      "Surface training t=1012, loss=0.6820748150348663\n",
      "Surface training t=1013, loss=0.6850292086601257\n",
      "Surface training t=1014, loss=0.682645708322525\n",
      "Surface training t=1015, loss=0.6577948033809662\n",
      "Surface training t=1016, loss=0.6906583607196808\n",
      "Surface training t=1017, loss=0.7317796945571899\n",
      "Surface training t=1018, loss=0.7157589793205261\n",
      "Surface training t=1019, loss=0.6833800077438354\n",
      "Surface training t=1020, loss=0.6934943199157715\n",
      "Surface training t=1021, loss=0.724451333284378\n",
      "Surface training t=1022, loss=0.6733899116516113\n",
      "Surface training t=1023, loss=0.7203476428985596\n",
      "Surface training t=1024, loss=0.6905409693717957\n",
      "Surface training t=1025, loss=0.6703505218029022\n",
      "Surface training t=1026, loss=0.684205025434494\n",
      "Surface training t=1027, loss=0.7012571692466736\n",
      "Surface training t=1028, loss=0.7501386404037476\n",
      "Surface training t=1029, loss=0.6951741576194763\n",
      "Surface training t=1030, loss=0.7127212285995483\n",
      "Surface training t=1031, loss=0.6923122107982635\n",
      "Surface training t=1032, loss=0.7006103992462158\n",
      "Surface training t=1033, loss=0.7424720525741577\n",
      "Surface training t=1034, loss=0.6735334694385529\n",
      "Surface training t=1035, loss=0.6910350620746613\n",
      "Surface training t=1036, loss=0.6839320659637451\n",
      "Surface training t=1037, loss=0.6575536131858826\n",
      "Surface training t=1038, loss=0.6720835864543915\n",
      "Surface training t=1039, loss=0.7692006826400757\n",
      "Surface training t=1040, loss=0.6397226452827454\n",
      "Surface training t=1041, loss=0.6778115630149841\n",
      "Surface training t=1042, loss=0.6932428479194641\n",
      "Surface training t=1043, loss=0.7050404250621796\n",
      "Surface training t=1044, loss=0.6900419294834137\n",
      "Surface training t=1045, loss=0.705246090888977\n",
      "Surface training t=1046, loss=0.6574556231498718\n",
      "Surface training t=1047, loss=0.6896183490753174\n",
      "Surface training t=1048, loss=0.6849222183227539\n",
      "Surface training t=1049, loss=0.6790200173854828\n",
      "Surface training t=1050, loss=0.7250482439994812\n",
      "Surface training t=1051, loss=0.6751938462257385\n",
      "Surface training t=1052, loss=0.6538428068161011\n",
      "Surface training t=1053, loss=0.6833003461360931\n",
      "Surface training t=1054, loss=0.6630050837993622\n",
      "Surface training t=1055, loss=0.6859205961227417\n",
      "Surface training t=1056, loss=0.7172695100307465\n",
      "Surface training t=1057, loss=0.6644984781742096\n",
      "Surface training t=1058, loss=0.6877329349517822\n",
      "Surface training t=1059, loss=0.6777876913547516\n",
      "Surface training t=1060, loss=0.6717898845672607\n",
      "Surface training t=1061, loss=0.6394211053848267\n",
      "Surface training t=1062, loss=0.6683301627635956\n",
      "Surface training t=1063, loss=0.6787392795085907\n",
      "Surface training t=1064, loss=0.6573807597160339\n",
      "Surface training t=1065, loss=0.6995311677455902\n",
      "Surface training t=1066, loss=0.7170377373695374\n",
      "Surface training t=1067, loss=0.675233006477356\n",
      "Surface training t=1068, loss=0.6754898130893707\n",
      "Surface training t=1069, loss=0.6852890849113464\n",
      "Surface training t=1070, loss=0.6326330304145813\n",
      "Surface training t=1071, loss=0.6960288882255554\n",
      "Surface training t=1072, loss=0.6929100751876831\n",
      "Surface training t=1073, loss=0.6749621629714966\n",
      "Surface training t=1074, loss=0.6524979174137115\n",
      "Surface training t=1075, loss=0.6650071144104004\n",
      "Surface training t=1076, loss=0.6878277063369751\n",
      "Surface training t=1077, loss=0.6175493001937866\n",
      "Surface training t=1078, loss=0.6348842084407806\n",
      "Surface training t=1079, loss=0.6711897253990173\n",
      "Surface training t=1080, loss=0.6558950841426849\n",
      "Surface training t=1081, loss=0.6567469835281372\n",
      "Surface training t=1082, loss=0.6420493125915527\n",
      "Surface training t=1083, loss=0.715196281671524\n",
      "Surface training t=1084, loss=0.6296580135822296\n",
      "Surface training t=1085, loss=0.6504335105419159\n",
      "Surface training t=1086, loss=0.6771742403507233\n",
      "Surface training t=1087, loss=0.6978764832019806\n",
      "Surface training t=1088, loss=0.6689033508300781\n",
      "Surface training t=1089, loss=0.6426859200000763\n",
      "Surface training t=1090, loss=0.7082830667495728\n",
      "Surface training t=1091, loss=0.6792106926441193\n",
      "Surface training t=1092, loss=0.6504223346710205\n",
      "Surface training t=1093, loss=0.709683895111084\n",
      "Surface training t=1094, loss=0.7190245985984802\n",
      "Surface training t=1095, loss=0.6753195524215698\n",
      "Surface training t=1096, loss=0.6548386216163635\n",
      "Surface training t=1097, loss=0.7234072387218475\n",
      "Surface training t=1098, loss=0.7324091494083405\n",
      "Surface training t=1099, loss=0.6913995742797852\n",
      "Surface training t=1100, loss=0.6487596333026886\n",
      "Surface training t=1101, loss=0.6640420258045197\n",
      "Surface training t=1102, loss=0.6729129552841187\n",
      "Surface training t=1103, loss=0.6575277149677277\n",
      "Surface training t=1104, loss=0.7010212242603302\n",
      "Surface training t=1105, loss=0.6671848297119141\n",
      "Surface training t=1106, loss=0.6866084635257721\n",
      "Surface training t=1107, loss=0.6665715873241425\n",
      "Surface training t=1108, loss=0.6222450435161591\n",
      "Surface training t=1109, loss=0.6572255492210388\n",
      "Surface training t=1110, loss=0.6247903406620026\n",
      "Surface training t=1111, loss=0.6860941648483276\n",
      "Surface training t=1112, loss=0.6846833229064941\n",
      "Surface training t=1113, loss=0.6997367441654205\n",
      "Surface training t=1114, loss=0.6372667551040649\n",
      "Surface training t=1115, loss=0.705870509147644\n",
      "Surface training t=1116, loss=0.6891512274742126\n",
      "Surface training t=1117, loss=0.6580718457698822\n",
      "Surface training t=1118, loss=0.6836693286895752\n",
      "Surface training t=1119, loss=0.6635927557945251\n",
      "Surface training t=1120, loss=0.671014815568924\n",
      "Surface training t=1121, loss=0.643324226140976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=1122, loss=0.7075830101966858\n",
      "Surface training t=1123, loss=0.6628279685974121\n",
      "Surface training t=1124, loss=0.6975196897983551\n",
      "Surface training t=1125, loss=0.7008666396141052\n",
      "Surface training t=1126, loss=0.6622483730316162\n",
      "Surface training t=1127, loss=0.7012901306152344\n",
      "Surface training t=1128, loss=0.7001358866691589\n",
      "Surface training t=1129, loss=0.694210022687912\n",
      "Surface training t=1130, loss=0.6750964522361755\n",
      "Surface training t=1131, loss=0.7082045078277588\n",
      "Surface training t=1132, loss=0.6631987690925598\n",
      "Surface training t=1133, loss=0.655221700668335\n",
      "Surface training t=1134, loss=0.6902758777141571\n",
      "Surface training t=1135, loss=0.6910734474658966\n",
      "Surface training t=1136, loss=0.6668674051761627\n",
      "Surface training t=1137, loss=0.7298831939697266\n",
      "Surface training t=1138, loss=0.7263806760311127\n",
      "Surface training t=1139, loss=0.651524007320404\n",
      "Surface training t=1140, loss=0.7008337676525116\n",
      "Surface training t=1141, loss=0.6203329265117645\n",
      "Surface training t=1142, loss=0.6630546450614929\n",
      "Surface training t=1143, loss=0.6686806082725525\n",
      "Surface training t=1144, loss=0.6528687179088593\n",
      "Surface training t=1145, loss=0.7186869084835052\n",
      "Surface training t=1146, loss=0.6973227262496948\n",
      "Surface training t=1147, loss=0.6867348253726959\n",
      "Surface training t=1148, loss=0.6844745576381683\n",
      "Surface training t=1149, loss=0.6696422398090363\n",
      "Surface training t=1150, loss=0.6601164937019348\n",
      "Surface training t=1151, loss=0.6384834945201874\n",
      "Surface training t=1152, loss=0.6620150804519653\n",
      "Surface training t=1153, loss=0.6510109603404999\n",
      "Surface training t=1154, loss=0.6617465019226074\n",
      "Surface training t=1155, loss=0.6885905265808105\n",
      "Surface training t=1156, loss=0.6939286589622498\n",
      "Surface training t=1157, loss=0.6461641490459442\n",
      "Surface training t=1158, loss=0.6386644542217255\n",
      "Surface training t=1159, loss=0.6656429469585419\n",
      "Surface training t=1160, loss=0.6572025716304779\n",
      "Surface training t=1161, loss=0.7286104559898376\n",
      "Surface training t=1162, loss=0.6540512144565582\n",
      "Surface training t=1163, loss=0.686859518289566\n",
      "Surface training t=1164, loss=0.6783392131328583\n",
      "Surface training t=1165, loss=0.6947443187236786\n",
      "Surface training t=1166, loss=0.6880854666233063\n",
      "Surface training t=1167, loss=0.666443943977356\n",
      "Surface training t=1168, loss=0.6729408204555511\n",
      "Surface training t=1169, loss=0.6639948785305023\n",
      "Surface training t=1170, loss=0.6917353272438049\n",
      "Surface training t=1171, loss=0.7110790312290192\n",
      "Surface training t=1172, loss=0.6686237454414368\n",
      "Surface training t=1173, loss=0.650213897228241\n",
      "Surface training t=1174, loss=0.6909910440444946\n",
      "Surface training t=1175, loss=0.6927328407764435\n",
      "Surface training t=1176, loss=0.6307791471481323\n",
      "Surface training t=1177, loss=0.7006763815879822\n",
      "Surface training t=1178, loss=0.6862099468708038\n",
      "Surface training t=1179, loss=0.7247652411460876\n",
      "Surface training t=1180, loss=0.669969916343689\n",
      "Surface training t=1181, loss=0.6664757430553436\n",
      "Surface training t=1182, loss=0.655024379491806\n",
      "Surface training t=1183, loss=0.6919271647930145\n",
      "Surface training t=1184, loss=0.6393730938434601\n",
      "Surface training t=1185, loss=0.6812922954559326\n",
      "Surface training t=1186, loss=0.7157495319843292\n",
      "Surface training t=1187, loss=0.6431835889816284\n",
      "Surface training t=1188, loss=0.6571968495845795\n",
      "Surface training t=1189, loss=0.5983819514513016\n",
      "Surface training t=1190, loss=0.663386881351471\n",
      "Surface training t=1191, loss=0.641249269247055\n",
      "Surface training t=1192, loss=0.6606305837631226\n",
      "Surface training t=1193, loss=0.6417627930641174\n",
      "Surface training t=1194, loss=0.6522400081157684\n",
      "Surface training t=1195, loss=0.6772884726524353\n",
      "Surface training t=1196, loss=0.6735355854034424\n",
      "Surface training t=1197, loss=0.6706971824169159\n",
      "Surface training t=1198, loss=0.6860787272453308\n",
      "Surface training t=1199, loss=0.6616505682468414\n",
      "Surface training t=1200, loss=0.6307619214057922\n",
      "Surface training t=1201, loss=0.6663891971111298\n",
      "Surface training t=1202, loss=0.6584620177745819\n",
      "Surface training t=1203, loss=0.6446681618690491\n",
      "Surface training t=1204, loss=0.6522848010063171\n",
      "Surface training t=1205, loss=0.687468945980072\n",
      "Surface training t=1206, loss=0.6709845960140228\n",
      "Surface training t=1207, loss=0.7025942802429199\n",
      "Surface training t=1208, loss=0.599884957075119\n",
      "Surface training t=1209, loss=0.7120988070964813\n",
      "Surface training t=1210, loss=0.6340690553188324\n",
      "Surface training t=1211, loss=0.6836814880371094\n",
      "Surface training t=1212, loss=0.6305160820484161\n",
      "Surface training t=1213, loss=0.6400548815727234\n",
      "Surface training t=1214, loss=0.6831071674823761\n",
      "Surface training t=1215, loss=0.6458635628223419\n",
      "Surface training t=1216, loss=0.6270315945148468\n",
      "Surface training t=1217, loss=0.6234356164932251\n",
      "Surface training t=1218, loss=0.6760910153388977\n",
      "Surface training t=1219, loss=0.6395479440689087\n",
      "Surface training t=1220, loss=0.663374662399292\n",
      "Surface training t=1221, loss=0.653458833694458\n",
      "Surface training t=1222, loss=0.6732891201972961\n",
      "Surface training t=1223, loss=0.6670283675193787\n",
      "Surface training t=1224, loss=0.5784599781036377\n",
      "Surface training t=1225, loss=0.6835560202598572\n",
      "Surface training t=1226, loss=0.6502988040447235\n",
      "Surface training t=1227, loss=0.6670026481151581\n",
      "Surface training t=1228, loss=0.6321229338645935\n",
      "Surface training t=1229, loss=0.6318168640136719\n",
      "Surface training t=1230, loss=0.6569034457206726\n",
      "Surface training t=1231, loss=0.7115351557731628\n",
      "Surface training t=1232, loss=0.607630580663681\n",
      "Surface training t=1233, loss=0.6395075917243958\n",
      "Surface training t=1234, loss=0.7037356197834015\n",
      "Surface training t=1235, loss=0.631188690662384\n",
      "Surface training t=1236, loss=0.6541767418384552\n",
      "Surface training t=1237, loss=0.6619177758693695\n",
      "Surface training t=1238, loss=0.6467519700527191\n",
      "Surface training t=1239, loss=0.6934819519519806\n",
      "Surface training t=1240, loss=0.672546923160553\n",
      "Surface training t=1241, loss=0.6248534023761749\n",
      "Surface training t=1242, loss=0.5748581141233444\n",
      "Surface training t=1243, loss=0.6615704894065857\n",
      "Surface training t=1244, loss=0.6432574689388275\n",
      "Surface training t=1245, loss=0.660497784614563\n",
      "Surface training t=1246, loss=0.6663821339607239\n",
      "Surface training t=1247, loss=0.6371552050113678\n",
      "Surface training t=1248, loss=0.6792635917663574\n",
      "Surface training t=1249, loss=0.6389552056789398\n",
      "Surface training t=1250, loss=0.6396661698818207\n",
      "Surface training t=1251, loss=0.6745524406433105\n",
      "Surface training t=1252, loss=0.5947019308805466\n",
      "Surface training t=1253, loss=0.6591916084289551\n",
      "Surface training t=1254, loss=0.6936317384243011\n",
      "Surface training t=1255, loss=0.6604065597057343\n",
      "Surface training t=1256, loss=0.7030053436756134\n",
      "Surface training t=1257, loss=0.5951824188232422\n",
      "Surface training t=1258, loss=0.6606370210647583\n",
      "Surface training t=1259, loss=0.6289448738098145\n",
      "Surface training t=1260, loss=0.652921050786972\n",
      "Surface training t=1261, loss=0.6444889008998871\n",
      "Surface training t=1262, loss=0.6737412810325623\n",
      "Surface training t=1263, loss=0.615460604429245\n",
      "Surface training t=1264, loss=0.6458952128887177\n",
      "Surface training t=1265, loss=0.6463958621025085\n",
      "Surface training t=1266, loss=0.6767255961894989\n",
      "Surface training t=1267, loss=0.6648136377334595\n",
      "Surface training t=1268, loss=0.6399444341659546\n",
      "Surface training t=1269, loss=0.6208926439285278\n",
      "Surface training t=1270, loss=0.6512565910816193\n",
      "Surface training t=1271, loss=0.6201372146606445\n",
      "Surface training t=1272, loss=0.678394615650177\n",
      "Surface training t=1273, loss=0.6767788529396057\n",
      "Surface training t=1274, loss=0.655029684305191\n",
      "Surface training t=1275, loss=0.6827405393123627\n",
      "Surface training t=1276, loss=0.6225075423717499\n",
      "Surface training t=1277, loss=0.7193161845207214\n",
      "Surface training t=1278, loss=0.6508383452892303\n",
      "Surface training t=1279, loss=0.6242869794368744\n",
      "Surface training t=1280, loss=0.672045886516571\n",
      "Surface training t=1281, loss=0.6954369843006134\n",
      "Surface training t=1282, loss=0.6526513397693634\n",
      "Surface training t=1283, loss=0.612136721611023\n",
      "Surface training t=1284, loss=0.6354774832725525\n",
      "Surface training t=1285, loss=0.6054664254188538\n",
      "Surface training t=1286, loss=0.6406698524951935\n",
      "Surface training t=1287, loss=0.6292072236537933\n",
      "Surface training t=1288, loss=0.698925644159317\n",
      "Surface training t=1289, loss=0.6244089901447296\n",
      "Surface training t=1290, loss=0.6891805529594421\n",
      "Surface training t=1291, loss=0.6551369428634644\n",
      "Surface training t=1292, loss=0.7123220264911652\n",
      "Surface training t=1293, loss=0.6881983280181885\n",
      "Surface training t=1294, loss=0.6276598572731018\n",
      "Surface training t=1295, loss=0.6488191485404968\n",
      "Surface training t=1296, loss=0.6301617622375488\n",
      "Surface training t=1297, loss=0.6713285148143768\n",
      "Surface training t=1298, loss=0.6333914399147034\n",
      "Surface training t=1299, loss=0.6676086187362671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=1300, loss=0.6603861153125763\n",
      "Surface training t=1301, loss=0.6215182542800903\n",
      "Surface training t=1302, loss=0.6846041083335876\n",
      "Surface training t=1303, loss=0.625732958316803\n",
      "Surface training t=1304, loss=0.7239500284194946\n",
      "Surface training t=1305, loss=0.6646137833595276\n",
      "Surface training t=1306, loss=0.6074694097042084\n",
      "Surface training t=1307, loss=0.6691764295101166\n",
      "Surface training t=1308, loss=0.6534204483032227\n",
      "Surface training t=1309, loss=0.6594311594963074\n",
      "Surface training t=1310, loss=0.5915285497903824\n",
      "Surface training t=1311, loss=0.6616460978984833\n",
      "Surface training t=1312, loss=0.6456650197505951\n",
      "Surface training t=1313, loss=0.685455858707428\n",
      "Surface training t=1314, loss=0.6400328278541565\n",
      "Surface training t=1315, loss=0.6523101031780243\n",
      "Surface training t=1316, loss=0.6646439731121063\n",
      "Surface training t=1317, loss=0.6451307237148285\n",
      "Surface training t=1318, loss=0.6849786341190338\n",
      "Surface training t=1319, loss=0.6363668143749237\n",
      "Surface training t=1320, loss=0.6163534224033356\n",
      "Surface training t=1321, loss=0.6460579633712769\n",
      "Surface training t=1322, loss=0.6236027777194977\n",
      "Surface training t=1323, loss=0.6420817077159882\n",
      "Surface training t=1324, loss=0.628928154706955\n",
      "Surface training t=1325, loss=0.6582735776901245\n",
      "Surface training t=1326, loss=0.6142328977584839\n",
      "Surface training t=1327, loss=0.6217162609100342\n",
      "Surface training t=1328, loss=0.6182471513748169\n",
      "Surface training t=1329, loss=0.6235172748565674\n",
      "Surface training t=1330, loss=0.6533673107624054\n",
      "Surface training t=1331, loss=0.6535628437995911\n",
      "Surface training t=1332, loss=0.6801361739635468\n",
      "Surface training t=1333, loss=0.6643293499946594\n",
      "Surface training t=1334, loss=0.5989289283752441\n",
      "Surface training t=1335, loss=0.6119298934936523\n",
      "Surface training t=1336, loss=0.6293862462043762\n",
      "Surface training t=1337, loss=0.6328511834144592\n",
      "Surface training t=1338, loss=0.7099388539791107\n",
      "Surface training t=1339, loss=0.602483481168747\n",
      "Surface training t=1340, loss=0.6020293831825256\n",
      "Surface training t=1341, loss=0.6365745961666107\n",
      "Surface training t=1342, loss=0.6390696167945862\n",
      "Surface training t=1343, loss=0.6771174669265747\n",
      "Surface training t=1344, loss=0.6101353168487549\n",
      "Surface training t=1345, loss=0.655613899230957\n",
      "Surface training t=1346, loss=0.6117248833179474\n",
      "Surface training t=1347, loss=0.6435916423797607\n",
      "Surface training t=1348, loss=0.6514075398445129\n",
      "Surface training t=1349, loss=0.6723738014698029\n",
      "Surface training t=1350, loss=0.6762326955795288\n",
      "Surface training t=1351, loss=0.6452055275440216\n",
      "Surface training t=1352, loss=0.6740718483924866\n",
      "Surface training t=1353, loss=0.6447959542274475\n",
      "Surface training t=1354, loss=0.6261891424655914\n",
      "Surface training t=1355, loss=0.6226188242435455\n",
      "Surface training t=1356, loss=0.6467366516590118\n",
      "Surface training t=1357, loss=0.614318460226059\n",
      "Surface training t=1358, loss=0.6037726700305939\n",
      "Surface training t=1359, loss=0.6718493103981018\n",
      "Surface training t=1360, loss=0.6347018182277679\n",
      "Surface training t=1361, loss=0.596694678068161\n",
      "Surface training t=1362, loss=0.6583932638168335\n",
      "Surface training t=1363, loss=0.6254995167255402\n",
      "Surface training t=1364, loss=0.6395244002342224\n",
      "Surface training t=1365, loss=0.6442410945892334\n",
      "Surface training t=1366, loss=0.6209297180175781\n",
      "Surface training t=1367, loss=0.5873103439807892\n",
      "Surface training t=1368, loss=0.6717439889907837\n",
      "Surface training t=1369, loss=0.5814811438322067\n",
      "Surface training t=1370, loss=0.6347451210021973\n",
      "Surface training t=1371, loss=0.5976554453372955\n",
      "Surface training t=1372, loss=0.6001757085323334\n",
      "Surface training t=1373, loss=0.6305055916309357\n",
      "Surface training t=1374, loss=0.6660210490226746\n",
      "Surface training t=1375, loss=0.6318836212158203\n",
      "Surface training t=1376, loss=0.607777327299118\n",
      "Surface training t=1377, loss=0.6415330767631531\n",
      "Surface training t=1378, loss=0.6094745397567749\n",
      "Surface training t=1379, loss=0.6414063274860382\n",
      "Surface training t=1380, loss=0.6859756112098694\n",
      "Surface training t=1381, loss=0.6196469962596893\n",
      "Surface training t=1382, loss=0.6083597242832184\n",
      "Surface training t=1383, loss=0.6437655687332153\n",
      "Surface training t=1384, loss=0.6411689519882202\n",
      "Surface training t=1385, loss=0.6179415285587311\n",
      "Surface training t=1386, loss=0.6480084359645844\n",
      "Surface training t=1387, loss=0.6324549615383148\n",
      "Surface training t=1388, loss=0.6592339873313904\n",
      "Surface training t=1389, loss=0.5950922667980194\n",
      "Surface training t=1390, loss=0.6426184177398682\n",
      "Surface training t=1391, loss=0.5942370593547821\n",
      "Surface training t=1392, loss=0.6434265971183777\n",
      "Surface training t=1393, loss=0.6577836275100708\n",
      "Surface training t=1394, loss=0.6854157149791718\n",
      "Surface training t=1395, loss=0.6253171563148499\n",
      "Surface training t=1396, loss=0.6162525415420532\n",
      "Surface training t=1397, loss=0.572499543428421\n",
      "Surface training t=1398, loss=0.627261221408844\n",
      "Surface training t=1399, loss=0.6211438179016113\n",
      "Surface training t=1400, loss=0.6077205538749695\n",
      "Surface training t=1401, loss=0.6506679356098175\n",
      "Surface training t=1402, loss=0.6103848516941071\n",
      "Surface training t=1403, loss=0.6838327646255493\n",
      "Surface training t=1404, loss=0.5655101239681244\n",
      "Surface training t=1405, loss=0.596840113401413\n",
      "Surface training t=1406, loss=0.6313133239746094\n",
      "Surface training t=1407, loss=0.6156461238861084\n",
      "Surface training t=1408, loss=0.6337225139141083\n",
      "Surface training t=1409, loss=0.6423831880092621\n",
      "Surface training t=1410, loss=0.5881217420101166\n",
      "Surface training t=1411, loss=0.6246346235275269\n",
      "Surface training t=1412, loss=0.5770542621612549\n",
      "Surface training t=1413, loss=0.6633304953575134\n",
      "Surface training t=1414, loss=0.6344187259674072\n",
      "Surface training t=1415, loss=0.598046064376831\n",
      "Surface training t=1416, loss=0.6247725784778595\n",
      "Surface training t=1417, loss=0.6133538782596588\n",
      "Surface training t=1418, loss=0.6641361713409424\n",
      "Surface training t=1419, loss=0.6069384813308716\n",
      "Surface training t=1420, loss=0.6253416538238525\n",
      "Surface training t=1421, loss=0.6260331869125366\n",
      "Surface training t=1422, loss=0.653234988451004\n",
      "Surface training t=1423, loss=0.5723119229078293\n",
      "Surface training t=1424, loss=0.6084069907665253\n",
      "Surface training t=1425, loss=0.5965034365653992\n",
      "Surface training t=1426, loss=0.5929324328899384\n",
      "Surface training t=1427, loss=0.5930534303188324\n",
      "Surface training t=1428, loss=0.5899311900138855\n",
      "Surface training t=1429, loss=0.6183781027793884\n",
      "Surface training t=1430, loss=0.6129874885082245\n",
      "Surface training t=1431, loss=0.6554121673107147\n",
      "Surface training t=1432, loss=0.6485585272312164\n",
      "Surface training t=1433, loss=0.6322175860404968\n",
      "Surface training t=1434, loss=0.6629759073257446\n",
      "Surface training t=1435, loss=0.6034106910228729\n",
      "Surface training t=1436, loss=0.5904211401939392\n",
      "Surface training t=1437, loss=0.6198680400848389\n",
      "Surface training t=1438, loss=0.6011269092559814\n",
      "Surface training t=1439, loss=0.6052214205265045\n",
      "Surface training t=1440, loss=0.6363197565078735\n",
      "Surface training t=1441, loss=0.622616320848465\n",
      "Surface training t=1442, loss=0.6346160471439362\n",
      "Surface training t=1443, loss=0.657717376947403\n",
      "Surface training t=1444, loss=0.5867609083652496\n",
      "Surface training t=1445, loss=0.577114075422287\n",
      "Surface training t=1446, loss=0.616787314414978\n",
      "Surface training t=1447, loss=0.6112343966960907\n",
      "Surface training t=1448, loss=0.5894627273082733\n",
      "Surface training t=1449, loss=0.5985011458396912\n",
      "Surface training t=1450, loss=0.624166876077652\n",
      "Surface training t=1451, loss=0.6214512884616852\n",
      "Surface training t=1452, loss=0.6438569724559784\n",
      "Surface training t=1453, loss=0.6844114661216736\n",
      "Surface training t=1454, loss=0.6033623218536377\n",
      "Surface training t=1455, loss=0.6059119701385498\n",
      "Surface training t=1456, loss=0.6420363485813141\n",
      "Surface training t=1457, loss=0.6185729503631592\n",
      "Surface training t=1458, loss=0.5946282744407654\n",
      "Surface training t=1459, loss=0.6950341165065765\n",
      "Surface training t=1460, loss=0.5916966795921326\n",
      "Surface training t=1461, loss=0.5611128211021423\n",
      "Surface training t=1462, loss=0.5852953493595123\n",
      "Surface training t=1463, loss=0.6101351380348206\n",
      "Surface training t=1464, loss=0.6130150854587555\n",
      "Surface training t=1465, loss=0.5958472192287445\n",
      "Surface training t=1466, loss=0.6162252426147461\n",
      "Surface training t=1467, loss=0.628896951675415\n",
      "Surface training t=1468, loss=0.6270131170749664\n",
      "Surface training t=1469, loss=0.5816408693790436\n",
      "Surface training t=1470, loss=0.5685837268829346\n",
      "Surface training t=1471, loss=0.5458214432001114\n",
      "Surface training t=1472, loss=0.6116551160812378\n",
      "Surface training t=1473, loss=0.6085171699523926\n",
      "Surface training t=1474, loss=0.5904819071292877\n",
      "Surface training t=1475, loss=0.5873957872390747\n",
      "Surface training t=1476, loss=0.6647518873214722\n",
      "Surface training t=1477, loss=0.6355721056461334\n",
      "Surface training t=1478, loss=0.6084147393703461\n",
      "Surface training t=1479, loss=0.546796441078186\n",
      "Surface training t=1480, loss=0.5876783132553101\n",
      "Surface training t=1481, loss=0.61070317029953\n",
      "Surface training t=1482, loss=0.5962350070476532\n",
      "Surface training t=1483, loss=0.5637257397174835\n",
      "Surface training t=1484, loss=0.6143780946731567\n",
      "Surface training t=1485, loss=0.6349098980426788\n",
      "Surface training t=1486, loss=0.5783853530883789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=1487, loss=0.5689104199409485\n",
      "Surface training t=1488, loss=0.5585772395133972\n",
      "Surface training t=1489, loss=0.5979010462760925\n",
      "Surface training t=1490, loss=0.6223300993442535\n",
      "Surface training t=1491, loss=0.6123720705509186\n",
      "Surface training t=1492, loss=0.5621511340141296\n",
      "Surface training t=1493, loss=0.6119587123394012\n",
      "Surface training t=1494, loss=0.6314526796340942\n",
      "Surface training t=1495, loss=0.5961701273918152\n",
      "Surface training t=1496, loss=0.6131268441677094\n",
      "Surface training t=1497, loss=0.578490674495697\n",
      "Surface training t=1498, loss=0.6054146587848663\n",
      "Surface training t=1499, loss=0.5906876027584076\n",
      "Surface training t=1500, loss=0.6114715337753296\n",
      "Surface training t=1501, loss=0.5967662036418915\n",
      "Surface training t=1502, loss=0.6745475828647614\n",
      "Surface training t=1503, loss=0.6350761651992798\n",
      "Surface training t=1504, loss=0.5549453496932983\n",
      "Surface training t=1505, loss=0.5673704743385315\n",
      "Surface training t=1506, loss=0.6004814207553864\n",
      "Surface training t=1507, loss=0.5915707051753998\n",
      "Surface training t=1508, loss=0.608356386423111\n",
      "Surface training t=1509, loss=0.6199952661991119\n",
      "Surface training t=1510, loss=0.5684323310852051\n",
      "Surface training t=1511, loss=0.5961944162845612\n",
      "Surface training t=1512, loss=0.6223937571048737\n",
      "Surface training t=1513, loss=0.5993738174438477\n",
      "Surface training t=1514, loss=0.5736366510391235\n",
      "Surface training t=1515, loss=0.5856226980686188\n",
      "Surface training t=1516, loss=0.6183093190193176\n",
      "Surface training t=1517, loss=0.5880855917930603\n",
      "Surface training t=1518, loss=0.6329732537269592\n",
      "Surface training t=1519, loss=0.5820735692977905\n",
      "Surface training t=1520, loss=0.544177457690239\n",
      "Surface training t=1521, loss=0.5710714757442474\n",
      "Surface training t=1522, loss=0.6082746982574463\n",
      "Surface training t=1523, loss=0.5667178332805634\n",
      "Surface training t=1524, loss=0.5881072282791138\n",
      "Surface training t=1525, loss=0.5782542526721954\n",
      "Surface training t=1526, loss=0.5898803770542145\n",
      "Surface training t=1527, loss=0.6108886897563934\n",
      "Surface training t=1528, loss=0.6118485927581787\n",
      "Surface training t=1529, loss=0.5505621135234833\n",
      "Surface training t=1530, loss=0.598789632320404\n",
      "Surface training t=1531, loss=0.5885669589042664\n",
      "Surface training t=1532, loss=0.5888790190219879\n",
      "Surface training t=1533, loss=0.6387850046157837\n",
      "Surface training t=1534, loss=0.619098573923111\n",
      "Surface training t=1535, loss=0.5651270151138306\n",
      "Surface training t=1536, loss=0.60708087682724\n",
      "Surface training t=1537, loss=0.5850948989391327\n",
      "Surface training t=1538, loss=0.5895655155181885\n",
      "Surface training t=1539, loss=0.5768668055534363\n",
      "Surface training t=1540, loss=0.5997496247291565\n",
      "Surface training t=1541, loss=0.5883563756942749\n",
      "Surface training t=1542, loss=0.6220763623714447\n",
      "Surface training t=1543, loss=0.5751241743564606\n",
      "Surface training t=1544, loss=0.655835747718811\n",
      "Surface training t=1545, loss=0.5634807348251343\n",
      "Surface training t=1546, loss=0.5576308220624924\n",
      "Surface training t=1547, loss=0.5597370117902756\n",
      "Surface training t=1548, loss=0.5483444333076477\n",
      "Surface training t=1549, loss=0.5703310966491699\n",
      "Surface training t=1550, loss=0.6276520788669586\n",
      "Surface training t=1551, loss=0.6052182614803314\n",
      "Surface training t=1552, loss=0.5719888806343079\n",
      "Surface training t=1553, loss=0.5923294425010681\n",
      "Surface training t=1554, loss=0.6032650470733643\n",
      "Surface training t=1555, loss=0.579056441783905\n",
      "Surface training t=1556, loss=0.5812872052192688\n",
      "Surface training t=1557, loss=0.5793512463569641\n",
      "Surface training t=1558, loss=0.6037356555461884\n",
      "Surface training t=1559, loss=0.6198834776878357\n",
      "Surface training t=1560, loss=0.5619344413280487\n",
      "Surface training t=1561, loss=0.5775902271270752\n",
      "Surface training t=1562, loss=0.5769711136817932\n",
      "Surface training t=1563, loss=0.5989309847354889\n",
      "Surface training t=1564, loss=0.5790144205093384\n",
      "Surface training t=1565, loss=0.5268832445144653\n",
      "Surface training t=1566, loss=0.610869824886322\n",
      "Surface training t=1567, loss=0.5688000321388245\n",
      "Surface training t=1568, loss=0.5731547176837921\n",
      "Surface training t=1569, loss=0.5578331649303436\n",
      "Surface training t=1570, loss=0.5773864090442657\n",
      "Surface training t=1571, loss=0.576775848865509\n",
      "Surface training t=1572, loss=0.5731522440910339\n",
      "Surface training t=1573, loss=0.5756945013999939\n",
      "Surface training t=1574, loss=0.5476462990045547\n",
      "Surface training t=1575, loss=0.5932696759700775\n",
      "Surface training t=1576, loss=0.5455142259597778\n",
      "Surface training t=1577, loss=0.5557557642459869\n",
      "Surface training t=1578, loss=0.5803872644901276\n",
      "Surface training t=1579, loss=0.5937509834766388\n",
      "Surface training t=1580, loss=0.5979250371456146\n",
      "Surface training t=1581, loss=0.5995366275310516\n",
      "Surface training t=1582, loss=0.5792238116264343\n",
      "Surface training t=1583, loss=0.5761803984642029\n",
      "Surface training t=1584, loss=0.5918707251548767\n",
      "Surface training t=1585, loss=0.6419258415699005\n",
      "Surface training t=1586, loss=0.5863294303417206\n",
      "Surface training t=1587, loss=0.5662004053592682\n",
      "Surface training t=1588, loss=0.5590190589427948\n",
      "Surface training t=1589, loss=0.5907467305660248\n",
      "Surface training t=1590, loss=0.5873545706272125\n",
      "Surface training t=1591, loss=0.5760540962219238\n",
      "Surface training t=1592, loss=0.5538704693317413\n",
      "Surface training t=1593, loss=0.5499265491962433\n",
      "Surface training t=1594, loss=0.6317692995071411\n",
      "Surface training t=1595, loss=0.5665058195590973\n",
      "Surface training t=1596, loss=0.5671867430210114\n",
      "Surface training t=1597, loss=0.6231180429458618\n",
      "Surface training t=1598, loss=0.5569963753223419\n",
      "Surface training t=1599, loss=0.5227815806865692\n",
      "Surface training t=1600, loss=0.6468845009803772\n",
      "Surface training t=1601, loss=0.5849844515323639\n",
      "Surface training t=1602, loss=0.5686385631561279\n",
      "Surface training t=1603, loss=0.5922043919563293\n",
      "Surface training t=1604, loss=0.5624347627162933\n",
      "Surface training t=1605, loss=0.5497264266014099\n",
      "Surface training t=1606, loss=0.6131913363933563\n",
      "Surface training t=1607, loss=0.5007703304290771\n",
      "Surface training t=1608, loss=0.5791040062904358\n",
      "Surface training t=1609, loss=0.5778333246707916\n",
      "Surface training t=1610, loss=0.5227562785148621\n",
      "Surface training t=1611, loss=0.5908627212047577\n",
      "Surface training t=1612, loss=0.5265398025512695\n",
      "Surface training t=1613, loss=0.566598504781723\n",
      "Surface training t=1614, loss=0.5701106786727905\n",
      "Surface training t=1615, loss=0.5481728315353394\n",
      "Surface training t=1616, loss=0.5555581450462341\n",
      "Surface training t=1617, loss=0.5702228844165802\n",
      "Surface training t=1618, loss=0.5744666457176208\n",
      "Surface training t=1619, loss=0.548890084028244\n",
      "Surface training t=1620, loss=0.5406183302402496\n",
      "Surface training t=1621, loss=0.5993082523345947\n",
      "Surface training t=1622, loss=0.5408166348934174\n",
      "Surface training t=1623, loss=0.5521177649497986\n",
      "Surface training t=1624, loss=0.5865168571472168\n",
      "Surface training t=1625, loss=0.6146858632564545\n",
      "Surface training t=1626, loss=0.5431950688362122\n",
      "Surface training t=1627, loss=0.5166088342666626\n",
      "Surface training t=1628, loss=0.531822144985199\n",
      "Surface training t=1629, loss=0.553172379732132\n",
      "Surface training t=1630, loss=0.5819437801837921\n",
      "Surface training t=1631, loss=0.5568383932113647\n",
      "Surface training t=1632, loss=0.5385760515928268\n",
      "Surface training t=1633, loss=0.5707316696643829\n",
      "Surface training t=1634, loss=0.5556356608867645\n",
      "Surface training t=1635, loss=0.5515784621238708\n",
      "Surface training t=1636, loss=0.5613628625869751\n",
      "Surface training t=1637, loss=0.5125326812267303\n",
      "Surface training t=1638, loss=0.5913249850273132\n",
      "Surface training t=1639, loss=0.5344155430793762\n",
      "Surface training t=1640, loss=0.5921924412250519\n",
      "Surface training t=1641, loss=0.6158732175827026\n",
      "Surface training t=1642, loss=0.5855247378349304\n",
      "Surface training t=1643, loss=0.5795102417469025\n",
      "Surface training t=1644, loss=0.5545766055583954\n",
      "Surface training t=1645, loss=0.6049953103065491\n",
      "Surface training t=1646, loss=0.5543981492519379\n",
      "Surface training t=1647, loss=0.5723985433578491\n",
      "Surface training t=1648, loss=0.5484710037708282\n",
      "Surface training t=1649, loss=0.5606903731822968\n",
      "Surface training t=1650, loss=0.5816543400287628\n",
      "Surface training t=1651, loss=0.5612285733222961\n",
      "Surface training t=1652, loss=0.5348918437957764\n",
      "Surface training t=1653, loss=0.6074813604354858\n",
      "Surface training t=1654, loss=0.566905677318573\n",
      "Surface training t=1655, loss=0.5626762509346008\n",
      "Surface training t=1656, loss=0.5642643868923187\n",
      "Surface training t=1657, loss=0.5802262723445892\n",
      "Surface training t=1658, loss=0.5444507002830505\n",
      "Surface training t=1659, loss=0.5343849062919617\n",
      "Surface training t=1660, loss=0.573631078004837\n",
      "Surface training t=1661, loss=0.5094979703426361\n",
      "Surface training t=1662, loss=0.5825095176696777\n",
      "Surface training t=1663, loss=0.602462649345398\n",
      "Surface training t=1664, loss=0.576489120721817\n",
      "Surface training t=1665, loss=0.5887234807014465\n",
      "Surface training t=1666, loss=0.5777486562728882\n",
      "Surface training t=1667, loss=0.521567314863205\n",
      "Surface training t=1668, loss=0.5717287063598633\n",
      "Surface training t=1669, loss=0.5248273015022278\n",
      "Surface training t=1670, loss=0.5803937911987305\n",
      "Surface training t=1671, loss=0.5550538897514343\n",
      "Surface training t=1672, loss=0.6025519371032715\n",
      "Surface training t=1673, loss=0.5425985157489777\n",
      "Surface training t=1674, loss=0.5517261922359467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=1675, loss=0.546603798866272\n",
      "Surface training t=1676, loss=0.5410367250442505\n",
      "Surface training t=1677, loss=0.580089271068573\n",
      "Surface training t=1678, loss=0.5583784282207489\n",
      "Surface training t=1679, loss=0.5700943171977997\n",
      "Surface training t=1680, loss=0.5673936307430267\n",
      "Surface training t=1681, loss=0.5926579535007477\n",
      "Surface training t=1682, loss=0.5694858133792877\n",
      "Surface training t=1683, loss=0.5569345951080322\n",
      "Surface training t=1684, loss=0.5718838572502136\n",
      "Surface training t=1685, loss=0.5598309934139252\n",
      "Surface training t=1686, loss=0.5310629606246948\n",
      "Surface training t=1687, loss=0.5134412497282028\n",
      "Surface training t=1688, loss=0.5408430099487305\n",
      "Surface training t=1689, loss=0.4971855580806732\n",
      "Surface training t=1690, loss=0.5153980553150177\n",
      "Surface training t=1691, loss=0.5599159002304077\n",
      "Surface training t=1692, loss=0.5497987568378448\n",
      "Surface training t=1693, loss=0.5258941054344177\n",
      "Surface training t=1694, loss=0.5803600549697876\n",
      "Surface training t=1695, loss=0.5122186839580536\n",
      "Surface training t=1696, loss=0.5919989347457886\n",
      "Surface training t=1697, loss=0.5478089451789856\n",
      "Surface training t=1698, loss=0.500688448548317\n",
      "Surface training t=1699, loss=0.5270397067070007\n",
      "Surface training t=1700, loss=0.5407401025295258\n",
      "Surface training t=1701, loss=0.5286331176757812\n",
      "Surface training t=1702, loss=0.5270324051380157\n",
      "Surface training t=1703, loss=0.540240615606308\n",
      "Surface training t=1704, loss=0.5780258774757385\n",
      "Surface training t=1705, loss=0.5481935739517212\n",
      "Surface training t=1706, loss=0.5930746793746948\n",
      "Surface training t=1707, loss=0.5528964698314667\n",
      "Surface training t=1708, loss=0.5215512812137604\n",
      "Surface training t=1709, loss=0.5258941948413849\n",
      "Surface training t=1710, loss=0.5513822436332703\n",
      "Surface training t=1711, loss=0.5144731104373932\n",
      "Surface training t=1712, loss=0.5353690981864929\n",
      "Surface training t=1713, loss=0.5732090175151825\n",
      "Surface training t=1714, loss=0.5169036090373993\n",
      "Surface training t=1715, loss=0.5816029906272888\n",
      "Surface training t=1716, loss=0.5428482592105865\n",
      "Surface training t=1717, loss=0.5601636171340942\n",
      "Surface training t=1718, loss=0.5389315783977509\n",
      "Surface training t=1719, loss=0.5381743013858795\n",
      "Surface training t=1720, loss=0.5679199695587158\n",
      "Surface training t=1721, loss=0.569322019815445\n",
      "Surface training t=1722, loss=0.5581793487071991\n",
      "Surface training t=1723, loss=0.5603066980838776\n",
      "Surface training t=1724, loss=0.5426735877990723\n",
      "Surface training t=1725, loss=0.5131924003362656\n",
      "Surface training t=1726, loss=0.5369418859481812\n",
      "Surface training t=1727, loss=0.45093728601932526\n",
      "Surface training t=1728, loss=0.569396048784256\n",
      "Surface training t=1729, loss=0.5294174551963806\n",
      "Surface training t=1730, loss=0.5470679402351379\n",
      "Surface training t=1731, loss=0.540203720331192\n",
      "Surface training t=1732, loss=0.5195617079734802\n",
      "Surface training t=1733, loss=0.5153538286685944\n",
      "Surface training t=1734, loss=0.5187582075595856\n",
      "Surface training t=1735, loss=0.5676852166652679\n",
      "Surface training t=1736, loss=0.522682398557663\n",
      "Surface training t=1737, loss=0.5287748277187347\n",
      "Surface training t=1738, loss=0.537882536649704\n",
      "Surface training t=1739, loss=0.5454129576683044\n",
      "Surface training t=1740, loss=0.5684202015399933\n",
      "Surface training t=1741, loss=0.5526989698410034\n",
      "Surface training t=1742, loss=0.5495752394199371\n",
      "Surface training t=1743, loss=0.511784240603447\n",
      "Surface training t=1744, loss=0.5319581627845764\n",
      "Surface training t=1745, loss=0.5332968831062317\n",
      "Surface training t=1746, loss=0.6103714406490326\n",
      "Surface training t=1747, loss=0.5597752630710602\n",
      "Surface training t=1748, loss=0.561674028635025\n",
      "Surface training t=1749, loss=0.539885938167572\n",
      "Surface training t=1750, loss=0.5090260803699493\n",
      "Surface training t=1751, loss=0.5710765719413757\n",
      "Surface training t=1752, loss=0.4914882034063339\n",
      "Surface training t=1753, loss=0.5842322111129761\n",
      "Surface training t=1754, loss=0.5215686559677124\n",
      "Surface training t=1755, loss=0.5189706087112427\n",
      "Surface training t=1756, loss=0.5071735382080078\n",
      "Surface training t=1757, loss=0.5631357729434967\n",
      "Surface training t=1758, loss=0.5523510277271271\n",
      "Surface training t=1759, loss=0.5009432733058929\n",
      "Surface training t=1760, loss=0.5101053714752197\n",
      "Surface training t=1761, loss=0.5172011852264404\n",
      "Surface training t=1762, loss=0.5104911923408508\n",
      "Surface training t=1763, loss=0.5118454992771149\n",
      "Surface training t=1764, loss=0.5224879384040833\n",
      "Surface training t=1765, loss=0.5022819936275482\n",
      "Surface training t=1766, loss=0.5072099566459656\n",
      "Surface training t=1767, loss=0.5502354800701141\n",
      "Surface training t=1768, loss=0.49551019072532654\n",
      "Surface training t=1769, loss=0.5721024870872498\n",
      "Surface training t=1770, loss=0.5015315264463425\n",
      "Surface training t=1771, loss=0.5170785784721375\n",
      "Surface training t=1772, loss=0.5093269348144531\n",
      "Surface training t=1773, loss=0.5095750689506531\n",
      "Surface training t=1774, loss=0.5443229675292969\n",
      "Surface training t=1775, loss=0.48660290241241455\n",
      "Surface training t=1776, loss=0.5727396607398987\n",
      "Surface training t=1777, loss=0.5272437930107117\n",
      "Surface training t=1778, loss=0.511463463306427\n",
      "Surface training t=1779, loss=0.472331702709198\n",
      "Surface training t=1780, loss=0.5293459594249725\n",
      "Surface training t=1781, loss=0.512712836265564\n",
      "Surface training t=1782, loss=0.5513966679573059\n",
      "Surface training t=1783, loss=0.5383711606264114\n",
      "Surface training t=1784, loss=0.5117442607879639\n",
      "Surface training t=1785, loss=0.509988397359848\n",
      "Surface training t=1786, loss=0.5553814023733139\n",
      "Surface training t=1787, loss=0.521844893693924\n",
      "Surface training t=1788, loss=0.5155487060546875\n",
      "Surface training t=1789, loss=0.5445486903190613\n",
      "Surface training t=1790, loss=0.5489479303359985\n",
      "Surface training t=1791, loss=0.5460342466831207\n",
      "Surface training t=1792, loss=0.5303633809089661\n",
      "Surface training t=1793, loss=0.5143677592277527\n",
      "Surface training t=1794, loss=0.49580827355384827\n",
      "Surface training t=1795, loss=0.5132316052913666\n",
      "Surface training t=1796, loss=0.5235854983329773\n",
      "Surface training t=1797, loss=0.4976385533809662\n",
      "Surface training t=1798, loss=0.5334295332431793\n",
      "Surface training t=1799, loss=0.5344972759485245\n",
      "Surface training t=1800, loss=0.478397861123085\n",
      "Surface training t=1801, loss=0.5057857930660248\n",
      "Surface training t=1802, loss=0.5322512090206146\n",
      "Surface training t=1803, loss=0.4983079135417938\n",
      "Surface training t=1804, loss=0.5171728730201721\n",
      "Surface training t=1805, loss=0.5287827253341675\n",
      "Surface training t=1806, loss=0.48571400344371796\n",
      "Surface training t=1807, loss=0.5144541263580322\n",
      "Surface training t=1808, loss=0.48613524436950684\n",
      "Surface training t=1809, loss=0.48911258578300476\n",
      "Surface training t=1810, loss=0.5242182314395905\n",
      "Surface training t=1811, loss=0.5421418100595474\n",
      "Surface training t=1812, loss=0.48947155475616455\n",
      "Surface training t=1813, loss=0.582664430141449\n",
      "Surface training t=1814, loss=0.521762490272522\n",
      "Surface training t=1815, loss=0.47981032729148865\n",
      "Surface training t=1816, loss=0.5375482439994812\n",
      "Surface training t=1817, loss=0.5118400156497955\n",
      "Surface training t=1818, loss=0.531592071056366\n",
      "Surface training t=1819, loss=0.49534980952739716\n",
      "Surface training t=1820, loss=0.49712875485420227\n",
      "Surface training t=1821, loss=0.5416240692138672\n",
      "Surface training t=1822, loss=0.5325545370578766\n",
      "Surface training t=1823, loss=0.5052334666252136\n",
      "Surface training t=1824, loss=0.4777977019548416\n",
      "Surface training t=1825, loss=0.5034140348434448\n",
      "Surface training t=1826, loss=0.4904908686876297\n",
      "Surface training t=1827, loss=0.46310292184352875\n",
      "Surface training t=1828, loss=0.5083224326372147\n",
      "Surface training t=1829, loss=0.5009748786687851\n",
      "Surface training t=1830, loss=0.5442925691604614\n",
      "Surface training t=1831, loss=0.5407524406909943\n",
      "Surface training t=1832, loss=0.5096377730369568\n",
      "Surface training t=1833, loss=0.4879950135946274\n",
      "Surface training t=1834, loss=0.5058115124702454\n",
      "Surface training t=1835, loss=0.5104922205209732\n",
      "Surface training t=1836, loss=0.48677369952201843\n",
      "Surface training t=1837, loss=0.5337058305740356\n",
      "Surface training t=1838, loss=0.5114421546459198\n",
      "Surface training t=1839, loss=0.5288587808609009\n",
      "Surface training t=1840, loss=0.505891740322113\n",
      "Surface training t=1841, loss=0.5081346929073334\n",
      "Surface training t=1842, loss=0.5119224786758423\n",
      "Surface training t=1843, loss=0.5111746490001678\n",
      "Surface training t=1844, loss=0.5089800953865051\n",
      "Surface training t=1845, loss=0.473514199256897\n",
      "Surface training t=1846, loss=0.49266883730888367\n",
      "Surface training t=1847, loss=0.4835021197795868\n",
      "Surface training t=1848, loss=0.46750161051750183\n",
      "Surface training t=1849, loss=0.47433990240097046\n",
      "Surface training t=1850, loss=0.5494632422924042\n",
      "Surface training t=1851, loss=0.4889235496520996\n",
      "Surface training t=1852, loss=0.49862371385097504\n",
      "Surface training t=1853, loss=0.512090265750885\n",
      "Surface training t=1854, loss=0.4781784266233444\n",
      "Surface training t=1855, loss=0.4959116578102112\n",
      "Surface training t=1856, loss=0.5122604072093964\n",
      "Surface training t=1857, loss=0.5763310194015503\n",
      "Surface training t=1858, loss=0.508299931883812\n",
      "Surface training t=1859, loss=0.5515112280845642\n",
      "Surface training t=1860, loss=0.5287187695503235\n",
      "Surface training t=1861, loss=0.5671471655368805\n",
      "Surface training t=1862, loss=0.4985746294260025\n",
      "Surface training t=1863, loss=0.5088127553462982\n",
      "Surface training t=1864, loss=0.5117132812738419\n",
      "Surface training t=1865, loss=0.4671809524297714\n",
      "Surface training t=1866, loss=0.48380447924137115\n",
      "Surface training t=1867, loss=0.5200865268707275\n",
      "Surface training t=1868, loss=0.5096611082553864\n",
      "Surface training t=1869, loss=0.5074110925197601\n",
      "Surface training t=1870, loss=0.46659351885318756\n",
      "Surface training t=1871, loss=0.5422630608081818\n",
      "Surface training t=1872, loss=0.4875025004148483\n",
      "Surface training t=1873, loss=0.532322883605957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=1874, loss=0.5379021018743515\n",
      "Surface training t=1875, loss=0.4973258078098297\n",
      "Surface training t=1876, loss=0.5177575051784515\n",
      "Surface training t=1877, loss=0.531436949968338\n",
      "Surface training t=1878, loss=0.49741213023662567\n",
      "Surface training t=1879, loss=0.5623323917388916\n",
      "Surface training t=1880, loss=0.5434858798980713\n",
      "Surface training t=1881, loss=0.49501122534275055\n",
      "Surface training t=1882, loss=0.4742158502340317\n",
      "Surface training t=1883, loss=0.5006833970546722\n",
      "Surface training t=1884, loss=0.4963088482618332\n",
      "Surface training t=1885, loss=0.49148157238960266\n",
      "Surface training t=1886, loss=0.49665313959121704\n",
      "Surface training t=1887, loss=0.47965218126773834\n",
      "Surface training t=1888, loss=0.4791076332330704\n",
      "Surface training t=1889, loss=0.47329235076904297\n",
      "Surface training t=1890, loss=0.47605228424072266\n",
      "Surface training t=1891, loss=0.5830558985471725\n",
      "Surface training t=1892, loss=0.49355660378932953\n",
      "Surface training t=1893, loss=0.4998473972082138\n",
      "Surface training t=1894, loss=0.47362636029720306\n",
      "Surface training t=1895, loss=0.5266425907611847\n",
      "Surface training t=1896, loss=0.504825085401535\n",
      "Surface training t=1897, loss=0.5256170630455017\n",
      "Surface training t=1898, loss=0.48414507508277893\n",
      "Surface training t=1899, loss=0.5083892643451691\n",
      "Surface training t=1900, loss=0.49895232915878296\n",
      "Surface training t=1901, loss=0.5060631185770035\n",
      "Surface training t=1902, loss=0.45790041983127594\n",
      "Surface training t=1903, loss=0.46450942754745483\n",
      "Surface training t=1904, loss=0.5000090897083282\n",
      "Surface training t=1905, loss=0.4911537170410156\n",
      "Surface training t=1906, loss=0.4844715744256973\n",
      "Surface training t=1907, loss=0.5131893306970596\n",
      "Surface training t=1908, loss=0.47566697001457214\n",
      "Surface training t=1909, loss=0.5054825842380524\n",
      "Surface training t=1910, loss=0.48316170275211334\n",
      "Surface training t=1911, loss=0.4266125559806824\n",
      "Surface training t=1912, loss=0.479911744594574\n",
      "Surface training t=1913, loss=0.4437848925590515\n",
      "Surface training t=1914, loss=0.506125271320343\n",
      "Surface training t=1915, loss=0.4516299217939377\n",
      "Surface training t=1916, loss=0.48587949573993683\n",
      "Surface training t=1917, loss=0.45795200765132904\n",
      "Surface training t=1918, loss=0.5042965412139893\n",
      "Surface training t=1919, loss=0.5513158291578293\n",
      "Surface training t=1920, loss=0.5321182459592819\n",
      "Surface training t=1921, loss=0.42460374534130096\n",
      "Surface training t=1922, loss=0.5049746185541153\n",
      "Surface training t=1923, loss=0.47365419566631317\n",
      "Surface training t=1924, loss=0.4792071580886841\n",
      "Surface training t=1925, loss=0.5086800903081894\n",
      "Surface training t=1926, loss=0.4743753522634506\n",
      "Surface training t=1927, loss=0.5255634188652039\n",
      "Surface training t=1928, loss=0.4891660213470459\n",
      "Surface training t=1929, loss=0.4503292441368103\n",
      "Surface training t=1930, loss=0.5045455247163773\n",
      "Surface training t=1931, loss=0.4554952085018158\n",
      "Surface training t=1932, loss=0.5123895555734634\n",
      "Surface training t=1933, loss=0.4520217031240463\n",
      "Surface training t=1934, loss=0.49207885563373566\n",
      "Surface training t=1935, loss=0.4829970598220825\n",
      "Surface training t=1936, loss=0.49479296803474426\n",
      "Surface training t=1937, loss=0.48372839391231537\n",
      "Surface training t=1938, loss=0.5077699571847916\n",
      "Surface training t=1939, loss=0.5194637179374695\n",
      "Surface training t=1940, loss=0.5039614737033844\n",
      "Surface training t=1941, loss=0.48030439019203186\n",
      "Surface training t=1942, loss=0.487669438123703\n",
      "Surface training t=1943, loss=0.46794140338897705\n",
      "Surface training t=1944, loss=0.5134047716856003\n",
      "Surface training t=1945, loss=0.5072907507419586\n",
      "Surface training t=1946, loss=0.5034403502941132\n",
      "Surface training t=1947, loss=0.512673944234848\n",
      "Surface training t=1948, loss=0.47749263048171997\n",
      "Surface training t=1949, loss=0.4709629714488983\n",
      "Surface training t=1950, loss=0.5111448019742966\n",
      "Surface training t=1951, loss=0.4514176696538925\n",
      "Surface training t=1952, loss=0.4781041145324707\n",
      "Surface training t=1953, loss=0.50590880215168\n",
      "Surface training t=1954, loss=0.534041628241539\n",
      "Surface training t=1955, loss=0.4838692843914032\n",
      "Surface training t=1956, loss=0.5268835872411728\n",
      "Surface training t=1957, loss=0.4747440814971924\n",
      "Surface training t=1958, loss=0.5568418353796005\n",
      "Surface training t=1959, loss=0.5049805641174316\n",
      "Surface training t=1960, loss=0.4372447431087494\n",
      "Surface training t=1961, loss=0.5042705833911896\n",
      "Surface training t=1962, loss=0.4908369183540344\n",
      "Surface training t=1963, loss=0.5015744864940643\n",
      "Surface training t=1964, loss=0.49862608313560486\n",
      "Surface training t=1965, loss=0.4489113986492157\n",
      "Surface training t=1966, loss=0.491102010011673\n",
      "Surface training t=1967, loss=0.5022270977497101\n",
      "Surface training t=1968, loss=0.4958924353122711\n",
      "Surface training t=1969, loss=0.5130857229232788\n",
      "Surface training t=1970, loss=0.4236487150192261\n",
      "Surface training t=1971, loss=0.4196661412715912\n",
      "Surface training t=1972, loss=0.46100619435310364\n",
      "Surface training t=1973, loss=0.5095604509115219\n",
      "Surface training t=1974, loss=0.5612853467464447\n",
      "Surface training t=1975, loss=0.49362534284591675\n",
      "Surface training t=1976, loss=0.4841706156730652\n",
      "Surface training t=1977, loss=0.45835816860198975\n",
      "Surface training t=1978, loss=0.4425622671842575\n",
      "Surface training t=1979, loss=0.48147398233413696\n",
      "Surface training t=1980, loss=0.5137482583522797\n",
      "Surface training t=1981, loss=0.4834415167570114\n",
      "Surface training t=1982, loss=0.48517800867557526\n",
      "Surface training t=1983, loss=0.5397340655326843\n",
      "Surface training t=1984, loss=0.45301999151706696\n",
      "Surface training t=1985, loss=0.4907718598842621\n",
      "Surface training t=1986, loss=0.4946315586566925\n",
      "Surface training t=1987, loss=0.49535781145095825\n",
      "Surface training t=1988, loss=0.5002654939889908\n",
      "Surface training t=1989, loss=0.5559131950139999\n",
      "Surface training t=1990, loss=0.4919121712446213\n",
      "Surface training t=1991, loss=0.4721379280090332\n",
      "Surface training t=1992, loss=0.481987327337265\n",
      "Surface training t=1993, loss=0.5610778480768204\n",
      "Surface training t=1994, loss=0.5179659128189087\n",
      "Surface training t=1995, loss=0.5131275206804276\n",
      "Surface training t=1996, loss=0.49628494679927826\n",
      "Surface training t=1997, loss=0.47715407609939575\n",
      "Surface training t=1998, loss=0.4898977130651474\n",
      "Surface training t=1999, loss=0.4905552566051483\n",
      "Surface training t=2000, loss=0.4167484939098358\n",
      "Surface training t=2001, loss=0.5216405540704727\n",
      "Surface training t=2002, loss=0.46292443573474884\n",
      "Surface training t=2003, loss=0.47390642762184143\n",
      "Surface training t=2004, loss=0.5046434551477432\n",
      "Surface training t=2005, loss=0.5133406817913055\n",
      "Surface training t=2006, loss=0.4871605187654495\n",
      "Surface training t=2007, loss=0.4770279675722122\n",
      "Surface training t=2008, loss=0.5059212148189545\n",
      "Surface training t=2009, loss=0.5222014784812927\n",
      "Surface training t=2010, loss=0.44510985910892487\n",
      "Surface training t=2011, loss=0.44832535088062286\n",
      "Surface training t=2012, loss=0.5115835815668106\n",
      "Surface training t=2013, loss=0.4741647094488144\n",
      "Surface training t=2014, loss=0.4352775663137436\n",
      "Surface training t=2015, loss=0.5015784054994583\n",
      "Surface training t=2016, loss=0.4830327332019806\n",
      "Surface training t=2017, loss=0.4974598288536072\n",
      "Surface training t=2018, loss=0.5236848443746567\n",
      "Surface training t=2019, loss=0.4920715391635895\n",
      "Surface training t=2020, loss=0.5202351063489914\n",
      "Surface training t=2021, loss=0.4857856035232544\n",
      "Surface training t=2022, loss=0.48419299721717834\n",
      "Surface training t=2023, loss=0.4453105330467224\n",
      "Surface training t=2024, loss=0.43877220153808594\n",
      "Surface training t=2025, loss=0.43351227045059204\n",
      "Surface training t=2026, loss=0.4895244985818863\n",
      "Surface training t=2027, loss=0.5026067644357681\n",
      "Surface training t=2028, loss=0.5207934230566025\n",
      "Surface training t=2029, loss=0.5216234028339386\n",
      "Surface training t=2030, loss=0.44078975915908813\n",
      "Surface training t=2031, loss=0.47101935744285583\n",
      "Surface training t=2032, loss=0.45899148285388947\n",
      "Surface training t=2033, loss=0.47256051003932953\n",
      "Surface training t=2034, loss=0.4730498343706131\n",
      "Surface training t=2035, loss=0.43821966648101807\n",
      "Surface training t=2036, loss=0.45331643521785736\n",
      "Surface training t=2037, loss=0.43782754242420197\n",
      "Surface training t=2038, loss=0.4712892472743988\n",
      "Surface training t=2039, loss=0.4959627091884613\n",
      "Surface training t=2040, loss=0.5312819629907608\n",
      "Surface training t=2041, loss=0.4623580276966095\n",
      "Surface training t=2042, loss=0.49866442382335663\n",
      "Surface training t=2043, loss=0.47704721987247467\n",
      "Surface training t=2044, loss=0.4562046080827713\n",
      "Surface training t=2045, loss=0.47056879103183746\n",
      "Surface training t=2046, loss=0.4770067483186722\n",
      "Surface training t=2047, loss=0.45990970730781555\n",
      "Surface training t=2048, loss=0.4728747010231018\n",
      "Surface training t=2049, loss=0.4792155474424362\n",
      "Surface training t=2050, loss=0.49826017022132874\n",
      "Surface training t=2051, loss=0.4300519973039627\n",
      "Surface training t=2052, loss=0.46831902861595154\n",
      "Surface training t=2053, loss=0.47275614738464355\n",
      "Surface training t=2054, loss=0.4997975528240204\n",
      "Surface training t=2055, loss=0.4238484799861908\n",
      "Surface training t=2056, loss=0.49787911772727966\n",
      "Surface training t=2057, loss=0.4646986573934555\n",
      "Surface training t=2058, loss=0.4898216128349304\n",
      "Surface training t=2059, loss=0.46394023299217224\n",
      "Surface training t=2060, loss=0.49670903384685516\n",
      "Surface training t=2061, loss=0.45136111974716187\n",
      "Surface training t=2062, loss=0.43717731535434723\n",
      "Surface training t=2063, loss=0.42503686249256134\n",
      "Surface training t=2064, loss=0.5191025733947754\n",
      "Surface training t=2065, loss=0.47303304076194763\n",
      "Surface training t=2066, loss=0.48526301980018616\n",
      "Surface training t=2067, loss=0.4358968883752823\n",
      "Surface training t=2068, loss=0.4950833022594452\n",
      "Surface training t=2069, loss=0.4152902811765671\n",
      "Surface training t=2070, loss=0.5114808678627014\n",
      "Surface training t=2071, loss=0.5138852894306183\n",
      "Surface training t=2072, loss=0.48336076736450195\n",
      "Surface training t=2073, loss=0.4769268333911896\n",
      "Surface training t=2074, loss=0.47657062113285065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=2075, loss=0.46488356590270996\n",
      "Surface training t=2076, loss=0.5423241406679153\n",
      "Surface training t=2077, loss=0.5007453411817551\n",
      "Surface training t=2078, loss=0.4987400025129318\n",
      "Surface training t=2079, loss=0.4745458662509918\n",
      "Surface training t=2080, loss=0.4694027155637741\n",
      "Surface training t=2081, loss=0.4929600656032562\n",
      "Surface training t=2082, loss=0.46064721047878265\n",
      "Surface training t=2083, loss=0.4592372477054596\n",
      "Surface training t=2084, loss=0.4927379935979843\n",
      "Surface training t=2085, loss=0.48948240280151367\n",
      "Surface training t=2086, loss=0.4784228354692459\n",
      "Surface training t=2087, loss=0.5000792443752289\n",
      "Surface training t=2088, loss=0.43505507707595825\n",
      "Surface training t=2089, loss=0.42575179040431976\n",
      "Surface training t=2090, loss=0.4874122589826584\n",
      "Surface training t=2091, loss=0.4729376435279846\n",
      "Surface training t=2092, loss=0.5162804424762726\n",
      "Surface training t=2093, loss=0.45672507584095\n",
      "Surface training t=2094, loss=0.45309972763061523\n",
      "Surface training t=2095, loss=0.5249010175466537\n",
      "Surface training t=2096, loss=0.45743776857852936\n",
      "Surface training t=2097, loss=0.42898818850517273\n",
      "Surface training t=2098, loss=0.5454890877008438\n",
      "Surface training t=2099, loss=0.46313971281051636\n",
      "Surface training t=2100, loss=0.4857766926288605\n",
      "Surface training t=2101, loss=0.49287913739681244\n",
      "Surface training t=2102, loss=0.48002609610557556\n",
      "Surface training t=2103, loss=0.47370612621307373\n",
      "Surface training t=2104, loss=0.459817573428154\n",
      "Surface training t=2105, loss=0.47454601526260376\n",
      "Surface training t=2106, loss=0.4686594605445862\n",
      "Surface training t=2107, loss=0.46781450510025024\n",
      "Surface training t=2108, loss=0.48482823371887207\n",
      "Surface training t=2109, loss=0.5147205740213394\n",
      "Surface training t=2110, loss=0.49714140594005585\n",
      "Surface training t=2111, loss=0.5191384702920914\n",
      "Surface training t=2112, loss=0.45508289337158203\n",
      "Surface training t=2113, loss=0.4975791573524475\n",
      "Surface training t=2114, loss=0.5214228928089142\n",
      "Surface training t=2115, loss=0.47849297523498535\n",
      "Surface training t=2116, loss=0.522653266787529\n",
      "Surface training t=2117, loss=0.4931619465351105\n",
      "Surface training t=2118, loss=0.525516539812088\n",
      "Surface training t=2119, loss=0.5071321874856949\n",
      "Surface training t=2120, loss=0.49627557396888733\n",
      "Surface training t=2121, loss=0.5057078748941422\n",
      "Surface training t=2122, loss=0.5004344582557678\n",
      "Surface training t=2123, loss=0.4533490091562271\n",
      "Surface training t=2124, loss=0.45445020496845245\n",
      "Surface training t=2125, loss=0.4919516444206238\n",
      "Surface training t=2126, loss=0.5383554995059967\n",
      "Surface training t=2127, loss=0.4935632348060608\n",
      "Surface training t=2128, loss=0.4721735417842865\n",
      "Surface training t=2129, loss=0.41917750239372253\n",
      "Surface training t=2130, loss=0.4885101318359375\n",
      "Surface training t=2131, loss=0.4865420013666153\n",
      "Surface training t=2132, loss=0.48627978563308716\n",
      "Surface training t=2133, loss=0.4629368185997009\n",
      "Surface training t=2134, loss=0.4557785242795944\n",
      "Surface training t=2135, loss=0.5004743188619614\n",
      "Surface training t=2136, loss=0.44006970524787903\n",
      "Surface training t=2137, loss=0.42503705620765686\n",
      "Surface training t=2138, loss=0.42780904471874237\n",
      "Surface training t=2139, loss=0.430559366941452\n",
      "Surface training t=2140, loss=0.4868127107620239\n",
      "Surface training t=2141, loss=0.45823128521442413\n",
      "Surface training t=2142, loss=0.48163339495658875\n",
      "Surface training t=2143, loss=0.47269317507743835\n",
      "Surface training t=2144, loss=0.45766711235046387\n",
      "Surface training t=2145, loss=0.46143537759780884\n",
      "Surface training t=2146, loss=0.45889002084732056\n",
      "Surface training t=2147, loss=0.508842945098877\n",
      "Surface training t=2148, loss=0.464867040514946\n",
      "Surface training t=2149, loss=0.4787631779909134\n",
      "Surface training t=2150, loss=0.4789429306983948\n",
      "Surface training t=2151, loss=0.4659637212753296\n",
      "Surface training t=2152, loss=0.519342303276062\n",
      "Surface training t=2153, loss=0.48184923827648163\n",
      "Surface training t=2154, loss=0.46959833800792694\n",
      "Surface training t=2155, loss=0.44323015213012695\n",
      "Surface training t=2156, loss=0.45063066482543945\n",
      "Surface training t=2157, loss=0.5030816793441772\n",
      "Surface training t=2158, loss=0.46294979751110077\n",
      "Surface training t=2159, loss=0.5011892765760422\n",
      "Surface training t=2160, loss=0.4550723433494568\n",
      "Surface training t=2161, loss=0.42470593750476837\n",
      "Surface training t=2162, loss=0.4700409770011902\n",
      "Surface training t=2163, loss=0.4968256950378418\n",
      "Surface training t=2164, loss=0.44506384432315826\n",
      "Surface training t=2165, loss=0.5493239313364029\n",
      "Surface training t=2166, loss=0.4769390970468521\n",
      "Surface training t=2167, loss=0.48264026641845703\n",
      "Surface training t=2168, loss=0.47097350656986237\n",
      "Surface training t=2169, loss=0.47485506534576416\n",
      "Surface training t=2170, loss=0.5034992098808289\n",
      "Surface training t=2171, loss=0.45858609676361084\n",
      "Surface training t=2172, loss=0.4913608133792877\n",
      "Surface training t=2173, loss=0.4857865869998932\n",
      "Surface training t=2174, loss=0.5103944092988968\n",
      "Surface training t=2175, loss=0.4910905659198761\n",
      "Surface training t=2176, loss=0.46528109908103943\n",
      "Surface training t=2177, loss=0.4844330847263336\n",
      "Surface training t=2178, loss=0.4450978636741638\n",
      "Surface training t=2179, loss=0.501603290438652\n",
      "Surface training t=2180, loss=0.4434077739715576\n",
      "Surface training t=2181, loss=0.5223994702100754\n",
      "Surface training t=2182, loss=0.47626253962516785\n",
      "Surface training t=2183, loss=0.4654204249382019\n",
      "Surface training t=2184, loss=0.4575188606977463\n",
      "Surface training t=2185, loss=0.4568076729774475\n",
      "Surface training t=2186, loss=0.43458178639411926\n",
      "Surface training t=2187, loss=0.4493234157562256\n",
      "Surface training t=2188, loss=0.4451456069946289\n",
      "Surface training t=2189, loss=0.453231081366539\n",
      "Surface training t=2190, loss=0.5079368948936462\n",
      "Surface training t=2191, loss=0.4499937742948532\n",
      "Surface training t=2192, loss=0.4174107164144516\n",
      "Surface training t=2193, loss=0.47814711928367615\n",
      "Surface training t=2194, loss=0.4790232926607132\n",
      "Surface training t=2195, loss=0.45606493949890137\n",
      "Surface training t=2196, loss=0.4949982762336731\n",
      "Surface training t=2197, loss=0.49691344797611237\n",
      "Surface training t=2198, loss=0.4795127660036087\n",
      "Surface training t=2199, loss=0.5282194018363953\n",
      "Surface training t=2200, loss=0.5503292679786682\n",
      "Surface training t=2201, loss=0.42937737703323364\n",
      "Surface training t=2202, loss=0.4946095496416092\n",
      "Surface training t=2203, loss=0.49185970425605774\n",
      "Surface training t=2204, loss=0.4848899394273758\n",
      "Surface training t=2205, loss=0.4613574147224426\n",
      "Surface training t=2206, loss=0.46661506593227386\n",
      "Surface training t=2207, loss=0.4727509319782257\n",
      "Surface training t=2208, loss=0.51520636677742\n",
      "Surface training t=2209, loss=0.4551607370376587\n",
      "Surface training t=2210, loss=0.49787014722824097\n",
      "Surface training t=2211, loss=0.5027516037225723\n",
      "Surface training t=2212, loss=0.4975229799747467\n",
      "Surface training t=2213, loss=0.46220429241657257\n",
      "Surface training t=2214, loss=0.4244663715362549\n",
      "Surface training t=2215, loss=0.499215304851532\n",
      "Surface training t=2216, loss=0.47994759678840637\n",
      "Surface training t=2217, loss=0.4744897335767746\n",
      "Surface training t=2218, loss=0.4345564842224121\n",
      "Surface training t=2219, loss=0.46689847111701965\n",
      "Surface training t=2220, loss=0.4550541043281555\n",
      "Surface training t=2221, loss=0.4755503833293915\n",
      "Surface training t=2222, loss=0.45454154908657074\n",
      "Surface training t=2223, loss=0.46296900510787964\n",
      "Surface training t=2224, loss=0.49488501250743866\n",
      "Surface training t=2225, loss=0.5177054554224014\n",
      "Surface training t=2226, loss=0.5709320455789566\n",
      "Surface training t=2227, loss=0.4499702751636505\n",
      "Surface training t=2228, loss=0.5450514554977417\n",
      "Surface training t=2229, loss=0.4862492084503174\n",
      "Surface training t=2230, loss=0.49016593396663666\n",
      "Surface training t=2231, loss=0.5114628076553345\n",
      "Surface training t=2232, loss=0.4213905781507492\n",
      "Surface training t=2233, loss=0.45766738057136536\n",
      "Surface training t=2234, loss=0.5255933701992035\n",
      "Surface training t=2235, loss=0.4768713116645813\n",
      "Surface training t=2236, loss=0.4687572568655014\n",
      "Surface training t=2237, loss=0.4713568538427353\n",
      "Surface training t=2238, loss=0.47982384264469147\n",
      "Surface training t=2239, loss=0.4651360958814621\n",
      "Surface training t=2240, loss=0.3901226222515106\n",
      "Surface training t=2241, loss=0.41973939538002014\n",
      "Surface training t=2242, loss=0.4568171054124832\n",
      "Surface training t=2243, loss=0.4725368171930313\n",
      "Surface training t=2244, loss=0.42284877598285675\n",
      "Surface training t=2245, loss=0.4579278975725174\n",
      "Surface training t=2246, loss=0.4885680675506592\n",
      "Surface training t=2247, loss=0.444024920463562\n",
      "Surface training t=2248, loss=0.4774513393640518\n",
      "Surface training t=2249, loss=0.44254130125045776\n",
      "Surface training t=2250, loss=0.46775220334529877\n",
      "Surface training t=2251, loss=0.5271599590778351\n",
      "Surface training t=2252, loss=0.4378230571746826\n",
      "Surface training t=2253, loss=0.47785186767578125\n",
      "Surface training t=2254, loss=0.45240624248981476\n",
      "Surface training t=2255, loss=0.44092774391174316\n",
      "Surface training t=2256, loss=0.47701460123062134\n",
      "Surface training t=2257, loss=0.4952920824289322\n",
      "Surface training t=2258, loss=0.43764612078666687\n",
      "Surface training t=2259, loss=0.4332226812839508\n",
      "Surface training t=2260, loss=0.4105566293001175\n",
      "Surface training t=2261, loss=0.45078764855861664\n",
      "Surface training t=2262, loss=0.48952215909957886\n",
      "Surface training t=2263, loss=0.4828062504529953\n",
      "Surface training t=2264, loss=0.49010200798511505\n",
      "Surface training t=2265, loss=0.3935987651348114\n",
      "Surface training t=2266, loss=0.43920665979385376\n",
      "Surface training t=2267, loss=0.46313804388046265\n",
      "Surface training t=2268, loss=0.44617804884910583\n",
      "Surface training t=2269, loss=0.5113305449485779\n",
      "Surface training t=2270, loss=0.49629129469394684\n",
      "Surface training t=2271, loss=0.4149736166000366\n",
      "Surface training t=2272, loss=0.4724368453025818\n",
      "Surface training t=2273, loss=0.3994073122739792\n",
      "Surface training t=2274, loss=0.4756891280412674\n",
      "Surface training t=2275, loss=0.5012193620204926\n",
      "Surface training t=2276, loss=0.4186559468507767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=2277, loss=0.5093806087970734\n",
      "Surface training t=2278, loss=0.4217377305030823\n",
      "Surface training t=2279, loss=0.45689284801483154\n",
      "Surface training t=2280, loss=0.450832262635231\n",
      "Surface training t=2281, loss=0.4928884506225586\n",
      "Surface training t=2282, loss=0.46605396270751953\n",
      "Surface training t=2283, loss=0.44463157653808594\n",
      "Surface training t=2284, loss=0.43005968630313873\n",
      "Surface training t=2285, loss=0.4916885495185852\n",
      "Surface training t=2286, loss=0.4701341986656189\n",
      "Surface training t=2287, loss=0.47022195160388947\n",
      "Surface training t=2288, loss=0.4440317451953888\n",
      "Surface training t=2289, loss=0.43848271667957306\n",
      "Surface training t=2290, loss=0.44702258706092834\n",
      "Surface training t=2291, loss=0.51389579474926\n",
      "Surface training t=2292, loss=0.4566081017255783\n",
      "Surface training t=2293, loss=0.48316872119903564\n",
      "Surface training t=2294, loss=0.45359261333942413\n",
      "Surface training t=2295, loss=0.46988068521022797\n",
      "Surface training t=2296, loss=0.47830232977867126\n",
      "Surface training t=2297, loss=0.4337712526321411\n",
      "Surface training t=2298, loss=0.43865035474300385\n",
      "Surface training t=2299, loss=0.44255274534225464\n",
      "Surface training t=2300, loss=0.4631293714046478\n",
      "Surface training t=2301, loss=0.4558151662349701\n",
      "Surface training t=2302, loss=0.49568571150302887\n",
      "Surface training t=2303, loss=0.42196136713027954\n",
      "Surface training t=2304, loss=0.4804787039756775\n",
      "Surface training t=2305, loss=0.4935883581638336\n",
      "Surface training t=2306, loss=0.46270039677619934\n",
      "Surface training t=2307, loss=0.46368081867694855\n",
      "Surface training t=2308, loss=0.4434983432292938\n",
      "Surface training t=2309, loss=0.4651482105255127\n",
      "Surface training t=2310, loss=0.41727881133556366\n",
      "Surface training t=2311, loss=0.45021912455558777\n",
      "Surface training t=2312, loss=0.4286007732152939\n",
      "Surface training t=2313, loss=0.44733719527721405\n",
      "Surface training t=2314, loss=0.43499311804771423\n",
      "Surface training t=2315, loss=0.43241868913173676\n",
      "Surface training t=2316, loss=0.44509342312812805\n",
      "Surface training t=2317, loss=0.4196122884750366\n",
      "Surface training t=2318, loss=0.4873697757720947\n",
      "Surface training t=2319, loss=0.46039384603500366\n",
      "Surface training t=2320, loss=0.3912816792726517\n",
      "Surface training t=2321, loss=0.4269917905330658\n",
      "Surface training t=2322, loss=0.45943647623062134\n",
      "Surface training t=2323, loss=0.5223762392997742\n",
      "Surface training t=2324, loss=0.4793076813220978\n",
      "Surface training t=2325, loss=0.431251585483551\n",
      "Surface training t=2326, loss=0.46846380829811096\n",
      "Surface training t=2327, loss=0.49397963285446167\n",
      "Surface training t=2328, loss=0.4830149710178375\n",
      "Surface training t=2329, loss=0.4470537006855011\n",
      "Surface training t=2330, loss=0.49213550984859467\n",
      "Surface training t=2331, loss=0.44267868995666504\n",
      "Surface training t=2332, loss=0.39004547894001007\n",
      "Surface training t=2333, loss=0.44076357781887054\n",
      "Surface training t=2334, loss=0.44840869307518005\n",
      "Surface training t=2335, loss=0.44045212864875793\n",
      "Surface training t=2336, loss=0.43420036137104034\n",
      "Surface training t=2337, loss=0.45925046503543854\n",
      "Surface training t=2338, loss=0.4364212304353714\n",
      "Surface training t=2339, loss=0.45167312026023865\n",
      "Surface training t=2340, loss=0.4694027304649353\n",
      "Surface training t=2341, loss=0.4539984166622162\n",
      "Surface training t=2342, loss=0.4000183641910553\n",
      "Surface training t=2343, loss=0.4372195899486542\n",
      "Surface training t=2344, loss=0.4870999902486801\n",
      "Surface training t=2345, loss=0.40228842198848724\n",
      "Surface training t=2346, loss=0.43937577307224274\n",
      "Surface training t=2347, loss=0.4618932604789734\n",
      "Surface training t=2348, loss=0.45053577423095703\n",
      "Surface training t=2349, loss=0.44140832126140594\n",
      "Surface training t=2350, loss=0.4536312371492386\n",
      "Surface training t=2351, loss=0.44680775701999664\n",
      "Surface training t=2352, loss=0.43483051657676697\n",
      "Surface training t=2353, loss=0.5009468793869019\n",
      "Surface training t=2354, loss=0.4452032744884491\n",
      "Surface training t=2355, loss=0.4594237357378006\n",
      "Surface training t=2356, loss=0.433262437582016\n",
      "Surface training t=2357, loss=0.4258800894021988\n",
      "Surface training t=2358, loss=0.4307253211736679\n",
      "Surface training t=2359, loss=0.475247859954834\n",
      "Surface training t=2360, loss=0.47493430972099304\n",
      "Surface training t=2361, loss=0.4906495213508606\n",
      "Surface training t=2362, loss=0.43377506732940674\n",
      "Surface training t=2363, loss=0.48437686264514923\n",
      "Surface training t=2364, loss=0.47097957134246826\n",
      "Surface training t=2365, loss=0.4258027821779251\n",
      "Surface training t=2366, loss=0.48248860239982605\n",
      "Surface training t=2367, loss=0.4821564853191376\n",
      "Surface training t=2368, loss=0.3986027091741562\n",
      "Surface training t=2369, loss=0.4882350265979767\n",
      "Surface training t=2370, loss=0.4286239296197891\n",
      "Surface training t=2371, loss=0.4762611836194992\n",
      "Surface training t=2372, loss=0.4388095587491989\n",
      "Surface training t=2373, loss=0.4587491750717163\n",
      "Surface training t=2374, loss=0.5022853165864944\n",
      "Surface training t=2375, loss=0.4394278824329376\n",
      "Surface training t=2376, loss=0.43494008481502533\n",
      "Surface training t=2377, loss=0.4737127125263214\n",
      "Surface training t=2378, loss=0.4708395004272461\n",
      "Surface training t=2379, loss=0.4924057424068451\n",
      "Surface training t=2380, loss=0.4299047887325287\n",
      "Surface training t=2381, loss=0.5172192454338074\n",
      "Surface training t=2382, loss=0.4599854648113251\n",
      "Surface training t=2383, loss=0.550648957490921\n",
      "Surface training t=2384, loss=0.46598103642463684\n",
      "Surface training t=2385, loss=0.4509255886077881\n",
      "Surface training t=2386, loss=0.3988208770751953\n",
      "Surface training t=2387, loss=0.4879069924354553\n",
      "Surface training t=2388, loss=0.4864855408668518\n",
      "Surface training t=2389, loss=0.4324488788843155\n",
      "Surface training t=2390, loss=0.43778882920742035\n",
      "Surface training t=2391, loss=0.45471222698688507\n",
      "Surface training t=2392, loss=0.48700959980487823\n",
      "Surface training t=2393, loss=0.4707252085208893\n",
      "Surface training t=2394, loss=0.47040748596191406\n",
      "Surface training t=2395, loss=0.4013660252094269\n",
      "Surface training t=2396, loss=0.44437040388584137\n",
      "Surface training t=2397, loss=0.5109895318746567\n",
      "Surface training t=2398, loss=0.4389312118291855\n",
      "Surface training t=2399, loss=0.4388284683227539\n",
      "Surface training t=2400, loss=0.41374440491199493\n",
      "Surface training t=2401, loss=0.4333238899707794\n",
      "Surface training t=2402, loss=0.45529904961586\n",
      "Surface training t=2403, loss=0.45828603208065033\n",
      "Surface training t=2404, loss=0.437972292304039\n",
      "Surface training t=2405, loss=0.47369593381881714\n",
      "Surface training t=2406, loss=0.41841503977775574\n",
      "Surface training t=2407, loss=0.42828039824962616\n",
      "Surface training t=2408, loss=0.47863076627254486\n",
      "Surface training t=2409, loss=0.4130120277404785\n",
      "Surface training t=2410, loss=0.4321991801261902\n",
      "Surface training t=2411, loss=0.4156711846590042\n",
      "Surface training t=2412, loss=0.46732522547245026\n",
      "Surface training t=2413, loss=0.47314275801181793\n",
      "Surface training t=2414, loss=0.4316529631614685\n",
      "Surface training t=2415, loss=0.440229207277298\n",
      "Surface training t=2416, loss=0.43377916514873505\n",
      "Surface training t=2417, loss=0.47597864270210266\n",
      "Surface training t=2418, loss=0.45246751606464386\n",
      "Surface training t=2419, loss=0.48665158450603485\n",
      "Surface training t=2420, loss=0.4310016334056854\n",
      "Surface training t=2421, loss=0.4725162386894226\n",
      "Surface training t=2422, loss=0.4424178898334503\n",
      "Surface training t=2423, loss=0.46576982736587524\n",
      "Surface training t=2424, loss=0.4601992219686508\n",
      "Surface training t=2425, loss=0.41612525284290314\n",
      "Surface training t=2426, loss=0.4840676486492157\n",
      "Surface training t=2427, loss=0.42648138105869293\n",
      "Surface training t=2428, loss=0.42969028651714325\n",
      "Surface training t=2429, loss=0.42430083453655243\n",
      "Surface training t=2430, loss=0.465731605887413\n",
      "Surface training t=2431, loss=0.41511285305023193\n",
      "Surface training t=2432, loss=0.4499029666185379\n",
      "Surface training t=2433, loss=0.4532727152109146\n",
      "Surface training t=2434, loss=0.47585882246494293\n",
      "Surface training t=2435, loss=0.4430667459964752\n",
      "Surface training t=2436, loss=0.4682779163122177\n",
      "Surface training t=2437, loss=0.46477261185646057\n",
      "Surface training t=2438, loss=0.4819622337818146\n",
      "Surface training t=2439, loss=0.43913380801677704\n",
      "Surface training t=2440, loss=0.42452189326286316\n",
      "Surface training t=2441, loss=0.4427340179681778\n",
      "Surface training t=2442, loss=0.4344731420278549\n",
      "Surface training t=2443, loss=0.4543614089488983\n",
      "Surface training t=2444, loss=0.4728992283344269\n",
      "Surface training t=2445, loss=0.444390207529068\n",
      "Surface training t=2446, loss=0.47522930800914764\n",
      "Surface training t=2447, loss=0.41572311520576477\n",
      "Surface training t=2448, loss=0.42966993153095245\n",
      "Surface training t=2449, loss=0.45334626734256744\n",
      "Surface training t=2450, loss=0.47299206256866455\n",
      "Surface training t=2451, loss=0.4200698584318161\n",
      "Surface training t=2452, loss=0.4316685050725937\n",
      "Surface training t=2453, loss=0.45093488693237305\n",
      "Surface training t=2454, loss=0.4357118457555771\n",
      "Surface training t=2455, loss=0.4494685232639313\n",
      "Surface training t=2456, loss=0.4536765515804291\n",
      "Surface training t=2457, loss=0.4496036022901535\n",
      "Surface training t=2458, loss=0.4317857623100281\n",
      "Surface training t=2459, loss=0.4196336567401886\n",
      "Surface training t=2460, loss=0.5081759244203568\n",
      "Surface training t=2461, loss=0.4289526641368866\n",
      "Surface training t=2462, loss=0.46765629947185516\n",
      "Surface training t=2463, loss=0.4216556251049042\n",
      "Surface training t=2464, loss=0.4165830761194229\n",
      "Surface training t=2465, loss=0.46583667397499084\n",
      "Surface training t=2466, loss=0.40364281833171844\n",
      "Surface training t=2467, loss=0.44845037162303925\n",
      "Surface training t=2468, loss=0.4771161377429962\n",
      "Surface training t=2469, loss=0.4268873333930969\n",
      "Surface training t=2470, loss=0.45924462378025055\n",
      "Surface training t=2471, loss=0.4332585036754608\n",
      "Surface training t=2472, loss=0.431717649102211\n",
      "Surface training t=2473, loss=0.4319503754377365\n",
      "Surface training t=2474, loss=0.4158182591199875\n",
      "Surface training t=2475, loss=0.45471732318401337\n",
      "Surface training t=2476, loss=0.4729878604412079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=2477, loss=0.4280197471380234\n",
      "Surface training t=2478, loss=0.4886616915464401\n",
      "Surface training t=2479, loss=0.44441643357276917\n",
      "Surface training t=2480, loss=0.4214712530374527\n",
      "Surface training t=2481, loss=0.4630506932735443\n",
      "Surface training t=2482, loss=0.48081274330616\n",
      "Surface training t=2483, loss=0.44096820056438446\n",
      "Surface training t=2484, loss=0.49130500853061676\n",
      "Surface training t=2485, loss=0.4279588311910629\n",
      "Surface training t=2486, loss=0.46644924581050873\n",
      "Surface training t=2487, loss=0.4289615750312805\n",
      "Surface training t=2488, loss=0.48505645990371704\n",
      "Surface training t=2489, loss=0.4006112962961197\n",
      "Surface training t=2490, loss=0.45519615709781647\n",
      "Surface training t=2491, loss=0.4473772495985031\n",
      "Surface training t=2492, loss=0.4332427978515625\n",
      "Surface training t=2493, loss=0.4443042278289795\n",
      "Surface training t=2494, loss=0.44020161032676697\n",
      "Surface training t=2495, loss=0.4436393976211548\n",
      "Surface training t=2496, loss=0.5029331296682358\n",
      "Surface training t=2497, loss=0.4669591784477234\n",
      "Surface training t=2498, loss=0.44315752387046814\n",
      "Surface training t=2499, loss=0.4648231118917465\n",
      "Surface training t=2500, loss=0.45199868083000183\n",
      "Surface training t=2501, loss=0.4640672951936722\n",
      "Surface training t=2502, loss=0.4305734634399414\n",
      "Surface training t=2503, loss=0.5249668806791306\n",
      "Surface training t=2504, loss=0.43087083101272583\n",
      "Surface training t=2505, loss=0.42239515483379364\n",
      "Surface training t=2506, loss=0.46435263752937317\n",
      "Surface training t=2507, loss=0.45488667488098145\n",
      "Surface training t=2508, loss=0.4509362429380417\n",
      "Surface training t=2509, loss=0.4402446746826172\n",
      "Surface training t=2510, loss=0.47126948833465576\n",
      "Surface training t=2511, loss=0.4422680139541626\n",
      "Surface training t=2512, loss=0.37894558906555176\n",
      "Surface training t=2513, loss=0.4734024405479431\n",
      "Surface training t=2514, loss=0.44413718581199646\n",
      "Surface training t=2515, loss=0.45683471858501434\n",
      "Surface training t=2516, loss=0.4257318675518036\n",
      "Surface training t=2517, loss=0.4432451128959656\n",
      "Surface training t=2518, loss=0.42278316617012024\n",
      "Surface training t=2519, loss=0.4185015708208084\n",
      "Surface training t=2520, loss=0.47250717878341675\n",
      "Surface training t=2521, loss=0.4380248486995697\n",
      "Surface training t=2522, loss=0.41288116574287415\n",
      "Surface training t=2523, loss=0.46707941591739655\n",
      "Surface training t=2524, loss=0.45406731963157654\n",
      "Surface training t=2525, loss=0.41599535942077637\n",
      "Surface training t=2526, loss=0.44359634816646576\n",
      "Surface training t=2527, loss=0.4616897702217102\n",
      "Surface training t=2528, loss=0.4202815890312195\n",
      "Surface training t=2529, loss=0.44808363914489746\n",
      "Surface training t=2530, loss=0.4574175477027893\n",
      "Surface training t=2531, loss=0.4542705565690994\n",
      "Surface training t=2532, loss=0.41611798107624054\n",
      "Surface training t=2533, loss=0.43645329773426056\n",
      "Surface training t=2534, loss=0.4642383009195328\n",
      "Surface training t=2535, loss=0.5259026139974594\n",
      "Surface training t=2536, loss=0.44450725615024567\n",
      "Surface training t=2537, loss=0.4147041290998459\n",
      "Surface training t=2538, loss=0.45469440519809723\n",
      "Surface training t=2539, loss=0.4294178932905197\n",
      "Surface training t=2540, loss=0.43891093134880066\n",
      "Surface training t=2541, loss=0.4244363456964493\n",
      "Surface training t=2542, loss=0.4125492125749588\n",
      "Surface training t=2543, loss=0.4248572885990143\n",
      "Surface training t=2544, loss=0.46120527386665344\n",
      "Surface training t=2545, loss=0.46373724937438965\n",
      "Surface training t=2546, loss=0.4085434526205063\n",
      "Surface training t=2547, loss=0.4442037343978882\n",
      "Surface training t=2548, loss=0.43130330741405487\n",
      "Surface training t=2549, loss=0.4515143930912018\n",
      "Surface training t=2550, loss=0.3743239790201187\n",
      "Surface training t=2551, loss=0.44419972598552704\n",
      "Surface training t=2552, loss=0.4385967254638672\n",
      "Surface training t=2553, loss=0.40435370802879333\n",
      "Surface training t=2554, loss=0.43701331317424774\n",
      "Surface training t=2555, loss=0.47660787403583527\n",
      "Surface training t=2556, loss=0.5002947747707367\n",
      "Surface training t=2557, loss=0.47061188519001007\n",
      "Surface training t=2558, loss=0.48263661563396454\n",
      "Surface training t=2559, loss=0.4547279328107834\n",
      "Surface training t=2560, loss=0.41406387090682983\n",
      "Surface training t=2561, loss=0.43766218423843384\n",
      "Surface training t=2562, loss=0.4329909235239029\n",
      "Surface training t=2563, loss=0.4480447769165039\n",
      "Surface training t=2564, loss=0.43004412949085236\n",
      "Surface training t=2565, loss=0.41512855887413025\n",
      "Surface training t=2566, loss=0.40321068465709686\n",
      "Surface training t=2567, loss=0.39019380509853363\n",
      "Surface training t=2568, loss=0.4736756980419159\n",
      "Surface training t=2569, loss=0.4424511343240738\n",
      "Surface training t=2570, loss=0.42125286161899567\n",
      "Surface training t=2571, loss=0.4645068198442459\n",
      "Surface training t=2572, loss=0.42306968569755554\n",
      "Surface training t=2573, loss=0.48407626152038574\n",
      "Surface training t=2574, loss=0.41244570910930634\n",
      "Surface training t=2575, loss=0.4961950480937958\n",
      "Surface training t=2576, loss=0.46937738358974457\n",
      "Surface training t=2577, loss=0.3967142701148987\n",
      "Surface training t=2578, loss=0.40771155059337616\n",
      "Surface training t=2579, loss=0.4777066260576248\n",
      "Surface training t=2580, loss=0.445117712020874\n",
      "Surface training t=2581, loss=0.43871840834617615\n",
      "Surface training t=2582, loss=0.49974825978279114\n",
      "Surface training t=2583, loss=0.41184937953948975\n",
      "Surface training t=2584, loss=0.4538731426000595\n",
      "Surface training t=2585, loss=0.4112272411584854\n",
      "Surface training t=2586, loss=0.41998255252838135\n",
      "Surface training t=2587, loss=0.4105670154094696\n",
      "Surface training t=2588, loss=0.519885241985321\n",
      "Surface training t=2589, loss=0.459293469786644\n",
      "Surface training t=2590, loss=0.45693761110305786\n",
      "Surface training t=2591, loss=0.42364414036273956\n",
      "Surface training t=2592, loss=0.4119683653116226\n",
      "Surface training t=2593, loss=0.45598144829273224\n",
      "Surface training t=2594, loss=0.4012940675020218\n",
      "Surface training t=2595, loss=0.4296187609434128\n",
      "Surface training t=2596, loss=0.4521905183792114\n",
      "Surface training t=2597, loss=0.4932403713464737\n",
      "Surface training t=2598, loss=0.4649076610803604\n",
      "Surface training t=2599, loss=0.43575598299503326\n",
      "Surface training t=2600, loss=0.43169161677360535\n",
      "Surface training t=2601, loss=0.41752275824546814\n",
      "Surface training t=2602, loss=0.48210498690605164\n",
      "Surface training t=2603, loss=0.42785754799842834\n",
      "Surface training t=2604, loss=0.4336940497159958\n",
      "Surface training t=2605, loss=0.4114547520875931\n",
      "Surface training t=2606, loss=0.43976281583309174\n",
      "Surface training t=2607, loss=0.4307340830564499\n",
      "Surface training t=2608, loss=0.4915078580379486\n",
      "Surface training t=2609, loss=0.44639524817466736\n",
      "Surface training t=2610, loss=0.49705109000205994\n",
      "Surface training t=2611, loss=0.5064425319433212\n",
      "Surface training t=2612, loss=0.4478997141122818\n",
      "Surface training t=2613, loss=0.45986348390579224\n",
      "Surface training t=2614, loss=0.43456290662288666\n",
      "Surface training t=2615, loss=0.4126190096139908\n",
      "Surface training t=2616, loss=0.44706369936466217\n",
      "Surface training t=2617, loss=0.42827436327934265\n",
      "Surface training t=2618, loss=0.42196378111839294\n",
      "Surface training t=2619, loss=0.4543697088956833\n",
      "Surface training t=2620, loss=0.44955630600452423\n",
      "Surface training t=2621, loss=0.4411451369524002\n",
      "Surface training t=2622, loss=0.4623919278383255\n",
      "Surface training t=2623, loss=0.5017364621162415\n",
      "Surface training t=2624, loss=0.45995956659317017\n",
      "Surface training t=2625, loss=0.49504706263542175\n",
      "Surface training t=2626, loss=0.40752506256103516\n",
      "Surface training t=2627, loss=0.45029202103614807\n",
      "Surface training t=2628, loss=0.45550285279750824\n",
      "Surface training t=2629, loss=0.4426627457141876\n",
      "Surface training t=2630, loss=0.4165128469467163\n",
      "Surface training t=2631, loss=0.49611788988113403\n",
      "Surface training t=2632, loss=0.4465514123439789\n",
      "Surface training t=2633, loss=0.42610426247119904\n",
      "Surface training t=2634, loss=0.46928828954696655\n",
      "Surface training t=2635, loss=0.40565550327301025\n",
      "Surface training t=2636, loss=0.43115757405757904\n",
      "Surface training t=2637, loss=0.4354996830224991\n",
      "Surface training t=2638, loss=0.4242754578590393\n",
      "Surface training t=2639, loss=0.44471125304698944\n",
      "Surface training t=2640, loss=0.4549597352743149\n",
      "Surface training t=2641, loss=0.43518272042274475\n",
      "Surface training t=2642, loss=0.45391468703746796\n",
      "Surface training t=2643, loss=0.4800315648317337\n",
      "Surface training t=2644, loss=0.4453864246606827\n",
      "Surface training t=2645, loss=0.45177948474884033\n",
      "Surface training t=2646, loss=0.43288157880306244\n",
      "Surface training t=2647, loss=0.4523323029279709\n",
      "Surface training t=2648, loss=0.4442186504602432\n",
      "Surface training t=2649, loss=0.4233606308698654\n",
      "Surface training t=2650, loss=0.45107637345790863\n",
      "Surface training t=2651, loss=0.4120365083217621\n",
      "Surface training t=2652, loss=0.3959147185087204\n",
      "Surface training t=2653, loss=0.438400536775589\n",
      "Surface training t=2654, loss=0.4438493251800537\n",
      "Surface training t=2655, loss=0.46538907289505005\n",
      "Surface training t=2656, loss=0.4375161975622177\n",
      "Surface training t=2657, loss=0.3720599114894867\n",
      "Surface training t=2658, loss=0.48364800214767456\n",
      "Surface training t=2659, loss=0.47564224898815155\n",
      "Surface training t=2660, loss=0.46973955631256104\n",
      "Surface training t=2661, loss=0.4612117111682892\n",
      "Surface training t=2662, loss=0.47907841205596924\n",
      "Surface training t=2663, loss=0.4884217083454132\n",
      "Surface training t=2664, loss=0.480089470744133\n",
      "Surface training t=2665, loss=0.40639549493789673\n",
      "Surface training t=2666, loss=0.4787805527448654\n",
      "Surface training t=2667, loss=0.43754807114601135\n",
      "Surface training t=2668, loss=0.4500379413366318\n",
      "Surface training t=2669, loss=0.38408389687538147\n",
      "Surface training t=2670, loss=0.35808632522821426\n",
      "Surface training t=2671, loss=0.42525237798690796\n",
      "Surface training t=2672, loss=0.4377374202013016\n",
      "Surface training t=2673, loss=0.4997027516365051\n",
      "Surface training t=2674, loss=0.4433935284614563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=2675, loss=0.40685024857521057\n",
      "Surface training t=2676, loss=0.4492455869913101\n",
      "Surface training t=2677, loss=0.45587649941444397\n",
      "Surface training t=2678, loss=0.4870077818632126\n",
      "Surface training t=2679, loss=0.4429570734500885\n",
      "Surface training t=2680, loss=0.40543481707572937\n",
      "Surface training t=2681, loss=0.4968390315771103\n",
      "Surface training t=2682, loss=0.4439271092414856\n",
      "Surface training t=2683, loss=0.410059317946434\n",
      "Surface training t=2684, loss=0.4116511791944504\n",
      "Surface training t=2685, loss=0.47885146737098694\n",
      "Surface training t=2686, loss=0.4134751111268997\n",
      "Surface training t=2687, loss=0.44608037173748016\n",
      "Surface training t=2688, loss=0.434186726808548\n",
      "Surface training t=2689, loss=0.4574678838253021\n",
      "Surface training t=2690, loss=0.4294685274362564\n",
      "Surface training t=2691, loss=0.5232710242271423\n",
      "Surface training t=2692, loss=0.4624559134244919\n",
      "Surface training t=2693, loss=0.4327257573604584\n",
      "Surface training t=2694, loss=0.44765983521938324\n",
      "Surface training t=2695, loss=0.41630080342292786\n",
      "Surface training t=2696, loss=0.43385568261146545\n",
      "Surface training t=2697, loss=0.37840044498443604\n",
      "Surface training t=2698, loss=0.4577694237232208\n",
      "Surface training t=2699, loss=0.4280238747596741\n",
      "Surface training t=2700, loss=0.4579979479312897\n",
      "Surface training t=2701, loss=0.43824203312397003\n",
      "Surface training t=2702, loss=0.4271388202905655\n",
      "Surface training t=2703, loss=0.44134531915187836\n",
      "Surface training t=2704, loss=0.4961259514093399\n",
      "Surface training t=2705, loss=0.45740118622779846\n",
      "Surface training t=2706, loss=0.43609002232551575\n",
      "Surface training t=2707, loss=0.44066865742206573\n",
      "Surface training t=2708, loss=0.4381219297647476\n",
      "Surface training t=2709, loss=0.43751390278339386\n",
      "Surface training t=2710, loss=0.4582587629556656\n",
      "Surface training t=2711, loss=0.38575005531311035\n",
      "Surface training t=2712, loss=0.3898153603076935\n",
      "Surface training t=2713, loss=0.4643641710281372\n",
      "Surface training t=2714, loss=0.42262808978557587\n",
      "Surface training t=2715, loss=0.43325796723365784\n",
      "Surface training t=2716, loss=0.43029919266700745\n",
      "Surface training t=2717, loss=0.44821402430534363\n",
      "Surface training t=2718, loss=0.4148426949977875\n",
      "Surface training t=2719, loss=0.46901607513427734\n",
      "Surface training t=2720, loss=0.41337350010871887\n",
      "Surface training t=2721, loss=0.4321979880332947\n",
      "Surface training t=2722, loss=0.43596938252449036\n",
      "Surface training t=2723, loss=0.4342954307794571\n",
      "Surface training t=2724, loss=0.4474518299102783\n",
      "Surface training t=2725, loss=0.4613483250141144\n",
      "Surface training t=2726, loss=0.43699343502521515\n",
      "Surface training t=2727, loss=0.41136325895786285\n",
      "Surface training t=2728, loss=0.4414520114660263\n",
      "Surface training t=2729, loss=0.44109226763248444\n",
      "Surface training t=2730, loss=0.43062809109687805\n",
      "Surface training t=2731, loss=0.4210311323404312\n",
      "Surface training t=2732, loss=0.43390825390815735\n",
      "Surface training t=2733, loss=0.4096633195877075\n",
      "Surface training t=2734, loss=0.42255227267742157\n",
      "Surface training t=2735, loss=0.38460031151771545\n",
      "Surface training t=2736, loss=0.4880220592021942\n",
      "Surface training t=2737, loss=0.4372742474079132\n",
      "Surface training t=2738, loss=0.44589026272296906\n",
      "Surface training t=2739, loss=0.46676020324230194\n",
      "Surface training t=2740, loss=0.4352492541074753\n",
      "Surface training t=2741, loss=0.40974539518356323\n",
      "Surface training t=2742, loss=0.42337577044963837\n",
      "Surface training t=2743, loss=0.4686381369829178\n",
      "Surface training t=2744, loss=0.42550028860569\n",
      "Surface training t=2745, loss=0.44719041883945465\n",
      "Surface training t=2746, loss=0.4143693447113037\n",
      "Surface training t=2747, loss=0.43588295578956604\n",
      "Surface training t=2748, loss=0.4022800922393799\n",
      "Surface training t=2749, loss=0.38430823385715485\n",
      "Surface training t=2750, loss=0.4945787042379379\n",
      "Surface training t=2751, loss=0.43626657128334045\n",
      "Surface training t=2752, loss=0.4482137858867645\n",
      "Surface training t=2753, loss=0.456814780831337\n",
      "Surface training t=2754, loss=0.4568330943584442\n",
      "Surface training t=2755, loss=0.49306926131248474\n",
      "Surface training t=2756, loss=0.48817265033721924\n",
      "Surface training t=2757, loss=0.43063901364803314\n",
      "Surface training t=2758, loss=0.4937821924686432\n",
      "Surface training t=2759, loss=0.46469929814338684\n",
      "Surface training t=2760, loss=0.4288076162338257\n",
      "Surface training t=2761, loss=0.4093495309352875\n",
      "Surface training t=2762, loss=0.4424913823604584\n",
      "Surface training t=2763, loss=0.4870847165584564\n",
      "Surface training t=2764, loss=0.41144855320453644\n",
      "Surface training t=2765, loss=0.43266111612319946\n",
      "Surface training t=2766, loss=0.454938679933548\n",
      "Surface training t=2767, loss=0.4750462621450424\n",
      "Surface training t=2768, loss=0.48598727583885193\n",
      "Surface training t=2769, loss=0.4288644641637802\n",
      "Surface training t=2770, loss=0.44131138920783997\n",
      "Surface training t=2771, loss=0.4129468649625778\n",
      "Surface training t=2772, loss=0.485460564494133\n",
      "Surface training t=2773, loss=0.41418930888175964\n",
      "Surface training t=2774, loss=0.3986743092536926\n",
      "Surface training t=2775, loss=0.41010087728500366\n",
      "Surface training t=2776, loss=0.3915896564722061\n",
      "Surface training t=2777, loss=0.4538984000682831\n",
      "Surface training t=2778, loss=0.452263280749321\n",
      "Surface training t=2779, loss=0.43318694829940796\n",
      "Surface training t=2780, loss=0.3711920827627182\n",
      "Surface training t=2781, loss=0.3557734042406082\n",
      "Surface training t=2782, loss=0.4071514904499054\n",
      "Surface training t=2783, loss=0.46036916971206665\n",
      "Surface training t=2784, loss=0.439655065536499\n",
      "Surface training t=2785, loss=0.36641643941402435\n",
      "Surface training t=2786, loss=0.40098807215690613\n",
      "Surface training t=2787, loss=0.4680723249912262\n",
      "Surface training t=2788, loss=0.4471549689769745\n",
      "Surface training t=2789, loss=0.45218120515346527\n",
      "Surface training t=2790, loss=0.4499516487121582\n",
      "Surface training t=2791, loss=0.46250583231449127\n",
      "Surface training t=2792, loss=0.4847807139158249\n",
      "Surface training t=2793, loss=0.5026731044054031\n",
      "Surface training t=2794, loss=0.40303607285022736\n",
      "Surface training t=2795, loss=0.46399061381816864\n",
      "Surface training t=2796, loss=0.4402104318141937\n",
      "Surface training t=2797, loss=0.4312276691198349\n",
      "Surface training t=2798, loss=0.4539404511451721\n",
      "Surface training t=2799, loss=0.423775315284729\n",
      "Surface training t=2800, loss=0.43706484138965607\n",
      "Surface training t=2801, loss=0.43663859367370605\n",
      "Surface training t=2802, loss=0.46721693873405457\n",
      "Surface training t=2803, loss=0.5030122995376587\n",
      "Surface training t=2804, loss=0.4221787303686142\n",
      "Surface training t=2805, loss=0.440659299492836\n",
      "Surface training t=2806, loss=0.45385056734085083\n",
      "Surface training t=2807, loss=0.48489952087402344\n",
      "Surface training t=2808, loss=0.41641269624233246\n",
      "Surface training t=2809, loss=0.4443153738975525\n",
      "Surface training t=2810, loss=0.3833140730857849\n",
      "Surface training t=2811, loss=0.4452705830335617\n",
      "Surface training t=2812, loss=0.41272851824760437\n",
      "Surface training t=2813, loss=0.4331370145082474\n",
      "Surface training t=2814, loss=0.41153572499752045\n",
      "Surface training t=2815, loss=0.42851802706718445\n",
      "Surface training t=2816, loss=0.43474453687667847\n",
      "Surface training t=2817, loss=0.4740803390741348\n",
      "Surface training t=2818, loss=0.4021419882774353\n",
      "Surface training t=2819, loss=0.4129765182733536\n",
      "Surface training t=2820, loss=0.45524367690086365\n",
      "Surface training t=2821, loss=0.39998874068260193\n",
      "Surface training t=2822, loss=0.4531801640987396\n",
      "Surface training t=2823, loss=0.4562435746192932\n",
      "Surface training t=2824, loss=0.40314000844955444\n",
      "Surface training t=2825, loss=0.4368685930967331\n",
      "Surface training t=2826, loss=0.45514941215515137\n",
      "Surface training t=2827, loss=0.4304534047842026\n",
      "Surface training t=2828, loss=0.4066314101219177\n",
      "Surface training t=2829, loss=0.3911987990140915\n",
      "Surface training t=2830, loss=0.46972352266311646\n",
      "Surface training t=2831, loss=0.4650869816541672\n",
      "Surface training t=2832, loss=0.39995259046554565\n",
      "Surface training t=2833, loss=0.5050683915615082\n",
      "Surface training t=2834, loss=0.4082129895687103\n",
      "Surface training t=2835, loss=0.44089898467063904\n",
      "Surface training t=2836, loss=0.4624529629945755\n",
      "Surface training t=2837, loss=0.4511287063360214\n",
      "Surface training t=2838, loss=0.4644586741924286\n",
      "Surface training t=2839, loss=0.44390422105789185\n",
      "Surface training t=2840, loss=0.42986758053302765\n",
      "Surface training t=2841, loss=0.4079495519399643\n",
      "Surface training t=2842, loss=0.395296573638916\n",
      "Surface training t=2843, loss=0.43780672550201416\n",
      "Surface training t=2844, loss=0.4530857503414154\n",
      "Surface training t=2845, loss=0.41939398646354675\n",
      "Surface training t=2846, loss=0.4091823399066925\n",
      "Surface training t=2847, loss=0.47213341295719147\n",
      "Surface training t=2848, loss=0.39167381823062897\n",
      "Surface training t=2849, loss=0.4379121959209442\n",
      "Surface training t=2850, loss=0.41094522178173065\n",
      "Surface training t=2851, loss=0.38709139823913574\n",
      "Surface training t=2852, loss=0.41488490998744965\n",
      "Surface training t=2853, loss=0.37886300683021545\n",
      "Surface training t=2854, loss=0.4015091061592102\n",
      "Surface training t=2855, loss=0.4179067313671112\n",
      "Surface training t=2856, loss=0.4372371733188629\n",
      "Surface training t=2857, loss=0.40035372972488403\n",
      "Surface training t=2858, loss=0.4165664613246918\n",
      "Surface training t=2859, loss=0.43324099481105804\n",
      "Surface training t=2860, loss=0.43059056997299194\n",
      "Surface training t=2861, loss=0.4587452858686447\n",
      "Surface training t=2862, loss=0.4591493010520935\n",
      "Surface training t=2863, loss=0.48545321822166443\n",
      "Surface training t=2864, loss=0.4294491410255432\n",
      "Surface training t=2865, loss=0.47589850425720215\n",
      "Surface training t=2866, loss=0.4588399827480316\n",
      "Surface training t=2867, loss=0.4292658567428589\n",
      "Surface training t=2868, loss=0.4313245713710785\n",
      "Surface training t=2869, loss=0.4190388023853302\n",
      "Surface training t=2870, loss=0.4386388957500458\n",
      "Surface training t=2871, loss=0.4163713604211807\n",
      "Surface training t=2872, loss=0.4193822741508484\n",
      "Surface training t=2873, loss=0.4308767765760422\n",
      "Surface training t=2874, loss=0.4493454396724701\n",
      "Surface training t=2875, loss=0.4765673875808716\n",
      "Surface training t=2876, loss=0.48470868170261383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=2877, loss=0.48279042541980743\n",
      "Surface training t=2878, loss=0.4419693797826767\n",
      "Surface training t=2879, loss=0.4657391756772995\n",
      "Surface training t=2880, loss=0.43593497574329376\n",
      "Surface training t=2881, loss=0.42491425573825836\n",
      "Surface training t=2882, loss=0.46391403675079346\n",
      "Surface training t=2883, loss=0.4265882819890976\n",
      "Surface training t=2884, loss=0.4214331656694412\n",
      "Surface training t=2885, loss=0.4486294090747833\n",
      "Surface training t=2886, loss=0.4373985081911087\n",
      "Surface training t=2887, loss=0.41185885667800903\n",
      "Surface training t=2888, loss=0.39825139939785004\n",
      "Surface training t=2889, loss=0.4791064113378525\n",
      "Surface training t=2890, loss=0.44182732701301575\n",
      "Surface training t=2891, loss=0.46555787324905396\n",
      "Surface training t=2892, loss=0.44631753861904144\n",
      "Surface training t=2893, loss=0.4982694685459137\n",
      "Surface training t=2894, loss=0.4083549827337265\n",
      "Surface training t=2895, loss=0.44999605417251587\n",
      "Surface training t=2896, loss=0.4513818323612213\n",
      "Surface training t=2897, loss=0.43882906436920166\n",
      "Surface training t=2898, loss=0.5470249950885773\n",
      "Surface training t=2899, loss=0.3974836468696594\n",
      "Surface training t=2900, loss=0.3847958594560623\n",
      "Surface training t=2901, loss=0.4767402559518814\n",
      "Surface training t=2902, loss=0.4381906986236572\n",
      "Surface training t=2903, loss=0.45652469992637634\n",
      "Surface training t=2904, loss=0.42978061735630035\n",
      "Surface training t=2905, loss=0.47186821699142456\n",
      "Surface training t=2906, loss=0.4219720959663391\n",
      "Surface training t=2907, loss=0.39311501383781433\n",
      "Surface training t=2908, loss=0.4539540112018585\n",
      "Surface training t=2909, loss=0.45203544199466705\n",
      "Surface training t=2910, loss=0.4623064398765564\n",
      "Surface training t=2911, loss=0.4796244353055954\n",
      "Surface training t=2912, loss=0.39180684089660645\n",
      "Surface training t=2913, loss=0.44209466874599457\n",
      "Surface training t=2914, loss=0.4018833041191101\n",
      "Surface training t=2915, loss=0.4571758061647415\n",
      "Surface training t=2916, loss=0.4085492193698883\n",
      "Surface training t=2917, loss=0.4333761930465698\n",
      "Surface training t=2918, loss=0.4277759790420532\n",
      "Surface training t=2919, loss=0.3903719037771225\n",
      "Surface training t=2920, loss=0.43932919204235077\n",
      "Surface training t=2921, loss=0.41900739073753357\n",
      "Surface training t=2922, loss=0.44807909429073334\n",
      "Surface training t=2923, loss=0.4468911737203598\n",
      "Surface training t=2924, loss=0.4091973900794983\n",
      "Surface training t=2925, loss=0.42410072684288025\n",
      "Surface training t=2926, loss=0.41399116814136505\n",
      "Surface training t=2927, loss=0.48580801486968994\n",
      "Surface training t=2928, loss=0.4170319139957428\n",
      "Surface training t=2929, loss=0.42780935764312744\n",
      "Surface training t=2930, loss=0.45273457467556\n",
      "Surface training t=2931, loss=0.4856119453907013\n",
      "Surface training t=2932, loss=0.42046838998794556\n",
      "Surface training t=2933, loss=0.41308633983135223\n",
      "Surface training t=2934, loss=0.44731786847114563\n",
      "Surface training t=2935, loss=0.4824207276105881\n",
      "Surface training t=2936, loss=0.4252213388681412\n",
      "Surface training t=2937, loss=0.43298882246017456\n",
      "Surface training t=2938, loss=0.39631301164627075\n",
      "Surface training t=2939, loss=0.44215549528598785\n",
      "Surface training t=2940, loss=0.39704813063144684\n",
      "Surface training t=2941, loss=0.4136830270290375\n",
      "Surface training t=2942, loss=0.4322805255651474\n",
      "Surface training t=2943, loss=0.3976449817419052\n",
      "Surface training t=2944, loss=0.40882545709609985\n",
      "Surface training t=2945, loss=0.42195814847946167\n",
      "Surface training t=2946, loss=0.43144963681697845\n",
      "Surface training t=2947, loss=0.46865232288837433\n",
      "Surface training t=2948, loss=0.41064155101776123\n",
      "Surface training t=2949, loss=0.4416223168373108\n",
      "Surface training t=2950, loss=0.4520150423049927\n",
      "Surface training t=2951, loss=0.42230501770973206\n",
      "Surface training t=2952, loss=0.44138793647289276\n",
      "Surface training t=2953, loss=0.4402788430452347\n",
      "Surface training t=2954, loss=0.3900262862443924\n",
      "Surface training t=2955, loss=0.4125775247812271\n",
      "Surface training t=2956, loss=0.46475376188755035\n",
      "Surface training t=2957, loss=0.4433797597885132\n",
      "Surface training t=2958, loss=0.4696706533432007\n",
      "Surface training t=2959, loss=0.383124515414238\n",
      "Surface training t=2960, loss=0.4891268461942673\n",
      "Surface training t=2961, loss=0.41780008375644684\n",
      "Surface training t=2962, loss=0.42790544033050537\n",
      "Surface training t=2963, loss=0.4121829867362976\n",
      "Surface training t=2964, loss=0.43773478269577026\n",
      "Surface training t=2965, loss=0.4333590120077133\n",
      "Surface training t=2966, loss=0.4215322732925415\n",
      "Surface training t=2967, loss=0.4367086738348007\n",
      "Surface training t=2968, loss=0.46162885427474976\n",
      "Surface training t=2969, loss=0.44147244095802307\n",
      "Surface training t=2970, loss=0.38912928104400635\n",
      "Surface training t=2971, loss=0.48150382936000824\n",
      "Surface training t=2972, loss=0.42503561079502106\n",
      "Surface training t=2973, loss=0.41733649373054504\n",
      "Surface training t=2974, loss=0.4517136663198471\n",
      "Surface training t=2975, loss=0.4507593363523483\n",
      "Surface training t=2976, loss=0.46554602682590485\n",
      "Surface training t=2977, loss=0.4412979185581207\n",
      "Surface training t=2978, loss=0.4589996784925461\n",
      "Surface training t=2979, loss=0.4222857654094696\n",
      "Surface training t=2980, loss=0.44328197836875916\n",
      "Surface training t=2981, loss=0.44112372398376465\n",
      "Surface training t=2982, loss=0.4163595288991928\n",
      "Surface training t=2983, loss=0.4440448135137558\n",
      "Surface training t=2984, loss=0.3938806802034378\n",
      "Surface training t=2985, loss=0.41211169958114624\n",
      "Surface training t=2986, loss=0.41306236386299133\n",
      "Surface training t=2987, loss=0.43033671379089355\n",
      "Surface training t=2988, loss=0.4553738087415695\n",
      "Surface training t=2989, loss=0.4000406116247177\n",
      "Surface training t=2990, loss=0.4152120351791382\n",
      "Surface training t=2991, loss=0.4259119927883148\n",
      "Surface training t=2992, loss=0.4189242571592331\n",
      "Surface training t=2993, loss=0.45908282697200775\n",
      "Surface training t=2994, loss=0.3944312632083893\n",
      "Surface training t=2995, loss=0.39340756833553314\n",
      "Surface training t=2996, loss=0.43473368883132935\n",
      "Surface training t=2997, loss=0.4026937186717987\n",
      "Surface training t=2998, loss=0.4226559102535248\n",
      "Surface training t=2999, loss=0.4158366173505783\n",
      "Surface training t=3000, loss=0.4257752150297165\n",
      "Surface training t=3001, loss=0.4487629234790802\n",
      "Surface training t=3002, loss=0.4120959937572479\n",
      "Surface training t=3003, loss=0.42722684144973755\n",
      "Surface training t=3004, loss=0.4137829393148422\n",
      "Surface training t=3005, loss=0.4317459762096405\n",
      "Surface training t=3006, loss=0.4227207601070404\n",
      "Surface training t=3007, loss=0.47735731303691864\n",
      "Surface training t=3008, loss=0.4075806140899658\n",
      "Surface training t=3009, loss=0.43061865866184235\n",
      "Surface training t=3010, loss=0.4234689325094223\n",
      "Surface training t=3011, loss=0.4305606633424759\n",
      "Surface training t=3012, loss=0.46495941281318665\n",
      "Surface training t=3013, loss=0.4692373722791672\n",
      "Surface training t=3014, loss=0.40350715816020966\n",
      "Surface training t=3015, loss=0.45261669158935547\n",
      "Surface training t=3016, loss=0.4523047208786011\n",
      "Surface training t=3017, loss=0.5275103449821472\n",
      "Surface training t=3018, loss=0.41067303717136383\n",
      "Surface training t=3019, loss=0.38681773841381073\n",
      "Surface training t=3020, loss=0.40507353842258453\n",
      "Surface training t=3021, loss=0.4252922087907791\n",
      "Surface training t=3022, loss=0.43994593620300293\n",
      "Surface training t=3023, loss=0.45785993337631226\n",
      "Surface training t=3024, loss=0.38979779183864594\n",
      "Surface training t=3025, loss=0.4249631315469742\n",
      "Surface training t=3026, loss=0.41491614282131195\n",
      "Surface training t=3027, loss=0.46236538887023926\n",
      "Surface training t=3028, loss=0.4257398396730423\n",
      "Surface training t=3029, loss=0.4253612160682678\n",
      "Surface training t=3030, loss=0.41722263395786285\n",
      "Surface training t=3031, loss=0.4051024913787842\n",
      "Surface training t=3032, loss=0.4293500930070877\n",
      "Surface training t=3033, loss=0.4662810266017914\n",
      "Surface training t=3034, loss=0.39821311831474304\n",
      "Surface training t=3035, loss=0.409147247672081\n",
      "Surface training t=3036, loss=0.34060972183942795\n",
      "Surface training t=3037, loss=0.38842932879924774\n",
      "Surface training t=3038, loss=0.4578295797109604\n",
      "Surface training t=3039, loss=0.42650045454502106\n",
      "Surface training t=3040, loss=0.4241417795419693\n",
      "Surface training t=3041, loss=0.39116841554641724\n",
      "Surface training t=3042, loss=0.4140794724225998\n",
      "Surface training t=3043, loss=0.4338454604148865\n",
      "Surface training t=3044, loss=0.4821525663137436\n",
      "Surface training t=3045, loss=0.4496123194694519\n",
      "Surface training t=3046, loss=0.43700945377349854\n",
      "Surface training t=3047, loss=0.4222859889268875\n",
      "Surface training t=3048, loss=0.41856588423252106\n",
      "Surface training t=3049, loss=0.4236930012702942\n",
      "Surface training t=3050, loss=0.41859646141529083\n",
      "Surface training t=3051, loss=0.42180676758289337\n",
      "Surface training t=3052, loss=0.47377678751945496\n",
      "Surface training t=3053, loss=0.42509497702121735\n",
      "Surface training t=3054, loss=0.4445233643054962\n",
      "Surface training t=3055, loss=0.4388369619846344\n",
      "Surface training t=3056, loss=0.44670458137989044\n",
      "Surface training t=3057, loss=0.44038885831832886\n",
      "Surface training t=3058, loss=0.42377613484859467\n",
      "Surface training t=3059, loss=0.44731663167476654\n",
      "Surface training t=3060, loss=0.42074504494667053\n",
      "Surface training t=3061, loss=0.47520725429058075\n",
      "Surface training t=3062, loss=0.4021306037902832\n",
      "Surface training t=3063, loss=0.4277746081352234\n",
      "Surface training t=3064, loss=0.405879482626915\n",
      "Surface training t=3065, loss=0.40522925555706024\n",
      "Surface training t=3066, loss=0.414023220539093\n",
      "Surface training t=3067, loss=0.4301135987043381\n",
      "Surface training t=3068, loss=0.42443162202835083\n",
      "Surface training t=3069, loss=0.3900146484375\n",
      "Surface training t=3070, loss=0.4106752723455429\n",
      "Surface training t=3071, loss=0.4002106934785843\n",
      "Surface training t=3072, loss=0.4334873706102371\n",
      "Surface training t=3073, loss=0.37594833970069885\n",
      "Surface training t=3074, loss=0.43328922986984253\n",
      "Surface training t=3075, loss=0.41034993529319763\n",
      "Surface training t=3076, loss=0.4434642940759659\n",
      "Surface training t=3077, loss=0.41963285207748413\n",
      "Surface training t=3078, loss=0.3875427544116974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=3079, loss=0.42405180633068085\n",
      "Surface training t=3080, loss=0.4115127772092819\n",
      "Surface training t=3081, loss=0.38243745267391205\n",
      "Surface training t=3082, loss=0.44410568475723267\n",
      "Surface training t=3083, loss=0.42696329951286316\n",
      "Surface training t=3084, loss=0.4374200403690338\n",
      "Surface training t=3085, loss=0.43699538707733154\n",
      "Surface training t=3086, loss=0.38679561018943787\n",
      "Surface training t=3087, loss=0.47015997767448425\n",
      "Surface training t=3088, loss=0.43570704758167267\n",
      "Surface training t=3089, loss=0.42498932778835297\n",
      "Surface training t=3090, loss=0.4007487893104553\n",
      "Surface training t=3091, loss=0.4019118845462799\n",
      "Surface training t=3092, loss=0.44524966180324554\n",
      "Surface training t=3093, loss=0.384991392493248\n",
      "Surface training t=3094, loss=0.4535289704799652\n",
      "Surface training t=3095, loss=0.4595429003238678\n",
      "Surface training t=3096, loss=0.3837016224861145\n",
      "Surface training t=3097, loss=0.43622009456157684\n",
      "Surface training t=3098, loss=0.41728827357292175\n",
      "Surface training t=3099, loss=0.4232490211725235\n",
      "Surface training t=3100, loss=0.47745630145072937\n",
      "Surface training t=3101, loss=0.402076855301857\n",
      "Surface training t=3102, loss=0.40393778681755066\n",
      "Surface training t=3103, loss=0.3862992227077484\n",
      "Surface training t=3104, loss=0.35673126578330994\n",
      "Surface training t=3105, loss=0.3821159601211548\n",
      "Surface training t=3106, loss=0.4362305551767349\n",
      "Surface training t=3107, loss=0.4200492650270462\n",
      "Surface training t=3108, loss=0.4077787697315216\n",
      "Surface training t=3109, loss=0.4627789705991745\n",
      "Surface training t=3110, loss=0.45680108666419983\n",
      "Surface training t=3111, loss=0.4057403951883316\n",
      "Surface training t=3112, loss=0.4563547670841217\n",
      "Surface training t=3113, loss=0.4499344676733017\n",
      "Surface training t=3114, loss=0.4691333919763565\n",
      "Surface training t=3115, loss=0.47503596544265747\n",
      "Surface training t=3116, loss=0.44493669271469116\n",
      "Surface training t=3117, loss=0.4208315461874008\n",
      "Surface training t=3118, loss=0.38898028433322906\n",
      "Surface training t=3119, loss=0.4181153476238251\n",
      "Surface training t=3120, loss=0.4512633830308914\n",
      "Surface training t=3121, loss=0.4047412723302841\n",
      "Surface training t=3122, loss=0.46916893124580383\n",
      "Surface training t=3123, loss=0.403327539563179\n",
      "Surface training t=3124, loss=0.4125575125217438\n",
      "Surface training t=3125, loss=0.4412606805562973\n",
      "Surface training t=3126, loss=0.47967350482940674\n",
      "Surface training t=3127, loss=0.42566530406475067\n",
      "Surface training t=3128, loss=0.4993511587381363\n",
      "Surface training t=3129, loss=0.46613578498363495\n",
      "Surface training t=3130, loss=0.404192715883255\n",
      "Surface training t=3131, loss=0.4128665030002594\n",
      "Surface training t=3132, loss=0.42738108336925507\n",
      "Surface training t=3133, loss=0.420260950922966\n",
      "Surface training t=3134, loss=0.47838757932186127\n",
      "Surface training t=3135, loss=0.4852907359600067\n",
      "Surface training t=3136, loss=0.4357563853263855\n",
      "Surface training t=3137, loss=0.41786515712738037\n",
      "Surface training t=3138, loss=0.39082206785678864\n",
      "Surface training t=3139, loss=0.38341017067432404\n",
      "Surface training t=3140, loss=0.3834252655506134\n",
      "Surface training t=3141, loss=0.442009761929512\n",
      "Surface training t=3142, loss=0.42867235839366913\n",
      "Surface training t=3143, loss=0.3961540460586548\n",
      "Surface training t=3144, loss=0.40945257246494293\n",
      "Surface training t=3145, loss=0.41623713076114655\n",
      "Surface training t=3146, loss=0.4248872846364975\n",
      "Surface training t=3147, loss=0.38912995159626007\n",
      "Surface training t=3148, loss=0.4346236288547516\n",
      "Surface training t=3149, loss=0.40439572930336\n",
      "Surface training t=3150, loss=0.4149267077445984\n",
      "Surface training t=3151, loss=0.4142120033502579\n",
      "Surface training t=3152, loss=0.42030733823776245\n",
      "Surface training t=3153, loss=0.4455641508102417\n",
      "Surface training t=3154, loss=0.4644536226987839\n",
      "Surface training t=3155, loss=0.4633050858974457\n",
      "Surface training t=3156, loss=0.4250975251197815\n",
      "Surface training t=3157, loss=0.4541427195072174\n",
      "Surface training t=3158, loss=0.4173954278230667\n",
      "Surface training t=3159, loss=0.40836700797080994\n",
      "Surface training t=3160, loss=0.41319403052330017\n",
      "Surface training t=3161, loss=0.36760249733924866\n",
      "Surface training t=3162, loss=0.35758906602859497\n",
      "Surface training t=3163, loss=0.39760488271713257\n",
      "Surface training t=3164, loss=0.464606374502182\n",
      "Surface training t=3165, loss=0.47825996577739716\n",
      "Surface training t=3166, loss=0.46544161438941956\n",
      "Surface training t=3167, loss=0.415699765086174\n",
      "Surface training t=3168, loss=0.4204069525003433\n",
      "Surface training t=3169, loss=0.4179016202688217\n",
      "Surface training t=3170, loss=0.4139139950275421\n",
      "Surface training t=3171, loss=0.41100597381591797\n",
      "Surface training t=3172, loss=0.4148916155099869\n",
      "Surface training t=3173, loss=0.4039037525653839\n",
      "Surface training t=3174, loss=0.43093761801719666\n",
      "Surface training t=3175, loss=0.4638066440820694\n",
      "Surface training t=3176, loss=0.4394580274820328\n",
      "Surface training t=3177, loss=0.4184209704399109\n",
      "Surface training t=3178, loss=0.41274209320545197\n",
      "Surface training t=3179, loss=0.4291650354862213\n",
      "Surface training t=3180, loss=0.39597417414188385\n",
      "Surface training t=3181, loss=0.4341096580028534\n",
      "Surface training t=3182, loss=0.4592021405696869\n",
      "Surface training t=3183, loss=0.44444236159324646\n",
      "Surface training t=3184, loss=0.4557925909757614\n",
      "Surface training t=3185, loss=0.39996324479579926\n",
      "Surface training t=3186, loss=0.40280982851982117\n",
      "Surface training t=3187, loss=0.44585856795310974\n",
      "Surface training t=3188, loss=0.4090643525123596\n",
      "Surface training t=3189, loss=0.4587113708257675\n",
      "Surface training t=3190, loss=0.4244159460067749\n",
      "Surface training t=3191, loss=0.41003669798374176\n",
      "Surface training t=3192, loss=0.405429944396019\n",
      "Surface training t=3193, loss=0.48098529875278473\n",
      "Surface training t=3194, loss=0.4395507127046585\n",
      "Surface training t=3195, loss=0.4884573519229889\n",
      "Surface training t=3196, loss=0.4242125600576401\n",
      "Surface training t=3197, loss=0.4268989861011505\n",
      "Surface training t=3198, loss=0.43862855434417725\n",
      "Surface training t=3199, loss=0.3994443714618683\n",
      "Surface training t=3200, loss=0.3952530026435852\n",
      "Surface training t=3201, loss=0.41303113102912903\n",
      "Surface training t=3202, loss=0.4628641605377197\n",
      "Surface training t=3203, loss=0.43347376585006714\n",
      "Surface training t=3204, loss=0.41403360664844513\n",
      "Surface training t=3205, loss=0.43005944788455963\n",
      "Surface training t=3206, loss=0.44011153280735016\n",
      "Surface training t=3207, loss=0.4558145999908447\n",
      "Surface training t=3208, loss=0.3963277339935303\n",
      "Surface training t=3209, loss=0.4377833753824234\n",
      "Surface training t=3210, loss=0.45511654019355774\n",
      "Surface training t=3211, loss=0.4499349296092987\n",
      "Surface training t=3212, loss=0.4437277764081955\n",
      "Surface training t=3213, loss=0.4278107285499573\n",
      "Surface training t=3214, loss=0.4203846901655197\n",
      "Surface training t=3215, loss=0.3991104066371918\n",
      "Surface training t=3216, loss=0.3945137709379196\n",
      "Surface training t=3217, loss=0.4327003210783005\n",
      "Surface training t=3218, loss=0.39594072103500366\n",
      "Surface training t=3219, loss=0.38712744414806366\n",
      "Surface training t=3220, loss=0.44501306116580963\n",
      "Surface training t=3221, loss=0.44530247151851654\n",
      "Surface training t=3222, loss=0.4397497773170471\n",
      "Surface training t=3223, loss=0.4240996986627579\n",
      "Surface training t=3224, loss=0.42774930596351624\n",
      "Surface training t=3225, loss=0.3731255382299423\n",
      "Surface training t=3226, loss=0.4063316583633423\n",
      "Surface training t=3227, loss=0.43364351987838745\n",
      "Surface training t=3228, loss=0.41390232741832733\n",
      "Surface training t=3229, loss=0.3759287893772125\n",
      "Surface training t=3230, loss=0.40265414118766785\n",
      "Surface training t=3231, loss=0.44371160864830017\n",
      "Surface training t=3232, loss=0.4379241615533829\n",
      "Surface training t=3233, loss=0.4109162390232086\n",
      "Surface training t=3234, loss=0.3929404765367508\n",
      "Surface training t=3235, loss=0.40117867290973663\n",
      "Surface training t=3236, loss=0.4440494477748871\n",
      "Surface training t=3237, loss=0.44560113549232483\n",
      "Surface training t=3238, loss=0.4274427592754364\n",
      "Surface training t=3239, loss=0.4110921323299408\n",
      "Surface training t=3240, loss=0.3873293697834015\n",
      "Surface training t=3241, loss=0.39865221083164215\n",
      "Surface training t=3242, loss=0.4333992898464203\n",
      "Surface training t=3243, loss=0.3800322711467743\n",
      "Surface training t=3244, loss=0.41463547945022583\n",
      "Surface training t=3245, loss=0.4401187598705292\n",
      "Surface training t=3246, loss=0.4133055806159973\n",
      "Surface training t=3247, loss=0.4680447578430176\n",
      "Surface training t=3248, loss=0.4170420169830322\n",
      "Surface training t=3249, loss=0.4161995202302933\n",
      "Surface training t=3250, loss=0.45430268347263336\n",
      "Surface training t=3251, loss=0.42310018837451935\n",
      "Surface training t=3252, loss=0.39951369166374207\n",
      "Surface training t=3253, loss=0.4028497338294983\n",
      "Surface training t=3254, loss=0.3816314935684204\n",
      "Surface training t=3255, loss=0.4565126448869705\n",
      "Surface training t=3256, loss=0.371734082698822\n",
      "Surface training t=3257, loss=0.47353535890579224\n",
      "Surface training t=3258, loss=0.4408200681209564\n",
      "Surface training t=3259, loss=0.4112176150083542\n",
      "Surface training t=3260, loss=0.4435349106788635\n",
      "Surface training t=3261, loss=0.39620743691921234\n",
      "Surface training t=3262, loss=0.43613874912261963\n",
      "Surface training t=3263, loss=0.38724492490291595\n",
      "Surface training t=3264, loss=0.4303742051124573\n",
      "Surface training t=3265, loss=0.4217597544193268\n",
      "Surface training t=3266, loss=0.39590267837047577\n",
      "Surface training t=3267, loss=0.4103217124938965\n",
      "Surface training t=3268, loss=0.4542650878429413\n",
      "Surface training t=3269, loss=0.475455641746521\n",
      "Surface training t=3270, loss=0.46213656663894653\n",
      "Surface training t=3271, loss=0.4284319132566452\n",
      "Surface training t=3272, loss=0.41567739844322205\n",
      "Surface training t=3273, loss=0.44131216406822205\n",
      "Surface training t=3274, loss=0.4142257571220398\n",
      "Surface training t=3275, loss=0.412133052945137\n",
      "Surface training t=3276, loss=0.4210359901189804\n",
      "Surface training t=3277, loss=0.39336083829402924\n",
      "Surface training t=3278, loss=0.44184769690036774\n",
      "Surface training t=3279, loss=0.48124559223651886\n",
      "Surface training t=3280, loss=0.48076023161411285\n",
      "Surface training t=3281, loss=0.3965475857257843\n",
      "Surface training t=3282, loss=0.4122270792722702\n",
      "Surface training t=3283, loss=0.43998320400714874\n",
      "Surface training t=3284, loss=0.38954153656959534\n",
      "Surface training t=3285, loss=0.4725320190191269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=3286, loss=0.4345614016056061\n",
      "Surface training t=3287, loss=0.4541679620742798\n",
      "Surface training t=3288, loss=0.4129186272621155\n",
      "Surface training t=3289, loss=0.38835692405700684\n",
      "Surface training t=3290, loss=0.438097208738327\n",
      "Surface training t=3291, loss=0.46631211042404175\n",
      "Surface training t=3292, loss=0.4509003013372421\n",
      "Surface training t=3293, loss=0.4177183210849762\n",
      "Surface training t=3294, loss=0.43794526159763336\n",
      "Surface training t=3295, loss=0.3999045193195343\n",
      "Surface training t=3296, loss=0.46429985761642456\n",
      "Surface training t=3297, loss=0.4278258979320526\n",
      "Surface training t=3298, loss=0.4046638607978821\n",
      "Surface training t=3299, loss=0.3725349009037018\n",
      "Surface training t=3300, loss=0.4927196353673935\n",
      "Surface training t=3301, loss=0.4291251301765442\n",
      "Surface training t=3302, loss=0.40412183105945587\n",
      "Surface training t=3303, loss=0.442422479391098\n",
      "Surface training t=3304, loss=0.38035283982753754\n",
      "Surface training t=3305, loss=0.3921457976102829\n",
      "Surface training t=3306, loss=0.3856400102376938\n",
      "Surface training t=3307, loss=0.4049958735704422\n",
      "Surface training t=3308, loss=0.404806986451149\n",
      "Surface training t=3309, loss=0.42432597279548645\n",
      "Surface training t=3310, loss=0.40110306441783905\n",
      "Surface training t=3311, loss=0.39588314294815063\n",
      "Surface training t=3312, loss=0.40248630940914154\n",
      "Surface training t=3313, loss=0.4015888571739197\n",
      "Surface training t=3314, loss=0.42679552733898163\n",
      "Surface training t=3315, loss=0.4589639902114868\n",
      "Surface training t=3316, loss=0.40525127947330475\n",
      "Surface training t=3317, loss=0.35535623133182526\n",
      "Surface training t=3318, loss=0.43509942293167114\n",
      "Surface training t=3319, loss=0.42455819249153137\n",
      "Surface training t=3320, loss=0.45881515741348267\n",
      "Surface training t=3321, loss=0.4446222633123398\n",
      "Surface training t=3322, loss=0.3546772301197052\n",
      "Surface training t=3323, loss=0.40651844441890717\n",
      "Surface training t=3324, loss=0.41842903196811676\n",
      "Surface training t=3325, loss=0.44042791426181793\n",
      "Surface training t=3326, loss=0.41568732261657715\n",
      "Surface training t=3327, loss=0.3913002014160156\n",
      "Surface training t=3328, loss=0.36852042376995087\n",
      "Surface training t=3329, loss=0.4360719919204712\n",
      "Surface training t=3330, loss=0.4449445754289627\n",
      "Surface training t=3331, loss=0.4554610550403595\n",
      "Surface training t=3332, loss=0.43203431367874146\n",
      "Surface training t=3333, loss=0.4426300525665283\n",
      "Surface training t=3334, loss=0.37652355432510376\n",
      "Surface training t=3335, loss=0.3975777328014374\n",
      "Surface training t=3336, loss=0.39875347912311554\n",
      "Surface training t=3337, loss=0.4361858367919922\n",
      "Surface training t=3338, loss=0.4183109700679779\n",
      "Surface training t=3339, loss=0.40269726514816284\n",
      "Surface training t=3340, loss=0.40350355207920074\n",
      "Surface training t=3341, loss=0.4126443862915039\n",
      "Surface training t=3342, loss=0.3977590650320053\n",
      "Surface training t=3343, loss=0.4823860228061676\n",
      "Surface training t=3344, loss=0.3913048803806305\n",
      "Surface training t=3345, loss=0.3986761122941971\n",
      "Surface training t=3346, loss=0.37846551835536957\n",
      "Surface training t=3347, loss=0.39199957251548767\n",
      "Surface training t=3348, loss=0.40696610510349274\n",
      "Surface training t=3349, loss=0.39657261967658997\n",
      "Surface training t=3350, loss=0.40715526044368744\n",
      "Surface training t=3351, loss=0.428027018904686\n",
      "Surface training t=3352, loss=0.42289939522743225\n",
      "Surface training t=3353, loss=0.44746944308280945\n",
      "Surface training t=3354, loss=0.410013347864151\n",
      "Surface training t=3355, loss=0.47983427345752716\n",
      "Surface training t=3356, loss=0.41429226100444794\n",
      "Surface training t=3357, loss=0.47210563719272614\n",
      "Surface training t=3358, loss=0.3902500122785568\n",
      "Surface training t=3359, loss=0.4169016629457474\n",
      "Surface training t=3360, loss=0.41274620592594147\n",
      "Surface training t=3361, loss=0.4441736191511154\n",
      "Surface training t=3362, loss=0.44344595074653625\n",
      "Surface training t=3363, loss=0.42229776084423065\n",
      "Surface training t=3364, loss=0.4767402857542038\n",
      "Surface training t=3365, loss=0.4086155295372009\n",
      "Surface training t=3366, loss=0.4214150607585907\n",
      "Surface training t=3367, loss=0.37775859236717224\n",
      "Surface training t=3368, loss=0.38678233325481415\n",
      "Surface training t=3369, loss=0.44574378430843353\n",
      "Surface training t=3370, loss=0.4317063093185425\n",
      "Surface training t=3371, loss=0.47160233557224274\n",
      "Surface training t=3372, loss=0.384520560503006\n",
      "Surface training t=3373, loss=0.40879739820957184\n",
      "Surface training t=3374, loss=0.4180970638990402\n",
      "Surface training t=3375, loss=0.3953411877155304\n",
      "Surface training t=3376, loss=0.4181976318359375\n",
      "Surface training t=3377, loss=0.42614389955997467\n",
      "Surface training t=3378, loss=0.3582979440689087\n",
      "Surface training t=3379, loss=0.4181254655122757\n",
      "Surface training t=3380, loss=0.4176379591226578\n",
      "Surface training t=3381, loss=0.489976242184639\n",
      "Surface training t=3382, loss=0.4390326291322708\n",
      "Surface training t=3383, loss=0.4540012627840042\n",
      "Surface training t=3384, loss=0.40015582740306854\n",
      "Surface training t=3385, loss=0.4513511657714844\n",
      "Surface training t=3386, loss=0.41026148200035095\n",
      "Surface training t=3387, loss=0.3832870125770569\n",
      "Surface training t=3388, loss=0.3481913357973099\n",
      "Surface training t=3389, loss=0.43173372745513916\n",
      "Surface training t=3390, loss=0.3876558989286423\n",
      "Surface training t=3391, loss=0.4194665551185608\n",
      "Surface training t=3392, loss=0.43021225929260254\n",
      "Surface training t=3393, loss=0.4327698200941086\n",
      "Surface training t=3394, loss=0.47087036073207855\n",
      "Surface training t=3395, loss=0.3727024793624878\n",
      "Surface training t=3396, loss=0.3747462034225464\n",
      "Surface training t=3397, loss=0.402268648147583\n",
      "Surface training t=3398, loss=0.4529234766960144\n",
      "Surface training t=3399, loss=0.38070589303970337\n",
      "Surface training t=3400, loss=0.38637030124664307\n",
      "Surface training t=3401, loss=0.40969444811344147\n",
      "Surface training t=3402, loss=0.47006362676620483\n",
      "Surface training t=3403, loss=0.4299580454826355\n",
      "Surface training t=3404, loss=0.3804013431072235\n",
      "Surface training t=3405, loss=0.47156165540218353\n",
      "Surface training t=3406, loss=0.4153963774442673\n",
      "Surface training t=3407, loss=0.3922395408153534\n",
      "Surface training t=3408, loss=0.41879257559776306\n",
      "Surface training t=3409, loss=0.4634588658809662\n",
      "Surface training t=3410, loss=0.45916566252708435\n",
      "Surface training t=3411, loss=0.4529654383659363\n",
      "Surface training t=3412, loss=0.3767513334751129\n",
      "Surface training t=3413, loss=0.42404963076114655\n",
      "Surface training t=3414, loss=0.4290003031492233\n",
      "Surface training t=3415, loss=0.4055783897638321\n",
      "Surface training t=3416, loss=0.40047702193260193\n",
      "Surface training t=3417, loss=0.3991536498069763\n",
      "Surface training t=3418, loss=0.41697797179222107\n",
      "Surface training t=3419, loss=0.45547623932361603\n",
      "Surface training t=3420, loss=0.4126613438129425\n",
      "Surface training t=3421, loss=0.4204474091529846\n",
      "Surface training t=3422, loss=0.37089912593364716\n",
      "Surface training t=3423, loss=0.4388750344514847\n",
      "Surface training t=3424, loss=0.4635936766862869\n",
      "Surface training t=3425, loss=0.4281909465789795\n",
      "Surface training t=3426, loss=0.41473473608493805\n",
      "Surface training t=3427, loss=0.36460499465465546\n",
      "Surface training t=3428, loss=0.41033151745796204\n",
      "Surface training t=3429, loss=0.42432044446468353\n",
      "Surface training t=3430, loss=0.37643320858478546\n",
      "Surface training t=3431, loss=0.42885032296180725\n",
      "Surface training t=3432, loss=0.4407549798488617\n",
      "Surface training t=3433, loss=0.39431989192962646\n",
      "Surface training t=3434, loss=0.3922864943742752\n",
      "Surface training t=3435, loss=0.37985509634017944\n",
      "Surface training t=3436, loss=0.4217343330383301\n",
      "Surface training t=3437, loss=0.35271086543798447\n",
      "Surface training t=3438, loss=0.4206223338842392\n",
      "Surface training t=3439, loss=0.3891605883836746\n",
      "Surface training t=3440, loss=0.46697182953357697\n",
      "Surface training t=3441, loss=0.3821747303009033\n",
      "Surface training t=3442, loss=0.4010210931301117\n",
      "Surface training t=3443, loss=0.40310119092464447\n",
      "Surface training t=3444, loss=0.4492611885070801\n",
      "Surface training t=3445, loss=0.40325993299484253\n",
      "Surface training t=3446, loss=0.4154094010591507\n",
      "Surface training t=3447, loss=0.42895571887493134\n",
      "Surface training t=3448, loss=0.4383080452680588\n",
      "Surface training t=3449, loss=0.4515818953514099\n",
      "Surface training t=3450, loss=0.45191119611263275\n",
      "Surface training t=3451, loss=0.4089052081108093\n",
      "Surface training t=3452, loss=0.4302482157945633\n",
      "Surface training t=3453, loss=0.3445961996912956\n",
      "Surface training t=3454, loss=0.41235680878162384\n",
      "Surface training t=3455, loss=0.42167967557907104\n",
      "Surface training t=3456, loss=0.4785168021917343\n",
      "Surface training t=3457, loss=0.353370726108551\n",
      "Surface training t=3458, loss=0.4310981184244156\n",
      "Surface training t=3459, loss=0.3550240993499756\n",
      "Surface training t=3460, loss=0.42528408765792847\n",
      "Surface training t=3461, loss=0.41396112740039825\n",
      "Surface training t=3462, loss=0.39927075803279877\n",
      "Surface training t=3463, loss=0.4275428056716919\n",
      "Surface training t=3464, loss=0.40710215270519257\n",
      "Surface training t=3465, loss=0.388123020529747\n",
      "Surface training t=3466, loss=0.3824656307697296\n",
      "Surface training t=3467, loss=0.46506139636039734\n",
      "Surface training t=3468, loss=0.4113391935825348\n",
      "Surface training t=3469, loss=0.4052019715309143\n",
      "Surface training t=3470, loss=0.3628711700439453\n",
      "Surface training t=3471, loss=0.4125394821166992\n",
      "Surface training t=3472, loss=0.3728106915950775\n",
      "Surface training t=3473, loss=0.3740396499633789\n",
      "Surface training t=3474, loss=0.42864713072776794\n",
      "Surface training t=3475, loss=0.41626840829849243\n",
      "Surface training t=3476, loss=0.4209483116865158\n",
      "Surface training t=3477, loss=0.3875781297683716\n",
      "Surface training t=3478, loss=0.418461412191391\n",
      "Surface training t=3479, loss=0.4325912743806839\n",
      "Surface training t=3480, loss=0.413703978061676\n",
      "Surface training t=3481, loss=0.4349515736103058\n",
      "Surface training t=3482, loss=0.4020168036222458\n",
      "Surface training t=3483, loss=0.3901880383491516\n",
      "Surface training t=3484, loss=0.402623251080513\n",
      "Surface training t=3485, loss=0.3600553572177887\n",
      "Surface training t=3486, loss=0.38034312427043915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=3487, loss=0.40228308737277985\n",
      "Surface training t=3488, loss=0.41720297932624817\n",
      "Surface training t=3489, loss=0.3733544796705246\n",
      "Surface training t=3490, loss=0.3727937191724777\n",
      "Surface training t=3491, loss=0.4424431622028351\n",
      "Surface training t=3492, loss=0.40957750380039215\n",
      "Surface training t=3493, loss=0.4078883230686188\n",
      "Surface training t=3494, loss=0.40270504355430603\n",
      "Surface training t=3495, loss=0.41943252086639404\n",
      "Surface training t=3496, loss=0.3980149030685425\n",
      "Surface training t=3497, loss=0.47766514122486115\n",
      "Surface training t=3498, loss=0.39800719916820526\n",
      "Surface training t=3499, loss=0.38983073830604553\n",
      "Surface training t=3500, loss=0.4042108952999115\n",
      "Surface training t=3501, loss=0.4414053410291672\n",
      "Surface training t=3502, loss=0.41442902386188507\n",
      "Surface training t=3503, loss=0.3841669261455536\n",
      "Surface training t=3504, loss=0.3712655305862427\n",
      "Surface training t=3505, loss=0.42662693560123444\n",
      "Surface training t=3506, loss=0.40665242075920105\n",
      "Surface training t=3507, loss=0.38735489547252655\n",
      "Surface training t=3508, loss=0.4316225200891495\n",
      "Surface training t=3509, loss=0.4260714501142502\n",
      "Surface training t=3510, loss=0.41587962210178375\n",
      "Surface training t=3511, loss=0.383186936378479\n",
      "Surface training t=3512, loss=0.39787523448467255\n",
      "Surface training t=3513, loss=0.37066029012203217\n",
      "Surface training t=3514, loss=0.3925458937883377\n",
      "Surface training t=3515, loss=0.40268294513225555\n",
      "Surface training t=3516, loss=0.464722216129303\n",
      "Surface training t=3517, loss=0.48489683866500854\n",
      "Surface training t=3518, loss=0.3728925734758377\n",
      "Surface training t=3519, loss=0.3834977298974991\n",
      "Surface training t=3520, loss=0.3760637938976288\n",
      "Surface training t=3521, loss=0.4318679869174957\n",
      "Surface training t=3522, loss=0.43902595341205597\n",
      "Surface training t=3523, loss=0.38303452730178833\n",
      "Surface training t=3524, loss=0.3897535055875778\n",
      "Surface training t=3525, loss=0.4085176885128021\n",
      "Surface training t=3526, loss=0.4260907769203186\n",
      "Surface training t=3527, loss=0.4313267171382904\n",
      "Surface training t=3528, loss=0.4623508006334305\n",
      "Surface training t=3529, loss=0.4090587943792343\n",
      "Surface training t=3530, loss=0.37363460659980774\n",
      "Surface training t=3531, loss=0.39021769165992737\n",
      "Surface training t=3532, loss=0.3894852101802826\n",
      "Surface training t=3533, loss=0.4444199353456497\n",
      "Surface training t=3534, loss=0.34432409703731537\n",
      "Surface training t=3535, loss=0.39843323826789856\n",
      "Surface training t=3536, loss=0.41804929077625275\n",
      "Surface training t=3537, loss=0.41934868693351746\n",
      "Surface training t=3538, loss=0.5058681815862656\n",
      "Surface training t=3539, loss=0.4162169247865677\n",
      "Surface training t=3540, loss=0.351846843957901\n",
      "Surface training t=3541, loss=0.402733251452446\n",
      "Surface training t=3542, loss=0.36502134799957275\n",
      "Surface training t=3543, loss=0.4021197259426117\n",
      "Surface training t=3544, loss=0.45105360448360443\n",
      "Surface training t=3545, loss=0.40385107696056366\n",
      "Surface training t=3546, loss=0.45740818977355957\n",
      "Surface training t=3547, loss=0.3539073318243027\n",
      "Surface training t=3548, loss=0.3780725598335266\n",
      "Surface training t=3549, loss=0.4138365834951401\n",
      "Surface training t=3550, loss=0.41162411868572235\n",
      "Surface training t=3551, loss=0.43180420994758606\n",
      "Surface training t=3552, loss=0.4069443792104721\n",
      "Surface training t=3553, loss=0.43183809518814087\n",
      "Surface training t=3554, loss=0.38648684322834015\n",
      "Surface training t=3555, loss=0.41699954867362976\n",
      "Surface training t=3556, loss=0.4283899962902069\n",
      "Surface training t=3557, loss=0.3937527984380722\n",
      "Surface training t=3558, loss=0.3995814621448517\n",
      "Surface training t=3559, loss=0.39965076744556427\n",
      "Surface training t=3560, loss=0.37720775604248047\n",
      "Surface training t=3561, loss=0.42534518241882324\n",
      "Surface training t=3562, loss=0.43313464522361755\n",
      "Surface training t=3563, loss=0.41597096621990204\n",
      "Surface training t=3564, loss=0.38163919746875763\n",
      "Surface training t=3565, loss=0.44182737171649933\n",
      "Surface training t=3566, loss=0.41487841308116913\n",
      "Surface training t=3567, loss=0.41729454696178436\n",
      "Surface training t=3568, loss=0.3578677624464035\n",
      "Surface training t=3569, loss=0.4331076145172119\n",
      "Surface training t=3570, loss=0.3851621448993683\n",
      "Surface training t=3571, loss=0.39203034341335297\n",
      "Surface training t=3572, loss=0.39710159599781036\n",
      "Surface training t=3573, loss=0.4316404312849045\n",
      "Surface training t=3574, loss=0.386929526925087\n",
      "Surface training t=3575, loss=0.45597583055496216\n",
      "Surface training t=3576, loss=0.3455219268798828\n",
      "Surface training t=3577, loss=0.4165022224187851\n",
      "Surface training t=3578, loss=0.38123832643032074\n",
      "Surface training t=3579, loss=0.40466785430908203\n",
      "Surface training t=3580, loss=0.39123933017253876\n",
      "Surface training t=3581, loss=0.3850105553865433\n",
      "Surface training t=3582, loss=0.41390059888362885\n",
      "Surface training t=3583, loss=0.378118634223938\n",
      "Surface training t=3584, loss=0.4235777407884598\n",
      "Surface training t=3585, loss=0.41543737053871155\n",
      "Surface training t=3586, loss=0.4015570729970932\n",
      "Surface training t=3587, loss=0.35980215668678284\n",
      "Surface training t=3588, loss=0.40254826843738556\n",
      "Surface training t=3589, loss=0.4081897586584091\n",
      "Surface training t=3590, loss=0.40995945036411285\n",
      "Surface training t=3591, loss=0.3896564394235611\n",
      "Surface training t=3592, loss=0.4022190272808075\n",
      "Surface training t=3593, loss=0.43299874663352966\n",
      "Surface training t=3594, loss=0.39310064911842346\n",
      "Surface training t=3595, loss=0.4328601658344269\n",
      "Surface training t=3596, loss=0.3859991580247879\n",
      "Surface training t=3597, loss=0.4370043873786926\n",
      "Surface training t=3598, loss=0.37249214947223663\n",
      "Surface training t=3599, loss=0.3933229148387909\n",
      "Surface training t=3600, loss=0.4358699321746826\n",
      "Surface training t=3601, loss=0.43389786779880524\n",
      "Surface training t=3602, loss=0.4039624482393265\n",
      "Surface training t=3603, loss=0.42377854883670807\n",
      "Surface training t=3604, loss=0.4183361381292343\n",
      "Surface training t=3605, loss=0.33039064705371857\n",
      "Surface training t=3606, loss=0.3820832371711731\n",
      "Surface training t=3607, loss=0.4426205903291702\n",
      "Surface training t=3608, loss=0.39365053176879883\n",
      "Surface training t=3609, loss=0.4195591062307358\n",
      "Surface training t=3610, loss=0.3938687592744827\n",
      "Surface training t=3611, loss=0.4481259882450104\n",
      "Surface training t=3612, loss=0.4428499639034271\n",
      "Surface training t=3613, loss=0.4255550056695938\n",
      "Surface training t=3614, loss=0.41397541761398315\n",
      "Surface training t=3615, loss=0.35993795096874237\n",
      "Surface training t=3616, loss=0.44719281792640686\n",
      "Surface training t=3617, loss=0.3975573778152466\n",
      "Surface training t=3618, loss=0.46797293424606323\n",
      "Surface training t=3619, loss=0.40079978108406067\n",
      "Surface training t=3620, loss=0.3972943127155304\n",
      "Surface training t=3621, loss=0.4131454825401306\n",
      "Surface training t=3622, loss=0.43877893686294556\n",
      "Surface training t=3623, loss=0.4470537453889847\n",
      "Surface training t=3624, loss=0.37414321303367615\n",
      "Surface training t=3625, loss=0.38507869839668274\n",
      "Surface training t=3626, loss=0.4094766080379486\n",
      "Surface training t=3627, loss=0.3898916095495224\n",
      "Surface training t=3628, loss=0.3881508857011795\n",
      "Surface training t=3629, loss=0.35818716883659363\n",
      "Surface training t=3630, loss=0.4629838913679123\n",
      "Surface training t=3631, loss=0.4538951516151428\n",
      "Surface training t=3632, loss=0.42987994849681854\n",
      "Surface training t=3633, loss=0.4019870311021805\n",
      "Surface training t=3634, loss=0.39289338886737823\n",
      "Surface training t=3635, loss=0.3569370359182358\n",
      "Surface training t=3636, loss=0.3353290781378746\n",
      "Surface training t=3637, loss=0.38829636573791504\n",
      "Surface training t=3638, loss=0.39164917171001434\n",
      "Surface training t=3639, loss=0.4026705473661423\n",
      "Surface training t=3640, loss=0.4291002005338669\n",
      "Surface training t=3641, loss=0.37447309494018555\n",
      "Surface training t=3642, loss=0.3568629324436188\n",
      "Surface training t=3643, loss=0.4362647980451584\n",
      "Surface training t=3644, loss=0.3940742462873459\n",
      "Surface training t=3645, loss=0.4053639620542526\n",
      "Surface training t=3646, loss=0.4222984313964844\n",
      "Surface training t=3647, loss=0.3903055638074875\n",
      "Surface training t=3648, loss=0.42766907811164856\n",
      "Surface training t=3649, loss=0.4207676351070404\n",
      "Surface training t=3650, loss=0.3624044209718704\n",
      "Surface training t=3651, loss=0.4719863831996918\n",
      "Surface training t=3652, loss=0.423002153635025\n",
      "Surface training t=3653, loss=0.3685721158981323\n",
      "Surface training t=3654, loss=0.40746472775936127\n",
      "Surface training t=3655, loss=0.4021735042333603\n",
      "Surface training t=3656, loss=0.4391801655292511\n",
      "Surface training t=3657, loss=0.45804761350154877\n",
      "Surface training t=3658, loss=0.4425836205482483\n",
      "Surface training t=3659, loss=0.4269881695508957\n",
      "Surface training t=3660, loss=0.4299767017364502\n",
      "Surface training t=3661, loss=0.3913196474313736\n",
      "Surface training t=3662, loss=0.3637401759624481\n",
      "Surface training t=3663, loss=0.39576759934425354\n",
      "Surface training t=3664, loss=0.3838638961315155\n",
      "Surface training t=3665, loss=0.4062506705522537\n",
      "Surface training t=3666, loss=0.3986184298992157\n",
      "Surface training t=3667, loss=0.3852640837430954\n",
      "Surface training t=3668, loss=0.35523344576358795\n",
      "Surface training t=3669, loss=0.41889359056949615\n",
      "Surface training t=3670, loss=0.4066902846097946\n",
      "Surface training t=3671, loss=0.38025297224521637\n",
      "Surface training t=3672, loss=0.4344877451658249\n",
      "Surface training t=3673, loss=0.4675385355949402\n",
      "Surface training t=3674, loss=0.42479555308818817\n",
      "Surface training t=3675, loss=0.41397911310195923\n",
      "Surface training t=3676, loss=0.37401147186756134\n",
      "Surface training t=3677, loss=0.400039941072464\n",
      "Surface training t=3678, loss=0.3806566298007965\n",
      "Surface training t=3679, loss=0.37736837565898895\n",
      "Surface training t=3680, loss=0.38147488236427307\n",
      "Surface training t=3681, loss=0.3978186398744583\n",
      "Surface training t=3682, loss=0.3969861567020416\n",
      "Surface training t=3683, loss=0.36411498486995697\n",
      "Surface training t=3684, loss=0.4341927170753479\n",
      "Surface training t=3685, loss=0.43571290373802185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=3686, loss=0.4614860564470291\n",
      "Surface training t=3687, loss=0.4128866344690323\n",
      "Surface training t=3688, loss=0.4465310871601105\n",
      "Surface training t=3689, loss=0.35154400765895844\n",
      "Surface training t=3690, loss=0.418986976146698\n",
      "Surface training t=3691, loss=0.40213197469711304\n",
      "Surface training t=3692, loss=0.39991454780101776\n",
      "Surface training t=3693, loss=0.4119623899459839\n",
      "Surface training t=3694, loss=0.3798840641975403\n",
      "Surface training t=3695, loss=0.35434170067310333\n",
      "Surface training t=3696, loss=0.40182188153266907\n",
      "Surface training t=3697, loss=0.3714784234762192\n",
      "Surface training t=3698, loss=0.34777551889419556\n",
      "Surface training t=3699, loss=0.36471064388751984\n",
      "Surface training t=3700, loss=0.38661518692970276\n",
      "Surface training t=3701, loss=0.36750657856464386\n",
      "Surface training t=3702, loss=0.43145619332790375\n",
      "Surface training t=3703, loss=0.41595734655857086\n",
      "Surface training t=3704, loss=0.41410768032073975\n",
      "Surface training t=3705, loss=0.3772621303796768\n",
      "Surface training t=3706, loss=0.42050181329250336\n",
      "Surface training t=3707, loss=0.3864767849445343\n",
      "Surface training t=3708, loss=0.4159497320652008\n",
      "Surface training t=3709, loss=0.3811473697423935\n",
      "Surface training t=3710, loss=0.3705185502767563\n",
      "Surface training t=3711, loss=0.3977174907922745\n",
      "Surface training t=3712, loss=0.3873118609189987\n",
      "Surface training t=3713, loss=0.41153278946876526\n",
      "Surface training t=3714, loss=0.41571810841560364\n",
      "Surface training t=3715, loss=0.4008481204509735\n",
      "Surface training t=3716, loss=0.4436929374933243\n",
      "Surface training t=3717, loss=0.4201730191707611\n",
      "Surface training t=3718, loss=0.4119427353143692\n",
      "Surface training t=3719, loss=0.40684159100055695\n",
      "Surface training t=3720, loss=0.37326158583164215\n",
      "Surface training t=3721, loss=0.4404856711626053\n",
      "Surface training t=3722, loss=0.42768992483615875\n",
      "Surface training t=3723, loss=0.384817898273468\n",
      "Surface training t=3724, loss=0.41057121753692627\n",
      "Surface training t=3725, loss=0.41818128526210785\n",
      "Surface training t=3726, loss=0.43001367151737213\n",
      "Surface training t=3727, loss=0.4015935957431793\n",
      "Surface training t=3728, loss=0.3460698425769806\n",
      "Surface training t=3729, loss=0.3717762231826782\n",
      "Surface training t=3730, loss=0.35580874979496\n",
      "Surface training t=3731, loss=0.40974724292755127\n",
      "Surface training t=3732, loss=0.36200766265392303\n",
      "Surface training t=3733, loss=0.4097868800163269\n",
      "Surface training t=3734, loss=0.4173985868692398\n",
      "Surface training t=3735, loss=0.36462023854255676\n",
      "Surface training t=3736, loss=0.4431215524673462\n",
      "Surface training t=3737, loss=0.42679961025714874\n",
      "Surface training t=3738, loss=0.36411114037036896\n",
      "Surface training t=3739, loss=0.41382794082164764\n",
      "Surface training t=3740, loss=0.40376000106334686\n",
      "Surface training t=3741, loss=0.41952575743198395\n",
      "Surface training t=3742, loss=0.4667031019926071\n",
      "Surface training t=3743, loss=0.4293315261602402\n",
      "Surface training t=3744, loss=0.43884240090847015\n",
      "Surface training t=3745, loss=0.3699595332145691\n",
      "Surface training t=3746, loss=0.39469246566295624\n",
      "Surface training t=3747, loss=0.36471299827098846\n",
      "Surface training t=3748, loss=0.4234427362680435\n",
      "Surface training t=3749, loss=0.436571940779686\n",
      "Surface training t=3750, loss=0.3907364308834076\n",
      "Surface training t=3751, loss=0.4536646604537964\n",
      "Surface training t=3752, loss=0.40696652233600616\n",
      "Surface training t=3753, loss=0.36122438311576843\n",
      "Surface training t=3754, loss=0.40497928857803345\n",
      "Surface training t=3755, loss=0.36988280713558197\n",
      "Surface training t=3756, loss=0.344209685921669\n",
      "Surface training t=3757, loss=0.38695961236953735\n",
      "Surface training t=3758, loss=0.40397968888282776\n",
      "Surface training t=3759, loss=0.3802668899297714\n",
      "Surface training t=3760, loss=0.49184660613536835\n",
      "Surface training t=3761, loss=0.3737316280603409\n",
      "Surface training t=3762, loss=0.42562222480773926\n",
      "Surface training t=3763, loss=0.39880433678627014\n",
      "Surface training t=3764, loss=0.41031210124492645\n",
      "Surface training t=3765, loss=0.3954891562461853\n",
      "Surface training t=3766, loss=0.42809709906578064\n",
      "Surface training t=3767, loss=0.4144077003002167\n",
      "Surface training t=3768, loss=0.4025924801826477\n",
      "Surface training t=3769, loss=0.41385112702846527\n",
      "Surface training t=3770, loss=0.42184846103191376\n",
      "Surface training t=3771, loss=0.42711469531059265\n",
      "Surface training t=3772, loss=0.46526627242565155\n",
      "Surface training t=3773, loss=0.41075606644153595\n",
      "Surface training t=3774, loss=0.3901883065700531\n",
      "Surface training t=3775, loss=0.39348727464675903\n",
      "Surface training t=3776, loss=0.3715956509113312\n",
      "Surface training t=3777, loss=0.4631786495447159\n",
      "Surface training t=3778, loss=0.4253021329641342\n",
      "Surface training t=3779, loss=0.3732531666755676\n",
      "Surface training t=3780, loss=0.4069055914878845\n",
      "Surface training t=3781, loss=0.44440214335918427\n",
      "Surface training t=3782, loss=0.4069933146238327\n",
      "Surface training t=3783, loss=0.40050238370895386\n",
      "Surface training t=3784, loss=0.375227689743042\n",
      "Surface training t=3785, loss=0.4167502075433731\n",
      "Surface training t=3786, loss=0.41265465319156647\n",
      "Surface training t=3787, loss=0.37637875974178314\n",
      "Surface training t=3788, loss=0.40780700743198395\n",
      "Surface training t=3789, loss=0.3777640014886856\n",
      "Surface training t=3790, loss=0.37646709382534027\n",
      "Surface training t=3791, loss=0.40408240258693695\n",
      "Surface training t=3792, loss=0.40129703283309937\n",
      "Surface training t=3793, loss=0.41591258347034454\n",
      "Surface training t=3794, loss=0.4249241501092911\n",
      "Surface training t=3795, loss=0.38835394382476807\n",
      "Surface training t=3796, loss=0.39470410346984863\n",
      "Surface training t=3797, loss=0.41432857513427734\n",
      "Surface training t=3798, loss=0.42496544122695923\n",
      "Surface training t=3799, loss=0.404966801404953\n",
      "Surface training t=3800, loss=0.4014527350664139\n",
      "Surface training t=3801, loss=0.35146912932395935\n",
      "Surface training t=3802, loss=0.43449005484580994\n",
      "Surface training t=3803, loss=0.412505567073822\n",
      "Surface training t=3804, loss=0.42299938201904297\n",
      "Surface training t=3805, loss=0.43570955097675323\n",
      "Surface training t=3806, loss=0.38713520765304565\n",
      "Surface training t=3807, loss=0.44605299830436707\n",
      "Surface training t=3808, loss=0.3923485279083252\n",
      "Surface training t=3809, loss=0.40696685016155243\n",
      "Surface training t=3810, loss=0.3788508474826813\n",
      "Surface training t=3811, loss=0.4475942403078079\n",
      "Surface training t=3812, loss=0.3966699689626694\n",
      "Surface training t=3813, loss=0.41375432908535004\n",
      "Surface training t=3814, loss=0.40462371706962585\n",
      "Surface training t=3815, loss=0.41098831593990326\n",
      "Surface training t=3816, loss=0.4353414475917816\n",
      "Surface training t=3817, loss=0.38532155752182007\n",
      "Surface training t=3818, loss=0.37299612164497375\n",
      "Surface training t=3819, loss=0.37619951367378235\n",
      "Surface training t=3820, loss=0.4158749580383301\n",
      "Surface training t=3821, loss=0.4007754623889923\n",
      "Surface training t=3822, loss=0.39284248650074005\n",
      "Surface training t=3823, loss=0.34728942811489105\n",
      "Surface training t=3824, loss=0.4333714246749878\n",
      "Surface training t=3825, loss=0.37832868099212646\n",
      "Surface training t=3826, loss=0.38941168785095215\n",
      "Surface training t=3827, loss=0.413541316986084\n",
      "Surface training t=3828, loss=0.3649388700723648\n",
      "Surface training t=3829, loss=0.3713937997817993\n",
      "Surface training t=3830, loss=0.3162544444203377\n",
      "Surface training t=3831, loss=0.3923357129096985\n",
      "Surface training t=3832, loss=0.3590468019247055\n",
      "Surface training t=3833, loss=0.3981168568134308\n",
      "Surface training t=3834, loss=0.388483390212059\n",
      "Surface training t=3835, loss=0.395929753780365\n",
      "Surface training t=3836, loss=0.3535633534193039\n",
      "Surface training t=3837, loss=0.4047149270772934\n",
      "Surface training t=3838, loss=0.362693190574646\n",
      "Surface training t=3839, loss=0.36918848752975464\n",
      "Surface training t=3840, loss=0.3925859034061432\n",
      "Surface training t=3841, loss=0.3977292776107788\n",
      "Surface training t=3842, loss=0.41250234842300415\n",
      "Surface training t=3843, loss=0.3902607858181\n",
      "Surface training t=3844, loss=0.4081851691007614\n",
      "Surface training t=3845, loss=0.40367482602596283\n",
      "Surface training t=3846, loss=0.3772220015525818\n",
      "Surface training t=3847, loss=0.41526296734809875\n",
      "Surface training t=3848, loss=0.42244723439216614\n",
      "Surface training t=3849, loss=0.4207916110754013\n",
      "Surface training t=3850, loss=0.4143332690000534\n",
      "Surface training t=3851, loss=0.37096546590328217\n",
      "Surface training t=3852, loss=0.3638824373483658\n",
      "Surface training t=3853, loss=0.39341261982917786\n",
      "Surface training t=3854, loss=0.4447659105062485\n",
      "Surface training t=3855, loss=0.42821477353572845\n",
      "Surface training t=3856, loss=0.40236884355545044\n",
      "Surface training t=3857, loss=0.3870856463909149\n",
      "Surface training t=3858, loss=0.38243550062179565\n",
      "Surface training t=3859, loss=0.3531835079193115\n",
      "Surface training t=3860, loss=0.3712564557790756\n",
      "Surface training t=3861, loss=0.32792410999536514\n",
      "Surface training t=3862, loss=0.4590943604707718\n",
      "Surface training t=3863, loss=0.36978791654109955\n",
      "Surface training t=3864, loss=0.39221037924289703\n",
      "Surface training t=3865, loss=0.3654337525367737\n",
      "Surface training t=3866, loss=0.34084323048591614\n",
      "Surface training t=3867, loss=0.33294765651226044\n",
      "Surface training t=3868, loss=0.36370837688446045\n",
      "Surface training t=3869, loss=0.3843619078397751\n",
      "Surface training t=3870, loss=0.4135150909423828\n",
      "Surface training t=3871, loss=0.373654842376709\n",
      "Surface training t=3872, loss=0.4481412172317505\n",
      "Surface training t=3873, loss=0.37121425569057465\n",
      "Surface training t=3874, loss=0.36227135360240936\n",
      "Surface training t=3875, loss=0.38177230954170227\n",
      "Surface training t=3876, loss=0.416293740272522\n",
      "Surface training t=3877, loss=0.38004907965660095\n",
      "Surface training t=3878, loss=0.38429637253284454\n",
      "Surface training t=3879, loss=0.3485984653234482\n",
      "Surface training t=3880, loss=0.34944023191928864\n",
      "Surface training t=3881, loss=0.3675970584154129\n",
      "Surface training t=3882, loss=0.42056208848953247\n",
      "Surface training t=3883, loss=0.3694578856229782\n",
      "Surface training t=3884, loss=0.40312008559703827\n",
      "Surface training t=3885, loss=0.4315395951271057\n",
      "Surface training t=3886, loss=0.4139598459005356\n",
      "Surface training t=3887, loss=0.39058759808540344\n",
      "Surface training t=3888, loss=0.40753862261772156\n",
      "Surface training t=3889, loss=0.41365547478199005\n",
      "Surface training t=3890, loss=0.35065409541130066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=3891, loss=0.382595494389534\n",
      "Surface training t=3892, loss=0.3566640615463257\n",
      "Surface training t=3893, loss=0.3998414874076843\n",
      "Surface training t=3894, loss=0.42439933121204376\n",
      "Surface training t=3895, loss=0.4387615919113159\n",
      "Surface training t=3896, loss=0.3908649682998657\n",
      "Surface training t=3897, loss=0.3950778990983963\n",
      "Surface training t=3898, loss=0.4337518513202667\n",
      "Surface training t=3899, loss=0.365881010890007\n",
      "Surface training t=3900, loss=0.3963879197835922\n",
      "Surface training t=3901, loss=0.3779745548963547\n",
      "Surface training t=3902, loss=0.4579348564147949\n",
      "Surface training t=3903, loss=0.37012559175491333\n",
      "Surface training t=3904, loss=0.3796849548816681\n",
      "Surface training t=3905, loss=0.39015568792819977\n",
      "Surface training t=3906, loss=0.43914443254470825\n",
      "Surface training t=3907, loss=0.4375288188457489\n",
      "Surface training t=3908, loss=0.4303712546825409\n",
      "Surface training t=3909, loss=0.3774087727069855\n",
      "Surface training t=3910, loss=0.37381233274936676\n",
      "Surface training t=3911, loss=0.36021655797958374\n",
      "Surface training t=3912, loss=0.36969299614429474\n",
      "Surface training t=3913, loss=0.383622869849205\n",
      "Surface training t=3914, loss=0.38945919275283813\n",
      "Surface training t=3915, loss=0.34193308651447296\n",
      "Surface training t=3916, loss=0.41289372742176056\n",
      "Surface training t=3917, loss=0.43605104088783264\n",
      "Surface training t=3918, loss=0.4117777645587921\n",
      "Surface training t=3919, loss=0.39850129187107086\n",
      "Surface training t=3920, loss=0.39629845321178436\n",
      "Surface training t=3921, loss=0.39605532586574554\n",
      "Surface training t=3922, loss=0.43914756178855896\n",
      "Surface training t=3923, loss=0.3824612647294998\n",
      "Surface training t=3924, loss=0.42861656844615936\n",
      "Surface training t=3925, loss=0.4182557910680771\n",
      "Surface training t=3926, loss=0.43684227764606476\n",
      "Surface training t=3927, loss=0.3505563884973526\n",
      "Surface training t=3928, loss=0.4151829332113266\n",
      "Surface training t=3929, loss=0.4309549033641815\n",
      "Surface training t=3930, loss=0.39471442997455597\n",
      "Surface training t=3931, loss=0.4535412937402725\n",
      "Surface training t=3932, loss=0.40390630066394806\n",
      "Surface training t=3933, loss=0.38810895383358\n",
      "Surface training t=3934, loss=0.35541287064552307\n",
      "Surface training t=3935, loss=0.3631506413221359\n",
      "Surface training t=3936, loss=0.38959619402885437\n",
      "Surface training t=3937, loss=0.3456219583749771\n",
      "Surface training t=3938, loss=0.4245997965335846\n",
      "Surface training t=3939, loss=0.4389762729406357\n",
      "Surface training t=3940, loss=0.4158787280321121\n",
      "Surface training t=3941, loss=0.41510817408561707\n",
      "Surface training t=3942, loss=0.3934088498353958\n",
      "Surface training t=3943, loss=0.41329166293144226\n",
      "Surface training t=3944, loss=0.37385083734989166\n",
      "Surface training t=3945, loss=0.38788585364818573\n",
      "Surface training t=3946, loss=0.386840358376503\n",
      "Surface training t=3947, loss=0.37978649139404297\n",
      "Surface training t=3948, loss=0.4330186992883682\n",
      "Surface training t=3949, loss=0.432710736989975\n",
      "Surface training t=3950, loss=0.40179505944252014\n",
      "Surface training t=3951, loss=0.4228330999612808\n",
      "Surface training t=3952, loss=0.4243570864200592\n",
      "Surface training t=3953, loss=0.3699745237827301\n",
      "Surface training t=3954, loss=0.3642343580722809\n",
      "Surface training t=3955, loss=0.3716723322868347\n",
      "Surface training t=3956, loss=0.4569324553012848\n",
      "Surface training t=3957, loss=0.3993402123451233\n",
      "Surface training t=3958, loss=0.39399857819080353\n",
      "Surface training t=3959, loss=0.376907542347908\n",
      "Surface training t=3960, loss=0.39444904029369354\n",
      "Surface training t=3961, loss=0.4187479615211487\n",
      "Surface training t=3962, loss=0.3706560283899307\n",
      "Surface training t=3963, loss=0.3876808434724808\n",
      "Surface training t=3964, loss=0.39292606711387634\n",
      "Surface training t=3965, loss=0.4061991423368454\n",
      "Surface training t=3966, loss=0.44984787702560425\n",
      "Surface training t=3967, loss=0.3446061611175537\n",
      "Surface training t=3968, loss=0.34656041860580444\n",
      "Surface training t=3969, loss=0.37805232405662537\n",
      "Surface training t=3970, loss=0.4068017154932022\n",
      "Surface training t=3971, loss=0.38410866260528564\n",
      "Surface training t=3972, loss=0.3877701014280319\n",
      "Surface training t=3973, loss=0.34584277868270874\n",
      "Surface training t=3974, loss=0.378815233707428\n",
      "Surface training t=3975, loss=0.4185832291841507\n",
      "Surface training t=3976, loss=0.47080838680267334\n",
      "Surface training t=3977, loss=0.42339618504047394\n",
      "Surface training t=3978, loss=0.3918813169002533\n",
      "Surface training t=3979, loss=0.4075955003499985\n",
      "Surface training t=3980, loss=0.3716157227754593\n",
      "Surface training t=3981, loss=0.3353898674249649\n",
      "Surface training t=3982, loss=0.4159669578075409\n",
      "Surface training t=3983, loss=0.3521314561367035\n",
      "Surface training t=3984, loss=0.3344568908214569\n",
      "Surface training t=3985, loss=0.3833710849285126\n",
      "Surface training t=3986, loss=0.3508526682853699\n",
      "Surface training t=3987, loss=0.37997449934482574\n",
      "Surface training t=3988, loss=0.39501073956489563\n",
      "Surface training t=3989, loss=0.3567165583372116\n",
      "Surface training t=3990, loss=0.37947559356689453\n",
      "Surface training t=3991, loss=0.43692079186439514\n",
      "Surface training t=3992, loss=0.34080010652542114\n",
      "Surface training t=3993, loss=0.3960205018520355\n",
      "Surface training t=3994, loss=0.39740996062755585\n",
      "Surface training t=3995, loss=0.3756101429462433\n",
      "Surface training t=3996, loss=0.3641689121723175\n",
      "Surface training t=3997, loss=0.3583558052778244\n",
      "Surface training t=3998, loss=0.36836858093738556\n",
      "Surface training t=3999, loss=0.3435385674238205\n",
      "Surface training t=4000, loss=0.409637987613678\n",
      "Surface training t=4001, loss=0.38892732560634613\n",
      "Surface training t=4002, loss=0.4270562529563904\n",
      "Surface training t=4003, loss=0.4058266878128052\n",
      "Surface training t=4004, loss=0.37067826092243195\n",
      "Surface training t=4005, loss=0.3254050090909004\n",
      "Surface training t=4006, loss=0.35168980062007904\n",
      "Surface training t=4007, loss=0.3296745866537094\n",
      "Surface training t=4008, loss=0.37882469594478607\n",
      "Surface training t=4009, loss=0.3507038652896881\n",
      "Surface training t=4010, loss=0.3357045203447342\n",
      "Surface training t=4011, loss=0.38330164551734924\n",
      "Surface training t=4012, loss=0.42293302714824677\n",
      "Surface training t=4013, loss=0.36648617684841156\n",
      "Surface training t=4014, loss=0.3634517043828964\n",
      "Surface training t=4015, loss=0.351458802819252\n",
      "Surface training t=4016, loss=0.4743202179670334\n",
      "Surface training t=4017, loss=0.40084798634052277\n",
      "Surface training t=4018, loss=0.44031384587287903\n",
      "Surface training t=4019, loss=0.3767589330673218\n",
      "Surface training t=4020, loss=0.3844790607690811\n",
      "Surface training t=4021, loss=0.3724527060985565\n",
      "Surface training t=4022, loss=0.4045083820819855\n",
      "Surface training t=4023, loss=0.37398387491703033\n",
      "Surface training t=4024, loss=0.3809877783060074\n",
      "Surface training t=4025, loss=0.3374379277229309\n",
      "Surface training t=4026, loss=0.43137986958026886\n",
      "Surface training t=4027, loss=0.3759382367134094\n",
      "Surface training t=4028, loss=0.35098719596862793\n",
      "Surface training t=4029, loss=0.3728739470243454\n",
      "Surface training t=4030, loss=0.34928378462791443\n",
      "Surface training t=4031, loss=0.39423152804374695\n",
      "Surface training t=4032, loss=0.38743099570274353\n",
      "Surface training t=4033, loss=0.416525736451149\n",
      "Surface training t=4034, loss=0.38467973470687866\n",
      "Surface training t=4035, loss=0.38857659697532654\n",
      "Surface training t=4036, loss=0.4353574067354202\n",
      "Surface training t=4037, loss=0.3887976408004761\n",
      "Surface training t=4038, loss=0.39335185289382935\n",
      "Surface training t=4039, loss=0.38406993448734283\n",
      "Surface training t=4040, loss=0.373444139957428\n",
      "Surface training t=4041, loss=0.38323958218097687\n",
      "Surface training t=4042, loss=0.3265801966190338\n",
      "Surface training t=4043, loss=0.39211365580558777\n",
      "Surface training t=4044, loss=0.38867422938346863\n",
      "Surface training t=4045, loss=0.3873738497495651\n",
      "Surface training t=4046, loss=0.43757757544517517\n",
      "Surface training t=4047, loss=0.3677407056093216\n",
      "Surface training t=4048, loss=0.4077727198600769\n",
      "Surface training t=4049, loss=0.3965855538845062\n",
      "Surface training t=4050, loss=0.3979056179523468\n",
      "Surface training t=4051, loss=0.35740913450717926\n",
      "Surface training t=4052, loss=0.3610760420560837\n",
      "Surface training t=4053, loss=0.3967045396566391\n",
      "Surface training t=4054, loss=0.36939017474651337\n",
      "Surface training t=4055, loss=0.4434190094470978\n",
      "Surface training t=4056, loss=0.43022456765174866\n",
      "Surface training t=4057, loss=0.39645953476428986\n",
      "Surface training t=4058, loss=0.39086538553237915\n",
      "Surface training t=4059, loss=0.39733119308948517\n",
      "Surface training t=4060, loss=0.36998818814754486\n",
      "Surface training t=4061, loss=0.3852626532316208\n",
      "Surface training t=4062, loss=0.38113299012184143\n",
      "Surface training t=4063, loss=0.37375055253505707\n",
      "Surface training t=4064, loss=0.40432092547416687\n",
      "Surface training t=4065, loss=0.36766569316387177\n",
      "Surface training t=4066, loss=0.3465363681316376\n",
      "Surface training t=4067, loss=0.4257071912288666\n",
      "Surface training t=4068, loss=0.3449620008468628\n",
      "Surface training t=4069, loss=0.34773704409599304\n",
      "Surface training t=4070, loss=0.36228621006011963\n",
      "Surface training t=4071, loss=0.3666769117116928\n",
      "Surface training t=4072, loss=0.4189414530992508\n",
      "Surface training t=4073, loss=0.4410899579524994\n",
      "Surface training t=4074, loss=0.38597579300403595\n",
      "Surface training t=4075, loss=0.39998768270015717\n",
      "Surface training t=4076, loss=0.3465607762336731\n",
      "Surface training t=4077, loss=0.3859240859746933\n",
      "Surface training t=4078, loss=0.3571488857269287\n",
      "Surface training t=4079, loss=0.3607383519411087\n",
      "Surface training t=4080, loss=0.38721780478954315\n",
      "Surface training t=4081, loss=0.37213464081287384\n",
      "Surface training t=4082, loss=0.4252523332834244\n",
      "Surface training t=4083, loss=0.36385639011859894\n",
      "Surface training t=4084, loss=0.3973305821418762\n",
      "Surface training t=4085, loss=0.3575633019208908\n",
      "Surface training t=4086, loss=0.36962638795375824\n",
      "Surface training t=4087, loss=0.3753281831741333\n",
      "Surface training t=4088, loss=0.4019504189491272\n",
      "Surface training t=4089, loss=0.4139963537454605\n",
      "Surface training t=4090, loss=0.3780279606580734\n",
      "Surface training t=4091, loss=0.3772754669189453\n",
      "Surface training t=4092, loss=0.3817938417196274\n",
      "Surface training t=4093, loss=0.4328308701515198\n",
      "Surface training t=4094, loss=0.45881161093711853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=4095, loss=0.41994403302669525\n",
      "Surface training t=4096, loss=0.39601004123687744\n",
      "Surface training t=4097, loss=0.3545200079679489\n",
      "Surface training t=4098, loss=0.4010414034128189\n",
      "Surface training t=4099, loss=0.4276617467403412\n",
      "Surface training t=4100, loss=0.36783313751220703\n",
      "Surface training t=4101, loss=0.4488302767276764\n",
      "Surface training t=4102, loss=0.35793887078762054\n",
      "Surface training t=4103, loss=0.43075820803642273\n",
      "Surface training t=4104, loss=0.43811123073101044\n",
      "Surface training t=4105, loss=0.4268387407064438\n",
      "Surface training t=4106, loss=0.349450945854187\n",
      "Surface training t=4107, loss=0.43594884872436523\n",
      "Surface training t=4108, loss=0.4391603618860245\n",
      "Surface training t=4109, loss=0.4291580021381378\n",
      "Surface training t=4110, loss=0.4475468695163727\n",
      "Surface training t=4111, loss=0.47509847581386566\n",
      "Surface training t=4112, loss=0.41697831451892853\n",
      "Surface training t=4113, loss=0.3822086751461029\n",
      "Surface training t=4114, loss=0.39252781867980957\n",
      "Surface training t=4115, loss=0.40391072630882263\n",
      "Surface training t=4116, loss=0.37091633677482605\n",
      "Surface training t=4117, loss=0.39232976734638214\n",
      "Surface training t=4118, loss=0.4048681855201721\n",
      "Surface training t=4119, loss=0.3529878705739975\n",
      "Surface training t=4120, loss=0.3426922410726547\n",
      "Surface training t=4121, loss=0.4253324270248413\n",
      "Surface training t=4122, loss=0.4020538330078125\n",
      "Surface training t=4123, loss=0.3909311890602112\n",
      "Surface training t=4124, loss=0.41557714343070984\n",
      "Surface training t=4125, loss=0.34813132882118225\n",
      "Surface training t=4126, loss=0.38773106038570404\n",
      "Surface training t=4127, loss=0.3721057176589966\n",
      "Surface training t=4128, loss=0.42544397711753845\n",
      "Surface training t=4129, loss=0.42727823555469513\n",
      "Surface training t=4130, loss=0.3973812460899353\n",
      "Surface training t=4131, loss=0.40755386650562286\n",
      "Surface training t=4132, loss=0.38877080380916595\n",
      "Surface training t=4133, loss=0.3599332123994827\n",
      "Surface training t=4134, loss=0.32269053161144257\n",
      "Surface training t=4135, loss=0.37859803438186646\n",
      "Surface training t=4136, loss=0.4064124524593353\n",
      "Surface training t=4137, loss=0.39740459620952606\n",
      "Surface training t=4138, loss=0.3626261204481125\n",
      "Surface training t=4139, loss=0.39855483174324036\n",
      "Surface training t=4140, loss=0.40231335163116455\n",
      "Surface training t=4141, loss=0.42325013875961304\n",
      "Surface training t=4142, loss=0.38463257253170013\n",
      "Surface training t=4143, loss=0.366448774933815\n",
      "Surface training t=4144, loss=0.3800498694181442\n",
      "Surface training t=4145, loss=0.3542203903198242\n",
      "Surface training t=4146, loss=0.3474828749895096\n",
      "Surface training t=4147, loss=0.36078429222106934\n",
      "Surface training t=4148, loss=0.37698711454868317\n",
      "Surface training t=4149, loss=0.3963042050600052\n",
      "Surface training t=4150, loss=0.36357901990413666\n",
      "Surface training t=4151, loss=0.3718159645795822\n",
      "Surface training t=4152, loss=0.4092058539390564\n",
      "Surface training t=4153, loss=0.35153262317180634\n",
      "Surface training t=4154, loss=0.42049872875213623\n",
      "Surface training t=4155, loss=0.3856717497110367\n",
      "Surface training t=4156, loss=0.3175186365842819\n",
      "Surface training t=4157, loss=0.36749987304210663\n",
      "Surface training t=4158, loss=0.3502698093652725\n",
      "Surface training t=4159, loss=0.34319645166397095\n",
      "Surface training t=4160, loss=0.3721591681241989\n",
      "Surface training t=4161, loss=0.3698679059743881\n",
      "Surface training t=4162, loss=0.3867473900318146\n",
      "Surface training t=4163, loss=0.34956149756908417\n",
      "Surface training t=4164, loss=0.3543694466352463\n",
      "Surface training t=4165, loss=0.35541488230228424\n",
      "Surface training t=4166, loss=0.41137368977069855\n",
      "Surface training t=4167, loss=0.3678307384252548\n",
      "Surface training t=4168, loss=0.37762030959129333\n",
      "Surface training t=4169, loss=0.3714462220668793\n",
      "Surface training t=4170, loss=0.3581669479608536\n",
      "Surface training t=4171, loss=0.3692961037158966\n",
      "Surface training t=4172, loss=0.4193257987499237\n",
      "Surface training t=4173, loss=0.37316685914993286\n",
      "Surface training t=4174, loss=0.3735458254814148\n",
      "Surface training t=4175, loss=0.36882010102272034\n",
      "Surface training t=4176, loss=0.3998199850320816\n",
      "Surface training t=4177, loss=0.3797539323568344\n",
      "Surface training t=4178, loss=0.39830824732780457\n",
      "Surface training t=4179, loss=0.3956163376569748\n",
      "Surface training t=4180, loss=0.37247322499752045\n",
      "Surface training t=4181, loss=0.38055330514907837\n",
      "Surface training t=4182, loss=0.40685053169727325\n",
      "Surface training t=4183, loss=0.35422077775001526\n",
      "Surface training t=4184, loss=0.36972078680992126\n",
      "Surface training t=4185, loss=0.45684246718883514\n",
      "Surface training t=4186, loss=0.3901918828487396\n",
      "Surface training t=4187, loss=0.36839255690574646\n",
      "Surface training t=4188, loss=0.3414861857891083\n",
      "Surface training t=4189, loss=0.3951801210641861\n",
      "Surface training t=4190, loss=0.3494921028614044\n",
      "Surface training t=4191, loss=0.3527307063341141\n",
      "Surface training t=4192, loss=0.40332140028476715\n",
      "Surface training t=4193, loss=0.4383063316345215\n",
      "Surface training t=4194, loss=0.32969439029693604\n",
      "Surface training t=4195, loss=0.4088033586740494\n",
      "Surface training t=4196, loss=0.36945943534374237\n",
      "Surface training t=4197, loss=0.38277022540569305\n",
      "Surface training t=4198, loss=0.37135469913482666\n",
      "Surface training t=4199, loss=0.3509122133255005\n",
      "Surface training t=4200, loss=0.3685434013605118\n",
      "Surface training t=4201, loss=0.4316415935754776\n",
      "Surface training t=4202, loss=0.3265676125884056\n",
      "Surface training t=4203, loss=0.4227770119905472\n",
      "Surface training t=4204, loss=0.3956560641527176\n",
      "Surface training t=4205, loss=0.3766481727361679\n",
      "Surface training t=4206, loss=0.4196038842201233\n",
      "Surface training t=4207, loss=0.38544370234012604\n",
      "Surface training t=4208, loss=0.39068926870822906\n",
      "Surface training t=4209, loss=0.40415629744529724\n",
      "Surface training t=4210, loss=0.4677385985851288\n",
      "Surface training t=4211, loss=0.3539865016937256\n",
      "Surface training t=4212, loss=0.3675006479024887\n",
      "Surface training t=4213, loss=0.4006839990615845\n",
      "Surface training t=4214, loss=0.37274913489818573\n",
      "Surface training t=4215, loss=0.40927964448928833\n",
      "Surface training t=4216, loss=0.3708546906709671\n",
      "Surface training t=4217, loss=0.3998330533504486\n",
      "Surface training t=4218, loss=0.35461197793483734\n",
      "Surface training t=4219, loss=0.3931937515735626\n",
      "Surface training t=4220, loss=0.3133840039372444\n",
      "Surface training t=4221, loss=0.36659227311611176\n",
      "Surface training t=4222, loss=0.3409139811992645\n",
      "Surface training t=4223, loss=0.36081068217754364\n",
      "Surface training t=4224, loss=0.43566958606243134\n",
      "Surface training t=4225, loss=0.35886722803115845\n",
      "Surface training t=4226, loss=0.4151398688554764\n",
      "Surface training t=4227, loss=0.36254939436912537\n",
      "Surface training t=4228, loss=0.3685190975666046\n",
      "Surface training t=4229, loss=0.37547969818115234\n",
      "Surface training t=4230, loss=0.37221407890319824\n",
      "Surface training t=4231, loss=0.3747726082801819\n",
      "Surface training t=4232, loss=0.4145944267511368\n",
      "Surface training t=4233, loss=0.32387077808380127\n",
      "Surface training t=4234, loss=0.3859538584947586\n",
      "Surface training t=4235, loss=0.4018137902021408\n",
      "Surface training t=4236, loss=0.3604001849889755\n",
      "Surface training t=4237, loss=0.40909039974212646\n",
      "Surface training t=4238, loss=0.3869653344154358\n",
      "Surface training t=4239, loss=0.400825560092926\n",
      "Surface training t=4240, loss=0.3525598645210266\n",
      "Surface training t=4241, loss=0.35683633387088776\n",
      "Surface training t=4242, loss=0.32306596636772156\n",
      "Surface training t=4243, loss=0.35187433660030365\n",
      "Surface training t=4244, loss=0.38634093105793\n",
      "Surface training t=4245, loss=0.39483949542045593\n",
      "Surface training t=4246, loss=0.41561809182167053\n",
      "Surface training t=4247, loss=0.3929671049118042\n",
      "Surface training t=4248, loss=0.37398913502693176\n",
      "Surface training t=4249, loss=0.3760479837656021\n",
      "Surface training t=4250, loss=0.35255981981754303\n",
      "Surface training t=4251, loss=0.3584528863430023\n",
      "Surface training t=4252, loss=0.3208587020635605\n",
      "Surface training t=4253, loss=0.3483874350786209\n",
      "Surface training t=4254, loss=0.3341013044118881\n",
      "Surface training t=4255, loss=0.3511430025100708\n",
      "Surface training t=4256, loss=0.39988963305950165\n",
      "Surface training t=4257, loss=0.4129539728164673\n",
      "Surface training t=4258, loss=0.3908233195543289\n",
      "Surface training t=4259, loss=0.36181671917438507\n",
      "Surface training t=4260, loss=0.42447084188461304\n",
      "Surface training t=4261, loss=0.38360926508903503\n",
      "Surface training t=4262, loss=0.4091063290834427\n",
      "Surface training t=4263, loss=0.37716926634311676\n",
      "Surface training t=4264, loss=0.39614975452423096\n",
      "Surface training t=4265, loss=0.3753490746021271\n",
      "Surface training t=4266, loss=0.42440783977508545\n",
      "Surface training t=4267, loss=0.3837572932243347\n",
      "Surface training t=4268, loss=0.41328078508377075\n",
      "Surface training t=4269, loss=0.3799852877855301\n",
      "Surface training t=4270, loss=0.37348271906375885\n",
      "Surface training t=4271, loss=0.38784344494342804\n",
      "Surface training t=4272, loss=0.3934313505887985\n",
      "Surface training t=4273, loss=0.3926992565393448\n",
      "Surface training t=4274, loss=0.35267187654972076\n",
      "Surface training t=4275, loss=0.4444499611854553\n",
      "Surface training t=4276, loss=0.36108633875846863\n",
      "Surface training t=4277, loss=0.3628004491329193\n",
      "Surface training t=4278, loss=0.3530699163675308\n",
      "Surface training t=4279, loss=0.36622439324855804\n",
      "Surface training t=4280, loss=0.38559652864933014\n",
      "Surface training t=4281, loss=0.3681141883134842\n",
      "Surface training t=4282, loss=0.38887767493724823\n",
      "Surface training t=4283, loss=0.4231356382369995\n",
      "Surface training t=4284, loss=0.39899827539920807\n",
      "Surface training t=4285, loss=0.3789834678173065\n",
      "Surface training t=4286, loss=0.349868506193161\n",
      "Surface training t=4287, loss=0.39582115411758423\n",
      "Surface training t=4288, loss=0.3507246971130371\n",
      "Surface training t=4289, loss=0.37392280995845795\n",
      "Surface training t=4290, loss=0.32730312645435333\n",
      "Surface training t=4291, loss=0.35156576335430145\n",
      "Surface training t=4292, loss=0.3771182894706726\n",
      "Surface training t=4293, loss=0.42598100006580353\n",
      "Surface training t=4294, loss=0.40974435210227966\n",
      "Surface training t=4295, loss=0.3313213288784027\n",
      "Surface training t=4296, loss=0.38774310052394867\n",
      "Surface training t=4297, loss=0.3780954033136368\n",
      "Surface training t=4298, loss=0.43271537125110626\n",
      "Surface training t=4299, loss=0.3877466320991516\n",
      "Surface training t=4300, loss=0.38695840537548065\n",
      "Surface training t=4301, loss=0.35753315687179565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=4302, loss=0.37416818737983704\n",
      "Surface training t=4303, loss=0.38134732842445374\n",
      "Surface training t=4304, loss=0.3781530112028122\n",
      "Surface training t=4305, loss=0.3643421232700348\n",
      "Surface training t=4306, loss=0.3680584281682968\n",
      "Surface training t=4307, loss=0.3986388146877289\n",
      "Surface training t=4308, loss=0.38962027430534363\n",
      "Surface training t=4309, loss=0.3907150626182556\n",
      "Surface training t=4310, loss=0.3817341476678848\n",
      "Surface training t=4311, loss=0.3531882166862488\n",
      "Surface training t=4312, loss=0.3760717958211899\n",
      "Surface training t=4313, loss=0.40734662115573883\n",
      "Surface training t=4314, loss=0.3628798723220825\n",
      "Surface training t=4315, loss=0.4091508239507675\n",
      "Surface training t=4316, loss=0.34178614616394043\n",
      "Surface training t=4317, loss=0.40241488814353943\n",
      "Surface training t=4318, loss=0.398468941450119\n",
      "Surface training t=4319, loss=0.36917728185653687\n",
      "Surface training t=4320, loss=0.3768483102321625\n",
      "Surface training t=4321, loss=0.4205663800239563\n",
      "Surface training t=4322, loss=0.3845602869987488\n",
      "Surface training t=4323, loss=0.3772782236337662\n",
      "Surface training t=4324, loss=0.3747744709253311\n",
      "Surface training t=4325, loss=0.39269959926605225\n",
      "Surface training t=4326, loss=0.45843103528022766\n",
      "Surface training t=4327, loss=0.3879317045211792\n",
      "Surface training t=4328, loss=0.3375786393880844\n",
      "Surface training t=4329, loss=0.41481436789035797\n",
      "Surface training t=4330, loss=0.3537273108959198\n",
      "Surface training t=4331, loss=0.3769138753414154\n",
      "Surface training t=4332, loss=0.4170885384082794\n",
      "Surface training t=4333, loss=0.4118906408548355\n",
      "Surface training t=4334, loss=0.42412762343883514\n",
      "Surface training t=4335, loss=0.3857725113630295\n",
      "Surface training t=4336, loss=0.3564590960741043\n",
      "Surface training t=4337, loss=0.35500285029411316\n",
      "Surface training t=4338, loss=0.34384578466415405\n",
      "Surface training t=4339, loss=0.3884899169206619\n",
      "Surface training t=4340, loss=0.4181213974952698\n",
      "Surface training t=4341, loss=0.3181734159588814\n",
      "Surface training t=4342, loss=0.42288558185100555\n",
      "Surface training t=4343, loss=0.34617947041988373\n",
      "Surface training t=4344, loss=0.4405951499938965\n",
      "Surface training t=4345, loss=0.31235460937023163\n",
      "Surface training t=4346, loss=0.4012983441352844\n",
      "Surface training t=4347, loss=0.3937307894229889\n",
      "Surface training t=4348, loss=0.3397320806980133\n",
      "Surface training t=4349, loss=0.31667403876781464\n",
      "Surface training t=4350, loss=0.39592914283275604\n",
      "Surface training t=4351, loss=0.3545546233654022\n",
      "Surface training t=4352, loss=0.37972722947597504\n",
      "Surface training t=4353, loss=0.39214687049388885\n",
      "Surface training t=4354, loss=0.3416304439306259\n",
      "Surface training t=4355, loss=0.37028568983078003\n",
      "Surface training t=4356, loss=0.3455803543329239\n",
      "Surface training t=4357, loss=0.34553787112236023\n",
      "Surface training t=4358, loss=0.33478058874607086\n",
      "Surface training t=4359, loss=0.372174471616745\n",
      "Surface training t=4360, loss=0.34994155168533325\n",
      "Surface training t=4361, loss=0.3253207355737686\n",
      "Surface training t=4362, loss=0.37915845215320587\n",
      "Surface training t=4363, loss=0.4169373959302902\n",
      "Surface training t=4364, loss=0.3432408422231674\n",
      "Surface training t=4365, loss=0.36017055809497833\n",
      "Surface training t=4366, loss=0.37177030742168427\n",
      "Surface training t=4367, loss=0.39301586151123047\n",
      "Surface training t=4368, loss=0.34541475772857666\n",
      "Surface training t=4369, loss=0.35473762452602386\n",
      "Surface training t=4370, loss=0.4227055013179779\n",
      "Surface training t=4371, loss=0.4023296535015106\n",
      "Surface training t=4372, loss=0.4003516286611557\n",
      "Surface training t=4373, loss=0.39285245537757874\n",
      "Surface training t=4374, loss=0.3319564759731293\n",
      "Surface training t=4375, loss=0.4001583158969879\n",
      "Surface training t=4376, loss=0.4283875524997711\n",
      "Surface training t=4377, loss=0.38392408192157745\n",
      "Surface training t=4378, loss=0.36841341853141785\n",
      "Surface training t=4379, loss=0.3593503534793854\n",
      "Surface training t=4380, loss=0.3542510122060776\n",
      "Surface training t=4381, loss=0.3701048195362091\n",
      "Surface training t=4382, loss=0.3547619432210922\n",
      "Surface training t=4383, loss=0.40310579538345337\n",
      "Surface training t=4384, loss=0.3672349154949188\n",
      "Surface training t=4385, loss=0.3739467263221741\n",
      "Surface training t=4386, loss=0.36691808700561523\n",
      "Surface training t=4387, loss=0.35604386031627655\n",
      "Surface training t=4388, loss=0.3479558527469635\n",
      "Surface training t=4389, loss=0.37733401358127594\n",
      "Surface training t=4390, loss=0.3711944967508316\n",
      "Surface training t=4391, loss=0.34421688318252563\n",
      "Surface training t=4392, loss=0.4317270368337631\n",
      "Surface training t=4393, loss=0.36792542040348053\n",
      "Surface training t=4394, loss=0.31951098144054413\n",
      "Surface training t=4395, loss=0.36033694446086884\n",
      "Surface training t=4396, loss=0.379390224814415\n",
      "Surface training t=4397, loss=0.3505350947380066\n",
      "Surface training t=4398, loss=0.3869727849960327\n",
      "Surface training t=4399, loss=0.36493639647960663\n",
      "Surface training t=4400, loss=0.3528081029653549\n",
      "Surface training t=4401, loss=0.34022682905197144\n",
      "Surface training t=4402, loss=0.35573072731494904\n",
      "Surface training t=4403, loss=0.3499225527048111\n",
      "Surface training t=4404, loss=0.35486382246017456\n",
      "Surface training t=4405, loss=0.36691416800022125\n",
      "Surface training t=4406, loss=0.36063696444034576\n",
      "Surface training t=4407, loss=0.34796419739723206\n",
      "Surface training t=4408, loss=0.402608722448349\n",
      "Surface training t=4409, loss=0.371611624956131\n",
      "Surface training t=4410, loss=0.3657953590154648\n",
      "Surface training t=4411, loss=0.3909660875797272\n",
      "Surface training t=4412, loss=0.4032485634088516\n",
      "Surface training t=4413, loss=0.37316152453422546\n",
      "Surface training t=4414, loss=0.3520517945289612\n",
      "Surface training t=4415, loss=0.38107921183109283\n",
      "Surface training t=4416, loss=0.36524389684200287\n",
      "Surface training t=4417, loss=0.3681595027446747\n",
      "Surface training t=4418, loss=0.35095900297164917\n",
      "Surface training t=4419, loss=0.3640449047088623\n",
      "Surface training t=4420, loss=0.4061862975358963\n",
      "Surface training t=4421, loss=0.37502218782901764\n",
      "Surface training t=4422, loss=0.4401422441005707\n",
      "Surface training t=4423, loss=0.3404920846223831\n",
      "Surface training t=4424, loss=0.36102883517742157\n",
      "Surface training t=4425, loss=0.3995009660720825\n",
      "Surface training t=4426, loss=0.33152784407138824\n",
      "Surface training t=4427, loss=0.33734703063964844\n",
      "Surface training t=4428, loss=0.32043422758579254\n",
      "Surface training t=4429, loss=0.38645413517951965\n",
      "Surface training t=4430, loss=0.382725328207016\n",
      "Surface training t=4431, loss=0.382860004901886\n",
      "Surface training t=4432, loss=0.3888523578643799\n",
      "Surface training t=4433, loss=0.36120568215847015\n",
      "Surface training t=4434, loss=0.3106190487742424\n",
      "Surface training t=4435, loss=0.3742678016424179\n",
      "Surface training t=4436, loss=0.3242734670639038\n",
      "Surface training t=4437, loss=0.3148689717054367\n",
      "Surface training t=4438, loss=0.42700889706611633\n",
      "Surface training t=4439, loss=0.37403063476085663\n",
      "Surface training t=4440, loss=0.3247043117880821\n",
      "Surface training t=4441, loss=0.3767058104276657\n",
      "Surface training t=4442, loss=0.3977982699871063\n",
      "Surface training t=4443, loss=0.3738497793674469\n",
      "Surface training t=4444, loss=0.3952365815639496\n",
      "Surface training t=4445, loss=0.40496671199798584\n",
      "Surface training t=4446, loss=0.36646367609500885\n",
      "Surface training t=4447, loss=0.38503114879131317\n",
      "Surface training t=4448, loss=0.3294638693332672\n",
      "Surface training t=4449, loss=0.35898570716381073\n",
      "Surface training t=4450, loss=0.3792078197002411\n",
      "Surface training t=4451, loss=0.37381313741207123\n",
      "Surface training t=4452, loss=0.34670698642730713\n",
      "Surface training t=4453, loss=0.3379925638437271\n",
      "Surface training t=4454, loss=0.312766969203949\n",
      "Surface training t=4455, loss=0.3246275782585144\n",
      "Surface training t=4456, loss=0.3296464830636978\n",
      "Surface training t=4457, loss=0.31421442329883575\n",
      "Surface training t=4458, loss=0.38798047602176666\n",
      "Surface training t=4459, loss=0.35983774065971375\n",
      "Surface training t=4460, loss=0.33491238951683044\n",
      "Surface training t=4461, loss=0.3415924608707428\n",
      "Surface training t=4462, loss=0.35318075120449066\n",
      "Surface training t=4463, loss=0.3805788457393646\n",
      "Surface training t=4464, loss=0.3435753434896469\n",
      "Surface training t=4465, loss=0.3380352556705475\n",
      "Surface training t=4466, loss=0.3538895696401596\n",
      "Surface training t=4467, loss=0.3636568784713745\n",
      "Surface training t=4468, loss=0.432081863284111\n",
      "Surface training t=4469, loss=0.391595795750618\n",
      "Surface training t=4470, loss=0.3943723738193512\n",
      "Surface training t=4471, loss=0.3214152157306671\n",
      "Surface training t=4472, loss=0.30232080072164536\n",
      "Surface training t=4473, loss=0.33674293756484985\n",
      "Surface training t=4474, loss=0.36456628143787384\n",
      "Surface training t=4475, loss=0.35045158863067627\n",
      "Surface training t=4476, loss=0.3202579393982887\n",
      "Surface training t=4477, loss=0.3390187472105026\n",
      "Surface training t=4478, loss=0.36981113255023956\n",
      "Surface training t=4479, loss=0.4193192571401596\n",
      "Surface training t=4480, loss=0.4346388876438141\n",
      "Surface training t=4481, loss=0.38033419847488403\n",
      "Surface training t=4482, loss=0.3702719956636429\n",
      "Surface training t=4483, loss=0.36689913272857666\n",
      "Surface training t=4484, loss=0.35413530468940735\n",
      "Surface training t=4485, loss=0.3101692497730255\n",
      "Surface training t=4486, loss=0.3777613341808319\n",
      "Surface training t=4487, loss=0.3818369656801224\n",
      "Surface training t=4488, loss=0.37624460458755493\n",
      "Surface training t=4489, loss=0.3786255419254303\n",
      "Surface training t=4490, loss=0.35722778737545013\n",
      "Surface training t=4491, loss=0.3468325436115265\n",
      "Surface training t=4492, loss=0.41311104595661163\n",
      "Surface training t=4493, loss=0.36761048436164856\n",
      "Surface training t=4494, loss=0.33568279445171356\n",
      "Surface training t=4495, loss=0.35242754220962524\n",
      "Surface training t=4496, loss=0.38079413771629333\n",
      "Surface training t=4497, loss=0.3614285886287689\n",
      "Surface training t=4498, loss=0.36679908633232117\n",
      "Surface training t=4499, loss=0.3454326391220093\n",
      "Surface training t=4500, loss=0.34662647545337677\n",
      "Surface training t=4501, loss=0.3883886933326721\n",
      "Surface training t=4502, loss=0.3286457061767578\n",
      "Surface training t=4503, loss=0.3812432587146759\n",
      "Surface training t=4504, loss=0.363389328122139\n",
      "Surface training t=4505, loss=0.30353666096925735\n",
      "Surface training t=4506, loss=0.36332517862319946\n",
      "Surface training t=4507, loss=0.32256707549095154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=4508, loss=0.35294489562511444\n",
      "Surface training t=4509, loss=0.30525777488946915\n",
      "Surface training t=4510, loss=0.39752982556819916\n",
      "Surface training t=4511, loss=0.34861883521080017\n",
      "Surface training t=4512, loss=0.39940160512924194\n",
      "Surface training t=4513, loss=0.336132749915123\n",
      "Surface training t=4514, loss=0.36138124763965607\n",
      "Surface training t=4515, loss=0.3972701132297516\n",
      "Surface training t=4516, loss=0.32293180376291275\n",
      "Surface training t=4517, loss=0.3962497413158417\n",
      "Surface training t=4518, loss=0.382998526096344\n",
      "Surface training t=4519, loss=0.3656099736690521\n",
      "Surface training t=4520, loss=0.33271750807762146\n",
      "Surface training t=4521, loss=0.3487425446510315\n",
      "Surface training t=4522, loss=0.3864799439907074\n",
      "Surface training t=4523, loss=0.35701726377010345\n",
      "Surface training t=4524, loss=0.32848595082759857\n",
      "Surface training t=4525, loss=0.3744707256555557\n",
      "Surface training t=4526, loss=0.34485067427158356\n",
      "Surface training t=4527, loss=0.39946161210536957\n",
      "Surface training t=4528, loss=0.3228444755077362\n",
      "Surface training t=4529, loss=0.4011625051498413\n",
      "Surface training t=4530, loss=0.3238094300031662\n",
      "Surface training t=4531, loss=0.35599151253700256\n",
      "Surface training t=4532, loss=0.3940228223800659\n",
      "Surface training t=4533, loss=0.39141616225242615\n",
      "Surface training t=4534, loss=0.3737682104110718\n",
      "Surface training t=4535, loss=0.37499624490737915\n",
      "Surface training t=4536, loss=0.35205142199993134\n",
      "Surface training t=4537, loss=0.363216832280159\n",
      "Surface training t=4538, loss=0.35783466696739197\n",
      "Surface training t=4539, loss=0.3443835824728012\n",
      "Surface training t=4540, loss=0.3538028299808502\n",
      "Surface training t=4541, loss=0.3464362770318985\n",
      "Surface training t=4542, loss=0.3576887547969818\n",
      "Surface training t=4543, loss=0.378839835524559\n",
      "Surface training t=4544, loss=0.3379286676645279\n",
      "Surface training t=4545, loss=0.37465597689151764\n",
      "Surface training t=4546, loss=0.36155933141708374\n",
      "Surface training t=4547, loss=0.3763217329978943\n",
      "Surface training t=4548, loss=0.38714568316936493\n",
      "Surface training t=4549, loss=0.40397776663303375\n",
      "Surface training t=4550, loss=0.3954504281282425\n",
      "Surface training t=4551, loss=0.37967929244041443\n",
      "Surface training t=4552, loss=0.3590090870857239\n",
      "Surface training t=4553, loss=0.3781857043504715\n",
      "Surface training t=4554, loss=0.34687648713588715\n",
      "Surface training t=4555, loss=0.358994722366333\n",
      "Surface training t=4556, loss=0.3967878520488739\n",
      "Surface training t=4557, loss=0.3773282468318939\n",
      "Surface training t=4558, loss=0.38107360899448395\n",
      "Surface training t=4559, loss=0.3196386396884918\n",
      "Surface training t=4560, loss=0.34819504618644714\n",
      "Surface training t=4561, loss=0.34106819331645966\n",
      "Surface training t=4562, loss=0.3601728677749634\n",
      "Surface training t=4563, loss=0.30577630549669266\n",
      "Surface training t=4564, loss=0.321598619222641\n",
      "Surface training t=4565, loss=0.3781317472457886\n",
      "Surface training t=4566, loss=0.3458957076072693\n",
      "Surface training t=4567, loss=0.316767081618309\n",
      "Surface training t=4568, loss=0.3774903416633606\n",
      "Surface training t=4569, loss=0.39020244777202606\n",
      "Surface training t=4570, loss=0.3187175840139389\n",
      "Surface training t=4571, loss=0.3674771934747696\n",
      "Surface training t=4572, loss=0.3799215853214264\n",
      "Surface training t=4573, loss=0.4054121673107147\n",
      "Surface training t=4574, loss=0.3902060091495514\n",
      "Surface training t=4575, loss=0.3830926716327667\n",
      "Surface training t=4576, loss=0.38483820855617523\n",
      "Surface training t=4577, loss=0.3915509134531021\n",
      "Surface training t=4578, loss=0.3649221658706665\n",
      "Surface training t=4579, loss=0.36073482036590576\n",
      "Surface training t=4580, loss=0.4328763484954834\n",
      "Surface training t=4581, loss=0.41254378855228424\n",
      "Surface training t=4582, loss=0.478069007396698\n",
      "Surface training t=4583, loss=0.39864878356456757\n",
      "Surface training t=4584, loss=0.40513965487480164\n",
      "Surface training t=4585, loss=0.40900513529777527\n",
      "Surface training t=4586, loss=0.4422040432691574\n",
      "Surface training t=4587, loss=0.3658248484134674\n",
      "Surface training t=4588, loss=0.41193078458309174\n",
      "Surface training t=4589, loss=0.36952202022075653\n",
      "Surface training t=4590, loss=0.37535589933395386\n",
      "Surface training t=4591, loss=0.35220447182655334\n",
      "Surface training t=4592, loss=0.43900297582149506\n",
      "Surface training t=4593, loss=0.4089733362197876\n",
      "Surface training t=4594, loss=0.3889257460832596\n",
      "Surface training t=4595, loss=0.38490912318229675\n",
      "Surface training t=4596, loss=0.3716210424900055\n",
      "Surface training t=4597, loss=0.3683793395757675\n",
      "Surface training t=4598, loss=0.39986535906791687\n",
      "Surface training t=4599, loss=0.359115332365036\n",
      "Surface training t=4600, loss=0.37003548443317413\n",
      "Surface training t=4601, loss=0.3796766698360443\n",
      "Surface training t=4602, loss=0.36375147104263306\n",
      "Surface training t=4603, loss=0.3631678372621536\n",
      "Surface training t=4604, loss=0.3583451956510544\n",
      "Surface training t=4605, loss=0.39910224080085754\n",
      "Surface training t=4606, loss=0.3694910407066345\n",
      "Surface training t=4607, loss=0.39311932027339935\n",
      "Surface training t=4608, loss=0.4231205880641937\n",
      "Surface training t=4609, loss=0.3708811402320862\n",
      "Surface training t=4610, loss=0.3357953578233719\n",
      "Surface training t=4611, loss=0.393618568778038\n",
      "Surface training t=4612, loss=0.3344514071941376\n",
      "Surface training t=4613, loss=0.3717297166585922\n",
      "Surface training t=4614, loss=0.39689546823501587\n",
      "Surface training t=4615, loss=0.3530116677284241\n",
      "Surface training t=4616, loss=0.3464813828468323\n",
      "Surface training t=4617, loss=0.3354956805706024\n",
      "Surface training t=4618, loss=0.33438166975975037\n",
      "Surface training t=4619, loss=0.3424687385559082\n",
      "Surface training t=4620, loss=0.3704434931278229\n",
      "Surface training t=4621, loss=0.33510713279247284\n",
      "Surface training t=4622, loss=0.3415919840335846\n",
      "Surface training t=4623, loss=0.3551909625530243\n",
      "Surface training t=4624, loss=0.3573664426803589\n",
      "Surface training t=4625, loss=0.34049396216869354\n",
      "Surface training t=4626, loss=0.36195501685142517\n",
      "Surface training t=4627, loss=0.3181038051843643\n",
      "Surface training t=4628, loss=0.32798804342746735\n",
      "Surface training t=4629, loss=0.3217407763004303\n",
      "Surface training t=4630, loss=0.3853113204240799\n",
      "Surface training t=4631, loss=0.35586076974868774\n",
      "Surface training t=4632, loss=0.34820815920829773\n",
      "Surface training t=4633, loss=0.3677218407392502\n",
      "Surface training t=4634, loss=0.3393997848033905\n",
      "Surface training t=4635, loss=0.42260393500328064\n",
      "Surface training t=4636, loss=0.38695214688777924\n",
      "Surface training t=4637, loss=0.3682563900947571\n",
      "Surface training t=4638, loss=0.4041961133480072\n",
      "Surface training t=4639, loss=0.38951657712459564\n",
      "Surface training t=4640, loss=0.33623258769512177\n",
      "Surface training t=4641, loss=0.3647107630968094\n",
      "Surface training t=4642, loss=0.42259393632411957\n",
      "Surface training t=4643, loss=0.32884112000465393\n",
      "Surface training t=4644, loss=0.36407576501369476\n",
      "Surface training t=4645, loss=0.3878577649593353\n",
      "Surface training t=4646, loss=0.4463985115289688\n",
      "Surface training t=4647, loss=0.38334959745407104\n",
      "Surface training t=4648, loss=0.41183583438396454\n",
      "Surface training t=4649, loss=0.4305856078863144\n",
      "Surface training t=4650, loss=0.4443975239992142\n",
      "Surface training t=4651, loss=0.34607939422130585\n",
      "Surface training t=4652, loss=0.4207507073879242\n",
      "Surface training t=4653, loss=0.3691413551568985\n",
      "Surface training t=4654, loss=0.3824267238378525\n",
      "Surface training t=4655, loss=0.3916876018047333\n",
      "Surface training t=4656, loss=0.4695362448692322\n",
      "Surface training t=4657, loss=0.40651577711105347\n",
      "Surface training t=4658, loss=0.3876422345638275\n",
      "Surface training t=4659, loss=0.34728407859802246\n",
      "Surface training t=4660, loss=0.4125414937734604\n",
      "Surface training t=4661, loss=0.40472573041915894\n",
      "Surface training t=4662, loss=0.40897008776664734\n",
      "Surface training t=4663, loss=0.3781917542219162\n",
      "Surface training t=4664, loss=0.4093916565179825\n",
      "Surface training t=4665, loss=0.3657839447259903\n",
      "Surface training t=4666, loss=0.37420833110809326\n",
      "Surface training t=4667, loss=0.39356274902820587\n",
      "Surface training t=4668, loss=0.37300658226013184\n",
      "Surface training t=4669, loss=0.3980589807033539\n",
      "Surface training t=4670, loss=0.3215583711862564\n",
      "Surface training t=4671, loss=0.3589416444301605\n",
      "Surface training t=4672, loss=0.3586617708206177\n",
      "Surface training t=4673, loss=0.35255520045757294\n",
      "Surface training t=4674, loss=0.4310198277235031\n",
      "Surface training t=4675, loss=0.3296675533056259\n",
      "Surface training t=4676, loss=0.3538426607847214\n",
      "Surface training t=4677, loss=0.3321065902709961\n",
      "Surface training t=4678, loss=0.34450915455818176\n",
      "Surface training t=4679, loss=0.34496834874153137\n",
      "Surface training t=4680, loss=0.3844732195138931\n",
      "Surface training t=4681, loss=0.316301167011261\n",
      "Surface training t=4682, loss=0.30801568925380707\n",
      "Surface training t=4683, loss=0.35344570875167847\n",
      "Surface training t=4684, loss=0.387448325753212\n",
      "Surface training t=4685, loss=0.38780422508716583\n",
      "Surface training t=4686, loss=0.35513846576213837\n",
      "Surface training t=4687, loss=0.3616010546684265\n",
      "Surface training t=4688, loss=0.36283090710639954\n",
      "Surface training t=4689, loss=0.34629860520362854\n",
      "Surface training t=4690, loss=0.3755059391260147\n",
      "Surface training t=4691, loss=0.3642145097255707\n",
      "Surface training t=4692, loss=0.3723543733358383\n",
      "Surface training t=4693, loss=0.35281139612197876\n",
      "Surface training t=4694, loss=0.3390592336654663\n",
      "Surface training t=4695, loss=0.35113413631916046\n",
      "Surface training t=4696, loss=0.3374287039041519\n",
      "Surface training t=4697, loss=0.3301214128732681\n",
      "Surface training t=4698, loss=0.386724591255188\n",
      "Surface training t=4699, loss=0.3799232542514801\n",
      "Surface training t=4700, loss=0.3142611160874367\n",
      "Surface training t=4701, loss=0.3914482593536377\n",
      "Surface training t=4702, loss=0.30886702239513397\n",
      "Surface training t=4703, loss=0.3320941925048828\n",
      "Surface training t=4704, loss=0.33234554529190063\n",
      "Surface training t=4705, loss=0.34983108937740326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=4706, loss=0.36817994713783264\n",
      "Surface training t=4707, loss=0.2991291359066963\n",
      "Surface training t=4708, loss=0.36672520637512207\n",
      "Surface training t=4709, loss=0.34427401423454285\n",
      "Surface training t=4710, loss=0.36321890354156494\n",
      "Surface training t=4711, loss=0.35722239315509796\n",
      "Surface training t=4712, loss=0.3541698157787323\n",
      "Surface training t=4713, loss=0.33795951306819916\n",
      "Surface training t=4714, loss=0.32648633420467377\n",
      "Surface training t=4715, loss=0.35048262774944305\n",
      "Surface training t=4716, loss=0.36070995032787323\n",
      "Surface training t=4717, loss=0.32955893874168396\n",
      "Surface training t=4718, loss=0.3447103053331375\n",
      "Surface training t=4719, loss=0.38561876118183136\n",
      "Surface training t=4720, loss=0.32803651690483093\n",
      "Surface training t=4721, loss=0.31871648132801056\n",
      "Surface training t=4722, loss=0.35296526551246643\n",
      "Surface training t=4723, loss=0.35386501252651215\n",
      "Surface training t=4724, loss=0.34947003424167633\n",
      "Surface training t=4725, loss=0.3464714288711548\n",
      "Surface training t=4726, loss=0.3579832464456558\n",
      "Surface training t=4727, loss=0.3185901641845703\n",
      "Surface training t=4728, loss=0.31035933643579483\n",
      "Surface training t=4729, loss=0.3371572941541672\n",
      "Surface training t=4730, loss=0.32867611944675446\n",
      "Surface training t=4731, loss=0.3622114360332489\n",
      "Surface training t=4732, loss=0.3976222574710846\n",
      "Surface training t=4733, loss=0.3607126921415329\n",
      "Surface training t=4734, loss=0.3043099045753479\n",
      "Surface training t=4735, loss=0.38926348090171814\n",
      "Surface training t=4736, loss=0.3564392179250717\n",
      "Surface training t=4737, loss=0.38054780662059784\n",
      "Surface training t=4738, loss=0.35158489644527435\n",
      "Surface training t=4739, loss=0.3089229464530945\n",
      "Surface training t=4740, loss=0.3305133134126663\n",
      "Surface training t=4741, loss=0.30790266394615173\n",
      "Surface training t=4742, loss=0.33724159002304077\n",
      "Surface training t=4743, loss=0.27343565225601196\n",
      "Surface training t=4744, loss=0.3433024138212204\n",
      "Surface training t=4745, loss=0.33814558386802673\n",
      "Surface training t=4746, loss=0.32547444105148315\n",
      "Surface training t=4747, loss=0.33887919783592224\n",
      "Surface training t=4748, loss=0.3562484085559845\n",
      "Surface training t=4749, loss=0.35663099586963654\n",
      "Surface training t=4750, loss=0.3877490758895874\n",
      "Surface training t=4751, loss=0.3305843025445938\n",
      "Surface training t=4752, loss=0.3657403290271759\n",
      "Surface training t=4753, loss=0.3271320313215256\n",
      "Surface training t=4754, loss=0.36118726432323456\n",
      "Surface training t=4755, loss=0.35845886170864105\n",
      "Surface training t=4756, loss=0.31533050537109375\n",
      "Surface training t=4757, loss=0.3080063760280609\n",
      "Surface training t=4758, loss=0.3530910462141037\n",
      "Surface training t=4759, loss=0.35311993956565857\n",
      "Surface training t=4760, loss=0.3455332964658737\n",
      "Surface training t=4761, loss=0.3651703745126724\n",
      "Surface training t=4762, loss=0.36628711223602295\n",
      "Surface training t=4763, loss=0.38244879245758057\n",
      "Surface training t=4764, loss=0.37099991738796234\n",
      "Surface training t=4765, loss=0.3709573298692703\n",
      "Surface training t=4766, loss=0.3245709538459778\n",
      "Surface training t=4767, loss=0.319130040705204\n",
      "Surface training t=4768, loss=0.3583488464355469\n",
      "Surface training t=4769, loss=0.3501458019018173\n",
      "Surface training t=4770, loss=0.39026954770088196\n",
      "Surface training t=4771, loss=0.33193716406822205\n",
      "Surface training t=4772, loss=0.39734509587287903\n",
      "Surface training t=4773, loss=0.34866711497306824\n",
      "Surface training t=4774, loss=0.36379337310791016\n",
      "Surface training t=4775, loss=0.3791978359222412\n",
      "Surface training t=4776, loss=0.36265163123607635\n",
      "Surface training t=4777, loss=0.3507082909345627\n",
      "Surface training t=4778, loss=0.3862844556570053\n",
      "Surface training t=4779, loss=0.3683246374130249\n",
      "Surface training t=4780, loss=0.4104674607515335\n",
      "Surface training t=4781, loss=0.4076852351427078\n",
      "Surface training t=4782, loss=0.3420954793691635\n",
      "Surface training t=4783, loss=0.4049474596977234\n",
      "Surface training t=4784, loss=0.3591865450143814\n",
      "Surface training t=4785, loss=0.3494352549314499\n",
      "Surface training t=4786, loss=0.35347697138786316\n",
      "Surface training t=4787, loss=0.3600555956363678\n",
      "Surface training t=4788, loss=0.38043949007987976\n",
      "Surface training t=4789, loss=0.31629742681980133\n",
      "Surface training t=4790, loss=0.36473606526851654\n",
      "Surface training t=4791, loss=0.34667447209358215\n",
      "Surface training t=4792, loss=0.35855329036712646\n",
      "Surface training t=4793, loss=0.33096690475940704\n",
      "Surface training t=4794, loss=0.31840746104717255\n",
      "Surface training t=4795, loss=0.36668989062309265\n",
      "Surface training t=4796, loss=0.34504373371601105\n",
      "Surface training t=4797, loss=0.34357741475105286\n",
      "Surface training t=4798, loss=0.33180396258831024\n",
      "Surface training t=4799, loss=0.3370264321565628\n",
      "Surface training t=4800, loss=0.3651564419269562\n",
      "Surface training t=4801, loss=0.31795254349708557\n",
      "Surface training t=4802, loss=0.3411529213190079\n",
      "Surface training t=4803, loss=0.32686223089694977\n",
      "Surface training t=4804, loss=0.34000976383686066\n",
      "Surface training t=4805, loss=0.3492043763399124\n",
      "Surface training t=4806, loss=0.34844666719436646\n",
      "Surface training t=4807, loss=0.34666359424591064\n",
      "Surface training t=4808, loss=0.40211793780326843\n",
      "Surface training t=4809, loss=0.3853513300418854\n",
      "Surface training t=4810, loss=0.3136245310306549\n",
      "Surface training t=4811, loss=0.34616684913635254\n",
      "Surface training t=4812, loss=0.3297291547060013\n",
      "Surface training t=4813, loss=0.4100306034088135\n",
      "Surface training t=4814, loss=0.42301589250564575\n",
      "Surface training t=4815, loss=0.36196161806583405\n",
      "Surface training t=4816, loss=0.36324451863765717\n",
      "Surface training t=4817, loss=0.36012423038482666\n",
      "Surface training t=4818, loss=0.3938414752483368\n",
      "Surface training t=4819, loss=0.3602464199066162\n",
      "Surface training t=4820, loss=0.35456839203834534\n",
      "Surface training t=4821, loss=0.3768157958984375\n",
      "Surface training t=4822, loss=0.3227842301130295\n",
      "Surface training t=4823, loss=0.31772269308567047\n",
      "Surface training t=4824, loss=0.3012867271900177\n",
      "Surface training t=4825, loss=0.3309038281440735\n",
      "Surface training t=4826, loss=0.32625678181648254\n",
      "Surface training t=4827, loss=0.33914417028427124\n",
      "Surface training t=4828, loss=0.35071131587028503\n",
      "Surface training t=4829, loss=0.3634670525789261\n",
      "Surface training t=4830, loss=0.337959423661232\n",
      "Surface training t=4831, loss=0.3566763699054718\n",
      "Surface training t=4832, loss=0.3357463628053665\n",
      "Surface training t=4833, loss=0.3245605081319809\n",
      "Surface training t=4834, loss=0.2923673912882805\n",
      "Surface training t=4835, loss=0.3655222952365875\n",
      "Surface training t=4836, loss=0.3141457885503769\n",
      "Surface training t=4837, loss=0.36085692048072815\n",
      "Surface training t=4838, loss=0.3410634994506836\n",
      "Surface training t=4839, loss=0.29969698190689087\n",
      "Surface training t=4840, loss=0.32821793854236603\n",
      "Surface training t=4841, loss=0.29426806420087814\n",
      "Surface training t=4842, loss=0.3308296352624893\n",
      "Surface training t=4843, loss=0.3346896171569824\n",
      "Surface training t=4844, loss=0.3553932309150696\n",
      "Surface training t=4845, loss=0.357504740357399\n",
      "Surface training t=4846, loss=0.3548886328935623\n",
      "Surface training t=4847, loss=0.3367583006620407\n",
      "Surface training t=4848, loss=0.3784773647785187\n",
      "Surface training t=4849, loss=0.34593960642814636\n",
      "Surface training t=4850, loss=0.3441551774740219\n",
      "Surface training t=4851, loss=0.3018244132399559\n",
      "Surface training t=4852, loss=0.3421979546546936\n",
      "Surface training t=4853, loss=0.33538977801799774\n",
      "Surface training t=4854, loss=0.3206251561641693\n",
      "Surface training t=4855, loss=0.29176507890224457\n",
      "Surface training t=4856, loss=0.3301401287317276\n",
      "Surface training t=4857, loss=0.33803005516529083\n",
      "Surface training t=4858, loss=0.31050562858581543\n",
      "Surface training t=4859, loss=0.30931776762008667\n",
      "Surface training t=4860, loss=0.3420119434595108\n",
      "Surface training t=4861, loss=0.33066271245479584\n",
      "Surface training t=4862, loss=0.32381659746170044\n",
      "Surface training t=4863, loss=0.3511497229337692\n",
      "Surface training t=4864, loss=0.3257976323366165\n",
      "Surface training t=4865, loss=0.30340299010276794\n",
      "Surface training t=4866, loss=0.36182714998722076\n",
      "Surface training t=4867, loss=0.33455997705459595\n",
      "Surface training t=4868, loss=0.3825587183237076\n",
      "Surface training t=4869, loss=0.33208900690078735\n",
      "Surface training t=4870, loss=0.29730407893657684\n",
      "Surface training t=4871, loss=0.30391183495521545\n",
      "Surface training t=4872, loss=0.34452909231185913\n",
      "Surface training t=4873, loss=0.31840674579143524\n",
      "Surface training t=4874, loss=0.31174176931381226\n",
      "Surface training t=4875, loss=0.3311067968606949\n",
      "Surface training t=4876, loss=0.29250627756118774\n",
      "Surface training t=4877, loss=0.3374555706977844\n",
      "Surface training t=4878, loss=0.3070048391819\n",
      "Surface training t=4879, loss=0.3393526077270508\n",
      "Surface training t=4880, loss=0.3889262527227402\n",
      "Surface training t=4881, loss=0.40356874465942383\n",
      "Surface training t=4882, loss=0.28304241597652435\n",
      "Surface training t=4883, loss=0.30312979966402054\n",
      "Surface training t=4884, loss=0.325619101524353\n",
      "Surface training t=4885, loss=0.315571665763855\n",
      "Surface training t=4886, loss=0.33428333699703217\n",
      "Surface training t=4887, loss=0.34479811787605286\n",
      "Surface training t=4888, loss=0.32093317806720734\n",
      "Surface training t=4889, loss=0.3607652336359024\n",
      "Surface training t=4890, loss=0.3048575446009636\n",
      "Surface training t=4891, loss=0.32494497299194336\n",
      "Surface training t=4892, loss=0.35316579043865204\n",
      "Surface training t=4893, loss=0.35572807490825653\n",
      "Surface training t=4894, loss=0.3339609205722809\n",
      "Surface training t=4895, loss=0.35076984763145447\n",
      "Surface training t=4896, loss=0.3858302980661392\n",
      "Surface training t=4897, loss=0.26687951385974884\n",
      "Surface training t=4898, loss=0.33569417893886566\n",
      "Surface training t=4899, loss=0.31369221210479736\n",
      "Surface training t=4900, loss=0.3195297569036484\n",
      "Surface training t=4901, loss=0.3004414141178131\n",
      "Surface training t=4902, loss=0.33199629187583923\n",
      "Surface training t=4903, loss=0.34944286942481995\n",
      "Surface training t=4904, loss=0.33551979064941406\n",
      "Surface training t=4905, loss=0.30169762670993805\n",
      "Surface training t=4906, loss=0.36345718801021576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=4907, loss=0.3308414816856384\n",
      "Surface training t=4908, loss=0.31015458703041077\n",
      "Surface training t=4909, loss=0.30463728308677673\n",
      "Surface training t=4910, loss=0.32008279860019684\n",
      "Surface training t=4911, loss=0.2951040714979172\n",
      "Surface training t=4912, loss=0.33702829480171204\n",
      "Surface training t=4913, loss=0.35228487849235535\n",
      "Surface training t=4914, loss=0.3253467530012131\n",
      "Surface training t=4915, loss=0.31549473106861115\n",
      "Surface training t=4916, loss=0.3241606503725052\n",
      "Surface training t=4917, loss=0.3779056668281555\n",
      "Surface training t=4918, loss=0.3728039711713791\n",
      "Surface training t=4919, loss=0.3220500349998474\n",
      "Surface training t=4920, loss=0.30301307141780853\n",
      "Surface training t=4921, loss=0.340562641620636\n",
      "Surface training t=4922, loss=0.3344928175210953\n",
      "Surface training t=4923, loss=0.32043473422527313\n",
      "Surface training t=4924, loss=0.34644582867622375\n",
      "Surface training t=4925, loss=0.3452429920434952\n",
      "Surface training t=4926, loss=0.3113253712654114\n",
      "Surface training t=4927, loss=0.3246746212244034\n",
      "Surface training t=4928, loss=0.3507146090269089\n",
      "Surface training t=4929, loss=0.33540084958076477\n",
      "Surface training t=4930, loss=0.3244020491838455\n",
      "Surface training t=4931, loss=0.3661618083715439\n",
      "Surface training t=4932, loss=0.34582093358039856\n",
      "Surface training t=4933, loss=0.33185313642024994\n",
      "Surface training t=4934, loss=0.30788564682006836\n",
      "Surface training t=4935, loss=0.36575618386268616\n",
      "Surface training t=4936, loss=0.3551960587501526\n",
      "Surface training t=4937, loss=0.3737960606813431\n",
      "Surface training t=4938, loss=0.3404138684272766\n",
      "Surface training t=4939, loss=0.364915132522583\n",
      "Surface training t=4940, loss=0.39846308529376984\n",
      "Surface training t=4941, loss=0.38006289303302765\n",
      "Surface training t=4942, loss=0.32778748869895935\n",
      "Surface training t=4943, loss=0.3378603160381317\n",
      "Surface training t=4944, loss=0.2962803542613983\n",
      "Surface training t=4945, loss=0.3246782422065735\n",
      "Surface training t=4946, loss=0.3046301156282425\n",
      "Surface training t=4947, loss=0.3412448614835739\n",
      "Surface training t=4948, loss=0.3029187321662903\n",
      "Surface training t=4949, loss=0.29171767085790634\n",
      "Surface training t=4950, loss=0.3589441478252411\n",
      "Surface training t=4951, loss=0.34133104979991913\n",
      "Surface training t=4952, loss=0.3940960317850113\n",
      "Surface training t=4953, loss=0.36376072466373444\n",
      "Surface training t=4954, loss=0.35041598975658417\n",
      "Surface training t=4955, loss=0.3523437827825546\n",
      "Surface training t=4956, loss=0.3721865862607956\n",
      "Surface training t=4957, loss=0.32386527955532074\n",
      "Surface training t=4958, loss=0.3140314370393753\n",
      "Surface training t=4959, loss=0.28060276061296463\n",
      "Surface training t=4960, loss=0.3603641539812088\n",
      "Surface training t=4961, loss=0.31138940155506134\n",
      "Surface training t=4962, loss=0.3171911835670471\n",
      "Surface training t=4963, loss=0.3066091537475586\n",
      "Surface training t=4964, loss=0.29515208303928375\n",
      "Surface training t=4965, loss=0.3382185995578766\n",
      "Surface training t=4966, loss=0.33538947999477386\n",
      "Surface training t=4967, loss=0.28029797971248627\n",
      "Surface training t=4968, loss=0.3271774649620056\n",
      "Surface training t=4969, loss=0.32764557003974915\n",
      "Surface training t=4970, loss=0.34162838757038116\n",
      "Surface training t=4971, loss=0.3391285389661789\n",
      "Surface training t=4972, loss=0.30262291431427\n",
      "Surface training t=4973, loss=0.31420229375362396\n",
      "Surface training t=4974, loss=0.3697166293859482\n",
      "Surface training t=4975, loss=0.32632970809936523\n",
      "Surface training t=4976, loss=0.3026254177093506\n",
      "Surface training t=4977, loss=0.3532605767250061\n",
      "Surface training t=4978, loss=0.30645887553691864\n",
      "Surface training t=4979, loss=0.3437691926956177\n",
      "Surface training t=4980, loss=0.3165419399738312\n",
      "Surface training t=4981, loss=0.3072514683008194\n",
      "Surface training t=4982, loss=0.3565034717321396\n",
      "Surface training t=4983, loss=0.2978912591934204\n",
      "Surface training t=4984, loss=0.3520364761352539\n",
      "Surface training t=4985, loss=0.3495063930749893\n",
      "Surface training t=4986, loss=0.36801673471927643\n",
      "Surface training t=4987, loss=0.3428412526845932\n",
      "Surface training t=4988, loss=0.3384735882282257\n",
      "Surface training t=4989, loss=0.387606680393219\n",
      "Surface training t=4990, loss=0.3766317814588547\n",
      "Surface training t=4991, loss=0.338203027844429\n",
      "Surface training t=4992, loss=0.39408624172210693\n",
      "Surface training t=4993, loss=0.356275275349617\n",
      "Surface training t=4994, loss=0.39629240334033966\n",
      "Surface training t=4995, loss=0.3384665846824646\n",
      "Surface training t=4996, loss=0.4026898890733719\n",
      "Surface training t=4997, loss=0.34863682091236115\n",
      "Surface training t=4998, loss=0.3563470095396042\n",
      "Surface training t=4999, loss=0.4017273336648941\n",
      "Surface training t=5000, loss=0.37393513321876526\n",
      "Surface training t=5001, loss=0.3785364329814911\n",
      "Surface training t=5002, loss=0.3732006400823593\n",
      "Surface training t=5003, loss=0.3531656265258789\n",
      "Surface training t=5004, loss=0.3633383959531784\n",
      "Surface training t=5005, loss=0.3960649371147156\n",
      "Surface training t=5006, loss=0.3287232667207718\n",
      "Surface training t=5007, loss=0.35228803753852844\n",
      "Surface training t=5008, loss=0.35694481432437897\n",
      "Surface training t=5009, loss=0.30663490295410156\n",
      "Surface training t=5010, loss=0.3340197056531906\n",
      "Surface training t=5011, loss=0.317983940243721\n",
      "Surface training t=5012, loss=0.32604876160621643\n",
      "Surface training t=5013, loss=0.3056921064853668\n",
      "Surface training t=5014, loss=0.362693190574646\n",
      "Surface training t=5015, loss=0.3350290209054947\n",
      "Surface training t=5016, loss=0.3392331898212433\n",
      "Surface training t=5017, loss=0.4065302312374115\n",
      "Surface training t=5018, loss=0.35698170959949493\n",
      "Surface training t=5019, loss=0.36246106028556824\n",
      "Surface training t=5020, loss=0.3966373801231384\n",
      "Surface training t=5021, loss=0.3505736291408539\n",
      "Surface training t=5022, loss=0.32250915467739105\n",
      "Surface training t=5023, loss=0.3016819953918457\n",
      "Surface training t=5024, loss=0.32525525987148285\n",
      "Surface training t=5025, loss=0.3245408684015274\n",
      "Surface training t=5026, loss=0.30935530364513397\n",
      "Surface training t=5027, loss=0.3512042462825775\n",
      "Surface training t=5028, loss=0.305649071931839\n",
      "Surface training t=5029, loss=0.3038254678249359\n",
      "Surface training t=5030, loss=0.2866813912987709\n",
      "Surface training t=5031, loss=0.3126784861087799\n",
      "Surface training t=5032, loss=0.33396829664707184\n",
      "Surface training t=5033, loss=0.3579540550708771\n",
      "Surface training t=5034, loss=0.34195244312286377\n",
      "Surface training t=5035, loss=0.29794879257678986\n",
      "Surface training t=5036, loss=0.3168325573205948\n",
      "Surface training t=5037, loss=0.27554409950971603\n",
      "Surface training t=5038, loss=0.326405867934227\n",
      "Surface training t=5039, loss=0.3510781079530716\n",
      "Surface training t=5040, loss=0.29139018058776855\n",
      "Surface training t=5041, loss=0.33237215876579285\n",
      "Surface training t=5042, loss=0.3298225998878479\n",
      "Surface training t=5043, loss=0.3370172530412674\n",
      "Surface training t=5044, loss=0.35236912965774536\n",
      "Surface training t=5045, loss=0.30571249127388\n",
      "Surface training t=5046, loss=0.3181273490190506\n",
      "Surface training t=5047, loss=0.3052722364664078\n",
      "Surface training t=5048, loss=0.32137201726436615\n",
      "Surface training t=5049, loss=0.40604472160339355\n",
      "Surface training t=5050, loss=0.30490587651729584\n",
      "Surface training t=5051, loss=0.3336108773946762\n",
      "Surface training t=5052, loss=0.31789545714855194\n",
      "Surface training t=5053, loss=0.37427598237991333\n",
      "Surface training t=5054, loss=0.27456338703632355\n",
      "Surface training t=5055, loss=0.31868574023246765\n",
      "Surface training t=5056, loss=0.33535999059677124\n",
      "Surface training t=5057, loss=0.33639541268348694\n",
      "Surface training t=5058, loss=0.31113961338996887\n",
      "Surface training t=5059, loss=0.3320653587579727\n",
      "Surface training t=5060, loss=0.32970669865608215\n",
      "Surface training t=5061, loss=0.3436540812253952\n",
      "Surface training t=5062, loss=0.35555507242679596\n",
      "Surface training t=5063, loss=0.3302745670080185\n",
      "Surface training t=5064, loss=0.3070853650569916\n",
      "Surface training t=5065, loss=0.31401415169239044\n",
      "Surface training t=5066, loss=0.27949759364128113\n",
      "Surface training t=5067, loss=0.3778763711452484\n",
      "Surface training t=5068, loss=0.3409053534269333\n",
      "Surface training t=5069, loss=0.31061360239982605\n",
      "Surface training t=5070, loss=0.34573225677013397\n",
      "Surface training t=5071, loss=0.3926655948162079\n",
      "Surface training t=5072, loss=0.3155723810195923\n",
      "Surface training t=5073, loss=0.3356122821569443\n",
      "Surface training t=5074, loss=0.32923953235149384\n",
      "Surface training t=5075, loss=0.3358412832021713\n",
      "Surface training t=5076, loss=0.330840066075325\n",
      "Surface training t=5077, loss=0.33498451113700867\n",
      "Surface training t=5078, loss=0.34786106646060944\n",
      "Surface training t=5079, loss=0.35248924791812897\n",
      "Surface training t=5080, loss=0.31918953359127045\n",
      "Surface training t=5081, loss=0.34258170425891876\n",
      "Surface training t=5082, loss=0.326102614402771\n",
      "Surface training t=5083, loss=0.3332967013120651\n",
      "Surface training t=5084, loss=0.34429173171520233\n",
      "Surface training t=5085, loss=0.312836155295372\n",
      "Surface training t=5086, loss=0.3133603483438492\n",
      "Surface training t=5087, loss=0.28854088485240936\n",
      "Surface training t=5088, loss=0.3574047088623047\n",
      "Surface training t=5089, loss=0.3291769474744797\n",
      "Surface training t=5090, loss=0.32721956074237823\n",
      "Surface training t=5091, loss=0.31210191547870636\n",
      "Surface training t=5092, loss=0.3378921151161194\n",
      "Surface training t=5093, loss=0.35449180006980896\n",
      "Surface training t=5094, loss=0.30053049325942993\n",
      "Surface training t=5095, loss=0.342450350522995\n",
      "Surface training t=5096, loss=0.3298911303281784\n",
      "Surface training t=5097, loss=0.30693112313747406\n",
      "Surface training t=5098, loss=0.3775996118783951\n",
      "Surface training t=5099, loss=0.3450537472963333\n",
      "Surface training t=5100, loss=0.36946484446525574\n",
      "Surface training t=5101, loss=0.315378338098526\n",
      "Surface training t=5102, loss=0.3302224278450012\n",
      "Surface training t=5103, loss=0.3657698333263397\n",
      "Surface training t=5104, loss=0.3062877953052521\n",
      "Surface training t=5105, loss=0.31825053691864014\n",
      "Surface training t=5106, loss=0.3210266977548599\n",
      "Surface training t=5107, loss=0.374261274933815\n",
      "Surface training t=5108, loss=0.29082677513360977\n",
      "Surface training t=5109, loss=0.3620336949825287\n",
      "Surface training t=5110, loss=0.3069612979888916\n",
      "Surface training t=5111, loss=0.33348309993743896\n",
      "Surface training t=5112, loss=0.35028955340385437\n",
      "Surface training t=5113, loss=0.33586812019348145\n",
      "Surface training t=5114, loss=0.3450666218996048\n",
      "Surface training t=5115, loss=0.3159158527851105\n",
      "Surface training t=5116, loss=0.3435831069946289\n",
      "Surface training t=5117, loss=0.3492547571659088\n",
      "Surface training t=5118, loss=0.33061981201171875\n",
      "Surface training t=5119, loss=0.2839278429746628\n",
      "Surface training t=5120, loss=0.32116320729255676\n",
      "Surface training t=5121, loss=0.3218190521001816\n",
      "Surface training t=5122, loss=0.3376515805721283\n",
      "Surface training t=5123, loss=0.3453013598918915\n",
      "Surface training t=5124, loss=0.30270346999168396\n",
      "Surface training t=5125, loss=0.2980186343193054\n",
      "Surface training t=5126, loss=0.3301408588886261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=5127, loss=0.31235918402671814\n",
      "Surface training t=5128, loss=0.2967495620250702\n",
      "Surface training t=5129, loss=0.3284297585487366\n",
      "Surface training t=5130, loss=0.32267408072948456\n",
      "Surface training t=5131, loss=0.3147778809070587\n",
      "Surface training t=5132, loss=0.3006099611520767\n",
      "Surface training t=5133, loss=0.3322066217660904\n",
      "Surface training t=5134, loss=0.3064575344324112\n",
      "Surface training t=5135, loss=0.32586970925331116\n",
      "Surface training t=5136, loss=0.3398517370223999\n",
      "Surface training t=5137, loss=0.3024972528219223\n",
      "Surface training t=5138, loss=0.3421929031610489\n",
      "Surface training t=5139, loss=0.37816308438777924\n",
      "Surface training t=5140, loss=0.33107152581214905\n",
      "Surface training t=5141, loss=0.3079499751329422\n",
      "Surface training t=5142, loss=0.33051906526088715\n",
      "Surface training t=5143, loss=0.31433480978012085\n",
      "Surface training t=5144, loss=0.37207408249378204\n",
      "Surface training t=5145, loss=0.3656528890132904\n",
      "Surface training t=5146, loss=0.28585386276245117\n",
      "Surface training t=5147, loss=0.3087482899427414\n",
      "Surface training t=5148, loss=0.3339806944131851\n",
      "Surface training t=5149, loss=0.2772737741470337\n",
      "Surface training t=5150, loss=0.3330953121185303\n",
      "Surface training t=5151, loss=0.3201081156730652\n",
      "Surface training t=5152, loss=0.31957970559597015\n",
      "Surface training t=5153, loss=0.3925648629665375\n",
      "Surface training t=5154, loss=0.322244793176651\n",
      "Surface training t=5155, loss=0.3733319640159607\n",
      "Surface training t=5156, loss=0.33869417011737823\n",
      "Surface training t=5157, loss=0.35153207182884216\n",
      "Surface training t=5158, loss=0.36017192900180817\n",
      "Surface training t=5159, loss=0.35140009224414825\n",
      "Surface training t=5160, loss=0.3221583664417267\n",
      "Surface training t=5161, loss=0.32649077475070953\n",
      "Surface training t=5162, loss=0.3200366944074631\n",
      "Surface training t=5163, loss=0.3658405840396881\n",
      "Surface training t=5164, loss=0.3387824445962906\n",
      "Surface training t=5165, loss=0.35921627283096313\n",
      "Surface training t=5166, loss=0.3437203913927078\n",
      "Surface training t=5167, loss=0.3236856907606125\n",
      "Surface training t=5168, loss=0.3191101849079132\n",
      "Surface training t=5169, loss=0.35517778992652893\n",
      "Surface training t=5170, loss=0.3477075546979904\n",
      "Surface training t=5171, loss=0.33887770771980286\n",
      "Surface training t=5172, loss=0.3186117112636566\n",
      "Surface training t=5173, loss=0.3462630808353424\n",
      "Surface training t=5174, loss=0.3194810450077057\n",
      "Surface training t=5175, loss=0.2945423573255539\n",
      "Surface training t=5176, loss=0.27970387786626816\n",
      "Surface training t=5177, loss=0.2812473252415657\n",
      "Surface training t=5178, loss=0.3395417481660843\n",
      "Surface training t=5179, loss=0.34861965477466583\n",
      "Surface training t=5180, loss=0.3401271849870682\n",
      "Surface training t=5181, loss=0.32704608142375946\n",
      "Surface training t=5182, loss=0.3360053151845932\n",
      "Surface training t=5183, loss=0.3340822756290436\n",
      "Surface training t=5184, loss=0.29851366579532623\n",
      "Surface training t=5185, loss=0.26772399991750717\n",
      "Surface training t=5186, loss=0.2876310497522354\n",
      "Surface training t=5187, loss=0.31692464649677277\n",
      "Surface training t=5188, loss=0.32635755836963654\n",
      "Surface training t=5189, loss=0.3457022160291672\n",
      "Surface training t=5190, loss=0.3070163279771805\n",
      "Surface training t=5191, loss=0.3144734650850296\n",
      "Surface training t=5192, loss=0.29606133699417114\n",
      "Surface training t=5193, loss=0.35274988412857056\n",
      "Surface training t=5194, loss=0.3125286251306534\n",
      "Surface training t=5195, loss=0.36362960934638977\n",
      "Surface training t=5196, loss=0.362822562456131\n",
      "Surface training t=5197, loss=0.28442833572626114\n",
      "Surface training t=5198, loss=0.30875878036022186\n",
      "Surface training t=5199, loss=0.26590678840875626\n",
      "Surface training t=5200, loss=0.2839842289686203\n",
      "Surface training t=5201, loss=0.2611456662416458\n",
      "Surface training t=5202, loss=0.31302520632743835\n",
      "Surface training t=5203, loss=0.31763163208961487\n",
      "Surface training t=5204, loss=0.3118499517440796\n",
      "Surface training t=5205, loss=0.28228698670864105\n",
      "Surface training t=5206, loss=0.3182913213968277\n",
      "Surface training t=5207, loss=0.30041101574897766\n",
      "Surface training t=5208, loss=0.3291539251804352\n",
      "Surface training t=5209, loss=0.2746112644672394\n",
      "Surface training t=5210, loss=0.3271748721599579\n",
      "Surface training t=5211, loss=0.31908018887043\n",
      "Surface training t=5212, loss=0.29389628767967224\n",
      "Surface training t=5213, loss=0.29142312705516815\n",
      "Surface training t=5214, loss=0.2833636701107025\n",
      "Surface training t=5215, loss=0.3139423131942749\n",
      "Surface training t=5216, loss=0.27024422585964203\n",
      "Surface training t=5217, loss=0.3649779409170151\n",
      "Surface training t=5218, loss=0.28366396576166153\n",
      "Surface training t=5219, loss=0.3533956706523895\n",
      "Surface training t=5220, loss=0.289802722632885\n",
      "Surface training t=5221, loss=0.286709263920784\n",
      "Surface training t=5222, loss=0.28641752898693085\n",
      "Surface training t=5223, loss=0.3136758804321289\n",
      "Surface training t=5224, loss=0.36799444258213043\n",
      "Surface training t=5225, loss=0.30525609850883484\n",
      "Surface training t=5226, loss=0.30976687371730804\n",
      "Surface training t=5227, loss=0.2917463481426239\n",
      "Surface training t=5228, loss=0.24422594159841537\n",
      "Surface training t=5229, loss=0.32951758801937103\n",
      "Surface training t=5230, loss=0.2839035242795944\n",
      "Surface training t=5231, loss=0.31081482768058777\n",
      "Surface training t=5232, loss=0.3194231688976288\n",
      "Surface training t=5233, loss=0.29555778205394745\n",
      "Surface training t=5234, loss=0.3080359846353531\n",
      "Surface training t=5235, loss=0.34542316198349\n",
      "Surface training t=5236, loss=0.30305199325084686\n",
      "Surface training t=5237, loss=0.2829519659280777\n",
      "Surface training t=5238, loss=0.31136369705200195\n",
      "Surface training t=5239, loss=0.2844126522541046\n",
      "Surface training t=5240, loss=0.2855081111192703\n",
      "Surface training t=5241, loss=0.31801503896713257\n",
      "Surface training t=5242, loss=0.3354305177927017\n",
      "Surface training t=5243, loss=0.3014400452375412\n",
      "Surface training t=5244, loss=0.28869061172008514\n",
      "Surface training t=5245, loss=0.2764149233698845\n",
      "Surface training t=5246, loss=0.28735628724098206\n",
      "Surface training t=5247, loss=0.3087511658668518\n",
      "Surface training t=5248, loss=0.32195207476615906\n",
      "Surface training t=5249, loss=0.33974602818489075\n",
      "Surface training t=5250, loss=0.33505403995513916\n",
      "Surface training t=5251, loss=0.3104529082775116\n",
      "Surface training t=5252, loss=0.2795489579439163\n",
      "Surface training t=5253, loss=0.2762364521622658\n",
      "Surface training t=5254, loss=0.28088851273059845\n",
      "Surface training t=5255, loss=0.3075270652770996\n",
      "Surface training t=5256, loss=0.2796332612633705\n",
      "Surface training t=5257, loss=0.2957819700241089\n",
      "Surface training t=5258, loss=0.2990708202123642\n",
      "Surface training t=5259, loss=0.32436491549015045\n",
      "Surface training t=5260, loss=0.3032735288143158\n",
      "Surface training t=5261, loss=0.29087167978286743\n",
      "Surface training t=5262, loss=0.2982979863882065\n",
      "Surface training t=5263, loss=0.33239229023456573\n",
      "Surface training t=5264, loss=0.2668542116880417\n",
      "Surface training t=5265, loss=0.295545756816864\n",
      "Surface training t=5266, loss=0.3399112671613693\n",
      "Surface training t=5267, loss=0.3315669745206833\n",
      "Surface training t=5268, loss=0.32599781453609467\n",
      "Surface training t=5269, loss=0.35181064903736115\n",
      "Surface training t=5270, loss=0.28338204324245453\n",
      "Surface training t=5271, loss=0.2855370491743088\n",
      "Surface training t=5272, loss=0.33162136375904083\n",
      "Surface training t=5273, loss=0.34072771668434143\n",
      "Surface training t=5274, loss=0.30892015993595123\n",
      "Surface training t=5275, loss=0.2932841181755066\n",
      "Surface training t=5276, loss=0.30333344638347626\n",
      "Surface training t=5277, loss=0.3431232273578644\n",
      "Surface training t=5278, loss=0.3499206453561783\n",
      "Surface training t=5279, loss=0.27706025540828705\n",
      "Surface training t=5280, loss=0.30452045798301697\n",
      "Surface training t=5281, loss=0.3717424124479294\n",
      "Surface training t=5282, loss=0.3294285088777542\n",
      "Surface training t=5283, loss=0.2780197113752365\n",
      "Surface training t=5284, loss=0.28540097177028656\n",
      "Surface training t=5285, loss=0.3080756813287735\n",
      "Surface training t=5286, loss=0.29741905629634857\n",
      "Surface training t=5287, loss=0.2766420915722847\n",
      "Surface training t=5288, loss=0.30745406448841095\n",
      "Surface training t=5289, loss=0.3178475648164749\n",
      "Surface training t=5290, loss=0.2757110595703125\n",
      "Surface training t=5291, loss=0.2892886996269226\n",
      "Surface training t=5292, loss=0.27124103158712387\n",
      "Surface training t=5293, loss=0.3156990706920624\n",
      "Surface training t=5294, loss=0.30737820267677307\n",
      "Surface training t=5295, loss=0.2827487140893936\n",
      "Surface training t=5296, loss=0.35758647322654724\n",
      "Surface training t=5297, loss=0.3050578236579895\n",
      "Surface training t=5298, loss=0.31275439262390137\n",
      "Surface training t=5299, loss=0.294928178191185\n",
      "Surface training t=5300, loss=0.2899646759033203\n",
      "Surface training t=5301, loss=0.28297969698905945\n",
      "Surface training t=5302, loss=0.24603481590747833\n",
      "Surface training t=5303, loss=0.2800793796777725\n",
      "Surface training t=5304, loss=0.2912147790193558\n",
      "Surface training t=5305, loss=0.30129486322402954\n",
      "Surface training t=5306, loss=0.32453176379203796\n",
      "Surface training t=5307, loss=0.2821941524744034\n",
      "Surface training t=5308, loss=0.2616967260837555\n",
      "Surface training t=5309, loss=0.33688217401504517\n",
      "Surface training t=5310, loss=0.28318412601947784\n",
      "Surface training t=5311, loss=0.27297960221767426\n",
      "Surface training t=5312, loss=0.310537189245224\n",
      "Surface training t=5313, loss=0.28830520808696747\n",
      "Surface training t=5314, loss=0.2757919430732727\n",
      "Surface training t=5315, loss=0.3154882937669754\n",
      "Surface training t=5316, loss=0.29518888890743256\n",
      "Surface training t=5317, loss=0.29190076887607574\n",
      "Surface training t=5318, loss=0.3252597749233246\n",
      "Surface training t=5319, loss=0.2823193520307541\n",
      "Surface training t=5320, loss=0.2899266928434372\n",
      "Surface training t=5321, loss=0.3152923136949539\n",
      "Surface training t=5322, loss=0.3083191365003586\n",
      "Surface training t=5323, loss=0.31524188816547394\n",
      "Surface training t=5324, loss=0.31481538712978363\n",
      "Surface training t=5325, loss=0.34395258128643036\n",
      "Surface training t=5326, loss=0.3631097227334976\n",
      "Surface training t=5327, loss=0.32726915180683136\n",
      "Surface training t=5328, loss=0.3260206878185272\n",
      "Surface training t=5329, loss=0.31497257947921753\n",
      "Surface training t=5330, loss=0.3168669790029526\n",
      "Surface training t=5331, loss=0.30098024010658264\n",
      "Surface training t=5332, loss=0.3021669089794159\n",
      "Surface training t=5333, loss=0.30488647520542145\n",
      "Surface training t=5334, loss=0.3000537008047104\n",
      "Surface training t=5335, loss=0.2714517265558243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=5336, loss=0.3154228627681732\n",
      "Surface training t=5337, loss=0.3168509304523468\n",
      "Surface training t=5338, loss=0.28224068880081177\n",
      "Surface training t=5339, loss=0.2504410520195961\n",
      "Surface training t=5340, loss=0.32608552277088165\n",
      "Surface training t=5341, loss=0.30940762162208557\n",
      "Surface training t=5342, loss=0.27408795058727264\n",
      "Surface training t=5343, loss=0.3019694983959198\n",
      "Surface training t=5344, loss=0.2762930616736412\n",
      "Surface training t=5345, loss=0.31256940960884094\n",
      "Surface training t=5346, loss=0.3233474940061569\n",
      "Surface training t=5347, loss=0.2736572101712227\n",
      "Surface training t=5348, loss=0.3227270543575287\n",
      "Surface training t=5349, loss=0.3103865683078766\n",
      "Surface training t=5350, loss=0.3080551028251648\n",
      "Surface training t=5351, loss=0.26782675832509995\n",
      "Surface training t=5352, loss=0.30565614998340607\n",
      "Surface training t=5353, loss=0.26875555515289307\n",
      "Surface training t=5354, loss=0.31428076326847076\n",
      "Surface training t=5355, loss=0.27720676362514496\n",
      "Surface training t=5356, loss=0.3071555644273758\n",
      "Surface training t=5357, loss=0.2775801569223404\n",
      "Surface training t=5358, loss=0.2764958292245865\n",
      "Surface training t=5359, loss=0.30752623081207275\n",
      "Surface training t=5360, loss=0.2797190397977829\n",
      "Surface training t=5361, loss=0.3154224157333374\n",
      "Surface training t=5362, loss=0.3495301455259323\n",
      "Surface training t=5363, loss=0.31150591373443604\n",
      "Surface training t=5364, loss=0.3151181489229202\n",
      "Surface training t=5365, loss=0.29149240255355835\n",
      "Surface training t=5366, loss=0.2884548604488373\n",
      "Surface training t=5367, loss=0.30504658818244934\n",
      "Surface training t=5368, loss=0.3349509686231613\n",
      "Surface training t=5369, loss=0.27229519188404083\n",
      "Surface training t=5370, loss=0.30262263119220734\n",
      "Surface training t=5371, loss=0.33740057051181793\n",
      "Surface training t=5372, loss=0.311432808637619\n",
      "Surface training t=5373, loss=0.32609376311302185\n",
      "Surface training t=5374, loss=0.3384900391101837\n",
      "Surface training t=5375, loss=0.32255004346370697\n",
      "Surface training t=5376, loss=0.25786731392145157\n",
      "Surface training t=5377, loss=0.31840313971042633\n",
      "Surface training t=5378, loss=0.2858589142560959\n",
      "Surface training t=5379, loss=0.3159802407026291\n",
      "Surface training t=5380, loss=0.2520401254296303\n",
      "Surface training t=5381, loss=0.2748873308300972\n",
      "Surface training t=5382, loss=0.2965017408132553\n",
      "Surface training t=5383, loss=0.3010137528181076\n",
      "Surface training t=5384, loss=0.3055528849363327\n",
      "Surface training t=5385, loss=0.28253479301929474\n",
      "Surface training t=5386, loss=0.32275134325027466\n",
      "Surface training t=5387, loss=0.2955995798110962\n",
      "Surface training t=5388, loss=0.3002787083387375\n",
      "Surface training t=5389, loss=0.2856736183166504\n",
      "Surface training t=5390, loss=0.25656118243932724\n",
      "Surface training t=5391, loss=0.2957989573478699\n",
      "Surface training t=5392, loss=0.26488954573869705\n",
      "Surface training t=5393, loss=0.27138949930667877\n",
      "Surface training t=5394, loss=0.24850346148014069\n",
      "Surface training t=5395, loss=0.2515455558896065\n",
      "Surface training t=5396, loss=0.25767311453819275\n",
      "Surface training t=5397, loss=0.2536236643791199\n",
      "Surface training t=5398, loss=0.28068122267723083\n",
      "Surface training t=5399, loss=0.3079421669244766\n",
      "Surface training t=5400, loss=0.3072337508201599\n",
      "Surface training t=5401, loss=0.31400585174560547\n",
      "Surface training t=5402, loss=0.3057602792978287\n",
      "Surface training t=5403, loss=0.27873337268829346\n",
      "Surface training t=5404, loss=0.26237376034259796\n",
      "Surface training t=5405, loss=0.33534419536590576\n",
      "Surface training t=5406, loss=0.28370876610279083\n",
      "Surface training t=5407, loss=0.2611387372016907\n",
      "Surface training t=5408, loss=0.26614295691251755\n",
      "Surface training t=5409, loss=0.32271914184093475\n",
      "Surface training t=5410, loss=0.2885916531085968\n",
      "Surface training t=5411, loss=0.3603465110063553\n",
      "Surface training t=5412, loss=0.3324859291315079\n",
      "Surface training t=5413, loss=0.3189411461353302\n",
      "Surface training t=5414, loss=0.309375524520874\n",
      "Surface training t=5415, loss=0.26862654834985733\n",
      "Surface training t=5416, loss=0.29588085412979126\n",
      "Surface training t=5417, loss=0.29247719049453735\n",
      "Surface training t=5418, loss=0.2808109223842621\n",
      "Surface training t=5419, loss=0.3205348700284958\n",
      "Surface training t=5420, loss=0.25902434438467026\n",
      "Surface training t=5421, loss=0.3177907466888428\n",
      "Surface training t=5422, loss=0.2786378115415573\n",
      "Surface training t=5423, loss=0.3107255846261978\n",
      "Surface training t=5424, loss=0.2891990542411804\n",
      "Surface training t=5425, loss=0.287076935172081\n",
      "Surface training t=5426, loss=0.30963917076587677\n",
      "Surface training t=5427, loss=0.27823111414909363\n",
      "Surface training t=5428, loss=0.2507066875696182\n",
      "Surface training t=5429, loss=0.2834023982286453\n",
      "Surface training t=5430, loss=0.2939326763153076\n",
      "Surface training t=5431, loss=0.27959662675857544\n",
      "Surface training t=5432, loss=0.2889847755432129\n",
      "Surface training t=5433, loss=0.24871979653835297\n",
      "Surface training t=5434, loss=0.27621908485889435\n",
      "Surface training t=5435, loss=0.24632088094949722\n",
      "Surface training t=5436, loss=0.26472024619579315\n",
      "Surface training t=5437, loss=0.2506232485175133\n",
      "Surface training t=5438, loss=0.3039991110563278\n",
      "Surface training t=5439, loss=0.33138407766819\n",
      "Surface training t=5440, loss=0.3109571635723114\n",
      "Surface training t=5441, loss=0.25504396110773087\n",
      "Surface training t=5442, loss=0.299990713596344\n",
      "Surface training t=5443, loss=0.3032565414905548\n",
      "Surface training t=5444, loss=0.2612878829240799\n",
      "Surface training t=5445, loss=0.26880841702222824\n",
      "Surface training t=5446, loss=0.26142919808626175\n",
      "Surface training t=5447, loss=0.27344919741153717\n",
      "Surface training t=5448, loss=0.27583011984825134\n",
      "Surface training t=5449, loss=0.30835461616516113\n",
      "Surface training t=5450, loss=0.2793858051300049\n",
      "Surface training t=5451, loss=0.2662171274423599\n",
      "Surface training t=5452, loss=0.2994857579469681\n",
      "Surface training t=5453, loss=0.2735386937856674\n",
      "Surface training t=5454, loss=0.2794640064239502\n",
      "Surface training t=5455, loss=0.3171505928039551\n",
      "Surface training t=5456, loss=0.3002983033657074\n",
      "Surface training t=5457, loss=0.3573961853981018\n",
      "Surface training t=5458, loss=0.3292344659566879\n",
      "Surface training t=5459, loss=0.2958628684282303\n",
      "Surface training t=5460, loss=0.31247176229953766\n",
      "Surface training t=5461, loss=0.27524353563785553\n",
      "Surface training t=5462, loss=0.2796356752514839\n",
      "Surface training t=5463, loss=0.2971673756837845\n",
      "Surface training t=5464, loss=0.3098434954881668\n",
      "Surface training t=5465, loss=0.32059985399246216\n",
      "Surface training t=5466, loss=0.2761843055486679\n",
      "Surface training t=5467, loss=0.30048567056655884\n",
      "Surface training t=5468, loss=0.2842482030391693\n",
      "Surface training t=5469, loss=0.3106119930744171\n",
      "Surface training t=5470, loss=0.28260834515094757\n",
      "Surface training t=5471, loss=0.26915451139211655\n",
      "Surface training t=5472, loss=0.29749737679958344\n",
      "Surface training t=5473, loss=0.2910134345293045\n",
      "Surface training t=5474, loss=0.305224746465683\n",
      "Surface training t=5475, loss=0.3019418865442276\n",
      "Surface training t=5476, loss=0.28075218200683594\n",
      "Surface training t=5477, loss=0.30019836127758026\n",
      "Surface training t=5478, loss=0.3820078372955322\n",
      "Surface training t=5479, loss=0.2732817307114601\n",
      "Surface training t=5480, loss=0.3466426134109497\n",
      "Surface training t=5481, loss=0.3124508112668991\n",
      "Surface training t=5482, loss=0.2845621258020401\n",
      "Surface training t=5483, loss=0.30707378685474396\n",
      "Surface training t=5484, loss=0.29039452970027924\n",
      "Surface training t=5485, loss=0.3199286609888077\n",
      "Surface training t=5486, loss=0.3197631239891052\n",
      "Surface training t=5487, loss=0.27670401334762573\n",
      "Surface training t=5488, loss=0.26168742775917053\n",
      "Surface training t=5489, loss=0.2661653459072113\n",
      "Surface training t=5490, loss=0.2356085404753685\n",
      "Surface training t=5491, loss=0.26918022334575653\n",
      "Surface training t=5492, loss=0.24656807631254196\n",
      "Surface training t=5493, loss=0.32218673825263977\n",
      "Surface training t=5494, loss=0.28741365671157837\n",
      "Surface training t=5495, loss=0.24969105422496796\n",
      "Surface training t=5496, loss=0.31087730824947357\n",
      "Surface training t=5497, loss=0.2790236920118332\n",
      "Surface training t=5498, loss=0.2759190648794174\n",
      "Surface training t=5499, loss=0.28948312997817993\n",
      "Surface training t=5500, loss=0.2858654707670212\n",
      "Surface training t=5501, loss=0.271467462182045\n",
      "Surface training t=5502, loss=0.2934168726205826\n",
      "Surface training t=5503, loss=0.2782324254512787\n",
      "Surface training t=5504, loss=0.27977845072746277\n",
      "Surface training t=5505, loss=0.29194967448711395\n",
      "Surface training t=5506, loss=0.2778228223323822\n",
      "Surface training t=5507, loss=0.263792484998703\n",
      "Surface training t=5508, loss=0.3202380985021591\n",
      "Surface training t=5509, loss=0.2765244245529175\n",
      "Surface training t=5510, loss=0.3174542635679245\n",
      "Surface training t=5511, loss=0.2867610603570938\n",
      "Surface training t=5512, loss=0.27944140136241913\n",
      "Surface training t=5513, loss=0.2963864207267761\n",
      "Surface training t=5514, loss=0.2732263058423996\n",
      "Surface training t=5515, loss=0.25475041568279266\n",
      "Surface training t=5516, loss=0.2956255078315735\n",
      "Surface training t=5517, loss=0.27935677766799927\n",
      "Surface training t=5518, loss=0.2768433094024658\n",
      "Surface training t=5519, loss=0.3137855678796768\n",
      "Surface training t=5520, loss=0.28458085656166077\n",
      "Surface training t=5521, loss=0.32209503650665283\n",
      "Surface training t=5522, loss=0.3039305657148361\n",
      "Surface training t=5523, loss=0.32137535512447357\n",
      "Surface training t=5524, loss=0.29991112649440765\n",
      "Surface training t=5525, loss=0.310974583029747\n",
      "Surface training t=5526, loss=0.2821638733148575\n",
      "Surface training t=5527, loss=0.24271269142627716\n",
      "Surface training t=5528, loss=0.2859947234392166\n",
      "Surface training t=5529, loss=0.26540569216012955\n",
      "Surface training t=5530, loss=0.2941063791513443\n",
      "Surface training t=5531, loss=0.27333129942417145\n",
      "Surface training t=5532, loss=0.26267752051353455\n",
      "Surface training t=5533, loss=0.2423197254538536\n",
      "Surface training t=5534, loss=0.2731754183769226\n",
      "Surface training t=5535, loss=0.2915085256099701\n",
      "Surface training t=5536, loss=0.2604707181453705\n",
      "Surface training t=5537, loss=0.26023754477500916\n",
      "Surface training t=5538, loss=0.29541662335395813\n",
      "Surface training t=5539, loss=0.2839031517505646\n",
      "Surface training t=5540, loss=0.27245618402957916\n",
      "Surface training t=5541, loss=0.24934887886047363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=5542, loss=0.27138882875442505\n",
      "Surface training t=5543, loss=0.2844187170267105\n",
      "Surface training t=5544, loss=0.31356102228164673\n",
      "Surface training t=5545, loss=0.292723685503006\n",
      "Surface training t=5546, loss=0.25311287492513657\n",
      "Surface training t=5547, loss=0.27689386904239655\n",
      "Surface training t=5548, loss=0.232448011636734\n",
      "Surface training t=5549, loss=0.2797639071941376\n",
      "Surface training t=5550, loss=0.28144100308418274\n",
      "Surface training t=5551, loss=0.2877567410469055\n",
      "Surface training t=5552, loss=0.2668849527835846\n",
      "Surface training t=5553, loss=0.2677518427371979\n",
      "Surface training t=5554, loss=0.2960076481103897\n",
      "Surface training t=5555, loss=0.26236240565776825\n",
      "Surface training t=5556, loss=0.294985368847847\n",
      "Surface training t=5557, loss=0.2961774542927742\n",
      "Surface training t=5558, loss=0.24526813626289368\n",
      "Surface training t=5559, loss=0.25140950083732605\n",
      "Surface training t=5560, loss=0.2658272981643677\n",
      "Surface training t=5561, loss=0.2498021125793457\n",
      "Surface training t=5562, loss=0.30401240289211273\n",
      "Surface training t=5563, loss=0.2719496488571167\n",
      "Surface training t=5564, loss=0.25773918628692627\n",
      "Surface training t=5565, loss=0.2737438827753067\n",
      "Surface training t=5566, loss=0.30231404304504395\n",
      "Surface training t=5567, loss=0.2951057404279709\n",
      "Surface training t=5568, loss=0.27608321607112885\n",
      "Surface training t=5569, loss=0.2726520448923111\n",
      "Surface training t=5570, loss=0.2513929158449173\n",
      "Surface training t=5571, loss=0.28595873713493347\n",
      "Surface training t=5572, loss=0.28757618367671967\n",
      "Surface training t=5573, loss=0.24325910210609436\n",
      "Surface training t=5574, loss=0.26000887155532837\n",
      "Surface training t=5575, loss=0.2440582439303398\n",
      "Surface training t=5576, loss=0.2601131945848465\n",
      "Surface training t=5577, loss=0.26218388974666595\n",
      "Surface training t=5578, loss=0.28694024682044983\n",
      "Surface training t=5579, loss=0.29368405044078827\n",
      "Surface training t=5580, loss=0.2851390391588211\n",
      "Surface training t=5581, loss=0.26886405050754547\n",
      "Surface training t=5582, loss=0.27911077439785004\n",
      "Surface training t=5583, loss=0.2663371115922928\n",
      "Surface training t=5584, loss=0.25594988465309143\n",
      "Surface training t=5585, loss=0.21757669746875763\n",
      "Surface training t=5586, loss=0.2722439467906952\n",
      "Surface training t=5587, loss=0.22878342866897583\n",
      "Surface training t=5588, loss=0.2590966448187828\n",
      "Surface training t=5589, loss=0.2731727659702301\n",
      "Surface training t=5590, loss=0.28275375068187714\n",
      "Surface training t=5591, loss=0.27871301770210266\n",
      "Surface training t=5592, loss=0.2857295870780945\n",
      "Surface training t=5593, loss=0.3189517557621002\n",
      "Surface training t=5594, loss=0.28158918023109436\n",
      "Surface training t=5595, loss=0.29484932124614716\n",
      "Surface training t=5596, loss=0.30400918424129486\n",
      "Surface training t=5597, loss=0.2993856221437454\n",
      "Surface training t=5598, loss=0.27200205624103546\n",
      "Surface training t=5599, loss=0.30623704195022583\n",
      "Surface training t=5600, loss=0.26845061779022217\n",
      "Surface training t=5601, loss=0.25718916207551956\n",
      "Surface training t=5602, loss=0.28326085209846497\n",
      "Surface training t=5603, loss=0.30003419518470764\n",
      "Surface training t=5604, loss=0.23991231620311737\n",
      "Surface training t=5605, loss=0.27385224401950836\n",
      "Surface training t=5606, loss=0.2870698869228363\n",
      "Surface training t=5607, loss=0.3018124848604202\n",
      "Surface training t=5608, loss=0.27997085452079773\n",
      "Surface training t=5609, loss=0.3286374658346176\n",
      "Surface training t=5610, loss=0.3184300810098648\n",
      "Surface training t=5611, loss=0.2683255970478058\n",
      "Surface training t=5612, loss=0.271855428814888\n",
      "Surface training t=5613, loss=0.2505941241979599\n",
      "Surface training t=5614, loss=0.24834634363651276\n",
      "Surface training t=5615, loss=0.2617364674806595\n",
      "Surface training t=5616, loss=0.2427818924188614\n",
      "Surface training t=5617, loss=0.25957509875297546\n",
      "Surface training t=5618, loss=0.26846276223659515\n",
      "Surface training t=5619, loss=0.24651837348937988\n",
      "Surface training t=5620, loss=0.2638716995716095\n",
      "Surface training t=5621, loss=0.25387635827064514\n",
      "Surface training t=5622, loss=0.24597052484750748\n",
      "Surface training t=5623, loss=0.25522398948669434\n",
      "Surface training t=5624, loss=0.26750846952199936\n",
      "Surface training t=5625, loss=0.25618021935224533\n",
      "Surface training t=5626, loss=0.2969071567058563\n",
      "Surface training t=5627, loss=0.27289776504039764\n",
      "Surface training t=5628, loss=0.29026858508586884\n",
      "Surface training t=5629, loss=0.2521316111087799\n",
      "Surface training t=5630, loss=0.2752348482608795\n",
      "Surface training t=5631, loss=0.31527090072631836\n",
      "Surface training t=5632, loss=0.2528953030705452\n",
      "Surface training t=5633, loss=0.2850010097026825\n",
      "Surface training t=5634, loss=0.27734095603227615\n",
      "Surface training t=5635, loss=0.2594125419855118\n",
      "Surface training t=5636, loss=0.23811812698841095\n",
      "Surface training t=5637, loss=0.24894623458385468\n",
      "Surface training t=5638, loss=0.2695363312959671\n",
      "Surface training t=5639, loss=0.28653375804424286\n",
      "Surface training t=5640, loss=0.24064621329307556\n",
      "Surface training t=5641, loss=0.24331875145435333\n",
      "Surface training t=5642, loss=0.2547645941376686\n",
      "Surface training t=5643, loss=0.2678392305970192\n",
      "Surface training t=5644, loss=0.2293531596660614\n",
      "Surface training t=5645, loss=0.2438811957836151\n",
      "Surface training t=5646, loss=0.2520901635289192\n",
      "Surface training t=5647, loss=0.2493877336382866\n",
      "Surface training t=5648, loss=0.24569933861494064\n",
      "Surface training t=5649, loss=0.2593766003847122\n",
      "Surface training t=5650, loss=0.2594577670097351\n",
      "Surface training t=5651, loss=0.26512354612350464\n",
      "Surface training t=5652, loss=0.3112800195813179\n",
      "Surface training t=5653, loss=0.23411448299884796\n",
      "Surface training t=5654, loss=0.23659634590148926\n",
      "Surface training t=5655, loss=0.24505725502967834\n",
      "Surface training t=5656, loss=0.22829022258520126\n",
      "Surface training t=5657, loss=0.2425260692834854\n",
      "Surface training t=5658, loss=0.20204700529575348\n",
      "Surface training t=5659, loss=0.24042008817195892\n",
      "Surface training t=5660, loss=0.26664645969867706\n",
      "Surface training t=5661, loss=0.24775190651416779\n",
      "Surface training t=5662, loss=0.24065418541431427\n",
      "Surface training t=5663, loss=0.24357635527849197\n",
      "Surface training t=5664, loss=0.21948809921741486\n",
      "Surface training t=5665, loss=0.24532347917556763\n",
      "Surface training t=5666, loss=0.21521545946598053\n",
      "Surface training t=5667, loss=0.22890952229499817\n",
      "Surface training t=5668, loss=0.255230113863945\n",
      "Surface training t=5669, loss=0.23950286209583282\n",
      "Surface training t=5670, loss=0.24355542659759521\n",
      "Surface training t=5671, loss=0.24598385393619537\n",
      "Surface training t=5672, loss=0.26544634997844696\n",
      "Surface training t=5673, loss=0.2614247798919678\n",
      "Surface training t=5674, loss=0.2677955776453018\n",
      "Surface training t=5675, loss=0.26819784939289093\n",
      "Surface training t=5676, loss=0.31399428844451904\n",
      "Surface training t=5677, loss=0.23841558396816254\n",
      "Surface training t=5678, loss=0.3031114637851715\n",
      "Surface training t=5679, loss=0.2751857340335846\n",
      "Surface training t=5680, loss=0.2820971757173538\n",
      "Surface training t=5681, loss=0.3061434030532837\n",
      "Surface training t=5682, loss=0.2608427107334137\n",
      "Surface training t=5683, loss=0.26679010689258575\n",
      "Surface training t=5684, loss=0.2638688310980797\n",
      "Surface training t=5685, loss=0.3037513941526413\n",
      "Surface training t=5686, loss=0.24148042500019073\n",
      "Surface training t=5687, loss=0.27669382095336914\n",
      "Surface training t=5688, loss=0.2881956100463867\n",
      "Surface training t=5689, loss=0.26133888959884644\n",
      "Surface training t=5690, loss=0.24681229889392853\n",
      "Surface training t=5691, loss=0.27391237020492554\n",
      "Surface training t=5692, loss=0.26092658936977386\n",
      "Surface training t=5693, loss=0.31026582419872284\n",
      "Surface training t=5694, loss=0.2742907702922821\n",
      "Surface training t=5695, loss=0.29612063616514206\n",
      "Surface training t=5696, loss=0.27647264301776886\n",
      "Surface training t=5697, loss=0.2366505116224289\n",
      "Surface training t=5698, loss=0.2805522680282593\n",
      "Surface training t=5699, loss=0.2583799660205841\n",
      "Surface training t=5700, loss=0.28827178478240967\n",
      "Surface training t=5701, loss=0.2948344051837921\n",
      "Surface training t=5702, loss=0.23715005069971085\n",
      "Surface training t=5703, loss=0.24524802714586258\n",
      "Surface training t=5704, loss=0.24866118282079697\n",
      "Surface training t=5705, loss=0.23410479724407196\n",
      "Surface training t=5706, loss=0.2103646621108055\n",
      "Surface training t=5707, loss=0.23618943989276886\n",
      "Surface training t=5708, loss=0.24107129126787186\n",
      "Surface training t=5709, loss=0.2343919649720192\n",
      "Surface training t=5710, loss=0.2182186245918274\n",
      "Surface training t=5711, loss=0.27048371732234955\n",
      "Surface training t=5712, loss=0.23509840667247772\n",
      "Surface training t=5713, loss=0.2676534056663513\n",
      "Surface training t=5714, loss=0.21997057646512985\n",
      "Surface training t=5715, loss=0.22156639397144318\n",
      "Surface training t=5716, loss=0.2582215219736099\n",
      "Surface training t=5717, loss=0.2714954912662506\n",
      "Surface training t=5718, loss=0.24157509207725525\n",
      "Surface training t=5719, loss=0.2728704437613487\n",
      "Surface training t=5720, loss=0.2719624489545822\n",
      "Surface training t=5721, loss=0.28234247863292694\n",
      "Surface training t=5722, loss=0.24687814712524414\n",
      "Surface training t=5723, loss=0.23862841725349426\n",
      "Surface training t=5724, loss=0.2245400846004486\n",
      "Surface training t=5725, loss=0.25974874198436737\n",
      "Surface training t=5726, loss=0.22384586930274963\n",
      "Surface training t=5727, loss=0.23736846446990967\n",
      "Surface training t=5728, loss=0.25716813653707504\n",
      "Surface training t=5729, loss=0.26260292530059814\n",
      "Surface training t=5730, loss=0.24021179229021072\n",
      "Surface training t=5731, loss=0.26733438670635223\n",
      "Surface training t=5732, loss=0.2501276507973671\n",
      "Surface training t=5733, loss=0.2734862193465233\n",
      "Surface training t=5734, loss=0.26853734254837036\n",
      "Surface training t=5735, loss=0.26243139803409576\n",
      "Surface training t=5736, loss=0.2560267448425293\n",
      "Surface training t=5737, loss=0.23318372666835785\n",
      "Surface training t=5738, loss=0.21940502524375916\n",
      "Surface training t=5739, loss=0.22199906408786774\n",
      "Surface training t=5740, loss=0.21395358443260193\n",
      "Surface training t=5741, loss=0.21865315735340118\n",
      "Surface training t=5742, loss=0.2663719803094864\n",
      "Surface training t=5743, loss=0.20194685459136963\n",
      "Surface training t=5744, loss=0.2627224028110504\n",
      "Surface training t=5745, loss=0.2358788549900055\n",
      "Surface training t=5746, loss=0.21225757896900177\n",
      "Surface training t=5747, loss=0.2502650320529938\n",
      "Surface training t=5748, loss=0.26554616540670395\n",
      "Surface training t=5749, loss=0.251275010406971\n",
      "Surface training t=5750, loss=0.2339509353041649\n",
      "Surface training t=5751, loss=0.2591789662837982\n",
      "Surface training t=5752, loss=0.2300756499171257\n",
      "Surface training t=5753, loss=0.2483048439025879\n",
      "Surface training t=5754, loss=0.2508906349539757\n",
      "Surface training t=5755, loss=0.24239613115787506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=5756, loss=0.28181685507297516\n",
      "Surface training t=5757, loss=0.26344049721956253\n",
      "Surface training t=5758, loss=0.29710108041763306\n",
      "Surface training t=5759, loss=0.26785653829574585\n",
      "Surface training t=5760, loss=0.2803667336702347\n",
      "Surface training t=5761, loss=0.2631390243768692\n",
      "Surface training t=5762, loss=0.24363144487142563\n",
      "Surface training t=5763, loss=0.2489042654633522\n",
      "Surface training t=5764, loss=0.23085515946149826\n",
      "Surface training t=5765, loss=0.25608067214488983\n",
      "Surface training t=5766, loss=0.22413869947195053\n",
      "Surface training t=5767, loss=0.25395335257053375\n",
      "Surface training t=5768, loss=0.22599677741527557\n",
      "Surface training t=5769, loss=0.24030011147260666\n",
      "Surface training t=5770, loss=0.24892005324363708\n",
      "Surface training t=5771, loss=0.23929309844970703\n",
      "Surface training t=5772, loss=0.2633812129497528\n",
      "Surface training t=5773, loss=0.24674811959266663\n",
      "Surface training t=5774, loss=0.26250315457582474\n",
      "Surface training t=5775, loss=0.29356078058481216\n",
      "Surface training t=5776, loss=0.213335283100605\n",
      "Surface training t=5777, loss=0.19810620695352554\n",
      "Surface training t=5778, loss=0.24865950644016266\n",
      "Surface training t=5779, loss=0.2092345878481865\n",
      "Surface training t=5780, loss=0.22552520781755447\n",
      "Surface training t=5781, loss=0.2413126528263092\n",
      "Surface training t=5782, loss=0.1924886628985405\n",
      "Surface training t=5783, loss=0.247968889772892\n",
      "Surface training t=5784, loss=0.22858811169862747\n",
      "Surface training t=5785, loss=0.21570511162281036\n",
      "Surface training t=5786, loss=0.22778580337762833\n",
      "Surface training t=5787, loss=0.2542550787329674\n",
      "Surface training t=5788, loss=0.2166137844324112\n",
      "Surface training t=5789, loss=0.2316930741071701\n",
      "Surface training t=5790, loss=0.23551886528730392\n",
      "Surface training t=5791, loss=0.25839152932167053\n",
      "Surface training t=5792, loss=0.24083992093801498\n",
      "Surface training t=5793, loss=0.28645437955856323\n",
      "Surface training t=5794, loss=0.26577866822481155\n",
      "Surface training t=5795, loss=0.223770871758461\n",
      "Surface training t=5796, loss=0.25682928413152695\n",
      "Surface training t=5797, loss=0.2278820425271988\n",
      "Surface training t=5798, loss=0.25040098279714584\n",
      "Surface training t=5799, loss=0.23915944248437881\n",
      "Surface training t=5800, loss=0.25439295917749405\n",
      "Surface training t=5801, loss=0.2066636011004448\n",
      "Surface training t=5802, loss=0.24651185423135757\n",
      "Surface training t=5803, loss=0.2418351024389267\n",
      "Surface training t=5804, loss=0.21879281848669052\n",
      "Surface training t=5805, loss=0.2353498935699463\n",
      "Surface training t=5806, loss=0.2052706554532051\n",
      "Surface training t=5807, loss=0.21674738079309464\n",
      "Surface training t=5808, loss=0.19287899881601334\n",
      "Surface training t=5809, loss=0.2848002016544342\n",
      "Surface training t=5810, loss=0.22335894405841827\n",
      "Surface training t=5811, loss=0.2701161727309227\n",
      "Surface training t=5812, loss=0.300137922167778\n",
      "Surface training t=5813, loss=0.21339377015829086\n",
      "Surface training t=5814, loss=0.236129030585289\n",
      "Surface training t=5815, loss=0.22367924451828003\n",
      "Surface training t=5816, loss=0.23495107889175415\n",
      "Surface training t=5817, loss=0.23953159153461456\n",
      "Surface training t=5818, loss=0.2501211315393448\n",
      "Surface training t=5819, loss=0.21391016989946365\n",
      "Surface training t=5820, loss=0.21211907267570496\n",
      "Surface training t=5821, loss=0.24048027396202087\n",
      "Surface training t=5822, loss=0.2155677229166031\n",
      "Surface training t=5823, loss=0.2605423629283905\n",
      "Surface training t=5824, loss=0.22448408603668213\n",
      "Surface training t=5825, loss=0.21246473491191864\n",
      "Surface training t=5826, loss=0.21469055116176605\n",
      "Surface training t=5827, loss=0.23677664995193481\n",
      "Surface training t=5828, loss=0.20252244174480438\n",
      "Surface training t=5829, loss=0.1930719092488289\n",
      "Surface training t=5830, loss=0.1812382936477661\n",
      "Surface training t=5831, loss=0.24405019730329514\n",
      "Surface training t=5832, loss=0.24805141240358353\n",
      "Surface training t=5833, loss=0.20320583134889603\n",
      "Surface training t=5834, loss=0.2406870573759079\n",
      "Surface training t=5835, loss=0.18721923977136612\n",
      "Surface training t=5836, loss=0.24279669672250748\n",
      "Surface training t=5837, loss=0.2279002144932747\n",
      "Surface training t=5838, loss=0.22156088054180145\n",
      "Surface training t=5839, loss=0.2025519162416458\n",
      "Surface training t=5840, loss=0.23211894929409027\n",
      "Surface training t=5841, loss=0.2430356815457344\n",
      "Surface training t=5842, loss=0.2214917689561844\n",
      "Surface training t=5843, loss=0.21698939055204391\n",
      "Surface training t=5844, loss=0.212008498609066\n",
      "Surface training t=5845, loss=0.1825014129281044\n",
      "Surface training t=5846, loss=0.198260135948658\n",
      "Surface training t=5847, loss=0.2604723125696182\n",
      "Surface training t=5848, loss=0.2000882849097252\n",
      "Surface training t=5849, loss=0.19602368772029877\n",
      "Surface training t=5850, loss=0.2143305167555809\n",
      "Surface training t=5851, loss=0.21612969785928726\n",
      "Surface training t=5852, loss=0.21605177968740463\n",
      "Surface training t=5853, loss=0.19426245242357254\n",
      "Surface training t=5854, loss=0.21745772659778595\n",
      "Surface training t=5855, loss=0.22859733551740646\n",
      "Surface training t=5856, loss=0.23363152891397476\n",
      "Surface training t=5857, loss=0.22192441672086716\n",
      "Surface training t=5858, loss=0.22056526690721512\n",
      "Surface training t=5859, loss=0.20342914760112762\n",
      "Surface training t=5860, loss=0.24089495837688446\n",
      "Surface training t=5861, loss=0.2215849682688713\n",
      "Surface training t=5862, loss=0.23733220994472504\n",
      "Surface training t=5863, loss=0.20904089510440826\n",
      "Surface training t=5864, loss=0.1949523463845253\n",
      "Surface training t=5865, loss=0.2275334596633911\n",
      "Surface training t=5866, loss=0.19182147085666656\n",
      "Surface training t=5867, loss=0.17702482640743256\n",
      "Surface training t=5868, loss=0.22953535616397858\n",
      "Surface training t=5869, loss=0.22019324451684952\n",
      "Surface training t=5870, loss=0.21143320947885513\n",
      "Surface training t=5871, loss=0.16702890023589134\n",
      "Surface training t=5872, loss=0.20923428237438202\n",
      "Surface training t=5873, loss=0.2148767113685608\n",
      "Surface training t=5874, loss=0.24641714990139008\n",
      "Surface training t=5875, loss=0.2057771533727646\n",
      "Surface training t=5876, loss=0.21813170611858368\n",
      "Surface training t=5877, loss=0.20154976099729538\n",
      "Surface training t=5878, loss=0.21760380268096924\n",
      "Surface training t=5879, loss=0.212163545191288\n",
      "Surface training t=5880, loss=0.21061155945062637\n",
      "Surface training t=5881, loss=0.18671277910470963\n",
      "Surface training t=5882, loss=0.19573494046926498\n",
      "Surface training t=5883, loss=0.20265308022499084\n",
      "Surface training t=5884, loss=0.19894618541002274\n",
      "Surface training t=5885, loss=0.18432192504405975\n",
      "Surface training t=5886, loss=0.20718446373939514\n",
      "Surface training t=5887, loss=0.1978747546672821\n",
      "Surface training t=5888, loss=0.20380157977342606\n",
      "Surface training t=5889, loss=0.21617702394723892\n",
      "Surface training t=5890, loss=0.20712065696716309\n",
      "Surface training t=5891, loss=0.23238670080900192\n",
      "Surface training t=5892, loss=0.23626086115837097\n",
      "Surface training t=5893, loss=0.23688852041959763\n",
      "Surface training t=5894, loss=0.20436476916074753\n",
      "Surface training t=5895, loss=0.2080719843506813\n",
      "Surface training t=5896, loss=0.21066336333751678\n",
      "Surface training t=5897, loss=0.22162267565727234\n",
      "Surface training t=5898, loss=0.22503101080656052\n",
      "Surface training t=5899, loss=0.20504780858755112\n",
      "Surface training t=5900, loss=0.2423980012536049\n",
      "Surface training t=5901, loss=0.2361539676785469\n",
      "Surface training t=5902, loss=0.2547820433974266\n",
      "Surface training t=5903, loss=0.23236045241355896\n",
      "Surface training t=5904, loss=0.2464630827307701\n",
      "Surface training t=5905, loss=0.2636684402823448\n",
      "Surface training t=5906, loss=0.22602495551109314\n",
      "Surface training t=5907, loss=0.213431254029274\n",
      "Surface training t=5908, loss=0.21835853904485703\n",
      "Surface training t=5909, loss=0.21528293192386627\n",
      "Surface training t=5910, loss=0.20364543050527573\n",
      "Surface training t=5911, loss=0.21075048297643661\n",
      "Surface training t=5912, loss=0.19746337085962296\n",
      "Surface training t=5913, loss=0.2018422707915306\n",
      "Surface training t=5914, loss=0.19116342812776566\n",
      "Surface training t=5915, loss=0.19917356967926025\n",
      "Surface training t=5916, loss=0.2048327401280403\n",
      "Surface training t=5917, loss=0.23639246821403503\n",
      "Surface training t=5918, loss=0.17846745997667313\n",
      "Surface training t=5919, loss=0.20140960812568665\n",
      "Surface training t=5920, loss=0.20235729217529297\n",
      "Surface training t=5921, loss=0.17983155697584152\n",
      "Surface training t=5922, loss=0.18858501315116882\n",
      "Surface training t=5923, loss=0.24477872252464294\n",
      "Surface training t=5924, loss=0.20495326071977615\n",
      "Surface training t=5925, loss=0.19913718849420547\n",
      "Surface training t=5926, loss=0.19928117841482162\n",
      "Surface training t=5927, loss=0.19102788716554642\n",
      "Surface training t=5928, loss=0.19743959605693817\n",
      "Surface training t=5929, loss=0.2249089628458023\n",
      "Surface training t=5930, loss=0.17489437013864517\n",
      "Surface training t=5931, loss=0.21431419998407364\n",
      "Surface training t=5932, loss=0.2256087362766266\n",
      "Surface training t=5933, loss=0.20973558723926544\n",
      "Surface training t=5934, loss=0.20575639605522156\n",
      "Surface training t=5935, loss=0.18590712547302246\n",
      "Surface training t=5936, loss=0.205512136220932\n",
      "Surface training t=5937, loss=0.197860985994339\n",
      "Surface training t=5938, loss=0.19906559586524963\n",
      "Surface training t=5939, loss=0.20174548774957657\n",
      "Surface training t=5940, loss=0.21821367740631104\n",
      "Surface training t=5941, loss=0.20357928425073624\n",
      "Surface training t=5942, loss=0.21823624521493912\n",
      "Surface training t=5943, loss=0.20445464551448822\n",
      "Surface training t=5944, loss=0.19860658794641495\n",
      "Surface training t=5945, loss=0.23034381866455078\n",
      "Surface training t=5946, loss=0.20527391135692596\n",
      "Surface training t=5947, loss=0.18689145147800446\n",
      "Surface training t=5948, loss=0.19654285162687302\n",
      "Surface training t=5949, loss=0.159416351467371\n",
      "Surface training t=5950, loss=0.21861419081687927\n",
      "Surface training t=5951, loss=0.23171500861644745\n",
      "Surface training t=5952, loss=0.22413267940282822\n",
      "Surface training t=5953, loss=0.1849636659026146\n",
      "Surface training t=5954, loss=0.20860406756401062\n",
      "Surface training t=5955, loss=0.20105641335248947\n",
      "Surface training t=5956, loss=0.18109101802110672\n",
      "Surface training t=5957, loss=0.15256792679429054\n",
      "Surface training t=5958, loss=0.19623523205518723\n",
      "Surface training t=5959, loss=0.20081625878810883\n",
      "Surface training t=5960, loss=0.19822819530963898\n",
      "Surface training t=5961, loss=0.18463262170553207\n",
      "Surface training t=5962, loss=0.20018938928842545\n",
      "Surface training t=5963, loss=0.1989734023809433\n",
      "Surface training t=5964, loss=0.2366163432598114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=5965, loss=0.18715417385101318\n",
      "Surface training t=5966, loss=0.18970079720020294\n",
      "Surface training t=5967, loss=0.20867151021957397\n",
      "Surface training t=5968, loss=0.19249319285154343\n",
      "Surface training t=5969, loss=0.18866565078496933\n",
      "Surface training t=5970, loss=0.18043142557144165\n",
      "Surface training t=5971, loss=0.1899040788412094\n",
      "Surface training t=5972, loss=0.19732214510440826\n",
      "Surface training t=5973, loss=0.20987335592508316\n",
      "Surface training t=5974, loss=0.1936940923333168\n",
      "Surface training t=5975, loss=0.18381508439779282\n",
      "Surface training t=5976, loss=0.20701026171445847\n",
      "Surface training t=5977, loss=0.18307068198919296\n",
      "Surface training t=5978, loss=0.1885962411761284\n",
      "Surface training t=5979, loss=0.1919693499803543\n",
      "Surface training t=5980, loss=0.18375688791275024\n",
      "Surface training t=5981, loss=0.1890314668416977\n",
      "Surface training t=5982, loss=0.20804809033870697\n",
      "Surface training t=5983, loss=0.19155900925397873\n",
      "Surface training t=5984, loss=0.15697254985570908\n",
      "Surface training t=5985, loss=0.16652313619852066\n",
      "Surface training t=5986, loss=0.17939885705709457\n",
      "Surface training t=5987, loss=0.19632162153720856\n",
      "Surface training t=5988, loss=0.21900884062051773\n",
      "Surface training t=5989, loss=0.2639082074165344\n",
      "Surface training t=5990, loss=0.2094082310795784\n",
      "Surface training t=5991, loss=0.21374109387397766\n",
      "Surface training t=5992, loss=0.2354900911450386\n",
      "Surface training t=5993, loss=0.2108624428510666\n",
      "Surface training t=5994, loss=0.23406413197517395\n",
      "Surface training t=5995, loss=0.21394554525613785\n",
      "Surface training t=5996, loss=0.2129553258419037\n",
      "Surface training t=5997, loss=0.18984857201576233\n",
      "Surface training t=5998, loss=0.18797137588262558\n",
      "Surface training t=5999, loss=0.1899837851524353\n",
      "Surface training t=6000, loss=0.19832508265972137\n",
      "Surface training t=6001, loss=0.1865461841225624\n",
      "Surface training t=6002, loss=0.1901335045695305\n",
      "Surface training t=6003, loss=0.21264315396547318\n",
      "Surface training t=6004, loss=0.20572783052921295\n",
      "Surface training t=6005, loss=0.22472195327281952\n",
      "Surface training t=6006, loss=0.19215860962867737\n",
      "Surface training t=6007, loss=0.21991105377674103\n",
      "Surface training t=6008, loss=0.24481336027383804\n",
      "Surface training t=6009, loss=0.21904142200946808\n",
      "Surface training t=6010, loss=0.2228039726614952\n",
      "Surface training t=6011, loss=0.24296022206544876\n",
      "Surface training t=6012, loss=0.19611839950084686\n",
      "Surface training t=6013, loss=0.22378388792276382\n",
      "Surface training t=6014, loss=0.2114352583885193\n",
      "Surface training t=6015, loss=0.18609953671693802\n",
      "Surface training t=6016, loss=0.17513614147901535\n",
      "Surface training t=6017, loss=0.1735362932085991\n",
      "Surface training t=6018, loss=0.19227538257837296\n",
      "Surface training t=6019, loss=0.18912921845912933\n",
      "Surface training t=6020, loss=0.18029196560382843\n",
      "Surface training t=6021, loss=0.1800175905227661\n",
      "Surface training t=6022, loss=0.17112716287374496\n",
      "Surface training t=6023, loss=0.17715828120708466\n",
      "Surface training t=6024, loss=0.17394323647022247\n",
      "Surface training t=6025, loss=0.17342999577522278\n",
      "Surface training t=6026, loss=0.16581465303897858\n",
      "Surface training t=6027, loss=0.16484224051237106\n",
      "Surface training t=6028, loss=0.1843707263469696\n",
      "Surface training t=6029, loss=0.17964500933885574\n",
      "Surface training t=6030, loss=0.18024103343486786\n",
      "Surface training t=6031, loss=0.18149247765541077\n",
      "Surface training t=6032, loss=0.19934312254190445\n",
      "Surface training t=6033, loss=0.16371086239814758\n",
      "Surface training t=6034, loss=0.19118747115135193\n",
      "Surface training t=6035, loss=0.2021971493959427\n",
      "Surface training t=6036, loss=0.15982942283153534\n",
      "Surface training t=6037, loss=0.16306638717651367\n",
      "Surface training t=6038, loss=0.16637106239795685\n",
      "Surface training t=6039, loss=0.1570066139101982\n",
      "Surface training t=6040, loss=0.15793360024690628\n",
      "Surface training t=6041, loss=0.1881508156657219\n",
      "Surface training t=6042, loss=0.15959100425243378\n",
      "Surface training t=6043, loss=0.14585702121257782\n",
      "Surface training t=6044, loss=0.19715115427970886\n",
      "Surface training t=6045, loss=0.1699470803141594\n",
      "Surface training t=6046, loss=0.17874807864427567\n",
      "Surface training t=6047, loss=0.17225198447704315\n",
      "Surface training t=6048, loss=0.16481294482946396\n",
      "Surface training t=6049, loss=0.16333115845918655\n",
      "Surface training t=6050, loss=0.18151257187128067\n",
      "Surface training t=6051, loss=0.15541397035121918\n",
      "Surface training t=6052, loss=0.17766361683607101\n",
      "Surface training t=6053, loss=0.16292842477560043\n",
      "Surface training t=6054, loss=0.1604878082871437\n",
      "Surface training t=6055, loss=0.18467581272125244\n",
      "Surface training t=6056, loss=0.1613655611872673\n",
      "Surface training t=6057, loss=0.18563051521778107\n",
      "Surface training t=6058, loss=0.18722794950008392\n",
      "Surface training t=6059, loss=0.15147150307893753\n",
      "Surface training t=6060, loss=0.17135512828826904\n",
      "Surface training t=6061, loss=0.17242483794689178\n",
      "Surface training t=6062, loss=0.20720868557691574\n",
      "Surface training t=6063, loss=0.15071314573287964\n",
      "Surface training t=6064, loss=0.179229237139225\n",
      "Surface training t=6065, loss=0.18659765273332596\n",
      "Surface training t=6066, loss=0.1646461859345436\n",
      "Surface training t=6067, loss=0.15615691244602203\n",
      "Surface training t=6068, loss=0.1764150708913803\n",
      "Surface training t=6069, loss=0.18830852210521698\n",
      "Surface training t=6070, loss=0.16247264295816422\n",
      "Surface training t=6071, loss=0.16694226115942\n",
      "Surface training t=6072, loss=0.1594299077987671\n",
      "Surface training t=6073, loss=0.19008053839206696\n",
      "Surface training t=6074, loss=0.19994448125362396\n",
      "Surface training t=6075, loss=0.16393209993839264\n",
      "Surface training t=6076, loss=0.17231790721416473\n",
      "Surface training t=6077, loss=0.1891251504421234\n",
      "Surface training t=6078, loss=0.20096160471439362\n",
      "Surface training t=6079, loss=0.1912306323647499\n",
      "Surface training t=6080, loss=0.17138078808784485\n",
      "Surface training t=6081, loss=0.22999047487974167\n",
      "Surface training t=6082, loss=0.18284162878990173\n",
      "Surface training t=6083, loss=0.1756485253572464\n",
      "Surface training t=6084, loss=0.21336089074611664\n",
      "Surface training t=6085, loss=0.19242210686206818\n",
      "Surface training t=6086, loss=0.20155901461839676\n",
      "Surface training t=6087, loss=0.1980149820446968\n",
      "Surface training t=6088, loss=0.20113538205623627\n",
      "Surface training t=6089, loss=0.17224488407373428\n",
      "Surface training t=6090, loss=0.16425103694200516\n",
      "Surface training t=6091, loss=0.15363404154777527\n",
      "Surface training t=6092, loss=0.16860225051641464\n",
      "Surface training t=6093, loss=0.16284148395061493\n",
      "Surface training t=6094, loss=0.16186519712209702\n",
      "Surface training t=6095, loss=0.16134454309940338\n",
      "Surface training t=6096, loss=0.1707853153347969\n",
      "Surface training t=6097, loss=0.17414626479148865\n",
      "Surface training t=6098, loss=0.1597272753715515\n",
      "Surface training t=6099, loss=0.14549122750759125\n",
      "Surface training t=6100, loss=0.15662898123264313\n",
      "Surface training t=6101, loss=0.1611928641796112\n",
      "Surface training t=6102, loss=0.14328592270612717\n",
      "Surface training t=6103, loss=0.15351009368896484\n",
      "Surface training t=6104, loss=0.16349314153194427\n",
      "Surface training t=6105, loss=0.16887948662042618\n",
      "Surface training t=6106, loss=0.1773289069533348\n",
      "Surface training t=6107, loss=0.15109039843082428\n",
      "Surface training t=6108, loss=0.1524820476770401\n",
      "Surface training t=6109, loss=0.15974965691566467\n",
      "Surface training t=6110, loss=0.15048197656869888\n",
      "Surface training t=6111, loss=0.1412685140967369\n",
      "Surface training t=6112, loss=0.15804662555456161\n",
      "Surface training t=6113, loss=0.15471616387367249\n",
      "Surface training t=6114, loss=0.15389209985733032\n",
      "Surface training t=6115, loss=0.13702958077192307\n",
      "Surface training t=6116, loss=0.14338675141334534\n",
      "Surface training t=6117, loss=0.16241881251335144\n",
      "Surface training t=6118, loss=0.17675654590129852\n",
      "Surface training t=6119, loss=0.15096253901720047\n",
      "Surface training t=6120, loss=0.1549517661333084\n",
      "Surface training t=6121, loss=0.13712722808122635\n",
      "Surface training t=6122, loss=0.14682067185640335\n",
      "Surface training t=6123, loss=0.15664712339639664\n",
      "Surface training t=6124, loss=0.18149502575397491\n",
      "Surface training t=6125, loss=0.18715478479862213\n",
      "Surface training t=6126, loss=0.13342046737670898\n",
      "Surface training t=6127, loss=0.15386119484901428\n",
      "Surface training t=6128, loss=0.14574100822210312\n",
      "Surface training t=6129, loss=0.1634814590215683\n",
      "Surface training t=6130, loss=0.15608977526426315\n",
      "Surface training t=6131, loss=0.1797185316681862\n",
      "Surface training t=6132, loss=0.16630209237337112\n",
      "Surface training t=6133, loss=0.17187851667404175\n",
      "Surface training t=6134, loss=0.17876850068569183\n",
      "Surface training t=6135, loss=0.16445063054561615\n",
      "Surface training t=6136, loss=0.15072523802518845\n",
      "Surface training t=6137, loss=0.12960007786750793\n",
      "Surface training t=6138, loss=0.16233934462070465\n",
      "Surface training t=6139, loss=0.17872590571641922\n",
      "Surface training t=6140, loss=0.15210683643817902\n",
      "Surface training t=6141, loss=0.14653483033180237\n",
      "Surface training t=6142, loss=0.16884347051382065\n",
      "Surface training t=6143, loss=0.14243555068969727\n",
      "Surface training t=6144, loss=0.16416127979755402\n",
      "Surface training t=6145, loss=0.14030514657497406\n",
      "Surface training t=6146, loss=0.1545058861374855\n",
      "Surface training t=6147, loss=0.15918942540884018\n",
      "Surface training t=6148, loss=0.13944797590374947\n",
      "Surface training t=6149, loss=0.14602072536945343\n",
      "Surface training t=6150, loss=0.16203810274600983\n",
      "Surface training t=6151, loss=0.16175387054681778\n",
      "Surface training t=6152, loss=0.15407534688711166\n",
      "Surface training t=6153, loss=0.14263132214546204\n",
      "Surface training t=6154, loss=0.16691673547029495\n",
      "Surface training t=6155, loss=0.16779212653636932\n",
      "Surface training t=6156, loss=0.17559605091810226\n",
      "Surface training t=6157, loss=0.1485622227191925\n",
      "Surface training t=6158, loss=0.14536960422992706\n",
      "Surface training t=6159, loss=0.14237608015537262\n",
      "Surface training t=6160, loss=0.16344010084867477\n",
      "Surface training t=6161, loss=0.12204406410455704\n",
      "Surface training t=6162, loss=0.1694810539484024\n",
      "Surface training t=6163, loss=0.1389295756816864\n",
      "Surface training t=6164, loss=0.1396031379699707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=6165, loss=0.14169222116470337\n",
      "Surface training t=6166, loss=0.13859302550554276\n",
      "Surface training t=6167, loss=0.1399461254477501\n",
      "Surface training t=6168, loss=0.13400553166866302\n",
      "Surface training t=6169, loss=0.12007201462984085\n",
      "Surface training t=6170, loss=0.12699957191944122\n",
      "Surface training t=6171, loss=0.13865924626588821\n",
      "Surface training t=6172, loss=0.13573505729436874\n",
      "Surface training t=6173, loss=0.1321863755583763\n",
      "Surface training t=6174, loss=0.12254073843359947\n",
      "Surface training t=6175, loss=0.1411471739411354\n",
      "Surface training t=6176, loss=0.15004030615091324\n",
      "Surface training t=6177, loss=0.14053325355052948\n",
      "Surface training t=6178, loss=0.11494629085063934\n",
      "Surface training t=6179, loss=0.1549944281578064\n",
      "Surface training t=6180, loss=0.17885801196098328\n",
      "Surface training t=6181, loss=0.16520866751670837\n",
      "Surface training t=6182, loss=0.19184453040361404\n",
      "Surface training t=6183, loss=0.1764288693666458\n",
      "Surface training t=6184, loss=0.1491750255227089\n",
      "Surface training t=6185, loss=0.12190937995910645\n",
      "Surface training t=6186, loss=0.13083581626415253\n",
      "Surface training t=6187, loss=0.14355751127004623\n",
      "Surface training t=6188, loss=0.1232408806681633\n",
      "Surface training t=6189, loss=0.11710559204220772\n",
      "Surface training t=6190, loss=0.12678370997309685\n",
      "Surface training t=6191, loss=0.12433802336454391\n",
      "Surface training t=6192, loss=0.12141124531626701\n",
      "Surface training t=6193, loss=0.120465237647295\n",
      "Surface training t=6194, loss=0.14719729870557785\n",
      "Surface training t=6195, loss=0.13248514384031296\n",
      "Surface training t=6196, loss=0.14888101816177368\n",
      "Surface training t=6197, loss=0.12057901918888092\n",
      "Surface training t=6198, loss=0.1280447654426098\n",
      "Surface training t=6199, loss=0.12297289445996284\n",
      "Surface training t=6200, loss=0.1384391114115715\n",
      "Surface training t=6201, loss=0.12703319266438484\n",
      "Surface training t=6202, loss=0.14413084834814072\n",
      "Surface training t=6203, loss=0.12645482644438744\n",
      "Surface training t=6204, loss=0.13918715342879295\n",
      "Surface training t=6205, loss=0.13407820835709572\n",
      "Surface training t=6206, loss=0.10718812048435211\n",
      "Surface training t=6207, loss=0.1310325786471367\n",
      "Surface training t=6208, loss=0.11391391605138779\n",
      "Surface training t=6209, loss=0.13467802107334137\n",
      "Surface training t=6210, loss=0.12296168133616447\n",
      "Surface training t=6211, loss=0.1315583884716034\n",
      "Surface training t=6212, loss=0.1219753623008728\n",
      "Surface training t=6213, loss=0.11360089480876923\n",
      "Surface training t=6214, loss=0.12933935225009918\n",
      "Surface training t=6215, loss=0.14710277318954468\n",
      "Surface training t=6216, loss=0.12784706056118011\n",
      "Surface training t=6217, loss=0.1410617083311081\n",
      "Surface training t=6218, loss=0.13382410630583763\n",
      "Surface training t=6219, loss=0.13338936120271683\n",
      "Surface training t=6220, loss=0.13165835663676262\n",
      "Surface training t=6221, loss=0.15424025058746338\n",
      "Surface training t=6222, loss=0.13924867659807205\n",
      "Surface training t=6223, loss=0.14841028302907944\n",
      "Surface training t=6224, loss=0.12907447293400764\n",
      "Surface training t=6225, loss=0.12379607930779457\n",
      "Surface training t=6226, loss=0.11800078675150871\n",
      "Surface training t=6227, loss=0.11561201885342598\n",
      "Surface training t=6228, loss=0.12451492622494698\n",
      "Surface training t=6229, loss=0.11910755932331085\n",
      "Surface training t=6230, loss=0.13059014454483986\n",
      "Surface training t=6231, loss=0.12060251086950302\n",
      "Surface training t=6232, loss=0.10572578385472298\n",
      "Surface training t=6233, loss=0.12091754376888275\n",
      "Surface training t=6234, loss=0.11369778215885162\n",
      "Surface training t=6235, loss=0.11443190276622772\n",
      "Surface training t=6236, loss=0.12055366858839989\n",
      "Surface training t=6237, loss=0.11703043803572655\n",
      "Surface training t=6238, loss=0.11763092875480652\n",
      "Surface training t=6239, loss=0.09965915977954865\n",
      "Surface training t=6240, loss=0.11305494979023933\n",
      "Surface training t=6241, loss=0.1328577995300293\n",
      "Surface training t=6242, loss=0.12026013061404228\n",
      "Surface training t=6243, loss=0.1278861090540886\n",
      "Surface training t=6244, loss=0.12152859196066856\n",
      "Surface training t=6245, loss=0.1286592110991478\n",
      "Surface training t=6246, loss=0.12697313353419304\n",
      "Surface training t=6247, loss=0.10453671962022781\n",
      "Surface training t=6248, loss=0.0933302640914917\n",
      "Surface training t=6249, loss=0.11511707678437233\n",
      "Surface training t=6250, loss=0.12023818492889404\n",
      "Surface training t=6251, loss=0.1240900494158268\n",
      "Surface training t=6252, loss=0.13776439428329468\n",
      "Surface training t=6253, loss=0.14287276193499565\n",
      "Surface training t=6254, loss=0.17350435256958008\n",
      "Surface training t=6255, loss=0.13657470792531967\n",
      "Surface training t=6256, loss=0.13909109309315681\n",
      "Surface training t=6257, loss=0.1252162680029869\n",
      "Surface training t=6258, loss=0.1272376999258995\n",
      "Surface training t=6259, loss=0.11145465448498726\n",
      "Surface training t=6260, loss=0.10143030807375908\n",
      "Surface training t=6261, loss=0.1240747906267643\n",
      "Surface training t=6262, loss=0.11061912402510643\n",
      "Surface training t=6263, loss=0.1377483829855919\n",
      "Surface training t=6264, loss=0.13634102419018745\n",
      "Surface training t=6265, loss=0.12304287403821945\n",
      "Surface training t=6266, loss=0.12979355454444885\n",
      "Surface training t=6267, loss=0.1109202690422535\n",
      "Surface training t=6268, loss=0.13970839977264404\n",
      "Surface training t=6269, loss=0.11747585982084274\n",
      "Surface training t=6270, loss=0.10327755659818649\n",
      "Surface training t=6271, loss=0.10389763116836548\n",
      "Surface training t=6272, loss=0.09661239385604858\n",
      "Surface training t=6273, loss=0.11208087205886841\n",
      "Surface training t=6274, loss=0.0981617383658886\n",
      "Surface training t=6275, loss=0.10385242849588394\n",
      "Surface training t=6276, loss=0.09203502908349037\n",
      "Surface training t=6277, loss=0.10476534441113472\n",
      "Surface training t=6278, loss=0.11347486451268196\n",
      "Surface training t=6279, loss=0.11326542124152184\n",
      "Surface training t=6280, loss=0.10453790798783302\n",
      "Surface training t=6281, loss=0.09437520802021027\n",
      "Surface training t=6282, loss=0.0863831415772438\n",
      "Surface training t=6283, loss=0.08988593146204948\n",
      "Surface training t=6284, loss=0.09068583324551582\n",
      "Surface training t=6285, loss=0.10034262016415596\n",
      "Surface training t=6286, loss=0.09455830976366997\n",
      "Surface training t=6287, loss=0.12465077638626099\n",
      "Surface training t=6288, loss=0.12496170774102211\n",
      "Surface training t=6289, loss=0.11900858581066132\n",
      "Surface training t=6290, loss=0.11423804238438606\n",
      "Surface training t=6291, loss=0.10650434345006943\n",
      "Surface training t=6292, loss=0.11141633242368698\n",
      "Surface training t=6293, loss=0.12299909442663193\n",
      "Surface training t=6294, loss=0.1182461678981781\n",
      "Surface training t=6295, loss=0.11447746306657791\n",
      "Surface training t=6296, loss=0.1317930370569229\n",
      "Surface training t=6297, loss=0.11760444939136505\n",
      "Surface training t=6298, loss=0.12196676433086395\n",
      "Surface training t=6299, loss=0.11334113776683807\n",
      "Surface training t=6300, loss=0.10725197196006775\n",
      "Surface training t=6301, loss=0.09396778792142868\n",
      "Surface training t=6302, loss=0.0939830057322979\n",
      "Surface training t=6303, loss=0.09026018157601357\n",
      "Surface training t=6304, loss=0.08834217488765717\n",
      "Surface training t=6305, loss=0.10659286007285118\n",
      "Surface training t=6306, loss=0.12100407108664513\n",
      "Surface training t=6307, loss=0.12111103534698486\n",
      "Surface training t=6308, loss=0.10403259843587875\n",
      "Surface training t=6309, loss=0.10635790228843689\n",
      "Surface training t=6310, loss=0.10130785033106804\n",
      "Surface training t=6311, loss=0.09622466191649437\n",
      "Surface training t=6312, loss=0.10678102448582649\n",
      "Surface training t=6313, loss=0.09554547071456909\n",
      "Surface training t=6314, loss=0.10622081905603409\n",
      "Surface training t=6315, loss=0.07916982844471931\n",
      "Surface training t=6316, loss=0.07793348282575607\n",
      "Surface training t=6317, loss=0.09242960438132286\n",
      "Surface training t=6318, loss=0.09128560498356819\n",
      "Surface training t=6319, loss=0.10068169608712196\n",
      "Surface training t=6320, loss=0.07660332322120667\n",
      "Surface training t=6321, loss=0.07246863842010498\n",
      "Surface training t=6322, loss=0.07918097078800201\n",
      "Surface training t=6323, loss=0.07605034857988358\n",
      "Surface training t=6324, loss=0.0968756414949894\n",
      "Surface training t=6325, loss=0.09065381810069084\n",
      "Surface training t=6326, loss=0.08447947353124619\n",
      "Surface training t=6327, loss=0.07597498595714569\n",
      "Surface training t=6328, loss=0.0906515009701252\n",
      "Surface training t=6329, loss=0.08572664484381676\n",
      "Surface training t=6330, loss=0.06567506492137909\n",
      "Surface training t=6331, loss=0.08673300594091415\n",
      "Surface training t=6332, loss=0.10262828320264816\n",
      "Surface training t=6333, loss=0.09752796962857246\n",
      "Surface training t=6334, loss=0.1277221292257309\n",
      "Surface training t=6335, loss=0.09834958240389824\n",
      "Surface training t=6336, loss=0.11249848082661629\n",
      "Surface training t=6337, loss=0.1050686165690422\n",
      "Surface training t=6338, loss=0.11630981415510178\n",
      "Surface training t=6339, loss=0.11409225314855576\n",
      "Surface training t=6340, loss=0.1043352372944355\n",
      "Surface training t=6341, loss=0.12699782848358154\n",
      "Surface training t=6342, loss=0.10029268264770508\n",
      "Surface training t=6343, loss=0.11047562584280968\n",
      "Surface training t=6344, loss=0.09878901392221451\n",
      "Surface training t=6345, loss=0.10251739248633385\n",
      "Surface training t=6346, loss=0.07876493781805038\n",
      "Surface training t=6347, loss=0.09130226448178291\n",
      "Surface training t=6348, loss=0.07238997891545296\n",
      "Surface training t=6349, loss=0.08549825474619865\n",
      "Surface training t=6350, loss=0.09911378845572472\n",
      "Surface training t=6351, loss=0.10694431141018867\n",
      "Surface training t=6352, loss=0.08157498762011528\n",
      "Surface training t=6353, loss=0.08176742121577263\n",
      "Surface training t=6354, loss=0.08766279369592667\n",
      "Surface training t=6355, loss=0.10712090134620667\n",
      "Surface training t=6356, loss=0.10354938730597496\n",
      "Surface training t=6357, loss=0.08696227893233299\n",
      "Surface training t=6358, loss=0.11288385838270187\n",
      "Surface training t=6359, loss=0.10236290842294693\n",
      "Surface training t=6360, loss=0.08847342059016228\n",
      "Surface training t=6361, loss=0.10741666331887245\n",
      "Surface training t=6362, loss=0.09909989312291145\n",
      "Surface training t=6363, loss=0.09912761300802231\n",
      "Surface training t=6364, loss=0.09780840575695038\n",
      "Surface training t=6365, loss=0.07055509462952614\n",
      "Surface training t=6366, loss=0.07503162696957588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=6367, loss=0.0787622518837452\n",
      "Surface training t=6368, loss=0.06521524488925934\n",
      "Surface training t=6369, loss=0.07176929339766502\n",
      "Surface training t=6370, loss=0.07974477857351303\n",
      "Surface training t=6371, loss=0.07878698408603668\n",
      "Surface training t=6372, loss=0.09860457479953766\n",
      "Surface training t=6373, loss=0.10441012680530548\n",
      "Surface training t=6374, loss=0.11296579614281654\n",
      "Surface training t=6375, loss=0.12415020167827606\n",
      "Surface training t=6376, loss=0.09233628585934639\n",
      "Surface training t=6377, loss=0.10202889889478683\n",
      "Surface training t=6378, loss=0.09441499039530754\n",
      "Surface training t=6379, loss=0.08965088799595833\n",
      "Surface training t=6380, loss=0.09817822650074959\n",
      "Surface training t=6381, loss=0.08105646818876266\n",
      "Surface training t=6382, loss=0.07835578918457031\n",
      "Surface training t=6383, loss=0.09217649698257446\n",
      "Surface training t=6384, loss=0.10457371547818184\n",
      "Surface training t=6385, loss=0.08166946284472942\n",
      "Surface training t=6386, loss=0.08332228660583496\n",
      "Surface training t=6387, loss=0.06654095277190208\n",
      "Surface training t=6388, loss=0.0936763733625412\n",
      "Surface training t=6389, loss=0.07569503411650658\n",
      "Surface training t=6390, loss=0.07963017001748085\n",
      "Surface training t=6391, loss=0.05592386610805988\n",
      "Surface training t=6392, loss=0.0627307128161192\n",
      "Surface training t=6393, loss=0.06419874727725983\n",
      "Surface training t=6394, loss=0.07674234360456467\n",
      "Surface training t=6395, loss=0.0803149975836277\n",
      "Surface training t=6396, loss=0.08464843407273293\n",
      "Surface training t=6397, loss=0.07820525951683521\n",
      "Surface training t=6398, loss=0.08035193383693695\n",
      "Surface training t=6399, loss=0.0773887112736702\n",
      "Surface training t=6400, loss=0.0755675844848156\n",
      "Surface training t=6401, loss=0.07849320024251938\n",
      "Surface training t=6402, loss=0.08617449179291725\n",
      "Surface training t=6403, loss=0.07228778302669525\n",
      "Surface training t=6404, loss=0.06187979131937027\n",
      "Surface training t=6405, loss=0.09768931567668915\n",
      "Surface training t=6406, loss=0.07189011201262474\n",
      "Surface training t=6407, loss=0.07545600458979607\n",
      "Surface training t=6408, loss=0.08969200402498245\n",
      "Surface training t=6409, loss=0.10961342602968216\n",
      "Surface training t=6410, loss=0.095860056579113\n",
      "Surface training t=6411, loss=0.09085974469780922\n",
      "Surface training t=6412, loss=0.09289279580116272\n",
      "Surface training t=6413, loss=0.1143757775425911\n",
      "Surface training t=6414, loss=0.13264401629567146\n",
      "Surface training t=6415, loss=0.10964342951774597\n",
      "Surface training t=6416, loss=0.16619138419628143\n",
      "Surface training t=6417, loss=0.13546491414308548\n",
      "Surface training t=6418, loss=0.1067240834236145\n",
      "Surface training t=6419, loss=0.1203572042286396\n",
      "Surface training t=6420, loss=0.16365526616573334\n",
      "Surface training t=6421, loss=0.12100750580430031\n",
      "Surface training t=6422, loss=0.1517399437725544\n",
      "Surface training t=6423, loss=0.18564745038747787\n",
      "Surface training t=6424, loss=0.12331707403063774\n",
      "Surface training t=6425, loss=0.14881723374128342\n",
      "Surface training t=6426, loss=0.09361382573843002\n",
      "Surface training t=6427, loss=0.07340855151414871\n",
      "Surface training t=6428, loss=0.07699993066489697\n",
      "Surface training t=6429, loss=0.06195247545838356\n",
      "Surface training t=6430, loss=0.06633442267775536\n",
      "Surface training t=6431, loss=0.06322388350963593\n",
      "Surface training t=6432, loss=0.05148257315158844\n",
      "Surface training t=6433, loss=0.062096353620290756\n",
      "Surface training t=6434, loss=0.06840730831027031\n",
      "Surface training t=6435, loss=0.10372085124254227\n",
      "Surface training t=6436, loss=0.0867747962474823\n",
      "Surface training t=6437, loss=0.09552576020359993\n",
      "Surface training t=6438, loss=0.08368458598852158\n",
      "Surface training t=6439, loss=0.08645020052790642\n",
      "Surface training t=6440, loss=0.06976744532585144\n",
      "Surface training t=6441, loss=0.055551644414663315\n",
      "Surface training t=6442, loss=0.07214463874697685\n",
      "Surface training t=6443, loss=0.06237412802875042\n",
      "Surface training t=6444, loss=0.061785656958818436\n",
      "Surface training t=6445, loss=0.06200643256306648\n",
      "Surface training t=6446, loss=0.07610954344272614\n",
      "Surface training t=6447, loss=0.08186917752027512\n",
      "Surface training t=6448, loss=0.08141817152500153\n",
      "Surface training t=6449, loss=0.07421938329935074\n",
      "Surface training t=6450, loss=0.07476048730313778\n",
      "Surface training t=6451, loss=0.1144334003329277\n",
      "Surface training t=6452, loss=0.10395936295390129\n",
      "Surface training t=6453, loss=0.09622088074684143\n",
      "Surface training t=6454, loss=0.09895464777946472\n",
      "Surface training t=6455, loss=0.08720838278532028\n",
      "Surface training t=6456, loss=0.08515366166830063\n",
      "Surface training t=6457, loss=0.08122950792312622\n",
      "Surface training t=6458, loss=0.07286781072616577\n",
      "Surface training t=6459, loss=0.06892695277929306\n",
      "Surface training t=6460, loss=0.07332360371947289\n",
      "Surface training t=6461, loss=0.09571387246251106\n",
      "Surface training t=6462, loss=0.08217081055045128\n",
      "Surface training t=6463, loss=0.08011249266564846\n",
      "Surface training t=6464, loss=0.11127601936459541\n",
      "Surface training t=6465, loss=0.09425710514187813\n",
      "Surface training t=6466, loss=0.07409397885203362\n",
      "Surface training t=6467, loss=0.06567266210913658\n",
      "Surface training t=6468, loss=0.07982943579554558\n",
      "Surface training t=6469, loss=0.062171487137675285\n",
      "Surface training t=6470, loss=0.06902001611888409\n",
      "Surface training t=6471, loss=0.06854667514562607\n",
      "Surface training t=6472, loss=0.08863543346524239\n",
      "Surface training t=6473, loss=0.11466338112950325\n",
      "Surface training t=6474, loss=0.12136582657694817\n",
      "Surface training t=6475, loss=0.09994084015488625\n",
      "Surface training t=6476, loss=0.08348259329795837\n",
      "Surface training t=6477, loss=0.06651759333908558\n",
      "Surface training t=6478, loss=0.07047591544687748\n",
      "Surface training t=6479, loss=0.06782716326415539\n",
      "Surface training t=6480, loss=0.0570925697684288\n",
      "Surface training t=6481, loss=0.06300196424126625\n",
      "Surface training t=6482, loss=0.10611993446946144\n",
      "Surface training t=6483, loss=0.08197895437479019\n",
      "Surface training t=6484, loss=0.0784884188324213\n",
      "Surface training t=6485, loss=0.08397635444998741\n",
      "Surface training t=6486, loss=0.07298005186021328\n",
      "Surface training t=6487, loss=0.1377154290676117\n",
      "Surface training t=6488, loss=0.10218369588255882\n",
      "Surface training t=6489, loss=0.08158302493393421\n",
      "Surface training t=6490, loss=0.11488358303904533\n",
      "Surface training t=6491, loss=0.09577455371618271\n",
      "Surface training t=6492, loss=0.08665335550904274\n",
      "Surface training t=6493, loss=0.0856105238199234\n",
      "Surface training t=6494, loss=0.11165454238653183\n",
      "Surface training t=6495, loss=0.1027168482542038\n",
      "Surface training t=6496, loss=0.08369562774896622\n",
      "Surface training t=6497, loss=0.08363272622227669\n",
      "Surface training t=6498, loss=0.08234923705458641\n",
      "Surface training t=6499, loss=0.09276967868208885\n",
      "Surface training t=6500, loss=0.11607419699430466\n",
      "Surface training t=6501, loss=0.0945208165794611\n",
      "Surface training t=6502, loss=0.11772848665714264\n",
      "Surface training t=6503, loss=0.14818432182073593\n",
      "Surface training t=6504, loss=0.11674736812710762\n",
      "Surface training t=6505, loss=0.102546326816082\n",
      "Surface training t=6506, loss=0.1647746041417122\n",
      "Surface training t=6507, loss=0.13041241094470024\n",
      "Surface training t=6508, loss=0.18591656535863876\n",
      "Surface training t=6509, loss=0.10697466135025024\n",
      "Surface training t=6510, loss=0.07909291982650757\n",
      "Surface training t=6511, loss=0.1093275137245655\n",
      "Surface training t=6512, loss=0.13524342328310013\n",
      "Surface training t=6513, loss=0.09597013518214226\n",
      "Surface training t=6514, loss=0.11247427761554718\n",
      "Surface training t=6515, loss=0.15344467759132385\n",
      "Surface training t=6516, loss=0.11471601948142052\n",
      "Surface training t=6517, loss=0.09725265018641949\n",
      "Surface training t=6518, loss=0.14606104791164398\n",
      "Surface training t=6519, loss=0.09317990951240063\n",
      "Surface training t=6520, loss=0.11945315822958946\n",
      "Surface training t=6521, loss=0.15265124291181564\n",
      "Surface training t=6522, loss=0.10886674374341965\n",
      "Surface training t=6523, loss=0.1298053115606308\n",
      "Surface training t=6524, loss=0.1630808189511299\n",
      "Surface training t=6525, loss=0.11613558977842331\n",
      "Surface training t=6526, loss=0.13230882585048676\n",
      "Surface training t=6527, loss=0.15212421119213104\n",
      "Surface training t=6528, loss=0.10327658802270889\n",
      "Surface training t=6529, loss=0.11565600708127022\n",
      "Surface training t=6530, loss=0.11410248279571533\n",
      "Surface training t=6531, loss=0.08166379854083061\n",
      "Surface training t=6532, loss=0.0799659714102745\n",
      "Surface training t=6533, loss=0.0975094586610794\n",
      "Surface training t=6534, loss=0.10070207342505455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=6535, loss=0.08263273909687996\n",
      "Surface training t=6536, loss=0.07509677484631538\n",
      "Surface training t=6537, loss=0.06775899045169353\n",
      "Surface training t=6538, loss=0.05697708763182163\n",
      "Surface training t=6539, loss=0.062465643510222435\n",
      "Surface training t=6540, loss=0.06831344403326511\n",
      "Surface training t=6541, loss=0.0691518783569336\n",
      "Surface training t=6542, loss=0.07015266828238964\n",
      "Surface training t=6543, loss=0.07522526755928993\n",
      "Surface training t=6544, loss=0.05927532911300659\n",
      "Surface training t=6545, loss=0.09363263100385666\n",
      "Surface training t=6546, loss=0.06246895156800747\n",
      "Surface training t=6547, loss=0.06279245391488075\n",
      "Surface training t=6548, loss=0.08215402066707611\n",
      "Surface training t=6549, loss=0.0809592455625534\n",
      "Surface training t=6550, loss=0.06643397733569145\n",
      "Surface training t=6551, loss=0.055719420313835144\n",
      "Surface training t=6552, loss=0.08855758607387543\n",
      "Surface training t=6553, loss=0.06214790791273117\n",
      "Surface training t=6554, loss=0.06904836371541023\n",
      "Surface training t=6555, loss=0.05089796334505081\n",
      "Surface training t=6556, loss=0.05204209126532078\n",
      "Surface training t=6557, loss=0.05486822873353958\n",
      "Surface training t=6558, loss=0.0668561551719904\n",
      "Surface training t=6559, loss=0.0586121529340744\n",
      "Surface training t=6560, loss=0.051465338096022606\n",
      "Surface training t=6561, loss=0.05534493923187256\n",
      "Surface training t=6562, loss=0.0570825170725584\n",
      "Surface training t=6563, loss=0.055446427315473557\n",
      "Surface training t=6564, loss=0.08034413307905197\n",
      "Surface training t=6565, loss=0.06508777849376202\n",
      "Surface training t=6566, loss=0.0734159555286169\n",
      "Surface training t=6567, loss=0.053355615586042404\n",
      "Surface training t=6568, loss=0.06437218002974987\n",
      "Surface training t=6569, loss=0.0681879073381424\n",
      "Surface training t=6570, loss=0.0702652782201767\n",
      "Surface training t=6571, loss=0.0571643952280283\n",
      "Surface training t=6572, loss=0.07458909600973129\n",
      "Surface training t=6573, loss=0.10222658514976501\n",
      "Surface training t=6574, loss=0.05696292035281658\n",
      "Surface training t=6575, loss=0.05551867187023163\n",
      "Surface training t=6576, loss=0.058645810931921005\n",
      "Surface training t=6577, loss=0.06412734463810921\n",
      "Surface training t=6578, loss=0.06999019160866737\n",
      "Surface training t=6579, loss=0.06475582346320152\n",
      "Surface training t=6580, loss=0.09652753174304962\n",
      "Surface training t=6581, loss=0.06803357042372227\n",
      "Surface training t=6582, loss=0.06442196294665337\n",
      "Surface training t=6583, loss=0.06810127198696136\n",
      "Surface training t=6584, loss=0.05824914388358593\n",
      "Surface training t=6585, loss=0.0795835442841053\n",
      "Surface training t=6586, loss=0.06004784069955349\n",
      "Surface training t=6587, loss=0.061835478991270065\n",
      "Surface training t=6588, loss=0.06334215961396694\n",
      "Surface training t=6589, loss=0.0726240873336792\n",
      "Surface training t=6590, loss=0.06381619349122047\n",
      "Surface training t=6591, loss=0.06654654629528522\n",
      "Surface training t=6592, loss=0.07114574685692787\n",
      "Surface training t=6593, loss=0.058106496930122375\n",
      "Surface training t=6594, loss=0.06832762062549591\n",
      "Surface training t=6595, loss=0.05980901047587395\n",
      "Surface training t=6596, loss=0.07285495847463608\n",
      "Surface training t=6597, loss=0.04786033183336258\n",
      "Surface training t=6598, loss=0.050922760739922523\n",
      "Surface training t=6599, loss=0.06781729310750961\n",
      "Surface training t=6600, loss=0.05165375396609306\n",
      "Surface training t=6601, loss=0.04343603365123272\n",
      "Surface training t=6602, loss=0.053924936801195145\n",
      "Surface training t=6603, loss=0.05711314454674721\n",
      "Surface training t=6604, loss=0.05293353274464607\n",
      "Surface training t=6605, loss=0.047438645735383034\n",
      "Surface training t=6606, loss=0.052071915939450264\n",
      "Surface training t=6607, loss=0.051457347348332405\n",
      "Surface training t=6608, loss=0.05958336591720581\n",
      "Surface training t=6609, loss=0.048146480694413185\n",
      "Surface training t=6610, loss=0.055865246802568436\n",
      "Surface training t=6611, loss=0.05100358836352825\n",
      "Surface training t=6612, loss=0.04995092377066612\n",
      "Surface training t=6613, loss=0.05079924128949642\n",
      "Surface training t=6614, loss=0.0541802067309618\n",
      "Surface training t=6615, loss=0.04829112999141216\n",
      "Surface training t=6616, loss=0.05890970304608345\n",
      "Surface training t=6617, loss=0.05480232276022434\n",
      "Surface training t=6618, loss=0.04040177911520004\n",
      "Surface training t=6619, loss=0.049373287707567215\n",
      "Surface training t=6620, loss=0.053708143532276154\n",
      "Surface training t=6621, loss=0.05118122510612011\n",
      "Surface training t=6622, loss=0.05158263258635998\n",
      "Surface training t=6623, loss=0.05384488217532635\n",
      "Surface training t=6624, loss=0.05452554300427437\n",
      "Surface training t=6625, loss=0.0676688700914383\n",
      "Surface training t=6626, loss=0.04291494004428387\n",
      "Surface training t=6627, loss=0.05191992036998272\n",
      "Surface training t=6628, loss=0.05634714663028717\n",
      "Surface training t=6629, loss=0.055702462792396545\n",
      "Surface training t=6630, loss=0.05409005470573902\n",
      "Surface training t=6631, loss=0.046520233154296875\n",
      "Surface training t=6632, loss=0.05014405585825443\n",
      "Surface training t=6633, loss=0.0544965248554945\n",
      "Surface training t=6634, loss=0.06268368661403656\n",
      "Surface training t=6635, loss=0.052892837673425674\n",
      "Surface training t=6636, loss=0.0570847075432539\n",
      "Surface training t=6637, loss=0.06834745034575462\n",
      "Surface training t=6638, loss=0.04848978854715824\n",
      "Surface training t=6639, loss=0.0463049728423357\n",
      "Surface training t=6640, loss=0.0677587203681469\n",
      "Surface training t=6641, loss=0.06510374695062637\n",
      "Surface training t=6642, loss=0.05073880776762962\n",
      "Surface training t=6643, loss=0.0532929003238678\n",
      "Surface training t=6644, loss=0.048587899655103683\n",
      "Surface training t=6645, loss=0.04930643551051617\n",
      "Surface training t=6646, loss=0.0789974071085453\n",
      "Surface training t=6647, loss=0.06489753350615501\n",
      "Surface training t=6648, loss=0.06963972188532352\n",
      "Surface training t=6649, loss=0.06416494585573673\n",
      "Surface training t=6650, loss=0.05974812060594559\n",
      "Surface training t=6651, loss=0.0653834380209446\n",
      "Surface training t=6652, loss=0.06202646903693676\n",
      "Surface training t=6653, loss=0.05119881220161915\n",
      "Surface training t=6654, loss=0.056701915338635445\n",
      "Surface training t=6655, loss=0.054695405066013336\n",
      "Surface training t=6656, loss=0.04173432104289532\n",
      "Surface training t=6657, loss=0.0466703437268734\n",
      "Surface training t=6658, loss=0.039677875116467476\n",
      "Surface training t=6659, loss=0.054673340171575546\n",
      "Surface training t=6660, loss=0.0623239129781723\n",
      "Surface training t=6661, loss=0.0873384103178978\n",
      "Surface training t=6662, loss=0.05470678396522999\n",
      "Surface training t=6663, loss=0.06476274877786636\n",
      "Surface training t=6664, loss=0.06705522909760475\n",
      "Surface training t=6665, loss=0.07815146818757057\n",
      "Surface training t=6666, loss=0.07580738887190819\n",
      "Surface training t=6667, loss=0.06868822313845158\n",
      "Surface training t=6668, loss=0.07132414728403091\n",
      "Surface training t=6669, loss=0.04865003935992718\n",
      "Surface training t=6670, loss=0.062249865382909775\n",
      "Surface training t=6671, loss=0.05823610723018646\n",
      "Surface training t=6672, loss=0.06009959056973457\n",
      "Surface training t=6673, loss=0.057332951575517654\n",
      "Surface training t=6674, loss=0.06657889857888222\n",
      "Surface training t=6675, loss=0.106511440128088\n",
      "Surface training t=6676, loss=0.07746599614620209\n",
      "Surface training t=6677, loss=0.0717069935053587\n",
      "Surface training t=6678, loss=0.06362240016460419\n",
      "Surface training t=6679, loss=0.07056612893939018\n",
      "Surface training t=6680, loss=0.07034165598452091\n",
      "Surface training t=6681, loss=0.06777880899608135\n",
      "Surface training t=6682, loss=0.07087123766541481\n",
      "Surface training t=6683, loss=0.08958978578448296\n",
      "Surface training t=6684, loss=0.06539057940244675\n",
      "Surface training t=6685, loss=0.05335737578570843\n",
      "Surface training t=6686, loss=0.06667109206318855\n",
      "Surface training t=6687, loss=0.06048168987035751\n",
      "Surface training t=6688, loss=0.09965814277529716\n",
      "Surface training t=6689, loss=0.08013177663087845\n",
      "Surface training t=6690, loss=0.0759519636631012\n",
      "Surface training t=6691, loss=0.07031269930303097\n",
      "Surface training t=6692, loss=0.06470049917697906\n",
      "Surface training t=6693, loss=0.07526356540620327\n",
      "Surface training t=6694, loss=0.07024072855710983\n",
      "Surface training t=6695, loss=0.0767584852874279\n",
      "Surface training t=6696, loss=0.049207981675863266\n",
      "Surface training t=6697, loss=0.05204688757658005\n",
      "Surface training t=6698, loss=0.04643629305064678\n",
      "Surface training t=6699, loss=0.0486974511295557\n",
      "Surface training t=6700, loss=0.06254588067531586\n",
      "Surface training t=6701, loss=0.06799792312085629\n",
      "Surface training t=6702, loss=0.06339827552437782\n",
      "Surface training t=6703, loss=0.0529327318072319\n",
      "Surface training t=6704, loss=0.06868554279208183\n",
      "Surface training t=6705, loss=0.0683923028409481\n",
      "Surface training t=6706, loss=0.07156149670481682\n",
      "Surface training t=6707, loss=0.06538640521466732\n",
      "Surface training t=6708, loss=0.10515481606125832\n",
      "Surface training t=6709, loss=0.07215394824743271\n",
      "Surface training t=6710, loss=0.060595253482460976\n",
      "Surface training t=6711, loss=0.0618708822876215\n",
      "Surface training t=6712, loss=0.0703611746430397\n",
      "Surface training t=6713, loss=0.06503847241401672\n",
      "Surface training t=6714, loss=0.054217955097556114\n",
      "Surface training t=6715, loss=0.03762657940387726\n",
      "Surface training t=6716, loss=0.042860712856054306\n",
      "Surface training t=6717, loss=0.045344455167651176\n",
      "Surface training t=6718, loss=0.04144933633506298\n",
      "Surface training t=6719, loss=0.037418147549033165\n",
      "Surface training t=6720, loss=0.0385288055986166\n",
      "Surface training t=6721, loss=0.04132726788520813\n",
      "Surface training t=6722, loss=0.04262514412403107\n",
      "Surface training t=6723, loss=0.04335766099393368\n",
      "Surface training t=6724, loss=0.04732424579560757\n",
      "Surface training t=6725, loss=0.04716363176703453\n",
      "Surface training t=6726, loss=0.044431647285819054\n",
      "Surface training t=6727, loss=0.039917778223752975\n",
      "Surface training t=6728, loss=0.049020493403077126\n",
      "Surface training t=6729, loss=0.04077751189470291\n",
      "Surface training t=6730, loss=0.042125701904296875\n",
      "Surface training t=6731, loss=0.053271979093551636\n",
      "Surface training t=6732, loss=0.05297834239900112\n",
      "Surface training t=6733, loss=0.04758075252175331\n",
      "Surface training t=6734, loss=0.04307187721133232\n",
      "Surface training t=6735, loss=0.04182131960988045\n",
      "Surface training t=6736, loss=0.03770568408071995\n",
      "Surface training t=6737, loss=0.033346609212458134\n",
      "Surface training t=6738, loss=0.04013346880674362\n",
      "Surface training t=6739, loss=0.04197762534022331\n",
      "Surface training t=6740, loss=0.04531700909137726\n",
      "Surface training t=6741, loss=0.038737695664167404\n",
      "Surface training t=6742, loss=0.04240168444812298\n",
      "Surface training t=6743, loss=0.038845526054501534\n",
      "Surface training t=6744, loss=0.03815450705587864\n",
      "Surface training t=6745, loss=0.04417004995048046\n",
      "Surface training t=6746, loss=0.04459221288561821\n",
      "Surface training t=6747, loss=0.048390889540314674\n",
      "Surface training t=6748, loss=0.056356893852353096\n",
      "Surface training t=6749, loss=0.05790188908576965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=6750, loss=0.05680453963577747\n",
      "Surface training t=6751, loss=0.05918400548398495\n",
      "Surface training t=6752, loss=0.06044897437095642\n",
      "Surface training t=6753, loss=0.05117589421570301\n",
      "Surface training t=6754, loss=0.04509519226849079\n",
      "Surface training t=6755, loss=0.0514133982360363\n",
      "Surface training t=6756, loss=0.06894635036587715\n",
      "Surface training t=6757, loss=0.0689119566231966\n",
      "Surface training t=6758, loss=0.06560586765408516\n",
      "Surface training t=6759, loss=0.05514034256339073\n",
      "Surface training t=6760, loss=0.05582304298877716\n",
      "Surface training t=6761, loss=0.04877492040395737\n",
      "Surface training t=6762, loss=0.0577834527939558\n",
      "Surface training t=6763, loss=0.047458846122026443\n",
      "Surface training t=6764, loss=0.04555273987352848\n",
      "Surface training t=6765, loss=0.046605708077549934\n",
      "Surface training t=6766, loss=0.04774414375424385\n",
      "Surface training t=6767, loss=0.04327121935784817\n",
      "Surface training t=6768, loss=0.046833792701363564\n",
      "Surface training t=6769, loss=0.039917461574077606\n",
      "Surface training t=6770, loss=0.04767734743654728\n",
      "Surface training t=6771, loss=0.05963779613375664\n",
      "Surface training t=6772, loss=0.04274016432464123\n",
      "Surface training t=6773, loss=0.04640737548470497\n",
      "Surface training t=6774, loss=0.03927728906273842\n",
      "Surface training t=6775, loss=0.035963039845228195\n",
      "Surface training t=6776, loss=0.0374287124723196\n",
      "Surface training t=6777, loss=0.056640833616256714\n",
      "Surface training t=6778, loss=0.0331686045974493\n",
      "Surface training t=6779, loss=0.04376171715557575\n",
      "Surface training t=6780, loss=0.044779032468795776\n",
      "Surface training t=6781, loss=0.03559113293886185\n",
      "Surface training t=6782, loss=0.048077259212732315\n",
      "Surface training t=6783, loss=0.06704745814204216\n",
      "Surface training t=6784, loss=0.05272080563008785\n",
      "Surface training t=6785, loss=0.056473542004823685\n",
      "Surface training t=6786, loss=0.055515106767416\n",
      "Surface training t=6787, loss=0.046075908467173576\n",
      "Surface training t=6788, loss=0.04287556931376457\n",
      "Surface training t=6789, loss=0.0464485976845026\n",
      "Surface training t=6790, loss=0.040877221152186394\n",
      "Surface training t=6791, loss=0.04558095335960388\n",
      "Surface training t=6792, loss=0.054503509774804115\n",
      "Surface training t=6793, loss=0.04613749124109745\n",
      "Surface training t=6794, loss=0.043282195925712585\n",
      "Surface training t=6795, loss=0.04671158269047737\n",
      "Surface training t=6796, loss=0.04226808063685894\n",
      "Surface training t=6797, loss=0.04041628912091255\n",
      "Surface training t=6798, loss=0.037351710721850395\n",
      "Surface training t=6799, loss=0.040903838351368904\n",
      "Surface training t=6800, loss=0.036003848537802696\n",
      "Surface training t=6801, loss=0.04405613988637924\n",
      "Surface training t=6802, loss=0.03661175072193146\n",
      "Surface training t=6803, loss=0.036004478111863136\n",
      "Surface training t=6804, loss=0.04535919986665249\n",
      "Surface training t=6805, loss=0.05008687265217304\n",
      "Surface training t=6806, loss=0.06700899265706539\n",
      "Surface training t=6807, loss=0.05739838071167469\n",
      "Surface training t=6808, loss=0.061685504391789436\n",
      "Surface training t=6809, loss=0.0459253154695034\n",
      "Surface training t=6810, loss=0.06208847276866436\n",
      "Surface training t=6811, loss=0.04403620958328247\n",
      "Surface training t=6812, loss=0.06348088756203651\n",
      "Surface training t=6813, loss=0.044483186677098274\n",
      "Surface training t=6814, loss=0.06398420222103596\n",
      "Surface training t=6815, loss=0.04313371703028679\n",
      "Surface training t=6816, loss=0.044380323961377144\n",
      "Surface training t=6817, loss=0.0652238205075264\n",
      "Surface training t=6818, loss=0.04614422284066677\n",
      "Surface training t=6819, loss=0.05383044481277466\n",
      "Surface training t=6820, loss=0.05161551758646965\n",
      "Surface training t=6821, loss=0.04263623431324959\n",
      "Surface training t=6822, loss=0.05000704154372215\n",
      "Surface training t=6823, loss=0.043023910373449326\n",
      "Surface training t=6824, loss=0.036291858181357384\n",
      "Surface training t=6825, loss=0.047978343442082405\n",
      "Surface training t=6826, loss=0.05056559108197689\n",
      "Surface training t=6827, loss=0.04474089294672012\n",
      "Surface training t=6828, loss=0.0585990734398365\n",
      "Surface training t=6829, loss=0.060390111058950424\n",
      "Surface training t=6830, loss=0.05946025252342224\n",
      "Surface training t=6831, loss=0.080683384090662\n",
      "Surface training t=6832, loss=0.06415949203073978\n",
      "Surface training t=6833, loss=0.08901065215468407\n",
      "Surface training t=6834, loss=0.07837581634521484\n",
      "Surface training t=6835, loss=0.06631389632821083\n",
      "Surface training t=6836, loss=0.05135498382151127\n",
      "Surface training t=6837, loss=0.0667949952185154\n",
      "Surface training t=6838, loss=0.04666006751358509\n",
      "Surface training t=6839, loss=0.05345660820603371\n",
      "Surface training t=6840, loss=0.05863986536860466\n",
      "Surface training t=6841, loss=0.0467084776610136\n",
      "Surface training t=6842, loss=0.043893782421946526\n",
      "Surface training t=6843, loss=0.04446075297892094\n",
      "Surface training t=6844, loss=0.05492132902145386\n",
      "Surface training t=6845, loss=0.048746854066848755\n",
      "Surface training t=6846, loss=0.04601630941033363\n",
      "Surface training t=6847, loss=0.05942690186202526\n",
      "Surface training t=6848, loss=0.07638815604150295\n",
      "Surface training t=6849, loss=0.08046985417604446\n",
      "Surface training t=6850, loss=0.07203604467213154\n",
      "Surface training t=6851, loss=0.07357585616409779\n",
      "Surface training t=6852, loss=0.07306573167443275\n",
      "Surface training t=6853, loss=0.0780811458826065\n",
      "Surface training t=6854, loss=0.058193355798721313\n",
      "Surface training t=6855, loss=0.06467016041278839\n",
      "Surface training t=6856, loss=0.05361099913716316\n",
      "Surface training t=6857, loss=0.05452089384198189\n",
      "Surface training t=6858, loss=0.04865552298724651\n",
      "Surface training t=6859, loss=0.04872431606054306\n",
      "Surface training t=6860, loss=0.04388018324971199\n",
      "Surface training t=6861, loss=0.04414182901382446\n",
      "Surface training t=6862, loss=0.04128102771937847\n",
      "Surface training t=6863, loss=0.047495003789663315\n",
      "Surface training t=6864, loss=0.04968772642314434\n",
      "Surface training t=6865, loss=0.03808435797691345\n",
      "Surface training t=6866, loss=0.04217234253883362\n",
      "Surface training t=6867, loss=0.0469952579587698\n",
      "Surface training t=6868, loss=0.04181386157870293\n",
      "Surface training t=6869, loss=0.05082413926720619\n",
      "Surface training t=6870, loss=0.05454172007739544\n",
      "Surface training t=6871, loss=0.037902144715189934\n",
      "Surface training t=6872, loss=0.03406159020960331\n",
      "Surface training t=6873, loss=0.03251019027084112\n",
      "Surface training t=6874, loss=0.03940955176949501\n",
      "Surface training t=6875, loss=0.04519432969391346\n",
      "Surface training t=6876, loss=0.04573187045753002\n",
      "Surface training t=6877, loss=0.05549873784184456\n",
      "Surface training t=6878, loss=0.04976160079240799\n",
      "Surface training t=6879, loss=0.0461676437407732\n",
      "Surface training t=6880, loss=0.05690077319741249\n",
      "Surface training t=6881, loss=0.04864482581615448\n",
      "Surface training t=6882, loss=0.05327231623232365\n",
      "Surface training t=6883, loss=0.05773543566465378\n",
      "Surface training t=6884, loss=0.04059571586549282\n",
      "Surface training t=6885, loss=0.04647260531783104\n",
      "Surface training t=6886, loss=0.05500786565244198\n",
      "Surface training t=6887, loss=0.04792735539376736\n",
      "Surface training t=6888, loss=0.04920137859880924\n",
      "Surface training t=6889, loss=0.041598519310355186\n",
      "Surface training t=6890, loss=0.06241196021437645\n",
      "Surface training t=6891, loss=0.04092787951231003\n",
      "Surface training t=6892, loss=0.05727571249008179\n",
      "Surface training t=6893, loss=0.08691614121198654\n",
      "Surface training t=6894, loss=0.07121443748474121\n",
      "Surface training t=6895, loss=0.06296319514513016\n",
      "Surface training t=6896, loss=0.04631749354302883\n",
      "Surface training t=6897, loss=0.045278485864400864\n",
      "Surface training t=6898, loss=0.03554150462150574\n",
      "Surface training t=6899, loss=0.04096223786473274\n",
      "Surface training t=6900, loss=0.037617402151227\n",
      "Surface training t=6901, loss=0.04956452362239361\n",
      "Surface training t=6902, loss=0.04314143769443035\n",
      "Surface training t=6903, loss=0.03950625844299793\n",
      "Surface training t=6904, loss=0.03398263268172741\n",
      "Surface training t=6905, loss=0.04276730306446552\n",
      "Surface training t=6906, loss=0.032951321452856064\n",
      "Surface training t=6907, loss=0.03027869574725628\n",
      "Surface training t=6908, loss=0.047850579023361206\n",
      "Surface training t=6909, loss=0.05983016453683376\n",
      "Surface training t=6910, loss=0.07870491221547127\n",
      "Surface training t=6911, loss=0.06842116266489029\n",
      "Surface training t=6912, loss=0.050168320536613464\n",
      "Surface training t=6913, loss=0.05028023570775986\n",
      "Surface training t=6914, loss=0.03714112937450409\n",
      "Surface training t=6915, loss=0.03356854245066643\n",
      "Surface training t=6916, loss=0.041798802092671394\n",
      "Surface training t=6917, loss=0.04278023540973663\n",
      "Surface training t=6918, loss=0.0524508748203516\n",
      "Surface training t=6919, loss=0.05844436399638653\n",
      "Surface training t=6920, loss=0.049088047817349434\n",
      "Surface training t=6921, loss=0.04553530551493168\n",
      "Surface training t=6922, loss=0.05677792429924011\n",
      "Surface training t=6923, loss=0.04421226494014263\n",
      "Surface training t=6924, loss=0.048104315996170044\n",
      "Surface training t=6925, loss=0.038429515436291695\n",
      "Surface training t=6926, loss=0.052937911823391914\n",
      "Surface training t=6927, loss=0.05012564733624458\n",
      "Surface training t=6928, loss=0.03309015650302172\n",
      "Surface training t=6929, loss=0.03798754699528217\n",
      "Surface training t=6930, loss=0.04310588538646698\n",
      "Surface training t=6931, loss=0.04568480886518955\n",
      "Surface training t=6932, loss=0.06350985541939735\n",
      "Surface training t=6933, loss=0.07523258961737156\n",
      "Surface training t=6934, loss=0.0759334247559309\n",
      "Surface training t=6935, loss=0.06735300272703171\n",
      "Surface training t=6936, loss=0.07764855772256851\n",
      "Surface training t=6937, loss=0.11895551532506943\n",
      "Surface training t=6938, loss=0.08301113359630108\n",
      "Surface training t=6939, loss=0.08875121921300888\n",
      "Surface training t=6940, loss=0.14066891372203827\n",
      "Surface training t=6941, loss=0.08922161906957626\n",
      "Surface training t=6942, loss=0.13784771040081978\n",
      "Surface training t=6943, loss=0.1337289810180664\n",
      "Surface training t=6944, loss=0.09451157227158546\n",
      "Surface training t=6945, loss=0.12528903782367706\n",
      "Surface training t=6946, loss=0.08888543397188187\n",
      "Surface training t=6947, loss=0.05664394050836563\n",
      "Surface training t=6948, loss=0.059574030339717865\n",
      "Surface training t=6949, loss=0.05792236328125\n",
      "Surface training t=6950, loss=0.055765120312571526\n",
      "Surface training t=6951, loss=0.061242032796144485\n",
      "Surface training t=6952, loss=0.0437510721385479\n",
      "Surface training t=6953, loss=0.03811192326247692\n",
      "Surface training t=6954, loss=0.05270146392285824\n",
      "Surface training t=6955, loss=0.04345439933240414\n",
      "Surface training t=6956, loss=0.04389476962387562\n",
      "Surface training t=6957, loss=0.03831322863698006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=6958, loss=0.03997540660202503\n",
      "Surface training t=6959, loss=0.034204003401100636\n",
      "Surface training t=6960, loss=0.03698467090725899\n",
      "Surface training t=6961, loss=0.031872011721134186\n",
      "Surface training t=6962, loss=0.03813878819346428\n",
      "Surface training t=6963, loss=0.039637716487050056\n",
      "Surface training t=6964, loss=0.04164249636232853\n",
      "Surface training t=6965, loss=0.03388249687850475\n",
      "Surface training t=6966, loss=0.03253595717251301\n",
      "Surface training t=6967, loss=0.0301474342122674\n",
      "Surface training t=6968, loss=0.033372364938259125\n",
      "Surface training t=6969, loss=0.0355071984231472\n",
      "Surface training t=6970, loss=0.04580157622694969\n",
      "Surface training t=6971, loss=0.05048590898513794\n",
      "Surface training t=6972, loss=0.04722149111330509\n",
      "Surface training t=6973, loss=0.04344245605170727\n",
      "Surface training t=6974, loss=0.04829563573002815\n",
      "Surface training t=6975, loss=0.04401862807571888\n",
      "Surface training t=6976, loss=0.040138816460967064\n",
      "Surface training t=6977, loss=0.055313291028141975\n",
      "Surface training t=6978, loss=0.06723535433411598\n",
      "Surface training t=6979, loss=0.04657880589365959\n",
      "Surface training t=6980, loss=0.061288148164749146\n",
      "Surface training t=6981, loss=0.0800996832549572\n",
      "Surface training t=6982, loss=0.06388571299612522\n",
      "Surface training t=6983, loss=0.04396831430494785\n",
      "Surface training t=6984, loss=0.05065012164413929\n",
      "Surface training t=6985, loss=0.054792214184999466\n",
      "Surface training t=6986, loss=0.05250251665711403\n",
      "Surface training t=6987, loss=0.05811113491654396\n",
      "Surface training t=6988, loss=0.060188066214323044\n",
      "Surface training t=6989, loss=0.05351139046251774\n",
      "Surface training t=6990, loss=0.04550429619848728\n",
      "Surface training t=6991, loss=0.050107309594750404\n",
      "Surface training t=6992, loss=0.04762652702629566\n",
      "Surface training t=6993, loss=0.04658833146095276\n",
      "Surface training t=6994, loss=0.05637428164482117\n",
      "Surface training t=6995, loss=0.04728624224662781\n",
      "Surface training t=6996, loss=0.04837353900074959\n",
      "Surface training t=6997, loss=0.045103706419467926\n",
      "Surface training t=6998, loss=0.045534150674939156\n",
      "Surface training t=6999, loss=0.048868633806705475\n",
      "Surface training t=7000, loss=0.04848852567374706\n",
      "Surface training t=7001, loss=0.0664604939520359\n",
      "Surface training t=7002, loss=0.05591243505477905\n",
      "Surface training t=7003, loss=0.06438840553164482\n",
      "Surface training t=7004, loss=0.045081647112965584\n",
      "Surface training t=7005, loss=0.043405866250395775\n",
      "Surface training t=7006, loss=0.057374196127057076\n",
      "Surface training t=7007, loss=0.05070004239678383\n",
      "Surface training t=7008, loss=0.04897807724773884\n",
      "Surface training t=7009, loss=0.040214983746409416\n",
      "Surface training t=7010, loss=0.042447373270988464\n",
      "Surface training t=7011, loss=0.055314742028713226\n",
      "Surface training t=7012, loss=0.03490372281521559\n",
      "Surface training t=7013, loss=0.04194132890552282\n",
      "Surface training t=7014, loss=0.037951789796352386\n",
      "Surface training t=7015, loss=0.03705426212400198\n",
      "Surface training t=7016, loss=0.04478060454130173\n",
      "Surface training t=7017, loss=0.04668711870908737\n",
      "Surface training t=7018, loss=0.054271480068564415\n",
      "Surface training t=7019, loss=0.047727204859256744\n",
      "Surface training t=7020, loss=0.04009162820875645\n",
      "Surface training t=7021, loss=0.04246188700199127\n",
      "Surface training t=7022, loss=0.03959357179701328\n",
      "Surface training t=7023, loss=0.036851778626441956\n",
      "Surface training t=7024, loss=0.033850811421871185\n",
      "Surface training t=7025, loss=0.037967806681990623\n",
      "Surface training t=7026, loss=0.035714130848646164\n",
      "Surface training t=7027, loss=0.04257109761238098\n",
      "Surface training t=7028, loss=0.052366361021995544\n",
      "Surface training t=7029, loss=0.046496505849063396\n",
      "Surface training t=7030, loss=0.03487803228199482\n",
      "Surface training t=7031, loss=0.054001759737730026\n",
      "Surface training t=7032, loss=0.05401213467121124\n",
      "Surface training t=7033, loss=0.04033865965902805\n",
      "Surface training t=7034, loss=0.03868159931153059\n",
      "Surface training t=7035, loss=0.051905686035752296\n",
      "Surface training t=7036, loss=0.05914725922048092\n",
      "Surface training t=7037, loss=0.06262314133346081\n",
      "Surface training t=7038, loss=0.07568640634417534\n",
      "Surface training t=7039, loss=0.04868934489786625\n",
      "Surface training t=7040, loss=0.07549082487821579\n",
      "Surface training t=7041, loss=0.06150026619434357\n",
      "Surface training t=7042, loss=0.057531483471393585\n",
      "Surface training t=7043, loss=0.06042981147766113\n",
      "Surface training t=7044, loss=0.04628624953329563\n",
      "Surface training t=7045, loss=0.0405756663531065\n",
      "Surface training t=7046, loss=0.05176517739892006\n",
      "Surface training t=7047, loss=0.042429523542523384\n",
      "Surface training t=7048, loss=0.051529958844184875\n",
      "Surface training t=7049, loss=0.0354914627969265\n",
      "Surface training t=7050, loss=0.035198917612433434\n",
      "Surface training t=7051, loss=0.048917366191744804\n",
      "Surface training t=7052, loss=0.05241650342941284\n",
      "Surface training t=7053, loss=0.051735954359173775\n",
      "Surface training t=7054, loss=0.0559509489685297\n",
      "Surface training t=7055, loss=0.05592021159827709\n",
      "Surface training t=7056, loss=0.06451567448675632\n",
      "Surface training t=7057, loss=0.09245526790618896\n",
      "Surface training t=7058, loss=0.07085184007883072\n",
      "Surface training t=7059, loss=0.07142772153019905\n",
      "Surface training t=7060, loss=0.06018000841140747\n",
      "Surface training t=7061, loss=0.06455237790942192\n",
      "Surface training t=7062, loss=0.06983879953622818\n",
      "Surface training t=7063, loss=0.0634433850646019\n",
      "Surface training t=7064, loss=0.05453554168343544\n",
      "Surface training t=7065, loss=0.04798227362334728\n",
      "Surface training t=7066, loss=0.04847537353634834\n",
      "Surface training t=7067, loss=0.036500247195363045\n",
      "Surface training t=7068, loss=0.041340598836541176\n",
      "Surface training t=7069, loss=0.0372626855969429\n",
      "Surface training t=7070, loss=0.0369087178260088\n",
      "Surface training t=7071, loss=0.04089837893843651\n",
      "Surface training t=7072, loss=0.04048112779855728\n",
      "Surface training t=7073, loss=0.040820034220814705\n",
      "Surface training t=7074, loss=0.04475482739508152\n",
      "Surface training t=7075, loss=0.05922120809555054\n",
      "Surface training t=7076, loss=0.05658046156167984\n",
      "Surface training t=7077, loss=0.05167556740343571\n",
      "Surface training t=7078, loss=0.040950847789645195\n",
      "Surface training t=7079, loss=0.042707448825240135\n",
      "Surface training t=7080, loss=0.03648526594042778\n",
      "Surface training t=7081, loss=0.05363054759800434\n",
      "Surface training t=7082, loss=0.09422213956713676\n",
      "Surface training t=7083, loss=0.0713215097784996\n",
      "Surface training t=7084, loss=0.04961239732801914\n",
      "Surface training t=7085, loss=0.0482251551002264\n",
      "Surface training t=7086, loss=0.03582850471138954\n",
      "Surface training t=7087, loss=0.03196105919778347\n",
      "Surface training t=7088, loss=0.04036487638950348\n",
      "Surface training t=7089, loss=0.036963216960430145\n",
      "Surface training t=7090, loss=0.038987613283097744\n",
      "Surface training t=7091, loss=0.03930506110191345\n",
      "Surface training t=7092, loss=0.07809294760227203\n",
      "Surface training t=7093, loss=0.05720623955130577\n",
      "Surface training t=7094, loss=0.05699878931045532\n",
      "Surface training t=7095, loss=0.0663883425295353\n",
      "Surface training t=7096, loss=0.07160088047385216\n",
      "Surface training t=7097, loss=0.06550346128642559\n",
      "Surface training t=7098, loss=0.05266338586807251\n",
      "Surface training t=7099, loss=0.07111354544758797\n",
      "Surface training t=7100, loss=0.06068589724600315\n",
      "Surface training t=7101, loss=0.061822980642318726\n",
      "Surface training t=7102, loss=0.061978936195373535\n",
      "Surface training t=7103, loss=0.07280857861042023\n",
      "Surface training t=7104, loss=0.08351332694292068\n",
      "Surface training t=7105, loss=0.10721263661980629\n",
      "Surface training t=7106, loss=0.08636567369103432\n",
      "Surface training t=7107, loss=0.09684111177921295\n",
      "Surface training t=7108, loss=0.13722093403339386\n",
      "Surface training t=7109, loss=0.08216702938079834\n",
      "Surface training t=7110, loss=0.10867717489600182\n",
      "Surface training t=7111, loss=0.10603096708655357\n",
      "Surface training t=7112, loss=0.0767677053809166\n",
      "Surface training t=7113, loss=0.06865807808935642\n",
      "Surface training t=7114, loss=0.09182056039571762\n",
      "Surface training t=7115, loss=0.062089256942272186\n",
      "Surface training t=7116, loss=0.03941132500767708\n",
      "Surface training t=7117, loss=0.06573152914643288\n",
      "Surface training t=7118, loss=0.05673911049962044\n",
      "Surface training t=7119, loss=0.0736789908260107\n",
      "Surface training t=7120, loss=0.051700010895729065\n",
      "Surface training t=7121, loss=0.05016983486711979\n",
      "Surface training t=7122, loss=0.051413895562291145\n",
      "Surface training t=7123, loss=0.05204831622540951\n",
      "Surface training t=7124, loss=0.08126235753297806\n",
      "Surface training t=7125, loss=0.059480585157871246\n",
      "Surface training t=7126, loss=0.05189269781112671\n",
      "Surface training t=7127, loss=0.040177807211875916\n",
      "Surface training t=7128, loss=0.043223362416028976\n",
      "Surface training t=7129, loss=0.0406574010848999\n",
      "Surface training t=7130, loss=0.041750673204660416\n",
      "Surface training t=7131, loss=0.03755274601280689\n",
      "Surface training t=7132, loss=0.039860038086771965\n",
      "Surface training t=7133, loss=0.04731777682900429\n",
      "Surface training t=7134, loss=0.0412710327655077\n",
      "Surface training t=7135, loss=0.07073646411299706\n",
      "Surface training t=7136, loss=0.05228705704212189\n",
      "Surface training t=7137, loss=0.056125376373529434\n",
      "Surface training t=7138, loss=0.06268316879868507\n",
      "Surface training t=7139, loss=0.05227356217801571\n",
      "Surface training t=7140, loss=0.05511193722486496\n",
      "Surface training t=7141, loss=0.058553777635097504\n",
      "Surface training t=7142, loss=0.05965312570333481\n",
      "Surface training t=7143, loss=0.06734750792384148\n",
      "Surface training t=7144, loss=0.05804984085261822\n",
      "Surface training t=7145, loss=0.06587477959692478\n",
      "Surface training t=7146, loss=0.07383739948272705\n",
      "Surface training t=7147, loss=0.06601291336119175\n",
      "Surface training t=7148, loss=0.07027094811201096\n",
      "Surface training t=7149, loss=0.050738418474793434\n",
      "Surface training t=7150, loss=0.05840061604976654\n",
      "Surface training t=7151, loss=0.05488130450248718\n",
      "Surface training t=7152, loss=0.04034564457833767\n",
      "Surface training t=7153, loss=0.039345771074295044\n",
      "Surface training t=7154, loss=0.04365925118327141\n",
      "Surface training t=7155, loss=0.044206712394952774\n",
      "Surface training t=7156, loss=0.03151384275406599\n",
      "Surface training t=7157, loss=0.03042173571884632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=7158, loss=0.039857013151049614\n",
      "Surface training t=7159, loss=0.03572567366063595\n",
      "Surface training t=7160, loss=0.040665747597813606\n",
      "Surface training t=7161, loss=0.04028290510177612\n",
      "Surface training t=7162, loss=0.0505054946988821\n",
      "Surface training t=7163, loss=0.048755815252661705\n",
      "Surface training t=7164, loss=0.04793514311313629\n",
      "Surface training t=7165, loss=0.05106751434504986\n",
      "Surface training t=7166, loss=0.06215151026844978\n",
      "Surface training t=7167, loss=0.05996991507709026\n",
      "Surface training t=7168, loss=0.05185800604522228\n",
      "Surface training t=7169, loss=0.05069417878985405\n",
      "Surface training t=7170, loss=0.05913340672850609\n",
      "Surface training t=7171, loss=0.05077727138996124\n",
      "Surface training t=7172, loss=0.045458871871232986\n",
      "Surface training t=7173, loss=0.0415475033223629\n",
      "Surface training t=7174, loss=0.048806801438331604\n",
      "Surface training t=7175, loss=0.056321464478969574\n",
      "Surface training t=7176, loss=0.0650075227022171\n",
      "Surface training t=7177, loss=0.045565878972411156\n",
      "Surface training t=7178, loss=0.051630569621920586\n",
      "Surface training t=7179, loss=0.044037384912371635\n",
      "Surface training t=7180, loss=0.05129790119826794\n",
      "Surface training t=7181, loss=0.04607277922332287\n",
      "Surface training t=7182, loss=0.03809959627687931\n",
      "Surface training t=7183, loss=0.04043198935687542\n",
      "Surface training t=7184, loss=0.038306949660182\n",
      "Surface training t=7185, loss=0.03442291542887688\n",
      "Surface training t=7186, loss=0.031828299164772034\n",
      "Surface training t=7187, loss=0.02990327589213848\n",
      "Surface training t=7188, loss=0.032504575327038765\n",
      "Surface training t=7189, loss=0.028803851455450058\n",
      "Surface training t=7190, loss=0.030367029830813408\n",
      "Surface training t=7191, loss=0.03075243253260851\n",
      "Surface training t=7192, loss=0.03052990324795246\n",
      "Surface training t=7193, loss=0.0306731341406703\n",
      "Surface training t=7194, loss=0.029296468012034893\n",
      "Surface training t=7195, loss=0.03430876322090626\n",
      "Surface training t=7196, loss=0.02862386591732502\n",
      "Surface training t=7197, loss=0.03502068668603897\n",
      "Surface training t=7198, loss=0.04083430767059326\n",
      "Surface training t=7199, loss=0.031339993700385094\n",
      "Surface training t=7200, loss=0.03416942246258259\n",
      "Surface training t=7201, loss=0.041053393855690956\n",
      "Surface training t=7202, loss=0.04989922232925892\n",
      "Surface training t=7203, loss=0.05134611018002033\n",
      "Surface training t=7204, loss=0.04259142652153969\n",
      "Surface training t=7205, loss=0.038433799520134926\n",
      "Surface training t=7206, loss=0.03887411579489708\n",
      "Surface training t=7207, loss=0.04022935591638088\n",
      "Surface training t=7208, loss=0.06058656610548496\n",
      "Surface training t=7209, loss=0.05507960729300976\n",
      "Surface training t=7210, loss=0.08068656548857689\n",
      "Surface training t=7211, loss=0.07229074835777283\n",
      "Surface training t=7212, loss=0.08494248613715172\n",
      "Surface training t=7213, loss=0.10253705829381943\n",
      "Surface training t=7214, loss=0.07342435419559479\n",
      "Surface training t=7215, loss=0.06019164063036442\n",
      "Surface training t=7216, loss=0.06438025459647179\n",
      "Surface training t=7217, loss=0.11737504601478577\n",
      "Surface training t=7218, loss=0.08608995378017426\n",
      "Surface training t=7219, loss=0.06716455519199371\n",
      "Surface training t=7220, loss=0.08077678456902504\n",
      "Surface training t=7221, loss=0.051128214225172997\n",
      "Surface training t=7222, loss=0.030716054141521454\n",
      "Surface training t=7223, loss=0.029754476621747017\n",
      "Surface training t=7224, loss=0.03209080919623375\n",
      "Surface training t=7225, loss=0.031027061864733696\n",
      "Surface training t=7226, loss=0.041748590767383575\n",
      "Surface training t=7227, loss=0.03607637621462345\n",
      "Surface training t=7228, loss=0.03602680750191212\n",
      "Surface training t=7229, loss=0.03988860920071602\n",
      "Surface training t=7230, loss=0.03851282224059105\n",
      "Surface training t=7231, loss=0.03772715199738741\n",
      "Surface training t=7232, loss=0.03112609125673771\n",
      "Surface training t=7233, loss=0.03516828268766403\n",
      "Surface training t=7234, loss=0.037912722676992416\n",
      "Surface training t=7235, loss=0.039347209967672825\n",
      "Surface training t=7236, loss=0.0399656742811203\n",
      "Surface training t=7237, loss=0.061206622049212456\n",
      "Surface training t=7238, loss=0.051515666767954826\n",
      "Surface training t=7239, loss=0.04612361639738083\n",
      "Surface training t=7240, loss=0.05402091704308987\n",
      "Surface training t=7241, loss=0.0548968892544508\n",
      "Surface training t=7242, loss=0.04918750748038292\n",
      "Surface training t=7243, loss=0.04948662593960762\n",
      "Surface training t=7244, loss=0.059536054730415344\n",
      "Surface training t=7245, loss=0.04678059369325638\n",
      "Surface training t=7246, loss=0.04667394421994686\n",
      "Surface training t=7247, loss=0.0452923309057951\n",
      "Surface training t=7248, loss=0.04823669418692589\n",
      "Surface training t=7249, loss=0.04702278599143028\n",
      "Surface training t=7250, loss=0.05943410098552704\n",
      "Surface training t=7251, loss=0.057311199605464935\n",
      "Surface training t=7252, loss=0.06057071313261986\n",
      "Surface training t=7253, loss=0.055954042822122574\n",
      "Surface training t=7254, loss=0.058420563116669655\n",
      "Surface training t=7255, loss=0.05647539533674717\n",
      "Surface training t=7256, loss=0.054408276453614235\n",
      "Surface training t=7257, loss=0.05369636043906212\n",
      "Surface training t=7258, loss=0.039917788468301296\n",
      "Surface training t=7259, loss=0.046098385006189346\n",
      "Surface training t=7260, loss=0.04295805282890797\n",
      "Surface training t=7261, loss=0.03004782646894455\n",
      "Surface training t=7262, loss=0.035987209528684616\n",
      "Surface training t=7263, loss=0.043795064091682434\n",
      "Surface training t=7264, loss=0.04168171435594559\n",
      "Surface training t=7265, loss=0.03433569520711899\n",
      "Surface training t=7266, loss=0.042218420654535294\n",
      "Surface training t=7267, loss=0.04611283168196678\n",
      "Surface training t=7268, loss=0.04265117831528187\n",
      "Surface training t=7269, loss=0.045208364725112915\n",
      "Surface training t=7270, loss=0.05116444639861584\n",
      "Surface training t=7271, loss=0.04480055719614029\n",
      "Surface training t=7272, loss=0.05335213243961334\n",
      "Surface training t=7273, loss=0.04403471015393734\n",
      "Surface training t=7274, loss=0.048774879425764084\n",
      "Surface training t=7275, loss=0.041630446910858154\n",
      "Surface training t=7276, loss=0.04087973013520241\n",
      "Surface training t=7277, loss=0.046663250774145126\n",
      "Surface training t=7278, loss=0.03642917890101671\n",
      "Surface training t=7279, loss=0.034499457105994225\n",
      "Surface training t=7280, loss=0.035346854478120804\n",
      "Surface training t=7281, loss=0.03687486983835697\n",
      "Surface training t=7282, loss=0.04329502955079079\n",
      "Surface training t=7283, loss=0.0457515399903059\n",
      "Surface training t=7284, loss=0.06177694350481033\n",
      "Surface training t=7285, loss=0.06263291463255882\n",
      "Surface training t=7286, loss=0.044779665768146515\n",
      "Surface training t=7287, loss=0.050052838400006294\n",
      "Surface training t=7288, loss=0.03682264685630798\n",
      "Surface training t=7289, loss=0.029738757759332657\n",
      "Surface training t=7290, loss=0.03436045814305544\n",
      "Surface training t=7291, loss=0.03266321960836649\n",
      "Surface training t=7292, loss=0.0336856571957469\n",
      "Surface training t=7293, loss=0.03399818390607834\n",
      "Surface training t=7294, loss=0.0459832064807415\n",
      "Surface training t=7295, loss=0.04207763075828552\n",
      "Surface training t=7296, loss=0.04379417560994625\n",
      "Surface training t=7297, loss=0.03864976670593023\n",
      "Surface training t=7298, loss=0.03722698241472244\n",
      "Surface training t=7299, loss=0.04009758494794369\n",
      "Surface training t=7300, loss=0.040075041353702545\n",
      "Surface training t=7301, loss=0.05620561167597771\n",
      "Surface training t=7302, loss=0.03543802723288536\n",
      "Surface training t=7303, loss=0.038912251591682434\n",
      "Surface training t=7304, loss=0.04414434731006622\n",
      "Surface training t=7305, loss=0.04437902942299843\n",
      "Surface training t=7306, loss=0.04505426995456219\n",
      "Surface training t=7307, loss=0.04800271987915039\n",
      "Surface training t=7308, loss=0.08489679917693138\n",
      "Surface training t=7309, loss=0.06144098564982414\n",
      "Surface training t=7310, loss=0.05385489948093891\n",
      "Surface training t=7311, loss=0.0453768577426672\n",
      "Surface training t=7312, loss=0.0452869301661849\n",
      "Surface training t=7313, loss=0.044789137318730354\n",
      "Surface training t=7314, loss=0.03858406748622656\n",
      "Surface training t=7315, loss=0.04460859298706055\n",
      "Surface training t=7316, loss=0.041510699316859245\n",
      "Surface training t=7317, loss=0.03526029363274574\n",
      "Surface training t=7318, loss=0.031813813373446465\n",
      "Surface training t=7319, loss=0.0313197011128068\n",
      "Surface training t=7320, loss=0.025967574678361416\n",
      "Surface training t=7321, loss=0.034007478505373\n",
      "Surface training t=7322, loss=0.03937148116528988\n",
      "Surface training t=7323, loss=0.053195495158433914\n",
      "Surface training t=7324, loss=0.05808829702436924\n",
      "Surface training t=7325, loss=0.04077646695077419\n",
      "Surface training t=7326, loss=0.04642999917268753\n",
      "Surface training t=7327, loss=0.04560563154518604\n",
      "Surface training t=7328, loss=0.06388677656650543\n",
      "Surface training t=7329, loss=0.062250712886452675\n",
      "Surface training t=7330, loss=0.04893004149198532\n",
      "Surface training t=7331, loss=0.04275653511285782\n",
      "Surface training t=7332, loss=0.03735750261694193\n",
      "Surface training t=7333, loss=0.04314216785132885\n",
      "Surface training t=7334, loss=0.04536319151520729\n",
      "Surface training t=7335, loss=0.041284605860710144\n",
      "Surface training t=7336, loss=0.0477136243134737\n",
      "Surface training t=7337, loss=0.034659769386053085\n",
      "Surface training t=7338, loss=0.04086299426853657\n",
      "Surface training t=7339, loss=0.03516193013638258\n",
      "Surface training t=7340, loss=0.03398312535136938\n",
      "Surface training t=7341, loss=0.034129454754292965\n",
      "Surface training t=7342, loss=0.03414144180715084\n",
      "Surface training t=7343, loss=0.026229905895888805\n",
      "Surface training t=7344, loss=0.039219750091433525\n",
      "Surface training t=7345, loss=0.030800403095781803\n",
      "Surface training t=7346, loss=0.03049255721271038\n",
      "Surface training t=7347, loss=0.03959307819604874\n",
      "Surface training t=7348, loss=0.03100438602268696\n",
      "Surface training t=7349, loss=0.03648771718144417\n",
      "Surface training t=7350, loss=0.03173085115849972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=7351, loss=0.03671680763363838\n",
      "Surface training t=7352, loss=0.04072875529527664\n",
      "Surface training t=7353, loss=0.06944437138736248\n",
      "Surface training t=7354, loss=0.06746687740087509\n",
      "Surface training t=7355, loss=0.05263223499059677\n",
      "Surface training t=7356, loss=0.060009174048900604\n",
      "Surface training t=7357, loss=0.066951435059309\n",
      "Surface training t=7358, loss=0.05759923905134201\n",
      "Surface training t=7359, loss=0.0676748976111412\n",
      "Surface training t=7360, loss=0.054337095469236374\n",
      "Surface training t=7361, loss=0.05804967321455479\n",
      "Surface training t=7362, loss=0.06469338946044445\n",
      "Surface training t=7363, loss=0.05915950797498226\n",
      "Surface training t=7364, loss=0.049072328954935074\n",
      "Surface training t=7365, loss=0.0506149735301733\n",
      "Surface training t=7366, loss=0.05781533569097519\n",
      "Surface training t=7367, loss=0.07533631101250648\n",
      "Surface training t=7368, loss=0.06476636230945587\n",
      "Surface training t=7369, loss=0.04850519262254238\n",
      "Surface training t=7370, loss=0.04039045795798302\n",
      "Surface training t=7371, loss=0.06628168746829033\n",
      "Surface training t=7372, loss=0.04881373420357704\n",
      "Surface training t=7373, loss=0.05198846571147442\n",
      "Surface training t=7374, loss=0.03482411429286003\n",
      "Surface training t=7375, loss=0.04541521146893501\n",
      "Surface training t=7376, loss=0.05921219103038311\n",
      "Surface training t=7377, loss=0.054861798882484436\n",
      "Surface training t=7378, loss=0.0463736392557621\n",
      "Surface training t=7379, loss=0.05057297646999359\n",
      "Surface training t=7380, loss=0.033275676891207695\n",
      "Surface training t=7381, loss=0.03789830766618252\n",
      "Surface training t=7382, loss=0.029636387713253498\n",
      "Surface training t=7383, loss=0.037747275084257126\n",
      "Surface training t=7384, loss=0.042462464421987534\n",
      "Surface training t=7385, loss=0.039325025863945484\n",
      "Surface training t=7386, loss=0.042132679373025894\n",
      "Surface training t=7387, loss=0.057777369394898415\n",
      "Surface training t=7388, loss=0.04456315375864506\n",
      "Surface training t=7389, loss=0.04030921496450901\n",
      "Surface training t=7390, loss=0.04104047082364559\n",
      "Surface training t=7391, loss=0.029839816503226757\n",
      "Surface training t=7392, loss=0.03453825041651726\n",
      "Surface training t=7393, loss=0.03517352603375912\n",
      "Surface training t=7394, loss=0.05802213400602341\n",
      "Surface training t=7395, loss=0.03871791251003742\n",
      "Surface training t=7396, loss=0.035699816420674324\n",
      "Surface training t=7397, loss=0.03971487656235695\n",
      "Surface training t=7398, loss=0.048043590039014816\n",
      "Surface training t=7399, loss=0.04184835962951183\n",
      "Surface training t=7400, loss=0.03621280565857887\n",
      "Surface training t=7401, loss=0.03575399238616228\n",
      "Surface training t=7402, loss=0.038933174684643745\n",
      "Surface training t=7403, loss=0.06778491847217083\n",
      "Surface training t=7404, loss=0.047991929575800896\n",
      "Surface training t=7405, loss=0.06004432588815689\n",
      "Surface training t=7406, loss=0.03773492015898228\n",
      "Surface training t=7407, loss=0.04335523210465908\n",
      "Surface training t=7408, loss=0.0649011917412281\n",
      "Surface training t=7409, loss=0.049643728882074356\n",
      "Surface training t=7410, loss=0.044086722657084465\n",
      "Surface training t=7411, loss=0.04305637255311012\n",
      "Surface training t=7412, loss=0.04184269905090332\n",
      "Surface training t=7413, loss=0.06036017835140228\n",
      "Surface training t=7414, loss=0.04746948927640915\n",
      "Surface training t=7415, loss=0.04683096893131733\n",
      "Surface training t=7416, loss=0.05140865221619606\n",
      "Surface training t=7417, loss=0.03620337415486574\n",
      "Surface training t=7418, loss=0.029772842302918434\n",
      "Surface training t=7419, loss=0.03090868517756462\n",
      "Surface training t=7420, loss=0.030807497911155224\n",
      "Surface training t=7421, loss=0.030799285508692265\n",
      "Surface training t=7422, loss=0.034924207255244255\n",
      "Surface training t=7423, loss=0.03096597082912922\n",
      "Surface training t=7424, loss=0.03543272241950035\n",
      "Surface training t=7425, loss=0.03976088762283325\n",
      "Surface training t=7426, loss=0.04875587858259678\n",
      "Surface training t=7427, loss=0.048669399693608284\n",
      "Surface training t=7428, loss=0.04548405483365059\n",
      "Surface training t=7429, loss=0.05272074230015278\n",
      "Surface training t=7430, loss=0.054474152624607086\n",
      "Surface training t=7431, loss=0.05576740577816963\n",
      "Surface training t=7432, loss=0.052898915484547615\n",
      "Surface training t=7433, loss=0.04628100246191025\n",
      "Surface training t=7434, loss=0.05547080747783184\n",
      "Surface training t=7435, loss=0.04898780770599842\n",
      "Surface training t=7436, loss=0.03685707878321409\n",
      "Surface training t=7437, loss=0.039172058925032616\n",
      "Surface training t=7438, loss=0.04344281926751137\n",
      "Surface training t=7439, loss=0.032801808789372444\n",
      "Surface training t=7440, loss=0.039515700191259384\n",
      "Surface training t=7441, loss=0.029180568642914295\n",
      "Surface training t=7442, loss=0.037869371473789215\n",
      "Surface training t=7443, loss=0.03347174450755119\n",
      "Surface training t=7444, loss=0.03777723014354706\n",
      "Surface training t=7445, loss=0.03045985195785761\n",
      "Surface training t=7446, loss=0.037322286516427994\n",
      "Surface training t=7447, loss=0.0375575739890337\n",
      "Surface training t=7448, loss=0.03609997592866421\n",
      "Surface training t=7449, loss=0.038944780826568604\n",
      "Surface training t=7450, loss=0.029687009751796722\n",
      "Surface training t=7451, loss=0.03286115080118179\n",
      "Surface training t=7452, loss=0.03195726126432419\n",
      "Surface training t=7453, loss=0.02852684911340475\n",
      "Surface training t=7454, loss=0.028599812649190426\n",
      "Surface training t=7455, loss=0.03421599045395851\n",
      "Surface training t=7456, loss=0.032693736255168915\n",
      "Surface training t=7457, loss=0.031242238357663155\n",
      "Surface training t=7458, loss=0.02824427653104067\n",
      "Surface training t=7459, loss=0.034063566476106644\n",
      "Surface training t=7460, loss=0.027656007558107376\n",
      "Surface training t=7461, loss=0.03658691234886646\n",
      "Surface training t=7462, loss=0.049196431413292885\n",
      "Surface training t=7463, loss=0.04338075965642929\n",
      "Surface training t=7464, loss=0.04266495443880558\n",
      "Surface training t=7465, loss=0.04858998954296112\n",
      "Surface training t=7466, loss=0.03862902894616127\n",
      "Surface training t=7467, loss=0.03585305064916611\n",
      "Surface training t=7468, loss=0.05321606248617172\n",
      "Surface training t=7469, loss=0.03464031405746937\n",
      "Surface training t=7470, loss=0.04184821434319019\n",
      "Surface training t=7471, loss=0.06081654131412506\n",
      "Surface training t=7472, loss=0.07351404055953026\n",
      "Surface training t=7473, loss=0.06578400731086731\n",
      "Surface training t=7474, loss=0.05962267518043518\n",
      "Surface training t=7475, loss=0.061561159789562225\n",
      "Surface training t=7476, loss=0.05514015629887581\n",
      "Surface training t=7477, loss=0.05050152353942394\n",
      "Surface training t=7478, loss=0.051041459664702415\n",
      "Surface training t=7479, loss=0.08813750371336937\n",
      "Surface training t=7480, loss=0.07256461679935455\n",
      "Surface training t=7481, loss=0.06406118534505367\n",
      "Surface training t=7482, loss=0.06385402381420135\n",
      "Surface training t=7483, loss=0.052906718105077744\n",
      "Surface training t=7484, loss=0.044161705300211906\n",
      "Surface training t=7485, loss=0.03581900242716074\n",
      "Surface training t=7486, loss=0.03252662159502506\n",
      "Surface training t=7487, loss=0.034427862614393234\n",
      "Surface training t=7488, loss=0.0342786330729723\n",
      "Surface training t=7489, loss=0.03526666387915611\n",
      "Surface training t=7490, loss=0.030898060649633408\n",
      "Surface training t=7491, loss=0.037950363010168076\n",
      "Surface training t=7492, loss=0.0407226737588644\n",
      "Surface training t=7493, loss=0.04063343070447445\n",
      "Surface training t=7494, loss=0.03808092325925827\n",
      "Surface training t=7495, loss=0.044963590800762177\n",
      "Surface training t=7496, loss=0.03637799248099327\n",
      "Surface training t=7497, loss=0.03077127132564783\n",
      "Surface training t=7498, loss=0.03393091633915901\n",
      "Surface training t=7499, loss=0.03601200133562088\n",
      "Surface training t=7500, loss=0.038737816736102104\n",
      "Surface training t=7501, loss=0.034615283831954\n",
      "Surface training t=7502, loss=0.03033850435167551\n",
      "Surface training t=7503, loss=0.031088308431208134\n",
      "Surface training t=7504, loss=0.041657593101263046\n",
      "Surface training t=7505, loss=0.04236241430044174\n",
      "Surface training t=7506, loss=0.0437142476439476\n",
      "Surface training t=7507, loss=0.048364898189902306\n",
      "Surface training t=7508, loss=0.038565490394830704\n",
      "Surface training t=7509, loss=0.029024064540863037\n",
      "Surface training t=7510, loss=0.026325291022658348\n",
      "Surface training t=7511, loss=0.034270090982317924\n",
      "Surface training t=7512, loss=0.03356366604566574\n",
      "Surface training t=7513, loss=0.02822686918079853\n",
      "Surface training t=7514, loss=0.02851589396595955\n",
      "Surface training t=7515, loss=0.03705216199159622\n",
      "Surface training t=7516, loss=0.050111254677176476\n",
      "Surface training t=7517, loss=0.0891575962305069\n",
      "Surface training t=7518, loss=0.0518866702914238\n",
      "Surface training t=7519, loss=0.038322778418660164\n",
      "Surface training t=7520, loss=0.05388876982033253\n",
      "Surface training t=7521, loss=0.042890775948762894\n",
      "Surface training t=7522, loss=0.041861530393362045\n",
      "Surface training t=7523, loss=0.060737401247024536\n",
      "Surface training t=7524, loss=0.08302588760852814\n",
      "Surface training t=7525, loss=0.05256679654121399\n",
      "Surface training t=7526, loss=0.053705526515841484\n",
      "Surface training t=7527, loss=0.05834941193461418\n",
      "Surface training t=7528, loss=0.058547716587781906\n",
      "Surface training t=7529, loss=0.05297193489968777\n",
      "Surface training t=7530, loss=0.0481996014714241\n",
      "Surface training t=7531, loss=0.05045383237302303\n",
      "Surface training t=7532, loss=0.04258144833147526\n",
      "Surface training t=7533, loss=0.06704448908567429\n",
      "Surface training t=7534, loss=0.04938216879963875\n",
      "Surface training t=7535, loss=0.052327705547213554\n",
      "Surface training t=7536, loss=0.0472454447299242\n",
      "Surface training t=7537, loss=0.04262588918209076\n",
      "Surface training t=7538, loss=0.04546160437166691\n",
      "Surface training t=7539, loss=0.04186227545142174\n",
      "Surface training t=7540, loss=0.03510771691799164\n",
      "Surface training t=7541, loss=0.04571804031729698\n",
      "Surface training t=7542, loss=0.04745199717581272\n",
      "Surface training t=7543, loss=0.03459430951625109\n",
      "Surface training t=7544, loss=0.03655741270631552\n",
      "Surface training t=7545, loss=0.043675534427165985\n",
      "Surface training t=7546, loss=0.036305719055235386\n",
      "Surface training t=7547, loss=0.04190008528530598\n",
      "Surface training t=7548, loss=0.05985068529844284\n",
      "Surface training t=7549, loss=0.05143946409225464\n",
      "Surface training t=7550, loss=0.04784316197037697\n",
      "Surface training t=7551, loss=0.03772587701678276\n",
      "Surface training t=7552, loss=0.03468663804233074\n",
      "Surface training t=7553, loss=0.03377645090222359\n",
      "Surface training t=7554, loss=0.034146648831665516\n",
      "Surface training t=7555, loss=0.03426423668861389\n",
      "Surface training t=7556, loss=0.03968526050448418\n",
      "Surface training t=7557, loss=0.03390557877719402\n",
      "Surface training t=7558, loss=0.03263009246438742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=7559, loss=0.03752314858138561\n",
      "Surface training t=7560, loss=0.03702545911073685\n",
      "Surface training t=7561, loss=0.07589799910783768\n",
      "Surface training t=7562, loss=0.07456863671541214\n",
      "Surface training t=7563, loss=0.03838946111500263\n",
      "Surface training t=7564, loss=0.056856902316212654\n",
      "Surface training t=7565, loss=0.03406102769076824\n",
      "Surface training t=7566, loss=0.028543181717395782\n",
      "Surface training t=7567, loss=0.0268198074772954\n",
      "Surface training t=7568, loss=0.029810383915901184\n",
      "Surface training t=7569, loss=0.033697815611958504\n",
      "Surface training t=7570, loss=0.041385771706700325\n",
      "Surface training t=7571, loss=0.10010123625397682\n",
      "Surface training t=7572, loss=0.07220340333878994\n",
      "Surface training t=7573, loss=0.06700139120221138\n",
      "Surface training t=7574, loss=0.07772399485111237\n",
      "Surface training t=7575, loss=0.12788764387369156\n",
      "Surface training t=7576, loss=0.08751999959349632\n",
      "Surface training t=7577, loss=0.09235880337655544\n",
      "Surface training t=7578, loss=0.15938981622457504\n",
      "Surface training t=7579, loss=0.08451230637729168\n",
      "Surface training t=7580, loss=0.08076842501759529\n",
      "Surface training t=7581, loss=0.09834415465593338\n",
      "Surface training t=7582, loss=0.06361731700599194\n",
      "Surface training t=7583, loss=0.06339052692055702\n",
      "Surface training t=7584, loss=0.05483348108828068\n",
      "Surface training t=7585, loss=0.07852288335561752\n",
      "Surface training t=7586, loss=0.06978873908519745\n",
      "Surface training t=7587, loss=0.0599615853279829\n",
      "Surface training t=7588, loss=0.052545543760061264\n",
      "Surface training t=7589, loss=0.05464196391403675\n",
      "Surface training t=7590, loss=0.06557810679078102\n",
      "Surface training t=7591, loss=0.04935033153742552\n",
      "Surface training t=7592, loss=0.09696481376886368\n",
      "Surface training t=7593, loss=0.07917047291994095\n",
      "Surface training t=7594, loss=0.07618135213851929\n",
      "Surface training t=7595, loss=0.08835884928703308\n",
      "Surface training t=7596, loss=0.08473522216081619\n",
      "Surface training t=7597, loss=0.07096003741025925\n",
      "Surface training t=7598, loss=0.05157754197716713\n",
      "Surface training t=7599, loss=0.06225414015352726\n",
      "Surface training t=7600, loss=0.05736037530004978\n",
      "Surface training t=7601, loss=0.03200187906622887\n",
      "Surface training t=7602, loss=0.03961889259517193\n",
      "Surface training t=7603, loss=0.04790743254125118\n",
      "Surface training t=7604, loss=0.03578758426010609\n",
      "Surface training t=7605, loss=0.037265436723828316\n",
      "Surface training t=7606, loss=0.04264191724359989\n",
      "Surface training t=7607, loss=0.02944291476160288\n",
      "Surface training t=7608, loss=0.04120045155286789\n",
      "Surface training t=7609, loss=0.04302643798291683\n",
      "Surface training t=7610, loss=0.0573339257389307\n",
      "Surface training t=7611, loss=0.07543119788169861\n",
      "Surface training t=7612, loss=0.06301204115152359\n",
      "Surface training t=7613, loss=0.06826283968985081\n",
      "Surface training t=7614, loss=0.07709852233529091\n",
      "Surface training t=7615, loss=0.08169586211442947\n",
      "Surface training t=7616, loss=0.05044344626367092\n",
      "Surface training t=7617, loss=0.04172699339687824\n",
      "Surface training t=7618, loss=0.0631477553397417\n",
      "Surface training t=7619, loss=0.057759156450629234\n",
      "Surface training t=7620, loss=0.07272007316350937\n",
      "Surface training t=7621, loss=0.051986195147037506\n",
      "Surface training t=7622, loss=0.053754089400172234\n",
      "Surface training t=7623, loss=0.04569860175251961\n",
      "Surface training t=7624, loss=0.04189678467810154\n",
      "Surface training t=7625, loss=0.04523515701293945\n",
      "Surface training t=7626, loss=0.04945642873644829\n",
      "Surface training t=7627, loss=0.03920803498476744\n",
      "Surface training t=7628, loss=0.032468751072883606\n",
      "Surface training t=7629, loss=0.040448760613799095\n",
      "Surface training t=7630, loss=0.040479013696312904\n",
      "Surface training t=7631, loss=0.043408339843153954\n",
      "Surface training t=7632, loss=0.039026036858558655\n",
      "Surface training t=7633, loss=0.03653192427009344\n",
      "Surface training t=7634, loss=0.06476502865552902\n",
      "Surface training t=7635, loss=0.0506687443703413\n",
      "Surface training t=7636, loss=0.03952830098569393\n",
      "Surface training t=7637, loss=0.035808585584163666\n",
      "Surface training t=7638, loss=0.06157594546675682\n",
      "Surface training t=7639, loss=0.0672038160264492\n",
      "Surface training t=7640, loss=0.042527180165052414\n",
      "Surface training t=7641, loss=0.03912721574306488\n",
      "Surface training t=7642, loss=0.05591170862317085\n",
      "Surface training t=7643, loss=0.04919377528131008\n",
      "Surface training t=7644, loss=0.07520762458443642\n",
      "Surface training t=7645, loss=0.050753965973854065\n",
      "Surface training t=7646, loss=0.07267997413873672\n",
      "Surface training t=7647, loss=0.07116981409490108\n",
      "Surface training t=7648, loss=0.06527739576995373\n",
      "Surface training t=7649, loss=0.054595598950982094\n",
      "Surface training t=7650, loss=0.05396423488855362\n",
      "Surface training t=7651, loss=0.05182906799018383\n",
      "Surface training t=7652, loss=0.05988828092813492\n",
      "Surface training t=7653, loss=0.06922803819179535\n",
      "Surface training t=7654, loss=0.07170534506440163\n",
      "Surface training t=7655, loss=0.053435822017490864\n",
      "Surface training t=7656, loss=0.049546822905540466\n",
      "Surface training t=7657, loss=0.053628019988536835\n",
      "Surface training t=7658, loss=0.045185770839452744\n",
      "Surface training t=7659, loss=0.04871520213782787\n",
      "Surface training t=7660, loss=0.04466833360493183\n",
      "Surface training t=7661, loss=0.04132500384002924\n",
      "Surface training t=7662, loss=0.043588828295469284\n",
      "Surface training t=7663, loss=0.052794527262449265\n",
      "Surface training t=7664, loss=0.059305308386683464\n",
      "Surface training t=7665, loss=0.05832100100815296\n",
      "Surface training t=7666, loss=0.07816562056541443\n",
      "Surface training t=7667, loss=0.061951540410518646\n",
      "Surface training t=7668, loss=0.03904908522963524\n",
      "Surface training t=7669, loss=0.03921642154455185\n",
      "Surface training t=7670, loss=0.0423043817281723\n",
      "Surface training t=7671, loss=0.03598523698747158\n",
      "Surface training t=7672, loss=0.03379359468817711\n",
      "Surface training t=7673, loss=0.04101408086717129\n",
      "Surface training t=7674, loss=0.051848458126187325\n",
      "Surface training t=7675, loss=0.03867427073419094\n",
      "Surface training t=7676, loss=0.037267763167619705\n",
      "Surface training t=7677, loss=0.04636576771736145\n",
      "Surface training t=7678, loss=0.03500627540051937\n",
      "Surface training t=7679, loss=0.045020684599876404\n",
      "Surface training t=7680, loss=0.05773336626589298\n",
      "Surface training t=7681, loss=0.04020854830741882\n",
      "Surface training t=7682, loss=0.03866270836442709\n",
      "Surface training t=7683, loss=0.03538406081497669\n",
      "Surface training t=7684, loss=0.03223326988518238\n",
      "Surface training t=7685, loss=0.03431130759418011\n",
      "Surface training t=7686, loss=0.042401475831866264\n",
      "Surface training t=7687, loss=0.0418472345918417\n",
      "Surface training t=7688, loss=0.03389466926455498\n",
      "Surface training t=7689, loss=0.037357211112976074\n",
      "Surface training t=7690, loss=0.029099988751113415\n",
      "Surface training t=7691, loss=0.0362990852445364\n",
      "Surface training t=7692, loss=0.03998354449868202\n",
      "Surface training t=7693, loss=0.03975874185562134\n",
      "Surface training t=7694, loss=0.03376076463609934\n",
      "Surface training t=7695, loss=0.05231565609574318\n",
      "Surface training t=7696, loss=0.04335682652890682\n",
      "Surface training t=7697, loss=0.04543976113200188\n",
      "Surface training t=7698, loss=0.060506269335746765\n",
      "Surface training t=7699, loss=0.05523073486983776\n",
      "Surface training t=7700, loss=0.06465707905590534\n",
      "Surface training t=7701, loss=0.051183007657527924\n",
      "Surface training t=7702, loss=0.06597473844885826\n",
      "Surface training t=7703, loss=0.06173296645283699\n",
      "Surface training t=7704, loss=0.0578080452978611\n",
      "Surface training t=7705, loss=0.04939994402229786\n",
      "Surface training t=7706, loss=0.040398554876446724\n",
      "Surface training t=7707, loss=0.041449058800935745\n",
      "Surface training t=7708, loss=0.04366137832403183\n",
      "Surface training t=7709, loss=0.05281545780599117\n",
      "Surface training t=7710, loss=0.08812733367085457\n",
      "Surface training t=7711, loss=0.045053512789309025\n",
      "Surface training t=7712, loss=0.04149378091096878\n",
      "Surface training t=7713, loss=0.04872930981218815\n",
      "Surface training t=7714, loss=0.04767608176916838\n",
      "Surface training t=7715, loss=0.05893024057149887\n",
      "Surface training t=7716, loss=0.051919156685471535\n",
      "Surface training t=7717, loss=0.0483828354626894\n",
      "Surface training t=7718, loss=0.03684980422258377\n",
      "Surface training t=7719, loss=0.04917503893375397\n",
      "Surface training t=7720, loss=0.03927428647875786\n",
      "Surface training t=7721, loss=0.03527433052659035\n",
      "Surface training t=7722, loss=0.036097628995776176\n",
      "Surface training t=7723, loss=0.03883451968431473\n",
      "Surface training t=7724, loss=0.04040489159524441\n",
      "Surface training t=7725, loss=0.03439766447991133\n",
      "Surface training t=7726, loss=0.03805740363895893\n",
      "Surface training t=7727, loss=0.054508211091160774\n",
      "Surface training t=7728, loss=0.07361910864710808\n",
      "Surface training t=7729, loss=0.06233218498528004\n",
      "Surface training t=7730, loss=0.061517948284745216\n",
      "Surface training t=7731, loss=0.06568977795541286\n",
      "Surface training t=7732, loss=0.06652531400322914\n",
      "Surface training t=7733, loss=0.09898637980222702\n",
      "Surface training t=7734, loss=0.06466583721339703\n",
      "Surface training t=7735, loss=0.04551692493259907\n",
      "Surface training t=7736, loss=0.04622129164636135\n",
      "Surface training t=7737, loss=0.04884251952171326\n",
      "Surface training t=7738, loss=0.04352976381778717\n",
      "Surface training t=7739, loss=0.050022609531879425\n",
      "Surface training t=7740, loss=0.04949074424803257\n",
      "Surface training t=7741, loss=0.031088512390851974\n",
      "Surface training t=7742, loss=0.0324078481644392\n",
      "Surface training t=7743, loss=0.02790275402367115\n",
      "Surface training t=7744, loss=0.028135032393038273\n",
      "Surface training t=7745, loss=0.030805516056716442\n",
      "Surface training t=7746, loss=0.03333138860762119\n",
      "Surface training t=7747, loss=0.03754897788167\n",
      "Surface training t=7748, loss=0.03892248123884201\n",
      "Surface training t=7749, loss=0.040298450738191605\n",
      "Surface training t=7750, loss=0.044857291504740715\n",
      "Surface training t=7751, loss=0.04247133433818817\n",
      "Surface training t=7752, loss=0.041484703309834\n",
      "Surface training t=7753, loss=0.034855274483561516\n",
      "Surface training t=7754, loss=0.03822803869843483\n",
      "Surface training t=7755, loss=0.03050841111689806\n",
      "Surface training t=7756, loss=0.04967478848993778\n",
      "Surface training t=7757, loss=0.049545109272003174\n",
      "Surface training t=7758, loss=0.04125973582267761\n",
      "Surface training t=7759, loss=0.045114658772945404\n",
      "Surface training t=7760, loss=0.04228479601442814\n",
      "Surface training t=7761, loss=0.03768603503704071\n",
      "Surface training t=7762, loss=0.04471580870449543\n",
      "Surface training t=7763, loss=0.03816244564950466\n",
      "Surface training t=7764, loss=0.03462354000657797\n",
      "Surface training t=7765, loss=0.029556256718933582\n",
      "Surface training t=7766, loss=0.03559386543929577\n",
      "Surface training t=7767, loss=0.0255454545840621\n",
      "Surface training t=7768, loss=0.034752015955746174\n",
      "Surface training t=7769, loss=0.02633629646152258\n",
      "Surface training t=7770, loss=0.02758117113262415\n",
      "Surface training t=7771, loss=0.03540210798382759\n",
      "Surface training t=7772, loss=0.034584072418510914\n",
      "Surface training t=7773, loss=0.030998900532722473\n",
      "Surface training t=7774, loss=0.03467760980129242\n",
      "Surface training t=7775, loss=0.03533061034977436\n",
      "Surface training t=7776, loss=0.04142920859158039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=7777, loss=0.044762369245290756\n",
      "Surface training t=7778, loss=0.04742545820772648\n",
      "Surface training t=7779, loss=0.03678876906633377\n",
      "Surface training t=7780, loss=0.0387073690071702\n",
      "Surface training t=7781, loss=0.04501793719828129\n",
      "Surface training t=7782, loss=0.041009822860360146\n",
      "Surface training t=7783, loss=0.03657203633338213\n",
      "Surface training t=7784, loss=0.03992369957268238\n",
      "Surface training t=7785, loss=0.05063782446086407\n",
      "Surface training t=7786, loss=0.04253164678812027\n",
      "Surface training t=7787, loss=0.03677576594054699\n",
      "Surface training t=7788, loss=0.039170049130916595\n",
      "Surface training t=7789, loss=0.04646126180887222\n",
      "Surface training t=7790, loss=0.05096246302127838\n",
      "Surface training t=7791, loss=0.051091439090669155\n",
      "Surface training t=7792, loss=0.08685633167624474\n",
      "Surface training t=7793, loss=0.06547893956303596\n",
      "Surface training t=7794, loss=0.054038478061556816\n",
      "Surface training t=7795, loss=0.06512981653213501\n",
      "Surface training t=7796, loss=0.06663591600954533\n",
      "Surface training t=7797, loss=0.08440656960010529\n",
      "Surface training t=7798, loss=0.07284712046384811\n",
      "Surface training t=7799, loss=0.06963863037526608\n",
      "Surface training t=7800, loss=0.08610263466835022\n",
      "Surface training t=7801, loss=0.0931207463145256\n",
      "Surface training t=7802, loss=0.06998176500201225\n",
      "Surface training t=7803, loss=0.12765943259000778\n",
      "Surface training t=7804, loss=0.0952102579176426\n",
      "Surface training t=7805, loss=0.07258912175893784\n",
      "Surface training t=7806, loss=0.06613869778811932\n",
      "Surface training t=7807, loss=0.09009860083460808\n",
      "Surface training t=7808, loss=0.05564294941723347\n",
      "Surface training t=7809, loss=0.06406892091035843\n",
      "Surface training t=7810, loss=0.055453745648264885\n",
      "Surface training t=7811, loss=0.09631957113742828\n",
      "Surface training t=7812, loss=0.08933739736676216\n",
      "Surface training t=7813, loss=0.06229339726269245\n",
      "Surface training t=7814, loss=0.09882649034261703\n",
      "Surface training t=7815, loss=0.0658393781632185\n",
      "Surface training t=7816, loss=0.07388437539339066\n",
      "Surface training t=7817, loss=0.056618787348270416\n",
      "Surface training t=7818, loss=0.05585465393960476\n",
      "Surface training t=7819, loss=0.043544407933950424\n",
      "Surface training t=7820, loss=0.03411005064845085\n",
      "Surface training t=7821, loss=0.0387142114341259\n",
      "Surface training t=7822, loss=0.06086924485862255\n",
      "Surface training t=7823, loss=0.05028999038040638\n",
      "Surface training t=7824, loss=0.040401021018624306\n",
      "Surface training t=7825, loss=0.06797118671238422\n",
      "Surface training t=7826, loss=0.04317554831504822\n",
      "Surface training t=7827, loss=0.06348823383450508\n",
      "Surface training t=7828, loss=0.049864742904901505\n",
      "Surface training t=7829, loss=0.05255609564483166\n",
      "Surface training t=7830, loss=0.07016757130622864\n",
      "Surface training t=7831, loss=0.08348225057125092\n",
      "Surface training t=7832, loss=0.06690598279237747\n",
      "Surface training t=7833, loss=0.08718465082347393\n",
      "Surface training t=7834, loss=0.06679616309702396\n",
      "Surface training t=7835, loss=0.07466511055827141\n",
      "Surface training t=7836, loss=0.05887812003493309\n",
      "Surface training t=7837, loss=0.07110176049172878\n",
      "Surface training t=7838, loss=0.07960842177271843\n",
      "Surface training t=7839, loss=0.08446424081921577\n",
      "Surface training t=7840, loss=0.06834596768021584\n",
      "Surface training t=7841, loss=0.06405268795788288\n",
      "Surface training t=7842, loss=0.09420093521475792\n",
      "Surface training t=7843, loss=0.06105892360210419\n",
      "Surface training t=7844, loss=0.05174201354384422\n",
      "Surface training t=7845, loss=0.052678730338811874\n",
      "Surface training t=7846, loss=0.04173537343740463\n",
      "Surface training t=7847, loss=0.042495790868997574\n",
      "Surface training t=7848, loss=0.06347129866480827\n",
      "Surface training t=7849, loss=0.055803149938583374\n",
      "Surface training t=7850, loss=0.048223281279206276\n",
      "Surface training t=7851, loss=0.04980096220970154\n",
      "Surface training t=7852, loss=0.06102936342358589\n",
      "Surface training t=7853, loss=0.0663658045232296\n",
      "Surface training t=7854, loss=0.08218972757458687\n",
      "Surface training t=7855, loss=0.05602940171957016\n",
      "Surface training t=7856, loss=0.05152961052954197\n",
      "Surface training t=7857, loss=0.06640687957406044\n",
      "Surface training t=7858, loss=0.09412596747279167\n",
      "Surface training t=7859, loss=0.056007279083132744\n",
      "Surface training t=7860, loss=0.057521214708685875\n",
      "Surface training t=7861, loss=0.06396911479532719\n",
      "Surface training t=7862, loss=0.11131995543837547\n",
      "Surface training t=7863, loss=0.08593726716935635\n",
      "Surface training t=7864, loss=0.10917647927999496\n",
      "Surface training t=7865, loss=0.09680841490626335\n",
      "Surface training t=7866, loss=0.08098837733268738\n",
      "Surface training t=7867, loss=0.10175152495503426\n",
      "Surface training t=7868, loss=0.07912977412343025\n",
      "Surface training t=7869, loss=0.06585841998457909\n",
      "Surface training t=7870, loss=0.06660236418247223\n",
      "Surface training t=7871, loss=0.06339343637228012\n",
      "Surface training t=7872, loss=0.04456036165356636\n",
      "Surface training t=7873, loss=0.03717319294810295\n",
      "Surface training t=7874, loss=0.04177098162472248\n",
      "Surface training t=7875, loss=0.03324656840413809\n",
      "Surface training t=7876, loss=0.030641691759228706\n",
      "Surface training t=7877, loss=0.02800305001437664\n",
      "Surface training t=7878, loss=0.030504979193210602\n",
      "Surface training t=7879, loss=0.03621141240000725\n",
      "Surface training t=7880, loss=0.04153487645089626\n",
      "Surface training t=7881, loss=0.06152629479765892\n",
      "Surface training t=7882, loss=0.057503633201122284\n",
      "Surface training t=7883, loss=0.05350308492779732\n",
      "Surface training t=7884, loss=0.046292541548609734\n",
      "Surface training t=7885, loss=0.05453076213598251\n",
      "Surface training t=7886, loss=0.050278808921575546\n",
      "Surface training t=7887, loss=0.05043049156665802\n",
      "Surface training t=7888, loss=0.05044855736196041\n",
      "Surface training t=7889, loss=0.0454441886395216\n",
      "Surface training t=7890, loss=0.029965341091156006\n",
      "Surface training t=7891, loss=0.04263542592525482\n",
      "Surface training t=7892, loss=0.03765209764242172\n",
      "Surface training t=7893, loss=0.0321496706455946\n",
      "Surface training t=7894, loss=0.04159492440521717\n",
      "Surface training t=7895, loss=0.04311164282262325\n",
      "Surface training t=7896, loss=0.07431194558739662\n",
      "Surface training t=7897, loss=0.04432203061878681\n",
      "Surface training t=7898, loss=0.052381863817572594\n",
      "Surface training t=7899, loss=0.046128084883093834\n",
      "Surface training t=7900, loss=0.04217090085148811\n",
      "Surface training t=7901, loss=0.03787138685584068\n",
      "Surface training t=7902, loss=0.032065775245428085\n",
      "Surface training t=7903, loss=0.02772896084934473\n",
      "Surface training t=7904, loss=0.03172002546489239\n",
      "Surface training t=7905, loss=0.03892369195818901\n",
      "Surface training t=7906, loss=0.03312748670578003\n",
      "Surface training t=7907, loss=0.030060192570090294\n",
      "Surface training t=7908, loss=0.032175276428461075\n",
      "Surface training t=7909, loss=0.025472842156887054\n",
      "Surface training t=7910, loss=0.036776360124349594\n",
      "Surface training t=7911, loss=0.02769693173468113\n",
      "Surface training t=7912, loss=0.022818829864263535\n",
      "Surface training t=7913, loss=0.0277189239859581\n",
      "Surface training t=7914, loss=0.03415617719292641\n",
      "Surface training t=7915, loss=0.03247552178800106\n",
      "Surface training t=7916, loss=0.034227728843688965\n",
      "Surface training t=7917, loss=0.03202946484088898\n",
      "Surface training t=7918, loss=0.028375922702252865\n",
      "Surface training t=7919, loss=0.03416351415216923\n",
      "Surface training t=7920, loss=0.03384880814701319\n",
      "Surface training t=7921, loss=0.028413357213139534\n",
      "Surface training t=7922, loss=0.027423083782196045\n",
      "Surface training t=7923, loss=0.03259052708745003\n",
      "Surface training t=7924, loss=0.02904126327484846\n",
      "Surface training t=7925, loss=0.027559039182960987\n",
      "Surface training t=7926, loss=0.02714661695063114\n",
      "Surface training t=7927, loss=0.03146187402307987\n",
      "Surface training t=7928, loss=0.023318467661738396\n",
      "Surface training t=7929, loss=0.024432464502751827\n",
      "Surface training t=7930, loss=0.03390817157924175\n",
      "Surface training t=7931, loss=0.02862958237528801\n",
      "Surface training t=7932, loss=0.02953080739825964\n",
      "Surface training t=7933, loss=0.029891262762248516\n",
      "Surface training t=7934, loss=0.026202762499451637\n",
      "Surface training t=7935, loss=0.027217188850045204\n",
      "Surface training t=7936, loss=0.03432360850274563\n",
      "Surface training t=7937, loss=0.03397503308951855\n",
      "Surface training t=7938, loss=0.04454224556684494\n",
      "Surface training t=7939, loss=0.040551941841840744\n",
      "Surface training t=7940, loss=0.03259950876235962\n",
      "Surface training t=7941, loss=0.04965721070766449\n",
      "Surface training t=7942, loss=0.045989775098860264\n",
      "Surface training t=7943, loss=0.04399549029767513\n",
      "Surface training t=7944, loss=0.04249139316380024\n",
      "Surface training t=7945, loss=0.03511985018849373\n",
      "Surface training t=7946, loss=0.0270694587379694\n",
      "Surface training t=7947, loss=0.05228930152952671\n",
      "Surface training t=7948, loss=0.062398700043559074\n",
      "Surface training t=7949, loss=0.04786863923072815\n",
      "Surface training t=7950, loss=0.048993926495313644\n",
      "Surface training t=7951, loss=0.0409467164427042\n",
      "Surface training t=7952, loss=0.0494582187384367\n",
      "Surface training t=7953, loss=0.039420073851943016\n",
      "Surface training t=7954, loss=0.03804119676351547\n",
      "Surface training t=7955, loss=0.04022488184273243\n",
      "Surface training t=7956, loss=0.03199326619505882\n",
      "Surface training t=7957, loss=0.03425561264157295\n",
      "Surface training t=7958, loss=0.030557414516806602\n",
      "Surface training t=7959, loss=0.036431146785616875\n",
      "Surface training t=7960, loss=0.03235068265348673\n",
      "Surface training t=7961, loss=0.036528317257761955\n",
      "Surface training t=7962, loss=0.03467901796102524\n",
      "Surface training t=7963, loss=0.0364993866533041\n",
      "Surface training t=7964, loss=0.047278719022870064\n",
      "Surface training t=7965, loss=0.031996769830584526\n",
      "Surface training t=7966, loss=0.04913174919784069\n",
      "Surface training t=7967, loss=0.04456733725965023\n",
      "Surface training t=7968, loss=0.03241548966616392\n",
      "Surface training t=7969, loss=0.04564506560564041\n",
      "Surface training t=7970, loss=0.03386913053691387\n",
      "Surface training t=7971, loss=0.040433911606669426\n",
      "Surface training t=7972, loss=0.03873836062848568\n",
      "Surface training t=7973, loss=0.034350791946053505\n",
      "Surface training t=7974, loss=0.03300510346889496\n",
      "Surface training t=7975, loss=0.03815378248691559\n",
      "Surface training t=7976, loss=0.042170461267232895\n",
      "Surface training t=7977, loss=0.04186094366014004\n",
      "Surface training t=7978, loss=0.05559281446039677\n",
      "Surface training t=7979, loss=0.07859997823834419\n",
      "Surface training t=7980, loss=0.05774959363043308\n",
      "Surface training t=7981, loss=0.051230983808636665\n",
      "Surface training t=7982, loss=0.05141281522810459\n",
      "Surface training t=7983, loss=0.04786410555243492\n",
      "Surface training t=7984, loss=0.046678612008690834\n",
      "Surface training t=7985, loss=0.051049040630459785\n",
      "Surface training t=7986, loss=0.0531237181276083\n",
      "Surface training t=7987, loss=0.05079345591366291\n",
      "Surface training t=7988, loss=0.06396791711449623\n",
      "Surface training t=7989, loss=0.07324148342013359\n",
      "Surface training t=7990, loss=0.0625145323574543\n",
      "Surface training t=7991, loss=0.05921827629208565\n",
      "Surface training t=7992, loss=0.05260344035923481\n",
      "Surface training t=7993, loss=0.0619766004383564\n",
      "Surface training t=7994, loss=0.04767927527427673\n",
      "Surface training t=7995, loss=0.03972122259438038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=7996, loss=0.046960316598415375\n",
      "Surface training t=7997, loss=0.044606005772948265\n",
      "Surface training t=7998, loss=0.05748092383146286\n",
      "Surface training t=7999, loss=0.05876355990767479\n",
      "Surface training t=8000, loss=0.05244305171072483\n",
      "Surface training t=8001, loss=0.066917534917593\n",
      "Surface training t=8002, loss=0.056591471657156944\n",
      "Surface training t=8003, loss=0.052410783246159554\n",
      "Surface training t=8004, loss=0.04822654463350773\n",
      "Surface training t=8005, loss=0.0535406693816185\n",
      "Surface training t=8006, loss=0.0480313915759325\n",
      "Surface training t=8007, loss=0.04356429539620876\n",
      "Surface training t=8008, loss=0.07724859192967415\n",
      "Surface training t=8009, loss=0.06835866905748844\n",
      "Surface training t=8010, loss=0.06696313433349133\n",
      "Surface training t=8011, loss=0.05365658365190029\n",
      "Surface training t=8012, loss=0.06329476088285446\n",
      "Surface training t=8013, loss=0.06599670276045799\n",
      "Surface training t=8014, loss=0.051139770075678825\n",
      "Surface training t=8015, loss=0.04681192897260189\n",
      "Surface training t=8016, loss=0.04207531549036503\n",
      "Surface training t=8017, loss=0.06423193216323853\n",
      "Surface training t=8018, loss=0.037567080929875374\n",
      "Surface training t=8019, loss=0.03291802853345871\n",
      "Surface training t=8020, loss=0.0323818139731884\n",
      "Surface training t=8021, loss=0.03281418699771166\n",
      "Surface training t=8022, loss=0.029949942603707314\n",
      "Surface training t=8023, loss=0.053432734683156013\n",
      "Surface training t=8024, loss=0.05946579948067665\n",
      "Surface training t=8025, loss=0.0766550712287426\n",
      "Surface training t=8026, loss=0.05900576710700989\n",
      "Surface training t=8027, loss=0.0599431786686182\n",
      "Surface training t=8028, loss=0.038824426010251045\n",
      "Surface training t=8029, loss=0.048819306306540966\n",
      "Surface training t=8030, loss=0.057994117960333824\n",
      "Surface training t=8031, loss=0.06769570335745811\n",
      "Surface training t=8032, loss=0.060510050505399704\n",
      "Surface training t=8033, loss=0.04338706284761429\n",
      "Surface training t=8034, loss=0.041200061328709126\n",
      "Surface training t=8035, loss=0.038646871224045753\n",
      "Surface training t=8036, loss=0.04528047889471054\n",
      "Surface training t=8037, loss=0.03985291346907616\n",
      "Surface training t=8038, loss=0.0348511915653944\n",
      "Surface training t=8039, loss=0.04266376979649067\n",
      "Surface training t=8040, loss=0.041563187725842\n",
      "Surface training t=8041, loss=0.04743294417858124\n",
      "Surface training t=8042, loss=0.05247548036277294\n",
      "Surface training t=8043, loss=0.08906527981162071\n",
      "Surface training t=8044, loss=0.06625337339937687\n",
      "Surface training t=8045, loss=0.05950721725821495\n",
      "Surface training t=8046, loss=0.09865357726812363\n",
      "Surface training t=8047, loss=0.06875131651759148\n",
      "Surface training t=8048, loss=0.05947815626859665\n",
      "Surface training t=8049, loss=0.06133977137506008\n",
      "Surface training t=8050, loss=0.07875197380781174\n",
      "Surface training t=8051, loss=0.0554162971675396\n",
      "Surface training t=8052, loss=0.05148315615952015\n",
      "Surface training t=8053, loss=0.1051216684281826\n",
      "Surface training t=8054, loss=0.06054171547293663\n",
      "Surface training t=8055, loss=0.07218816876411438\n",
      "Surface training t=8056, loss=0.04793709143996239\n",
      "Surface training t=8057, loss=0.05391297489404678\n",
      "Surface training t=8058, loss=0.041096385568380356\n",
      "Surface training t=8059, loss=0.06381458416581154\n",
      "Surface training t=8060, loss=0.03942998684942722\n",
      "Surface training t=8061, loss=0.04822292272001505\n",
      "Surface training t=8062, loss=0.04987248592078686\n",
      "Surface training t=8063, loss=0.04977233335375786\n",
      "Surface training t=8064, loss=0.04943549446761608\n",
      "Surface training t=8065, loss=0.05246012471616268\n",
      "Surface training t=8066, loss=0.04710688814520836\n",
      "Surface training t=8067, loss=0.09130764380097389\n",
      "Surface training t=8068, loss=0.0691443495452404\n",
      "Surface training t=8069, loss=0.0719300638884306\n",
      "Surface training t=8070, loss=0.1155046857893467\n",
      "Surface training t=8071, loss=0.0863608717918396\n",
      "Surface training t=8072, loss=0.1290760338306427\n",
      "Surface training t=8073, loss=0.0770078394562006\n",
      "Surface training t=8074, loss=0.06481544114649296\n",
      "Surface training t=8075, loss=0.05543074384331703\n",
      "Surface training t=8076, loss=0.06603675335645676\n",
      "Surface training t=8077, loss=0.0395981278270483\n",
      "Surface training t=8078, loss=0.04835228994488716\n",
      "Surface training t=8079, loss=0.031959653832018375\n",
      "Surface training t=8080, loss=0.02995497640222311\n",
      "Surface training t=8081, loss=0.05588173680007458\n",
      "Surface training t=8082, loss=0.04186180792748928\n",
      "Surface training t=8083, loss=0.048685019835829735\n",
      "Surface training t=8084, loss=0.0433448851108551\n",
      "Surface training t=8085, loss=0.042452892288565636\n",
      "Surface training t=8086, loss=0.04365427419543266\n",
      "Surface training t=8087, loss=0.029289454221725464\n",
      "Surface training t=8088, loss=0.03348626662045717\n",
      "Surface training t=8089, loss=0.03793897107243538\n",
      "Surface training t=8090, loss=0.02468116581439972\n",
      "Surface training t=8091, loss=0.031907436437904835\n",
      "Surface training t=8092, loss=0.032317714765667915\n",
      "Surface training t=8093, loss=0.026563781313598156\n",
      "Surface training t=8094, loss=0.022727075032889843\n",
      "Surface training t=8095, loss=0.03579657804220915\n",
      "Surface training t=8096, loss=0.04424657113850117\n",
      "Surface training t=8097, loss=0.037659620866179466\n",
      "Surface training t=8098, loss=0.03282981179654598\n",
      "Surface training t=8099, loss=0.03481563739478588\n",
      "Surface training t=8100, loss=0.029408574104309082\n",
      "Surface training t=8101, loss=0.02693677321076393\n",
      "Surface training t=8102, loss=0.030976291745901108\n",
      "Surface training t=8103, loss=0.02750521060079336\n",
      "Surface training t=8104, loss=0.025818735361099243\n",
      "Surface training t=8105, loss=0.02655072696506977\n",
      "Surface training t=8106, loss=0.028271662071347237\n",
      "Surface training t=8107, loss=0.028822663240134716\n",
      "Surface training t=8108, loss=0.02427054289728403\n",
      "Surface training t=8109, loss=0.03222198970615864\n",
      "Surface training t=8110, loss=0.04110930021852255\n",
      "Surface training t=8111, loss=0.06116233766078949\n",
      "Surface training t=8112, loss=0.045671362429857254\n",
      "Surface training t=8113, loss=0.04673242010176182\n",
      "Surface training t=8114, loss=0.06310956366360188\n",
      "Surface training t=8115, loss=0.05656692199409008\n",
      "Surface training t=8116, loss=0.04347559995949268\n",
      "Surface training t=8117, loss=0.03094171267002821\n",
      "Surface training t=8118, loss=0.037192732095718384\n",
      "Surface training t=8119, loss=0.02900402620434761\n",
      "Surface training t=8120, loss=0.031122020445764065\n",
      "Surface training t=8121, loss=0.03804511111229658\n",
      "Surface training t=8122, loss=0.03477000817656517\n",
      "Surface training t=8123, loss=0.03067579586058855\n",
      "Surface training t=8124, loss=0.02995970007032156\n",
      "Surface training t=8125, loss=0.02678900584578514\n",
      "Surface training t=8126, loss=0.028556360863149166\n",
      "Surface training t=8127, loss=0.03184475935995579\n",
      "Surface training t=8128, loss=0.031061798334121704\n",
      "Surface training t=8129, loss=0.024627897888422012\n",
      "Surface training t=8130, loss=0.025856911204755306\n",
      "Surface training t=8131, loss=0.03638261556625366\n",
      "Surface training t=8132, loss=0.038384029641747475\n",
      "Surface training t=8133, loss=0.04047386161983013\n",
      "Surface training t=8134, loss=0.05561373755335808\n",
      "Surface training t=8135, loss=0.06265822239220142\n",
      "Surface training t=8136, loss=0.04124743863940239\n",
      "Surface training t=8137, loss=0.047410354018211365\n",
      "Surface training t=8138, loss=0.0400670412927866\n",
      "Surface training t=8139, loss=0.036313554272055626\n",
      "Surface training t=8140, loss=0.04476672783493996\n",
      "Surface training t=8141, loss=0.04183358885347843\n",
      "Surface training t=8142, loss=0.04391285218298435\n",
      "Surface training t=8143, loss=0.07128050923347473\n",
      "Surface training t=8144, loss=0.05363134853541851\n",
      "Surface training t=8145, loss=0.056044312193989754\n",
      "Surface training t=8146, loss=0.05099867284297943\n",
      "Surface training t=8147, loss=0.04563993867486715\n",
      "Surface training t=8148, loss=0.042807312682271004\n",
      "Surface training t=8149, loss=0.03869009669870138\n",
      "Surface training t=8150, loss=0.056853169575333595\n",
      "Surface training t=8151, loss=0.052692048251628876\n",
      "Surface training t=8152, loss=0.048433514311909676\n",
      "Surface training t=8153, loss=0.0515508521348238\n",
      "Surface training t=8154, loss=0.041842930018901825\n",
      "Surface training t=8155, loss=0.03878939338028431\n",
      "Surface training t=8156, loss=0.06360275484621525\n",
      "Surface training t=8157, loss=0.05042772740125656\n",
      "Surface training t=8158, loss=0.046639105305075645\n",
      "Surface training t=8159, loss=0.03837432060390711\n",
      "Surface training t=8160, loss=0.040753064677119255\n",
      "Surface training t=8161, loss=0.03596400283277035\n",
      "Surface training t=8162, loss=0.03594751749187708\n",
      "Surface training t=8163, loss=0.04476146213710308\n",
      "Surface training t=8164, loss=0.03665538877248764\n",
      "Surface training t=8165, loss=0.034072921611368656\n",
      "Surface training t=8166, loss=0.03747590258717537\n",
      "Surface training t=8167, loss=0.044404828920960426\n",
      "Surface training t=8168, loss=0.030271442607045174\n",
      "Surface training t=8169, loss=0.02947387844324112\n",
      "Surface training t=8170, loss=0.027790487743914127\n",
      "Surface training t=8171, loss=0.039407284930348396\n",
      "Surface training t=8172, loss=0.04433217830955982\n",
      "Surface training t=8173, loss=0.03123011626303196\n",
      "Surface training t=8174, loss=0.048464447259902954\n",
      "Surface training t=8175, loss=0.043179234489798546\n",
      "Surface training t=8176, loss=0.04622933454811573\n",
      "Surface training t=8177, loss=0.05165672302246094\n",
      "Surface training t=8178, loss=0.04566228576004505\n",
      "Surface training t=8179, loss=0.04229218140244484\n",
      "Surface training t=8180, loss=0.05138352885842323\n",
      "Surface training t=8181, loss=0.06289944797754288\n",
      "Surface training t=8182, loss=0.07240577973425388\n",
      "Surface training t=8183, loss=0.10117115452885628\n",
      "Surface training t=8184, loss=0.06165003776550293\n",
      "Surface training t=8185, loss=0.06251878291368484\n",
      "Surface training t=8186, loss=0.0598540473729372\n",
      "Surface training t=8187, loss=0.08690063282847404\n",
      "Surface training t=8188, loss=0.05849253013730049\n",
      "Surface training t=8189, loss=0.050717780366539955\n",
      "Surface training t=8190, loss=0.04376719333231449\n",
      "Surface training t=8191, loss=0.07949284464120865\n",
      "Surface training t=8192, loss=0.06556863710284233\n",
      "Surface training t=8193, loss=0.05963302031159401\n",
      "Surface training t=8194, loss=0.06973342224955559\n",
      "Surface training t=8195, loss=0.12080088257789612\n",
      "Surface training t=8196, loss=0.07484642416238785\n",
      "Surface training t=8197, loss=0.09888829663395882\n",
      "Surface training t=8198, loss=0.10479523614048958\n",
      "Surface training t=8199, loss=0.07088159956037998\n",
      "Surface training t=8200, loss=0.11270161718130112\n",
      "Surface training t=8201, loss=0.077692411839962\n",
      "Surface training t=8202, loss=0.10548760741949081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=8203, loss=0.1050790511071682\n",
      "Surface training t=8204, loss=0.06701423786580563\n",
      "Surface training t=8205, loss=0.051142552867531776\n",
      "Surface training t=8206, loss=0.06305070035159588\n",
      "Surface training t=8207, loss=0.05264591798186302\n",
      "Surface training t=8208, loss=0.041321054100990295\n",
      "Surface training t=8209, loss=0.037438973784446716\n",
      "Surface training t=8210, loss=0.028422240167856216\n",
      "Surface training t=8211, loss=0.029197183437645435\n",
      "Surface training t=8212, loss=0.02552660834044218\n",
      "Surface training t=8213, loss=0.02770450245589018\n",
      "Surface training t=8214, loss=0.026722535490989685\n",
      "Surface training t=8215, loss=0.025469756685197353\n",
      "Surface training t=8216, loss=0.03209039755165577\n",
      "Surface training t=8217, loss=0.03133351635187864\n",
      "Surface training t=8218, loss=0.03225865215063095\n",
      "Surface training t=8219, loss=0.035759568214416504\n",
      "Surface training t=8220, loss=0.04295361787080765\n",
      "Surface training t=8221, loss=0.050283435732126236\n",
      "Surface training t=8222, loss=0.04076569527387619\n",
      "Surface training t=8223, loss=0.04812800511717796\n",
      "Surface training t=8224, loss=0.040821123868227005\n",
      "Surface training t=8225, loss=0.03986472077667713\n",
      "Surface training t=8226, loss=0.040241312235593796\n",
      "Surface training t=8227, loss=0.03669113479554653\n",
      "Surface training t=8228, loss=0.02714124321937561\n",
      "Surface training t=8229, loss=0.03180541843175888\n",
      "Surface training t=8230, loss=0.03145965654402971\n",
      "Surface training t=8231, loss=0.027090751565992832\n",
      "Surface training t=8232, loss=0.027400050312280655\n",
      "Surface training t=8233, loss=0.03858805075287819\n",
      "Surface training t=8234, loss=0.030322750099003315\n",
      "Surface training t=8235, loss=0.03352586179971695\n",
      "Surface training t=8236, loss=0.038561574183404446\n",
      "Surface training t=8237, loss=0.03510492108762264\n",
      "Surface training t=8238, loss=0.04075220227241516\n",
      "Surface training t=8239, loss=0.04845240339636803\n",
      "Surface training t=8240, loss=0.04288442060351372\n",
      "Surface training t=8241, loss=0.04298478364944458\n",
      "Surface training t=8242, loss=0.04107731021940708\n",
      "Surface training t=8243, loss=0.039881715551018715\n",
      "Surface training t=8244, loss=0.03782412223517895\n",
      "Surface training t=8245, loss=0.03211548551917076\n",
      "Surface training t=8246, loss=0.02940355520695448\n",
      "Surface training t=8247, loss=0.030091166496276855\n",
      "Surface training t=8248, loss=0.04272838495671749\n",
      "Surface training t=8249, loss=0.0569680780172348\n",
      "Surface training t=8250, loss=0.03569667600095272\n",
      "Surface training t=8251, loss=0.04104631952941418\n",
      "Surface training t=8252, loss=0.06105398014187813\n",
      "Surface training t=8253, loss=0.04898250848054886\n",
      "Surface training t=8254, loss=0.04473084583878517\n",
      "Surface training t=8255, loss=0.03325364459306002\n",
      "Surface training t=8256, loss=0.03232808969914913\n",
      "Surface training t=8257, loss=0.03247377369552851\n",
      "Surface training t=8258, loss=0.046681586652994156\n",
      "Surface training t=8259, loss=0.030174356885254383\n",
      "Surface training t=8260, loss=0.031153300777077675\n",
      "Surface training t=8261, loss=0.032263096421957016\n",
      "Surface training t=8262, loss=0.04919477179646492\n",
      "Surface training t=8263, loss=0.037971118465065956\n",
      "Surface training t=8264, loss=0.050030261278152466\n",
      "Surface training t=8265, loss=0.055599767714738846\n",
      "Surface training t=8266, loss=0.04354579746723175\n",
      "Surface training t=8267, loss=0.03752061910927296\n",
      "Surface training t=8268, loss=0.03792121447622776\n",
      "Surface training t=8269, loss=0.032529592514038086\n",
      "Surface training t=8270, loss=0.030885319225490093\n",
      "Surface training t=8271, loss=0.03380708210170269\n",
      "Surface training t=8272, loss=0.026754316873848438\n",
      "Surface training t=8273, loss=0.03183779865503311\n",
      "Surface training t=8274, loss=0.03255602903664112\n",
      "Surface training t=8275, loss=0.03331843763589859\n",
      "Surface training t=8276, loss=0.03526376932859421\n",
      "Surface training t=8277, loss=0.034892475232481956\n",
      "Surface training t=8278, loss=0.02781333588063717\n",
      "Surface training t=8279, loss=0.03267665021121502\n",
      "Surface training t=8280, loss=0.03394125681370497\n",
      "Surface training t=8281, loss=0.03565680794417858\n",
      "Surface training t=8282, loss=0.03194417618215084\n",
      "Surface training t=8283, loss=0.03506183996796608\n",
      "Surface training t=8284, loss=0.0393892340362072\n",
      "Surface training t=8285, loss=0.04644952341914177\n",
      "Surface training t=8286, loss=0.03334442339837551\n",
      "Surface training t=8287, loss=0.044795384630560875\n",
      "Surface training t=8288, loss=0.0440982561558485\n",
      "Surface training t=8289, loss=0.03905697539448738\n",
      "Surface training t=8290, loss=0.04885396547615528\n",
      "Surface training t=8291, loss=0.06349024549126625\n",
      "Surface training t=8292, loss=0.048866186290979385\n",
      "Surface training t=8293, loss=0.05403919890522957\n",
      "Surface training t=8294, loss=0.044029600918293\n",
      "Surface training t=8295, loss=0.044656009413301945\n",
      "Surface training t=8296, loss=0.04117082431912422\n",
      "Surface training t=8297, loss=0.04333432111889124\n",
      "Surface training t=8298, loss=0.04931427538394928\n",
      "Surface training t=8299, loss=0.04870997555553913\n",
      "Surface training t=8300, loss=0.06307640112936497\n",
      "Surface training t=8301, loss=0.05061435513198376\n",
      "Surface training t=8302, loss=0.041544850915670395\n",
      "Surface training t=8303, loss=0.03594118356704712\n",
      "Surface training t=8304, loss=0.03341960348188877\n",
      "Surface training t=8305, loss=0.037882596254348755\n",
      "Surface training t=8306, loss=0.04215595871210098\n",
      "Surface training t=8307, loss=0.042546797543764114\n",
      "Surface training t=8308, loss=0.0715254582464695\n",
      "Surface training t=8309, loss=0.04326873365789652\n",
      "Surface training t=8310, loss=0.041619038209319115\n",
      "Surface training t=8311, loss=0.05803236924111843\n",
      "Surface training t=8312, loss=0.042247370816767216\n",
      "Surface training t=8313, loss=0.06798036023974419\n",
      "Surface training t=8314, loss=0.05275349318981171\n",
      "Surface training t=8315, loss=0.06363633833825588\n",
      "Surface training t=8316, loss=0.06704353354871273\n",
      "Surface training t=8317, loss=0.04450143314898014\n",
      "Surface training t=8318, loss=0.05504298582673073\n",
      "Surface training t=8319, loss=0.05227966792881489\n",
      "Surface training t=8320, loss=0.05487368814647198\n",
      "Surface training t=8321, loss=0.05600308068096638\n",
      "Surface training t=8322, loss=0.03367375861853361\n",
      "Surface training t=8323, loss=0.029225997626781464\n",
      "Surface training t=8324, loss=0.038667427375912666\n",
      "Surface training t=8325, loss=0.03169070743024349\n",
      "Surface training t=8326, loss=0.029472491703927517\n",
      "Surface training t=8327, loss=0.034587014466524124\n",
      "Surface training t=8328, loss=0.02668710146099329\n",
      "Surface training t=8329, loss=0.033562103286385536\n",
      "Surface training t=8330, loss=0.031366356648504734\n",
      "Surface training t=8331, loss=0.026886671781539917\n",
      "Surface training t=8332, loss=0.028743427246809006\n",
      "Surface training t=8333, loss=0.03432873263955116\n",
      "Surface training t=8334, loss=0.027495136484503746\n",
      "Surface training t=8335, loss=0.026656211353838444\n",
      "Surface training t=8336, loss=0.04255424253642559\n",
      "Surface training t=8337, loss=0.029918868094682693\n",
      "Surface training t=8338, loss=0.02684694714844227\n",
      "Surface training t=8339, loss=0.02610689029097557\n",
      "Surface training t=8340, loss=0.024049967527389526\n",
      "Surface training t=8341, loss=0.02337418496608734\n",
      "Surface training t=8342, loss=0.025275783613324165\n",
      "Surface training t=8343, loss=0.02615114487707615\n",
      "Surface training t=8344, loss=0.029808033257722855\n",
      "Surface training t=8345, loss=0.02931084856390953\n",
      "Surface training t=8346, loss=0.032859621569514275\n",
      "Surface training t=8347, loss=0.03852892108261585\n",
      "Surface training t=8348, loss=0.0383668914437294\n",
      "Surface training t=8349, loss=0.04166810028254986\n",
      "Surface training t=8350, loss=0.04321316536515951\n",
      "Surface training t=8351, loss=0.05153868347406387\n",
      "Surface training t=8352, loss=0.048231277614831924\n",
      "Surface training t=8353, loss=0.04611060582101345\n",
      "Surface training t=8354, loss=0.035844586789608\n",
      "Surface training t=8355, loss=0.0422224048525095\n",
      "Surface training t=8356, loss=0.03250591550022364\n",
      "Surface training t=8357, loss=0.03600284084677696\n",
      "Surface training t=8358, loss=0.03784950077533722\n",
      "Surface training t=8359, loss=0.03339073061943054\n",
      "Surface training t=8360, loss=0.03222723864018917\n",
      "Surface training t=8361, loss=0.03175214026123285\n",
      "Surface training t=8362, loss=0.026254245080053806\n",
      "Surface training t=8363, loss=0.02715477254241705\n",
      "Surface training t=8364, loss=0.0381023995578289\n",
      "Surface training t=8365, loss=0.04558165185153484\n",
      "Surface training t=8366, loss=0.03880011476576328\n",
      "Surface training t=8367, loss=0.05082217790186405\n",
      "Surface training t=8368, loss=0.05200410261750221\n",
      "Surface training t=8369, loss=0.03830001689493656\n",
      "Surface training t=8370, loss=0.03267601318657398\n",
      "Surface training t=8371, loss=0.04293033666908741\n",
      "Surface training t=8372, loss=0.02972140721976757\n",
      "Surface training t=8373, loss=0.03866324760019779\n",
      "Surface training t=8374, loss=0.030847922898828983\n",
      "Surface training t=8375, loss=0.028333532623946667\n",
      "Surface training t=8376, loss=0.03810955211520195\n",
      "Surface training t=8377, loss=0.04928925447165966\n",
      "Surface training t=8378, loss=0.028605449944734573\n",
      "Surface training t=8379, loss=0.02545198332518339\n",
      "Surface training t=8380, loss=0.031312370672822\n",
      "Surface training t=8381, loss=0.02683196309953928\n",
      "Surface training t=8382, loss=0.0257329223677516\n",
      "Surface training t=8383, loss=0.0344505263492465\n",
      "Surface training t=8384, loss=0.03682582266628742\n",
      "Surface training t=8385, loss=0.032837086357176304\n",
      "Surface training t=8386, loss=0.030305813997983932\n",
      "Surface training t=8387, loss=0.0403779037296772\n",
      "Surface training t=8388, loss=0.032369714230298996\n",
      "Surface training t=8389, loss=0.034729027189314365\n",
      "Surface training t=8390, loss=0.04590030387043953\n",
      "Surface training t=8391, loss=0.039511680603027344\n",
      "Surface training t=8392, loss=0.032553404569625854\n",
      "Surface training t=8393, loss=0.03553882986307144\n",
      "Surface training t=8394, loss=0.03719790652394295\n",
      "Surface training t=8395, loss=0.04524189978837967\n",
      "Surface training t=8396, loss=0.047144729644060135\n",
      "Surface training t=8397, loss=0.03634160757064819\n",
      "Surface training t=8398, loss=0.043530721217393875\n",
      "Surface training t=8399, loss=0.05275377258658409\n",
      "Surface training t=8400, loss=0.0672764852643013\n",
      "Surface training t=8401, loss=0.04361075162887573\n",
      "Surface training t=8402, loss=0.040113311260938644\n",
      "Surface training t=8403, loss=0.04099966585636139\n",
      "Surface training t=8404, loss=0.03041326440870762\n",
      "Surface training t=8405, loss=0.037634195759892464\n",
      "Surface training t=8406, loss=0.047636423259973526\n",
      "Surface training t=8407, loss=0.026269052177667618\n",
      "Surface training t=8408, loss=0.03270510025322437\n",
      "Surface training t=8409, loss=0.03162575140595436\n",
      "Surface training t=8410, loss=0.026964737102389336\n",
      "Surface training t=8411, loss=0.0223327549174428\n",
      "Surface training t=8412, loss=0.025426611304283142\n",
      "Surface training t=8413, loss=0.02558442112058401\n",
      "Surface training t=8414, loss=0.03787286952137947\n",
      "Surface training t=8415, loss=0.029593749903142452\n",
      "Surface training t=8416, loss=0.027819769456982613\n",
      "Surface training t=8417, loss=0.03179425932466984\n",
      "Surface training t=8418, loss=0.02364980150014162\n",
      "Surface training t=8419, loss=0.037036117166280746\n",
      "Surface training t=8420, loss=0.03300974331796169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=8421, loss=0.03806858882308006\n",
      "Surface training t=8422, loss=0.03419074974954128\n",
      "Surface training t=8423, loss=0.03777211345732212\n",
      "Surface training t=8424, loss=0.050778916105628014\n",
      "Surface training t=8425, loss=0.052591562271118164\n",
      "Surface training t=8426, loss=0.0322908740490675\n",
      "Surface training t=8427, loss=0.037472331896424294\n",
      "Surface training t=8428, loss=0.045466091483831406\n",
      "Surface training t=8429, loss=0.032402606680989265\n",
      "Surface training t=8430, loss=0.027437803335487843\n",
      "Surface training t=8431, loss=0.023364226333796978\n",
      "Surface training t=8432, loss=0.03125680051743984\n",
      "Surface training t=8433, loss=0.04286826401948929\n",
      "Surface training t=8434, loss=0.03168961778283119\n",
      "Surface training t=8435, loss=0.03529270365834236\n",
      "Surface training t=8436, loss=0.04103880934417248\n",
      "Surface training t=8437, loss=0.033197845332324505\n",
      "Surface training t=8438, loss=0.042486466467380524\n",
      "Surface training t=8439, loss=0.03246404882520437\n",
      "Surface training t=8440, loss=0.03483757562935352\n",
      "Surface training t=8441, loss=0.04459569416940212\n",
      "Surface training t=8442, loss=0.04324157629162073\n",
      "Surface training t=8443, loss=0.03239503502845764\n",
      "Surface training t=8444, loss=0.03425284381955862\n",
      "Surface training t=8445, loss=0.038818007335066795\n",
      "Surface training t=8446, loss=0.03394310548901558\n",
      "Surface training t=8447, loss=0.03562464192509651\n",
      "Surface training t=8448, loss=0.04485797602683306\n",
      "Surface training t=8449, loss=0.05094423331320286\n",
      "Surface training t=8450, loss=0.05288161151111126\n",
      "Surface training t=8451, loss=0.046684570610523224\n",
      "Surface training t=8452, loss=0.046377796679735184\n",
      "Surface training t=8453, loss=0.03279853332787752\n",
      "Surface training t=8454, loss=0.028990505263209343\n",
      "Surface training t=8455, loss=0.025056163780391216\n",
      "Surface training t=8456, loss=0.02564324624836445\n",
      "Surface training t=8457, loss=0.0271984301507473\n",
      "Surface training t=8458, loss=0.033239033073186874\n",
      "Surface training t=8459, loss=0.03610076941549778\n",
      "Surface training t=8460, loss=0.0387368518859148\n",
      "Surface training t=8461, loss=0.027498728595674038\n",
      "Surface training t=8462, loss=0.03640221059322357\n",
      "Surface training t=8463, loss=0.0367925688624382\n",
      "Surface training t=8464, loss=0.030356822535395622\n",
      "Surface training t=8465, loss=0.037000445649027824\n",
      "Surface training t=8466, loss=0.035992274060845375\n",
      "Surface training t=8467, loss=0.04097479581832886\n",
      "Surface training t=8468, loss=0.03730248287320137\n",
      "Surface training t=8469, loss=0.03252730518579483\n",
      "Surface training t=8470, loss=0.03213957138359547\n",
      "Surface training t=8471, loss=0.03400816582143307\n",
      "Surface training t=8472, loss=0.03718698490411043\n",
      "Surface training t=8473, loss=0.042735276743769646\n",
      "Surface training t=8474, loss=0.029819616116583347\n",
      "Surface training t=8475, loss=0.029783503152430058\n",
      "Surface training t=8476, loss=0.023450659587979317\n",
      "Surface training t=8477, loss=0.03718236833810806\n",
      "Surface training t=8478, loss=0.03208225034177303\n",
      "Surface training t=8479, loss=0.028389006853103638\n",
      "Surface training t=8480, loss=0.028559256345033646\n",
      "Surface training t=8481, loss=0.02862981054931879\n",
      "Surface training t=8482, loss=0.03089744597673416\n",
      "Surface training t=8483, loss=0.029224512167274952\n",
      "Surface training t=8484, loss=0.02565834391862154\n",
      "Surface training t=8485, loss=0.03221403528004885\n",
      "Surface training t=8486, loss=0.04047921672463417\n",
      "Surface training t=8487, loss=0.052191002294421196\n",
      "Surface training t=8488, loss=0.06453154049813747\n",
      "Surface training t=8489, loss=0.09307565540075302\n",
      "Surface training t=8490, loss=0.08618203923106194\n",
      "Surface training t=8491, loss=0.06189657002687454\n",
      "Surface training t=8492, loss=0.07159928232431412\n",
      "Surface training t=8493, loss=0.0886184312403202\n",
      "Surface training t=8494, loss=0.06568017415702343\n",
      "Surface training t=8495, loss=0.07179119810461998\n",
      "Surface training t=8496, loss=0.09717674180865288\n",
      "Surface training t=8497, loss=0.05346042197197676\n",
      "Surface training t=8498, loss=0.05747765488922596\n",
      "Surface training t=8499, loss=0.0552070178091526\n",
      "Surface training t=8500, loss=0.0552162267267704\n",
      "Surface training t=8501, loss=0.05395914614200592\n",
      "Surface training t=8502, loss=0.03648000955581665\n",
      "Surface training t=8503, loss=0.04512394033372402\n",
      "Surface training t=8504, loss=0.038451265543699265\n",
      "Surface training t=8505, loss=0.030515579506754875\n",
      "Surface training t=8506, loss=0.0325472317636013\n",
      "Surface training t=8507, loss=0.03455593064427376\n",
      "Surface training t=8508, loss=0.02706945687532425\n",
      "Surface training t=8509, loss=0.02977156825363636\n",
      "Surface training t=8510, loss=0.035434963181614876\n",
      "Surface training t=8511, loss=0.04139469750225544\n",
      "Surface training t=8512, loss=0.039951881393790245\n",
      "Surface training t=8513, loss=0.0402162317186594\n",
      "Surface training t=8514, loss=0.03363301698118448\n",
      "Surface training t=8515, loss=0.033002725802361965\n",
      "Surface training t=8516, loss=0.030719948932528496\n",
      "Surface training t=8517, loss=0.026906698942184448\n",
      "Surface training t=8518, loss=0.0331565048545599\n",
      "Surface training t=8519, loss=0.04133288562297821\n",
      "Surface training t=8520, loss=0.035303485579788685\n",
      "Surface training t=8521, loss=0.0372987249866128\n",
      "Surface training t=8522, loss=0.03956822492182255\n",
      "Surface training t=8523, loss=0.040961854159832\n",
      "Surface training t=8524, loss=0.04003574699163437\n",
      "Surface training t=8525, loss=0.034416559152305126\n",
      "Surface training t=8526, loss=0.06524260342121124\n",
      "Surface training t=8527, loss=0.04396708682179451\n",
      "Surface training t=8528, loss=0.04731481708586216\n",
      "Surface training t=8529, loss=0.03747159894555807\n",
      "Surface training t=8530, loss=0.043040016666054726\n",
      "Surface training t=8531, loss=0.06075005978345871\n",
      "Surface training t=8532, loss=0.05298566073179245\n",
      "Surface training t=8533, loss=0.05549537856131792\n",
      "Surface training t=8534, loss=0.06464127264916897\n",
      "Surface training t=8535, loss=0.05447326973080635\n",
      "Surface training t=8536, loss=0.04391613230109215\n",
      "Surface training t=8537, loss=0.06078867241740227\n",
      "Surface training t=8538, loss=0.05610815808176994\n",
      "Surface training t=8539, loss=0.03359811380505562\n",
      "Surface training t=8540, loss=0.04405696876347065\n",
      "Surface training t=8541, loss=0.055155180394649506\n",
      "Surface training t=8542, loss=0.05335861071944237\n",
      "Surface training t=8543, loss=0.04175317846238613\n",
      "Surface training t=8544, loss=0.028176273219287395\n",
      "Surface training t=8545, loss=0.04057690314948559\n",
      "Surface training t=8546, loss=0.034207940101623535\n",
      "Surface training t=8547, loss=0.028108949773013592\n",
      "Surface training t=8548, loss=0.0308383796364069\n",
      "Surface training t=8549, loss=0.027424631640315056\n",
      "Surface training t=8550, loss=0.042028434574604034\n",
      "Surface training t=8551, loss=0.04838795028626919\n",
      "Surface training t=8552, loss=0.07060546427965164\n",
      "Surface training t=8553, loss=0.053999973461031914\n",
      "Surface training t=8554, loss=0.05707959458231926\n",
      "Surface training t=8555, loss=0.048775430768728256\n",
      "Surface training t=8556, loss=0.056771181523799896\n",
      "Surface training t=8557, loss=0.04356921836733818\n",
      "Surface training t=8558, loss=0.04186958447098732\n",
      "Surface training t=8559, loss=0.058652590960264206\n",
      "Surface training t=8560, loss=0.03745941258966923\n",
      "Surface training t=8561, loss=0.054407453164458275\n",
      "Surface training t=8562, loss=0.038462741300463676\n",
      "Surface training t=8563, loss=0.026944664306938648\n",
      "Surface training t=8564, loss=0.030147714540362358\n",
      "Surface training t=8565, loss=0.04545678198337555\n",
      "Surface training t=8566, loss=0.042426759377121925\n",
      "Surface training t=8567, loss=0.04554985277354717\n",
      "Surface training t=8568, loss=0.04399576969444752\n",
      "Surface training t=8569, loss=0.043260009959340096\n",
      "Surface training t=8570, loss=0.05252891406416893\n",
      "Surface training t=8571, loss=0.05120967701077461\n",
      "Surface training t=8572, loss=0.04553987458348274\n",
      "Surface training t=8573, loss=0.04056198708713055\n",
      "Surface training t=8574, loss=0.034575385972857475\n",
      "Surface training t=8575, loss=0.038320910185575485\n",
      "Surface training t=8576, loss=0.05548761785030365\n",
      "Surface training t=8577, loss=0.03917786851525307\n",
      "Surface training t=8578, loss=0.0686364583671093\n",
      "Surface training t=8579, loss=0.058101486414670944\n",
      "Surface training t=8580, loss=0.053437111899256706\n",
      "Surface training t=8581, loss=0.06120077148079872\n",
      "Surface training t=8582, loss=0.0559074692428112\n",
      "Surface training t=8583, loss=0.06433731317520142\n",
      "Surface training t=8584, loss=0.0512600839138031\n",
      "Surface training t=8585, loss=0.054759399965405464\n",
      "Surface training t=8586, loss=0.05439741536974907\n",
      "Surface training t=8587, loss=0.060668254271149635\n",
      "Surface training t=8588, loss=0.04505122732371092\n",
      "Surface training t=8589, loss=0.04170176200568676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=8590, loss=0.04120366647839546\n",
      "Surface training t=8591, loss=0.04495367035269737\n",
      "Surface training t=8592, loss=0.06304826959967613\n",
      "Surface training t=8593, loss=0.040725940838456154\n",
      "Surface training t=8594, loss=0.04425687901675701\n",
      "Surface training t=8595, loss=0.050428127869963646\n",
      "Surface training t=8596, loss=0.03355045057833195\n",
      "Surface training t=8597, loss=0.034738507121801376\n",
      "Surface training t=8598, loss=0.04613933898508549\n",
      "Surface training t=8599, loss=0.04045627452433109\n",
      "Surface training t=8600, loss=0.04175115469843149\n",
      "Surface training t=8601, loss=0.045630889013409615\n",
      "Surface training t=8602, loss=0.03943430818617344\n",
      "Surface training t=8603, loss=0.045000456273555756\n",
      "Surface training t=8604, loss=0.061066970229148865\n",
      "Surface training t=8605, loss=0.043608732521533966\n",
      "Surface training t=8606, loss=0.05655316822230816\n",
      "Surface training t=8607, loss=0.05448123812675476\n",
      "Surface training t=8608, loss=0.07207123935222626\n",
      "Surface training t=8609, loss=0.06199603155255318\n",
      "Surface training t=8610, loss=0.06298527307808399\n",
      "Surface training t=8611, loss=0.05331350862979889\n",
      "Surface training t=8612, loss=0.05763895809650421\n",
      "Surface training t=8613, loss=0.04100610688328743\n",
      "Surface training t=8614, loss=0.04518002085387707\n",
      "Surface training t=8615, loss=0.060694461688399315\n",
      "Surface training t=8616, loss=0.054858364164829254\n",
      "Surface training t=8617, loss=0.03534819558262825\n",
      "Surface training t=8618, loss=0.035146718844771385\n",
      "Surface training t=8619, loss=0.04802039824426174\n",
      "Surface training t=8620, loss=0.04719438962638378\n",
      "Surface training t=8621, loss=0.03302959352731705\n",
      "Surface training t=8622, loss=0.03524607978761196\n",
      "Surface training t=8623, loss=0.02890037838369608\n",
      "Surface training t=8624, loss=0.027474738657474518\n",
      "Surface training t=8625, loss=0.03765398357063532\n",
      "Surface training t=8626, loss=0.03859684430062771\n",
      "Surface training t=8627, loss=0.0340210497379303\n",
      "Surface training t=8628, loss=0.033716704696416855\n",
      "Surface training t=8629, loss=0.032481599599123\n",
      "Surface training t=8630, loss=0.032912345603108406\n",
      "Surface training t=8631, loss=0.02870309818536043\n",
      "Surface training t=8632, loss=0.042248696088790894\n",
      "Surface training t=8633, loss=0.04313143342733383\n",
      "Surface training t=8634, loss=0.027073688805103302\n",
      "Surface training t=8635, loss=0.030360032804310322\n",
      "Surface training t=8636, loss=0.025079960003495216\n",
      "Surface training t=8637, loss=0.03550627175718546\n",
      "Surface training t=8638, loss=0.02797766774892807\n",
      "Surface training t=8639, loss=0.027065834030508995\n",
      "Surface training t=8640, loss=0.03247906267642975\n",
      "Surface training t=8641, loss=0.03101431205868721\n",
      "Surface training t=8642, loss=0.028507594019174576\n",
      "Surface training t=8643, loss=0.02986968494951725\n",
      "Surface training t=8644, loss=0.02500348538160324\n",
      "Surface training t=8645, loss=0.026994885876774788\n",
      "Surface training t=8646, loss=0.026348814368247986\n",
      "Surface training t=8647, loss=0.024944995529949665\n",
      "Surface training t=8648, loss=0.02743593044579029\n",
      "Surface training t=8649, loss=0.03018425777554512\n",
      "Surface training t=8650, loss=0.03739744517952204\n",
      "Surface training t=8651, loss=0.043032802641391754\n",
      "Surface training t=8652, loss=0.04849240183830261\n",
      "Surface training t=8653, loss=0.05252799764275551\n",
      "Surface training t=8654, loss=0.04672188684344292\n",
      "Surface training t=8655, loss=0.04419776052236557\n",
      "Surface training t=8656, loss=0.09107520058751106\n",
      "Surface training t=8657, loss=0.058635422959923744\n",
      "Surface training t=8658, loss=0.05776609480381012\n",
      "Surface training t=8659, loss=0.04633975122123957\n",
      "Surface training t=8660, loss=0.04493186995387077\n",
      "Surface training t=8661, loss=0.04326248541474342\n",
      "Surface training t=8662, loss=0.03413564059883356\n",
      "Surface training t=8663, loss=0.038307154551148415\n",
      "Surface training t=8664, loss=0.03544015809893608\n",
      "Surface training t=8665, loss=0.033610918559134007\n",
      "Surface training t=8666, loss=0.03527624253183603\n",
      "Surface training t=8667, loss=0.046721264719963074\n",
      "Surface training t=8668, loss=0.03470868431031704\n",
      "Surface training t=8669, loss=0.034260597079992294\n",
      "Surface training t=8670, loss=0.04322134144604206\n",
      "Surface training t=8671, loss=0.04057275131344795\n",
      "Surface training t=8672, loss=0.04312007874250412\n",
      "Surface training t=8673, loss=0.04690355062484741\n",
      "Surface training t=8674, loss=0.047358933836221695\n",
      "Surface training t=8675, loss=0.039667416363954544\n",
      "Surface training t=8676, loss=0.02807666826993227\n",
      "Surface training t=8677, loss=0.02808855101466179\n",
      "Surface training t=8678, loss=0.026092523708939552\n",
      "Surface training t=8679, loss=0.03608546871691942\n",
      "Surface training t=8680, loss=0.030877931974828243\n",
      "Surface training t=8681, loss=0.03989415429532528\n",
      "Surface training t=8682, loss=0.03224645461887121\n",
      "Surface training t=8683, loss=0.04041922464966774\n",
      "Surface training t=8684, loss=0.04219259321689606\n",
      "Surface training t=8685, loss=0.04188403859734535\n",
      "Surface training t=8686, loss=0.07089226692914963\n",
      "Surface training t=8687, loss=0.04874473251402378\n",
      "Surface training t=8688, loss=0.04496477730572224\n",
      "Surface training t=8689, loss=0.044590216130018234\n",
      "Surface training t=8690, loss=0.042676907032728195\n",
      "Surface training t=8691, loss=0.048286810517311096\n",
      "Surface training t=8692, loss=0.037013232707977295\n",
      "Surface training t=8693, loss=0.02616109699010849\n",
      "Surface training t=8694, loss=0.03047047834843397\n",
      "Surface training t=8695, loss=0.029797937721014023\n",
      "Surface training t=8696, loss=0.022512580268085003\n",
      "Surface training t=8697, loss=0.021005917340517044\n",
      "Surface training t=8698, loss=0.0324371550232172\n",
      "Surface training t=8699, loss=0.026143740862607956\n",
      "Surface training t=8700, loss=0.02790961228311062\n",
      "Surface training t=8701, loss=0.032369882799685\n",
      "Surface training t=8702, loss=0.033891184255480766\n",
      "Surface training t=8703, loss=0.03586271870881319\n",
      "Surface training t=8704, loss=0.034093535505235195\n",
      "Surface training t=8705, loss=0.025244192220270634\n",
      "Surface training t=8706, loss=0.03135286271572113\n",
      "Surface training t=8707, loss=0.03969708830118179\n",
      "Surface training t=8708, loss=0.03741838317364454\n",
      "Surface training t=8709, loss=0.0379132479429245\n",
      "Surface training t=8710, loss=0.02279147319495678\n",
      "Surface training t=8711, loss=0.022180035710334778\n",
      "Surface training t=8712, loss=0.026722377166152\n",
      "Surface training t=8713, loss=0.034969648346304893\n",
      "Surface training t=8714, loss=0.019953135401010513\n",
      "Surface training t=8715, loss=0.03164475876837969\n",
      "Surface training t=8716, loss=0.023827003315091133\n",
      "Surface training t=8717, loss=0.024665395729243755\n",
      "Surface training t=8718, loss=0.025349310599267483\n",
      "Surface training t=8719, loss=0.020983023568987846\n",
      "Surface training t=8720, loss=0.026507949456572533\n",
      "Surface training t=8721, loss=0.03166052792221308\n",
      "Surface training t=8722, loss=0.030377968214452267\n",
      "Surface training t=8723, loss=0.024193720892071724\n",
      "Surface training t=8724, loss=0.02679349295794964\n",
      "Surface training t=8725, loss=0.022703596390783787\n",
      "Surface training t=8726, loss=0.024787994101643562\n",
      "Surface training t=8727, loss=0.028730133548378944\n",
      "Surface training t=8728, loss=0.03595929313451052\n",
      "Surface training t=8729, loss=0.030977165326476097\n",
      "Surface training t=8730, loss=0.03611760959029198\n",
      "Surface training t=8731, loss=0.052209051325917244\n",
      "Surface training t=8732, loss=0.07369446754455566\n",
      "Surface training t=8733, loss=0.04886517766863108\n",
      "Surface training t=8734, loss=0.042969778180122375\n",
      "Surface training t=8735, loss=0.06354779191315174\n",
      "Surface training t=8736, loss=0.05123344622552395\n",
      "Surface training t=8737, loss=0.04008638486266136\n",
      "Surface training t=8738, loss=0.03701843414455652\n",
      "Surface training t=8739, loss=0.02650341670960188\n",
      "Surface training t=8740, loss=0.02235960867255926\n",
      "Surface training t=8741, loss=0.029902756214141846\n",
      "Surface training t=8742, loss=0.027565550059080124\n",
      "Surface training t=8743, loss=0.03545612283051014\n",
      "Surface training t=8744, loss=0.03368627745658159\n",
      "Surface training t=8745, loss=0.03821113519370556\n",
      "Surface training t=8746, loss=0.043413279578089714\n",
      "Surface training t=8747, loss=0.035452001728117466\n",
      "Surface training t=8748, loss=0.038919027894735336\n",
      "Surface training t=8749, loss=0.03100158181041479\n",
      "Surface training t=8750, loss=0.05060794949531555\n",
      "Surface training t=8751, loss=0.04435811750590801\n",
      "Surface training t=8752, loss=0.040321940556168556\n",
      "Surface training t=8753, loss=0.04711967892944813\n",
      "Surface training t=8754, loss=0.03672055900096893\n",
      "Surface training t=8755, loss=0.02929061558097601\n",
      "Surface training t=8756, loss=0.041964175179600716\n",
      "Surface training t=8757, loss=0.03550213947892189\n",
      "Surface training t=8758, loss=0.0299752289429307\n",
      "Surface training t=8759, loss=0.03368497174233198\n",
      "Surface training t=8760, loss=0.026621323078870773\n",
      "Surface training t=8761, loss=0.026269038207829\n",
      "Surface training t=8762, loss=0.03768421057611704\n",
      "Surface training t=8763, loss=0.047051817178726196\n",
      "Surface training t=8764, loss=0.046415332704782486\n",
      "Surface training t=8765, loss=0.03621060773730278\n",
      "Surface training t=8766, loss=0.03444654121994972\n",
      "Surface training t=8767, loss=0.03905180096626282\n",
      "Surface training t=8768, loss=0.04209697060286999\n",
      "Surface training t=8769, loss=0.027168993838131428\n",
      "Surface training t=8770, loss=0.032054221257567406\n",
      "Surface training t=8771, loss=0.04025077074766159\n",
      "Surface training t=8772, loss=0.034696588292717934\n",
      "Surface training t=8773, loss=0.03123406507074833\n",
      "Surface training t=8774, loss=0.04137144051492214\n",
      "Surface training t=8775, loss=0.03430458251386881\n",
      "Surface training t=8776, loss=0.03249218314886093\n",
      "Surface training t=8777, loss=0.03100114595144987\n",
      "Surface training t=8778, loss=0.049797963351011276\n",
      "Surface training t=8779, loss=0.03846661001443863\n",
      "Surface training t=8780, loss=0.04025913402438164\n",
      "Surface training t=8781, loss=0.05681147798895836\n",
      "Surface training t=8782, loss=0.032262000255286694\n",
      "Surface training t=8783, loss=0.04017559625208378\n",
      "Surface training t=8784, loss=0.04624457098543644\n",
      "Surface training t=8785, loss=0.03709207847714424\n",
      "Surface training t=8786, loss=0.047847453504800797\n",
      "Surface training t=8787, loss=0.040569037199020386\n",
      "Surface training t=8788, loss=0.07250053808093071\n",
      "Surface training t=8789, loss=0.03515064902603626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=8790, loss=0.03267552703619003\n",
      "Surface training t=8791, loss=0.04127473570406437\n",
      "Surface training t=8792, loss=0.05244019255042076\n",
      "Surface training t=8793, loss=0.058395497500896454\n",
      "Surface training t=8794, loss=0.0414341539144516\n",
      "Surface training t=8795, loss=0.03466613404452801\n",
      "Surface training t=8796, loss=0.049307992681860924\n",
      "Surface training t=8797, loss=0.024167467840015888\n",
      "Surface training t=8798, loss=0.04487140476703644\n",
      "Surface training t=8799, loss=0.06434222310781479\n",
      "Surface training t=8800, loss=0.06342291086912155\n",
      "Surface training t=8801, loss=0.046335915103554726\n",
      "Surface training t=8802, loss=0.05392441060394049\n",
      "Surface training t=8803, loss=0.06154932640492916\n",
      "Surface training t=8804, loss=0.06527073122560978\n",
      "Surface training t=8805, loss=0.08735622093081474\n",
      "Surface training t=8806, loss=0.054115382954478264\n",
      "Surface training t=8807, loss=0.05083785019814968\n",
      "Surface training t=8808, loss=0.047687262296676636\n",
      "Surface training t=8809, loss=0.060092490166425705\n",
      "Surface training t=8810, loss=0.05340146645903587\n",
      "Surface training t=8811, loss=0.055058807134628296\n",
      "Surface training t=8812, loss=0.06053851172327995\n",
      "Surface training t=8813, loss=0.0437497403472662\n",
      "Surface training t=8814, loss=0.06402922794222832\n",
      "Surface training t=8815, loss=0.042841874063014984\n",
      "Surface training t=8816, loss=0.05491653457283974\n",
      "Surface training t=8817, loss=0.03816364333033562\n",
      "Surface training t=8818, loss=0.037552485242486\n",
      "Surface training t=8819, loss=0.030756663531064987\n",
      "Surface training t=8820, loss=0.028098917566239834\n",
      "Surface training t=8821, loss=0.040945637971162796\n",
      "Surface training t=8822, loss=0.04188002645969391\n",
      "Surface training t=8823, loss=0.056816233322024345\n",
      "Surface training t=8824, loss=0.051932962611317635\n",
      "Surface training t=8825, loss=0.08518504723906517\n",
      "Surface training t=8826, loss=0.0576284546405077\n",
      "Surface training t=8827, loss=0.04797864519059658\n",
      "Surface training t=8828, loss=0.046034377068281174\n",
      "Surface training t=8829, loss=0.03716541640460491\n",
      "Surface training t=8830, loss=0.04314965382218361\n",
      "Surface training t=8831, loss=0.05357117112725973\n",
      "Surface training t=8832, loss=0.0542377233505249\n",
      "Surface training t=8833, loss=0.04565661773085594\n",
      "Surface training t=8834, loss=0.06404427252709866\n",
      "Surface training t=8835, loss=0.04825129359960556\n",
      "Surface training t=8836, loss=0.04805957153439522\n",
      "Surface training t=8837, loss=0.03818697854876518\n",
      "Surface training t=8838, loss=0.028053290210664272\n",
      "Surface training t=8839, loss=0.046796765178442\n",
      "Surface training t=8840, loss=0.03822582866996527\n",
      "Surface training t=8841, loss=0.06063270941376686\n",
      "Surface training t=8842, loss=0.05285369511693716\n",
      "Surface training t=8843, loss=0.03407614305615425\n",
      "Surface training t=8844, loss=0.03934296406805515\n",
      "Surface training t=8845, loss=0.03967551700770855\n",
      "Surface training t=8846, loss=0.03587019257247448\n",
      "Surface training t=8847, loss=0.04162111505866051\n",
      "Surface training t=8848, loss=0.02873521577566862\n",
      "Surface training t=8849, loss=0.026779165491461754\n",
      "Surface training t=8850, loss=0.028716477565467358\n",
      "Surface training t=8851, loss=0.030229682102799416\n",
      "Surface training t=8852, loss=0.0304533364251256\n",
      "Surface training t=8853, loss=0.03885871544480324\n",
      "Surface training t=8854, loss=0.03447168320417404\n",
      "Surface training t=8855, loss=0.03677883464843035\n",
      "Surface training t=8856, loss=0.043931061401963234\n",
      "Surface training t=8857, loss=0.03310988750308752\n",
      "Surface training t=8858, loss=0.029593439772725105\n",
      "Surface training t=8859, loss=0.046034326776862144\n",
      "Surface training t=8860, loss=0.04073456861078739\n",
      "Surface training t=8861, loss=0.03391414135694504\n",
      "Surface training t=8862, loss=0.0571452509611845\n",
      "Surface training t=8863, loss=0.035937740467488766\n",
      "Surface training t=8864, loss=0.05948411114513874\n",
      "Surface training t=8865, loss=0.0429141316562891\n",
      "Surface training t=8866, loss=0.039916057139635086\n",
      "Surface training t=8867, loss=0.04088976979255676\n",
      "Surface training t=8868, loss=0.036493049934506416\n",
      "Surface training t=8869, loss=0.04222449101507664\n",
      "Surface training t=8870, loss=0.04476003162562847\n",
      "Surface training t=8871, loss=0.03886500187218189\n",
      "Surface training t=8872, loss=0.03141728788614273\n",
      "Surface training t=8873, loss=0.03510068543255329\n",
      "Surface training t=8874, loss=0.031076669692993164\n",
      "Surface training t=8875, loss=0.05239298939704895\n",
      "Surface training t=8876, loss=0.038394853472709656\n",
      "Surface training t=8877, loss=0.03301810659468174\n",
      "Surface training t=8878, loss=0.03413145896047354\n",
      "Surface training t=8879, loss=0.03344156872481108\n",
      "Surface training t=8880, loss=0.04808161407709122\n",
      "Surface training t=8881, loss=0.03963588923215866\n",
      "Surface training t=8882, loss=0.04389322176575661\n",
      "Surface training t=8883, loss=0.03962460346519947\n",
      "Surface training t=8884, loss=0.03644478973001242\n",
      "Surface training t=8885, loss=0.04544940963387489\n",
      "Surface training t=8886, loss=0.05455924943089485\n",
      "Surface training t=8887, loss=0.07144764065742493\n",
      "Surface training t=8888, loss=0.057136958464980125\n",
      "Surface training t=8889, loss=0.06246192008256912\n",
      "Surface training t=8890, loss=0.08990675210952759\n",
      "Surface training t=8891, loss=0.06848908960819244\n",
      "Surface training t=8892, loss=0.05876639299094677\n",
      "Surface training t=8893, loss=0.0625324547290802\n",
      "Surface training t=8894, loss=0.04640280082821846\n",
      "Surface training t=8895, loss=0.0404199231415987\n",
      "Surface training t=8896, loss=0.04881717078387737\n",
      "Surface training t=8897, loss=0.04505310766398907\n",
      "Surface training t=8898, loss=0.043185457587242126\n",
      "Surface training t=8899, loss=0.05604764074087143\n",
      "Surface training t=8900, loss=0.044901760295033455\n",
      "Surface training t=8901, loss=0.05474299378693104\n",
      "Surface training t=8902, loss=0.032943896017968655\n",
      "Surface training t=8903, loss=0.03191487304866314\n",
      "Surface training t=8904, loss=0.029392301104962826\n",
      "Surface training t=8905, loss=0.030700547620654106\n",
      "Surface training t=8906, loss=0.030074849724769592\n",
      "Surface training t=8907, loss=0.030575374141335487\n",
      "Surface training t=8908, loss=0.028458578512072563\n",
      "Surface training t=8909, loss=0.021866166964173317\n",
      "Surface training t=8910, loss=0.03050656709820032\n",
      "Surface training t=8911, loss=0.03456718847155571\n",
      "Surface training t=8912, loss=0.03951781988143921\n",
      "Surface training t=8913, loss=0.04173392802476883\n",
      "Surface training t=8914, loss=0.046113098971545696\n",
      "Surface training t=8915, loss=0.058815715834498405\n",
      "Surface training t=8916, loss=0.06612926349043846\n",
      "Surface training t=8917, loss=0.0831293836236\n",
      "Surface training t=8918, loss=0.07065516710281372\n",
      "Surface training t=8919, loss=0.04838740453124046\n",
      "Surface training t=8920, loss=0.04787909984588623\n",
      "Surface training t=8921, loss=0.04134342633187771\n",
      "Surface training t=8922, loss=0.043442437425255775\n",
      "Surface training t=8923, loss=0.06206730008125305\n",
      "Surface training t=8924, loss=0.035973940044641495\n",
      "Surface training t=8925, loss=0.04057471454143524\n",
      "Surface training t=8926, loss=0.03838181495666504\n",
      "Surface training t=8927, loss=0.03783265873789787\n",
      "Surface training t=8928, loss=0.048112377524375916\n",
      "Surface training t=8929, loss=0.034121936187148094\n",
      "Surface training t=8930, loss=0.030019858852028847\n",
      "Surface training t=8931, loss=0.029580501839518547\n",
      "Surface training t=8932, loss=0.03441365621984005\n",
      "Surface training t=8933, loss=0.037479303777217865\n",
      "Surface training t=8934, loss=0.03270862624049187\n",
      "Surface training t=8935, loss=0.03214342426508665\n",
      "Surface training t=8936, loss=0.0372394397854805\n",
      "Surface training t=8937, loss=0.02988292183727026\n",
      "Surface training t=8938, loss=0.037693457677960396\n",
      "Surface training t=8939, loss=0.0328481700271368\n",
      "Surface training t=8940, loss=0.048864953219890594\n",
      "Surface training t=8941, loss=0.041367772966623306\n",
      "Surface training t=8942, loss=0.036646248772740364\n",
      "Surface training t=8943, loss=0.04363502189517021\n",
      "Surface training t=8944, loss=0.045190706849098206\n",
      "Surface training t=8945, loss=0.035919543355703354\n",
      "Surface training t=8946, loss=0.04402393102645874\n",
      "Surface training t=8947, loss=0.03275059536099434\n",
      "Surface training t=8948, loss=0.027905739843845367\n",
      "Surface training t=8949, loss=0.03830169886350632\n",
      "Surface training t=8950, loss=0.03127820324152708\n",
      "Surface training t=8951, loss=0.027855477295815945\n",
      "Surface training t=8952, loss=0.02529353555291891\n",
      "Surface training t=8953, loss=0.025236062705516815\n",
      "Surface training t=8954, loss=0.025244463235139847\n",
      "Surface training t=8955, loss=0.031062197871506214\n",
      "Surface training t=8956, loss=0.027699753642082214\n",
      "Surface training t=8957, loss=0.0348152257502079\n",
      "Surface training t=8958, loss=0.03342475742101669\n",
      "Surface training t=8959, loss=0.032992416992783546\n",
      "Surface training t=8960, loss=0.036759513430297375\n",
      "Surface training t=8961, loss=0.06354088708758354\n",
      "Surface training t=8962, loss=0.07081625051796436\n",
      "Surface training t=8963, loss=0.03772066906094551\n",
      "Surface training t=8964, loss=0.04189802147448063\n",
      "Surface training t=8965, loss=0.049039142206311226\n",
      "Surface training t=8966, loss=0.04630161635577679\n",
      "Surface training t=8967, loss=0.043664176017045975\n",
      "Surface training t=8968, loss=0.04488266259431839\n",
      "Surface training t=8969, loss=0.04502543807029724\n",
      "Surface training t=8970, loss=0.033645352348685265\n",
      "Surface training t=8971, loss=0.036694468930363655\n",
      "Surface training t=8972, loss=0.03540980443358421\n",
      "Surface training t=8973, loss=0.02345788199454546\n",
      "Surface training t=8974, loss=0.03962133079767227\n",
      "Surface training t=8975, loss=0.03388152830302715\n",
      "Surface training t=8976, loss=0.03181675914674997\n",
      "Surface training t=8977, loss=0.047220274806022644\n",
      "Surface training t=8978, loss=0.07876689359545708\n",
      "Surface training t=8979, loss=0.03376643359661102\n",
      "Surface training t=8980, loss=0.05381617695093155\n",
      "Surface training t=8981, loss=0.03831835463643074\n",
      "Surface training t=8982, loss=0.04595581814646721\n",
      "Surface training t=8983, loss=0.037121161818504333\n",
      "Surface training t=8984, loss=0.037405794486403465\n",
      "Surface training t=8985, loss=0.027786731719970703\n",
      "Surface training t=8986, loss=0.034312186762690544\n",
      "Surface training t=8987, loss=0.037164054811000824\n",
      "Surface training t=8988, loss=0.054625941440463066\n",
      "Surface training t=8989, loss=0.06320977210998535\n",
      "Surface training t=8990, loss=0.07178392820060253\n",
      "Surface training t=8991, loss=0.06653273478150368\n",
      "Surface training t=8992, loss=0.09005193784832954\n",
      "Surface training t=8993, loss=0.07381825521588326\n",
      "Surface training t=8994, loss=0.06228427402675152\n",
      "Surface training t=8995, loss=0.06544915772974491\n",
      "Surface training t=8996, loss=0.08557527884840965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=8997, loss=0.06467096880078316\n",
      "Surface training t=8998, loss=0.052940307185053825\n",
      "Surface training t=8999, loss=0.03787241503596306\n",
      "Surface training t=9000, loss=0.033488526940345764\n",
      "Surface training t=9001, loss=0.03219687845557928\n",
      "Surface training t=9002, loss=0.03747228439897299\n",
      "Surface training t=9003, loss=0.03788110613822937\n",
      "Surface training t=9004, loss=0.03870030306279659\n",
      "Surface training t=9005, loss=0.041955405846238136\n",
      "Surface training t=9006, loss=0.04036170430481434\n",
      "Surface training t=9007, loss=0.040683429688215256\n",
      "Surface training t=9008, loss=0.043087467551231384\n",
      "Surface training t=9009, loss=0.04776549153029919\n",
      "Surface training t=9010, loss=0.05009455792605877\n",
      "Surface training t=9011, loss=0.03985644690692425\n",
      "Surface training t=9012, loss=0.04667816869914532\n",
      "Surface training t=9013, loss=0.04011625424027443\n",
      "Surface training t=9014, loss=0.034620506688952446\n",
      "Surface training t=9015, loss=0.02624970953911543\n",
      "Surface training t=9016, loss=0.027836056426167488\n",
      "Surface training t=9017, loss=0.037137105129659176\n",
      "Surface training t=9018, loss=0.04319116473197937\n",
      "Surface training t=9019, loss=0.03235566150397062\n",
      "Surface training t=9020, loss=0.024478712119162083\n",
      "Surface training t=9021, loss=0.02199179958552122\n",
      "Surface training t=9022, loss=0.037324244156479836\n",
      "Surface training t=9023, loss=0.03840330895036459\n",
      "Surface training t=9024, loss=0.04401404783129692\n",
      "Surface training t=9025, loss=0.048093406483531\n",
      "Surface training t=9026, loss=0.03394410666078329\n",
      "Surface training t=9027, loss=0.03786455560475588\n",
      "Surface training t=9028, loss=0.032206080853939056\n",
      "Surface training t=9029, loss=0.03784734383225441\n",
      "Surface training t=9030, loss=0.032880269922316074\n",
      "Surface training t=9031, loss=0.03269075229763985\n",
      "Surface training t=9032, loss=0.037061724811792374\n",
      "Surface training t=9033, loss=0.04015375021845102\n",
      "Surface training t=9034, loss=0.051925303414464\n",
      "Surface training t=9035, loss=0.04680037498474121\n",
      "Surface training t=9036, loss=0.03883221745491028\n",
      "Surface training t=9037, loss=0.04090317711234093\n",
      "Surface training t=9038, loss=0.04098729137331247\n",
      "Surface training t=9039, loss=0.038076212629675865\n",
      "Surface training t=9040, loss=0.04655885323882103\n",
      "Surface training t=9041, loss=0.05995256267488003\n",
      "Surface training t=9042, loss=0.03851095587015152\n",
      "Surface training t=9043, loss=0.04969829320907593\n",
      "Surface training t=9044, loss=0.05582602135837078\n",
      "Surface training t=9045, loss=0.043964930810034275\n",
      "Surface training t=9046, loss=0.036029448732733727\n",
      "Surface training t=9047, loss=0.03308268263936043\n",
      "Surface training t=9048, loss=0.0335842277854681\n",
      "Surface training t=9049, loss=0.0396036421880126\n",
      "Surface training t=9050, loss=0.033639490604400635\n",
      "Surface training t=9051, loss=0.031475591473281384\n",
      "Surface training t=9052, loss=0.030819952487945557\n",
      "Surface training t=9053, loss=0.03993488475680351\n",
      "Surface training t=9054, loss=0.03514761105179787\n",
      "Surface training t=9055, loss=0.04291854240000248\n",
      "Surface training t=9056, loss=0.05935150943696499\n",
      "Surface training t=9057, loss=0.0678061880171299\n",
      "Surface training t=9058, loss=0.04943862184882164\n",
      "Surface training t=9059, loss=0.04873432777822018\n",
      "Surface training t=9060, loss=0.08518403396010399\n",
      "Surface training t=9061, loss=0.06017633527517319\n",
      "Surface training t=9062, loss=0.05508715845644474\n",
      "Surface training t=9063, loss=0.05144286900758743\n",
      "Surface training t=9064, loss=0.04486594721674919\n",
      "Surface training t=9065, loss=0.04012720659375191\n",
      "Surface training t=9066, loss=0.035870444029569626\n",
      "Surface training t=9067, loss=0.03351537697017193\n",
      "Surface training t=9068, loss=0.0307844216004014\n",
      "Surface training t=9069, loss=0.040039319545030594\n",
      "Surface training t=9070, loss=0.033331301994621754\n",
      "Surface training t=9071, loss=0.030270908027887344\n",
      "Surface training t=9072, loss=0.03880375251173973\n",
      "Surface training t=9073, loss=0.02470590267330408\n",
      "Surface training t=9074, loss=0.04070534557104111\n",
      "Surface training t=9075, loss=0.03553789108991623\n",
      "Surface training t=9076, loss=0.032645152881741524\n",
      "Surface training t=9077, loss=0.030232827179133892\n",
      "Surface training t=9078, loss=0.028690656647086143\n",
      "Surface training t=9079, loss=0.03079486731439829\n",
      "Surface training t=9080, loss=0.048640742897987366\n",
      "Surface training t=9081, loss=0.038928452879190445\n",
      "Surface training t=9082, loss=0.03324159886687994\n",
      "Surface training t=9083, loss=0.04902801290154457\n",
      "Surface training t=9084, loss=0.043975191190838814\n",
      "Surface training t=9085, loss=0.037504689767956734\n",
      "Surface training t=9086, loss=0.050688913092017174\n",
      "Surface training t=9087, loss=0.045283494517207146\n",
      "Surface training t=9088, loss=0.027833708561956882\n",
      "Surface training t=9089, loss=0.04413316398859024\n",
      "Surface training t=9090, loss=0.04087117128074169\n",
      "Surface training t=9091, loss=0.028810993768274784\n",
      "Surface training t=9092, loss=0.03670782409608364\n",
      "Surface training t=9093, loss=0.0316113093867898\n",
      "Surface training t=9094, loss=0.03291160985827446\n",
      "Surface training t=9095, loss=0.02771972119808197\n",
      "Surface training t=9096, loss=0.03679269179701805\n",
      "Surface training t=9097, loss=0.034975032322108746\n",
      "Surface training t=9098, loss=0.03007899969816208\n",
      "Surface training t=9099, loss=0.034401699900627136\n",
      "Surface training t=9100, loss=0.035838592797517776\n",
      "Surface training t=9101, loss=0.061604879796504974\n",
      "Surface training t=9102, loss=0.05947299860417843\n",
      "Surface training t=9103, loss=0.032221716828644276\n",
      "Surface training t=9104, loss=0.030785735696554184\n",
      "Surface training t=9105, loss=0.05586876720190048\n",
      "Surface training t=9106, loss=0.05331740528345108\n",
      "Surface training t=9107, loss=0.049060309305787086\n",
      "Surface training t=9108, loss=0.03260068502277136\n",
      "Surface training t=9109, loss=0.02880630549043417\n",
      "Surface training t=9110, loss=0.03592299297451973\n",
      "Surface training t=9111, loss=0.038310504518449306\n",
      "Surface training t=9112, loss=0.048649825155735016\n",
      "Surface training t=9113, loss=0.05243545025587082\n",
      "Surface training t=9114, loss=0.045891473069787025\n",
      "Surface training t=9115, loss=0.03487679548561573\n",
      "Surface training t=9116, loss=0.043696463108062744\n",
      "Surface training t=9117, loss=0.029888205230236053\n",
      "Surface training t=9118, loss=0.022275852970778942\n",
      "Surface training t=9119, loss=0.029515642672777176\n",
      "Surface training t=9120, loss=0.025464624166488647\n",
      "Surface training t=9121, loss=0.028793826699256897\n",
      "Surface training t=9122, loss=0.032421061769127846\n",
      "Surface training t=9123, loss=0.03389350324869156\n",
      "Surface training t=9124, loss=0.03659271448850632\n",
      "Surface training t=9125, loss=0.04011462815105915\n",
      "Surface training t=9126, loss=0.03899003192782402\n",
      "Surface training t=9127, loss=0.06606147810816765\n",
      "Surface training t=9128, loss=0.05050034448504448\n",
      "Surface training t=9129, loss=0.04190464783459902\n",
      "Surface training t=9130, loss=0.039187937043607235\n",
      "Surface training t=9131, loss=0.058000970631837845\n",
      "Surface training t=9132, loss=0.06189853325486183\n",
      "Surface training t=9133, loss=0.09725512191653252\n",
      "Surface training t=9134, loss=0.07205785252153873\n",
      "Surface training t=9135, loss=0.10755034908652306\n",
      "Surface training t=9136, loss=0.09898222237825394\n",
      "Surface training t=9137, loss=0.06438913196325302\n",
      "Surface training t=9138, loss=0.08461014181375504\n",
      "Surface training t=9139, loss=0.06807370483875275\n",
      "Surface training t=9140, loss=0.04372835345566273\n",
      "Surface training t=9141, loss=0.041680989786982536\n",
      "Surface training t=9142, loss=0.04135672375559807\n",
      "Surface training t=9143, loss=0.03843805566430092\n",
      "Surface training t=9144, loss=0.046891674399375916\n",
      "Surface training t=9145, loss=0.04844535514712334\n",
      "Surface training t=9146, loss=0.05069698952138424\n",
      "Surface training t=9147, loss=0.04858116805553436\n",
      "Surface training t=9148, loss=0.045431774109601974\n",
      "Surface training t=9149, loss=0.04343756474554539\n",
      "Surface training t=9150, loss=0.047162991017103195\n",
      "Surface training t=9151, loss=0.07286317646503448\n",
      "Surface training t=9152, loss=0.061069357208907604\n",
      "Surface training t=9153, loss=0.07063106447458267\n",
      "Surface training t=9154, loss=0.09271491691470146\n",
      "Surface training t=9155, loss=0.05489208549261093\n",
      "Surface training t=9156, loss=0.05059465207159519\n",
      "Surface training t=9157, loss=0.048085350543260574\n",
      "Surface training t=9158, loss=0.042243842035532\n",
      "Surface training t=9159, loss=0.03600376658141613\n",
      "Surface training t=9160, loss=0.04201754741370678\n",
      "Surface training t=9161, loss=0.04689886420965195\n",
      "Surface training t=9162, loss=0.03893079794943333\n",
      "Surface training t=9163, loss=0.04642428457736969\n",
      "Surface training t=9164, loss=0.039806803688406944\n",
      "Surface training t=9165, loss=0.05240930803120136\n",
      "Surface training t=9166, loss=0.061670588329434395\n",
      "Surface training t=9167, loss=0.043578751385211945\n",
      "Surface training t=9168, loss=0.049137845635414124\n",
      "Surface training t=9169, loss=0.04069693014025688\n",
      "Surface training t=9170, loss=0.05000240541994572\n",
      "Surface training t=9171, loss=0.061302945017814636\n",
      "Surface training t=9172, loss=0.053535765036940575\n",
      "Surface training t=9173, loss=0.055536188185214996\n",
      "Surface training t=9174, loss=0.04812537878751755\n",
      "Surface training t=9175, loss=0.04369666613638401\n",
      "Surface training t=9176, loss=0.05515441298484802\n",
      "Surface training t=9177, loss=0.055086007341742516\n",
      "Surface training t=9178, loss=0.07103319838643074\n",
      "Surface training t=9179, loss=0.05230259150266647\n",
      "Surface training t=9180, loss=0.041496869176626205\n",
      "Surface training t=9181, loss=0.0507239643484354\n",
      "Surface training t=9182, loss=0.06346852332353592\n",
      "Surface training t=9183, loss=0.044052327051758766\n",
      "Surface training t=9184, loss=0.04730119742453098\n",
      "Surface training t=9185, loss=0.04848405718803406\n",
      "Surface training t=9186, loss=0.049750762060284615\n",
      "Surface training t=9187, loss=0.05345262587070465\n",
      "Surface training t=9188, loss=0.0514921173453331\n",
      "Surface training t=9189, loss=0.053960489109158516\n",
      "Surface training t=9190, loss=0.05776619166135788\n",
      "Surface training t=9191, loss=0.05711659975349903\n",
      "Surface training t=9192, loss=0.05057850666344166\n",
      "Surface training t=9193, loss=0.0425436869263649\n",
      "Surface training t=9194, loss=0.0471696462482214\n",
      "Surface training t=9195, loss=0.04948009178042412\n",
      "Surface training t=9196, loss=0.039148999378085136\n",
      "Surface training t=9197, loss=0.03730692155659199\n",
      "Surface training t=9198, loss=0.035875122994184494\n",
      "Surface training t=9199, loss=0.03927031718194485\n",
      "Surface training t=9200, loss=0.03733363375067711\n",
      "Surface training t=9201, loss=0.04518562741577625\n",
      "Surface training t=9202, loss=0.04325328394770622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=9203, loss=0.036486171185970306\n",
      "Surface training t=9204, loss=0.043193329125642776\n",
      "Surface training t=9205, loss=0.0495010893791914\n",
      "Surface training t=9206, loss=0.06461868435144424\n",
      "Surface training t=9207, loss=0.06637059524655342\n",
      "Surface training t=9208, loss=0.07368624582886696\n",
      "Surface training t=9209, loss=0.0578827690333128\n",
      "Surface training t=9210, loss=0.07283424213528633\n",
      "Surface training t=9211, loss=0.06037662923336029\n",
      "Surface training t=9212, loss=0.053349947556853294\n",
      "Surface training t=9213, loss=0.042936135083436966\n",
      "Surface training t=9214, loss=0.04430665820837021\n",
      "Surface training t=9215, loss=0.04387260787189007\n",
      "Surface training t=9216, loss=0.04928137548267841\n",
      "Surface training t=9217, loss=0.03875306900590658\n",
      "Surface training t=9218, loss=0.037214649841189384\n",
      "Surface training t=9219, loss=0.033042967319488525\n",
      "Surface training t=9220, loss=0.03553689271211624\n",
      "Surface training t=9221, loss=0.030580446124076843\n",
      "Surface training t=9222, loss=0.026368238031864166\n",
      "Surface training t=9223, loss=0.033941869623959064\n",
      "Surface training t=9224, loss=0.03341324161738157\n",
      "Surface training t=9225, loss=0.03479900863021612\n",
      "Surface training t=9226, loss=0.02664579451084137\n",
      "Surface training t=9227, loss=0.04339221864938736\n",
      "Surface training t=9228, loss=0.03058135975152254\n",
      "Surface training t=9229, loss=0.035566315054893494\n",
      "Surface training t=9230, loss=0.03536365460604429\n",
      "Surface training t=9231, loss=0.028649779967963696\n",
      "Surface training t=9232, loss=0.023112209513783455\n",
      "Surface training t=9233, loss=0.03189937025308609\n",
      "Surface training t=9234, loss=0.03633125126361847\n",
      "Surface training t=9235, loss=0.03191683441400528\n",
      "Surface training t=9236, loss=0.036245932802557945\n",
      "Surface training t=9237, loss=0.029574118554592133\n",
      "Surface training t=9238, loss=0.051368676126003265\n",
      "Surface training t=9239, loss=0.058016423135995865\n",
      "Surface training t=9240, loss=0.033852278254926205\n",
      "Surface training t=9241, loss=0.04696212150156498\n",
      "Surface training t=9242, loss=0.024284101091325283\n",
      "Surface training t=9243, loss=0.023755304515361786\n",
      "Surface training t=9244, loss=0.026947504840791225\n",
      "Surface training t=9245, loss=0.02340209297835827\n",
      "Surface training t=9246, loss=0.0321287726983428\n",
      "Surface training t=9247, loss=0.032571932300925255\n",
      "Surface training t=9248, loss=0.026376718655228615\n",
      "Surface training t=9249, loss=0.036671316251158714\n",
      "Surface training t=9250, loss=0.03450730349868536\n",
      "Surface training t=9251, loss=0.04924037307500839\n",
      "Surface training t=9252, loss=0.03618144616484642\n",
      "Surface training t=9253, loss=0.02790053840726614\n",
      "Surface training t=9254, loss=0.03411268349736929\n",
      "Surface training t=9255, loss=0.05477692373096943\n",
      "Surface training t=9256, loss=0.03706091735512018\n",
      "Surface training t=9257, loss=0.06517073884606361\n",
      "Surface training t=9258, loss=0.046241129748523235\n",
      "Surface training t=9259, loss=0.06055423431098461\n",
      "Surface training t=9260, loss=0.06109444797039032\n",
      "Surface training t=9261, loss=0.08821826055645943\n",
      "Surface training t=9262, loss=0.06656152941286564\n",
      "Surface training t=9263, loss=0.05651385337114334\n",
      "Surface training t=9264, loss=0.04580812156200409\n",
      "Surface training t=9265, loss=0.08833030238747597\n",
      "Surface training t=9266, loss=0.06445923261344433\n",
      "Surface training t=9267, loss=0.05831065587699413\n",
      "Surface training t=9268, loss=0.06951471045613289\n",
      "Surface training t=9269, loss=0.05197586305439472\n",
      "Surface training t=9270, loss=0.06287333182990551\n",
      "Surface training t=9271, loss=0.04303093999624252\n",
      "Surface training t=9272, loss=0.04496546648442745\n",
      "Surface training t=9273, loss=0.0681769885122776\n",
      "Surface training t=9274, loss=0.06853461265563965\n",
      "Surface training t=9275, loss=0.036360934376716614\n",
      "Surface training t=9276, loss=0.049119507893919945\n",
      "Surface training t=9277, loss=0.04459652863442898\n",
      "Surface training t=9278, loss=0.03426731750369072\n",
      "Surface training t=9279, loss=0.033871859312057495\n",
      "Surface training t=9280, loss=0.035764046013355255\n",
      "Surface training t=9281, loss=0.03891127835959196\n",
      "Surface training t=9282, loss=0.04800350032746792\n",
      "Surface training t=9283, loss=0.03781034052371979\n",
      "Surface training t=9284, loss=0.032927073538303375\n",
      "Surface training t=9285, loss=0.025702700950205326\n",
      "Surface training t=9286, loss=0.042440904304385185\n",
      "Surface training t=9287, loss=0.03745051100850105\n",
      "Surface training t=9288, loss=0.04160894453525543\n",
      "Surface training t=9289, loss=0.03236892260611057\n",
      "Surface training t=9290, loss=0.050581714138388634\n",
      "Surface training t=9291, loss=0.03153417631983757\n",
      "Surface training t=9292, loss=0.033091310411691666\n",
      "Surface training t=9293, loss=0.0408382173627615\n",
      "Surface training t=9294, loss=0.022974719759076834\n",
      "Surface training t=9295, loss=0.03065919317305088\n",
      "Surface training t=9296, loss=0.03382048849016428\n",
      "Surface training t=9297, loss=0.026415525935590267\n",
      "Surface training t=9298, loss=0.027530022896826267\n",
      "Surface training t=9299, loss=0.027932923287153244\n",
      "Surface training t=9300, loss=0.022344586439430714\n",
      "Surface training t=9301, loss=0.02037116326391697\n",
      "Surface training t=9302, loss=0.028173301368951797\n",
      "Surface training t=9303, loss=0.028346607461571693\n",
      "Surface training t=9304, loss=0.03953727148473263\n",
      "Surface training t=9305, loss=0.056415801867842674\n",
      "Surface training t=9306, loss=0.03804071061313152\n",
      "Surface training t=9307, loss=0.0359942764043808\n",
      "Surface training t=9308, loss=0.041051821783185005\n",
      "Surface training t=9309, loss=0.02851182222366333\n",
      "Surface training t=9310, loss=0.028635852970182896\n",
      "Surface training t=9311, loss=0.04207841865718365\n",
      "Surface training t=9312, loss=0.029267544858157635\n",
      "Surface training t=9313, loss=0.0417544674128294\n",
      "Surface training t=9314, loss=0.03900420293211937\n",
      "Surface training t=9315, loss=0.04095614701509476\n",
      "Surface training t=9316, loss=0.05611594766378403\n",
      "Surface training t=9317, loss=0.03738176077604294\n",
      "Surface training t=9318, loss=0.04093070141971111\n",
      "Surface training t=9319, loss=0.058572057634592056\n",
      "Surface training t=9320, loss=0.049497995525598526\n",
      "Surface training t=9321, loss=0.05292282812297344\n",
      "Surface training t=9322, loss=0.04844074137508869\n",
      "Surface training t=9323, loss=0.044662196189165115\n",
      "Surface training t=9324, loss=0.03835511952638626\n",
      "Surface training t=9325, loss=0.045769838616251945\n",
      "Surface training t=9326, loss=0.03543853014707565\n",
      "Surface training t=9327, loss=0.029756435193121433\n",
      "Surface training t=9328, loss=0.0448266975581646\n",
      "Surface training t=9329, loss=0.03058804664760828\n",
      "Surface training t=9330, loss=0.036338645964860916\n",
      "Surface training t=9331, loss=0.027110785245895386\n",
      "Surface training t=9332, loss=0.03752036206424236\n",
      "Surface training t=9333, loss=0.03163269255310297\n",
      "Surface training t=9334, loss=0.031978134997189045\n",
      "Surface training t=9335, loss=0.029983241111040115\n",
      "Surface training t=9336, loss=0.028607944026589394\n",
      "Surface training t=9337, loss=0.0293679628521204\n",
      "Surface training t=9338, loss=0.029833978973329067\n",
      "Surface training t=9339, loss=0.035976434126496315\n",
      "Surface training t=9340, loss=0.03745867591351271\n",
      "Surface training t=9341, loss=0.030528325587511063\n",
      "Surface training t=9342, loss=0.052003320306539536\n",
      "Surface training t=9343, loss=0.05012279190123081\n",
      "Surface training t=9344, loss=0.08301601931452751\n",
      "Surface training t=9345, loss=0.055719008669257164\n",
      "Surface training t=9346, loss=0.07074682414531708\n",
      "Surface training t=9347, loss=0.04139501228928566\n",
      "Surface training t=9348, loss=0.05826233699917793\n",
      "Surface training t=9349, loss=0.03931807354092598\n",
      "Surface training t=9350, loss=0.03669351525604725\n",
      "Surface training t=9351, loss=0.04534534923732281\n",
      "Surface training t=9352, loss=0.033753467723727226\n",
      "Surface training t=9353, loss=0.037061186507344246\n",
      "Surface training t=9354, loss=0.0319995628669858\n",
      "Surface training t=9355, loss=0.02784980833530426\n",
      "Surface training t=9356, loss=0.028487990610301495\n",
      "Surface training t=9357, loss=0.03995837178081274\n",
      "Surface training t=9358, loss=0.032226323150098324\n",
      "Surface training t=9359, loss=0.03085580561310053\n",
      "Surface training t=9360, loss=0.03133447282016277\n",
      "Surface training t=9361, loss=0.03140360862016678\n",
      "Surface training t=9362, loss=0.043445490300655365\n",
      "Surface training t=9363, loss=0.04066374897956848\n",
      "Surface training t=9364, loss=0.05714358203113079\n",
      "Surface training t=9365, loss=0.03261050581932068\n",
      "Surface training t=9366, loss=0.03230946138501167\n",
      "Surface training t=9367, loss=0.035324775613844395\n",
      "Surface training t=9368, loss=0.030765896663069725\n",
      "Surface training t=9369, loss=0.02001866325736046\n",
      "Surface training t=9370, loss=0.018902079202234745\n",
      "Surface training t=9371, loss=0.02757173404097557\n",
      "Surface training t=9372, loss=0.02584206685423851\n",
      "Surface training t=9373, loss=0.03330542426556349\n",
      "Surface training t=9374, loss=0.028163423761725426\n",
      "Surface training t=9375, loss=0.04604594595730305\n",
      "Surface training t=9376, loss=0.05092925950884819\n",
      "Surface training t=9377, loss=0.04513335041701794\n",
      "Surface training t=9378, loss=0.04820854403078556\n",
      "Surface training t=9379, loss=0.05299373157322407\n",
      "Surface training t=9380, loss=0.04473670572042465\n",
      "Surface training t=9381, loss=0.03805047832429409\n",
      "Surface training t=9382, loss=0.04613860510289669\n",
      "Surface training t=9383, loss=0.037142981775105\n",
      "Surface training t=9384, loss=0.06951829232275486\n",
      "Surface training t=9385, loss=0.0453794002532959\n",
      "Surface training t=9386, loss=0.06345526315271854\n",
      "Surface training t=9387, loss=0.042913876473903656\n",
      "Surface training t=9388, loss=0.06635692715644836\n",
      "Surface training t=9389, loss=0.04261302016675472\n",
      "Surface training t=9390, loss=0.04871981404721737\n",
      "Surface training t=9391, loss=0.03719557449221611\n",
      "Surface training t=9392, loss=0.03875715937465429\n",
      "Surface training t=9393, loss=0.03954089619219303\n",
      "Surface training t=9394, loss=0.038342349231243134\n",
      "Surface training t=9395, loss=0.05606582760810852\n",
      "Surface training t=9396, loss=0.06858978793025017\n",
      "Surface training t=9397, loss=0.037738146260380745\n",
      "Surface training t=9398, loss=0.04273421689867973\n",
      "Surface training t=9399, loss=0.04124572314321995\n",
      "Surface training t=9400, loss=0.03920966573059559\n",
      "Surface training t=9401, loss=0.034065366722643375\n",
      "Surface training t=9402, loss=0.04092341288924217\n",
      "Surface training t=9403, loss=0.050320954993367195\n",
      "Surface training t=9404, loss=0.040520137175917625\n",
      "Surface training t=9405, loss=0.043500762432813644\n",
      "Surface training t=9406, loss=0.029458075761795044\n",
      "Surface training t=9407, loss=0.03322787210345268\n",
      "Surface training t=9408, loss=0.029567484743893147\n",
      "Surface training t=9409, loss=0.03264963999390602\n",
      "Surface training t=9410, loss=0.026248817332088947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=9411, loss=0.04531131125986576\n",
      "Surface training t=9412, loss=0.040717482566833496\n",
      "Surface training t=9413, loss=0.039907658472657204\n",
      "Surface training t=9414, loss=0.034401108510792255\n",
      "Surface training t=9415, loss=0.03297878336161375\n",
      "Surface training t=9416, loss=0.03137690760195255\n",
      "Surface training t=9417, loss=0.05117006041109562\n",
      "Surface training t=9418, loss=0.039555517956614494\n",
      "Surface training t=9419, loss=0.036402941681444645\n",
      "Surface training t=9420, loss=0.04861992411315441\n",
      "Surface training t=9421, loss=0.03445363789796829\n",
      "Surface training t=9422, loss=0.052827904000878334\n",
      "Surface training t=9423, loss=0.03459468297660351\n",
      "Surface training t=9424, loss=0.03765659686177969\n",
      "Surface training t=9425, loss=0.046241870149970055\n",
      "Surface training t=9426, loss=0.03740713093429804\n",
      "Surface training t=9427, loss=0.045330965891480446\n",
      "Surface training t=9428, loss=0.05775504559278488\n",
      "Surface training t=9429, loss=0.03484032861888409\n",
      "Surface training t=9430, loss=0.044250862672924995\n",
      "Surface training t=9431, loss=0.028882614336907864\n",
      "Surface training t=9432, loss=0.03816020768135786\n",
      "Surface training t=9433, loss=0.05114907957613468\n",
      "Surface training t=9434, loss=0.05488325096666813\n",
      "Surface training t=9435, loss=0.049530964344739914\n",
      "Surface training t=9436, loss=0.052753522992134094\n",
      "Surface training t=9437, loss=0.04983909800648689\n",
      "Surface training t=9438, loss=0.05326547473669052\n",
      "Surface training t=9439, loss=0.05096021853387356\n",
      "Surface training t=9440, loss=0.0358355138450861\n",
      "Surface training t=9441, loss=0.0524157527834177\n",
      "Surface training t=9442, loss=0.04549489915370941\n",
      "Surface training t=9443, loss=0.04544457979500294\n",
      "Surface training t=9444, loss=0.0357059296220541\n",
      "Surface training t=9445, loss=0.041278207674622536\n",
      "Surface training t=9446, loss=0.0339883454144001\n",
      "Surface training t=9447, loss=0.03597686253488064\n",
      "Surface training t=9448, loss=0.035731883719563484\n",
      "Surface training t=9449, loss=0.03898325189948082\n",
      "Surface training t=9450, loss=0.04403608478605747\n",
      "Surface training t=9451, loss=0.04800115153193474\n",
      "Surface training t=9452, loss=0.03999685309827328\n",
      "Surface training t=9453, loss=0.04132380895316601\n",
      "Surface training t=9454, loss=0.03684141859412193\n",
      "Surface training t=9455, loss=0.037521399557590485\n",
      "Surface training t=9456, loss=0.047760797664523125\n",
      "Surface training t=9457, loss=0.0471021942794323\n",
      "Surface training t=9458, loss=0.0420520044863224\n",
      "Surface training t=9459, loss=0.029831314459443092\n",
      "Surface training t=9460, loss=0.027477889321744442\n",
      "Surface training t=9461, loss=0.055155303329229355\n",
      "Surface training t=9462, loss=0.03407440148293972\n",
      "Surface training t=9463, loss=0.049710698425769806\n",
      "Surface training t=9464, loss=0.0561954602599144\n",
      "Surface training t=9465, loss=0.07604735903441906\n",
      "Surface training t=9466, loss=0.049185339361429214\n",
      "Surface training t=9467, loss=0.05046398192644119\n",
      "Surface training t=9468, loss=0.04254394583404064\n",
      "Surface training t=9469, loss=0.043941982090473175\n",
      "Surface training t=9470, loss=0.0347881093621254\n",
      "Surface training t=9471, loss=0.03149825241416693\n",
      "Surface training t=9472, loss=0.03080001100897789\n",
      "Surface training t=9473, loss=0.051015716046094894\n",
      "Surface training t=9474, loss=0.04858156852424145\n",
      "Surface training t=9475, loss=0.05505391024053097\n",
      "Surface training t=9476, loss=0.03902131970971823\n",
      "Surface training t=9477, loss=0.06572695076465607\n",
      "Surface training t=9478, loss=0.04334241338074207\n",
      "Surface training t=9479, loss=0.055795131251215935\n",
      "Surface training t=9480, loss=0.03108187858015299\n",
      "Surface training t=9481, loss=0.03228906821459532\n",
      "Surface training t=9482, loss=0.029160006903111935\n",
      "Surface training t=9483, loss=0.029204286634922028\n",
      "Surface training t=9484, loss=0.03280270006507635\n",
      "Surface training t=9485, loss=0.03810999169945717\n",
      "Surface training t=9486, loss=0.04148893617093563\n",
      "Surface training t=9487, loss=0.03385917190462351\n",
      "Surface training t=9488, loss=0.03500455059111118\n",
      "Surface training t=9489, loss=0.033200533129274845\n",
      "Surface training t=9490, loss=0.0463880468159914\n",
      "Surface training t=9491, loss=0.03972793556749821\n",
      "Surface training t=9492, loss=0.034790217876434326\n",
      "Surface training t=9493, loss=0.034410638734698296\n",
      "Surface training t=9494, loss=0.041108179837465286\n",
      "Surface training t=9495, loss=0.05083504505455494\n",
      "Surface training t=9496, loss=0.08036389201879501\n",
      "Surface training t=9497, loss=0.057935746386647224\n",
      "Surface training t=9498, loss=0.07737915404140949\n",
      "Surface training t=9499, loss=0.10892084240913391\n",
      "Surface training t=9500, loss=0.0636223703622818\n",
      "Surface training t=9501, loss=0.07314918003976345\n",
      "Surface training t=9502, loss=0.051253655925393105\n",
      "Surface training t=9503, loss=0.04204200580716133\n",
      "Surface training t=9504, loss=0.03301149234175682\n",
      "Surface training t=9505, loss=0.039969660341739655\n",
      "Surface training t=9506, loss=0.03747519478201866\n",
      "Surface training t=9507, loss=0.026881759986281395\n",
      "Surface training t=9508, loss=0.04110708460211754\n",
      "Surface training t=9509, loss=0.036814162507653236\n",
      "Surface training t=9510, loss=0.04259492829442024\n",
      "Surface training t=9511, loss=0.046477245166897774\n",
      "Surface training t=9512, loss=0.055951036512851715\n",
      "Surface training t=9513, loss=0.035708075389266014\n",
      "Surface training t=9514, loss=0.05418341048061848\n",
      "Surface training t=9515, loss=0.0514382217079401\n",
      "Surface training t=9516, loss=0.05224362947046757\n",
      "Surface training t=9517, loss=0.03968128003180027\n",
      "Surface training t=9518, loss=0.05413299240171909\n",
      "Surface training t=9519, loss=0.0551674272865057\n",
      "Surface training t=9520, loss=0.05456366017460823\n",
      "Surface training t=9521, loss=0.06000149995088577\n",
      "Surface training t=9522, loss=0.04813349526375532\n",
      "Surface training t=9523, loss=0.05920427292585373\n",
      "Surface training t=9524, loss=0.06854667142033577\n",
      "Surface training t=9525, loss=0.04329613596200943\n",
      "Surface training t=9526, loss=0.046027932316064835\n",
      "Surface training t=9527, loss=0.041057026013731956\n",
      "Surface training t=9528, loss=0.04484689235687256\n",
      "Surface training t=9529, loss=0.04337307345122099\n",
      "Surface training t=9530, loss=0.052431872114539146\n",
      "Surface training t=9531, loss=0.04126713238656521\n",
      "Surface training t=9532, loss=0.034216709434986115\n",
      "Surface training t=9533, loss=0.037354566156864166\n",
      "Surface training t=9534, loss=0.03213785961270332\n",
      "Surface training t=9535, loss=0.03480968717485666\n",
      "Surface training t=9536, loss=0.029013676568865776\n",
      "Surface training t=9537, loss=0.020104320254176855\n",
      "Surface training t=9538, loss=0.025962729938328266\n",
      "Surface training t=9539, loss=0.02905179839581251\n",
      "Surface training t=9540, loss=0.027658787555992603\n",
      "Surface training t=9541, loss=0.02678409032523632\n",
      "Surface training t=9542, loss=0.04070964828133583\n",
      "Surface training t=9543, loss=0.03279068320989609\n",
      "Surface training t=9544, loss=0.028811958618462086\n",
      "Surface training t=9545, loss=0.02688753604888916\n",
      "Surface training t=9546, loss=0.027422786690294743\n",
      "Surface training t=9547, loss=0.02712935209274292\n",
      "Surface training t=9548, loss=0.03227257076650858\n",
      "Surface training t=9549, loss=0.033889747224748135\n",
      "Surface training t=9550, loss=0.044934485107660294\n",
      "Surface training t=9551, loss=0.030478467233479023\n",
      "Surface training t=9552, loss=0.03049886878579855\n",
      "Surface training t=9553, loss=0.03997766971588135\n",
      "Surface training t=9554, loss=0.032717084512114525\n",
      "Surface training t=9555, loss=0.031025304459035397\n",
      "Surface training t=9556, loss=0.04944908432662487\n",
      "Surface training t=9557, loss=0.03520946949720383\n",
      "Surface training t=9558, loss=0.03867669869214296\n",
      "Surface training t=9559, loss=0.05273156613111496\n",
      "Surface training t=9560, loss=0.035710349678993225\n",
      "Surface training t=9561, loss=0.05747068673372269\n",
      "Surface training t=9562, loss=0.049215707927942276\n",
      "Surface training t=9563, loss=0.03515752870589495\n",
      "Surface training t=9564, loss=0.05236336961388588\n",
      "Surface training t=9565, loss=0.04030350036919117\n",
      "Surface training t=9566, loss=0.04783260449767113\n",
      "Surface training t=9567, loss=0.03758461866527796\n",
      "Surface training t=9568, loss=0.044380079954862595\n",
      "Surface training t=9569, loss=0.04594072513282299\n",
      "Surface training t=9570, loss=0.07116217724978924\n",
      "Surface training t=9571, loss=0.06522830575704575\n",
      "Surface training t=9572, loss=0.05965801887214184\n",
      "Surface training t=9573, loss=0.050160931423306465\n",
      "Surface training t=9574, loss=0.04306916892528534\n",
      "Surface training t=9575, loss=0.045991742983460426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=9576, loss=0.061697859317064285\n",
      "Surface training t=9577, loss=0.050147656351327896\n",
      "Surface training t=9578, loss=0.04457305930554867\n",
      "Surface training t=9579, loss=0.040787141770124435\n",
      "Surface training t=9580, loss=0.05068940483033657\n",
      "Surface training t=9581, loss=0.03779012709856033\n",
      "Surface training t=9582, loss=0.04037729650735855\n",
      "Surface training t=9583, loss=0.03928259201347828\n",
      "Surface training t=9584, loss=0.029398612678050995\n",
      "Surface training t=9585, loss=0.021959098055958748\n",
      "Surface training t=9586, loss=0.027613315731287003\n",
      "Surface training t=9587, loss=0.03114286530762911\n",
      "Surface training t=9588, loss=0.029583580791950226\n",
      "Surface training t=9589, loss=0.02349835354834795\n",
      "Surface training t=9590, loss=0.023457981646060944\n",
      "Surface training t=9591, loss=0.029532795771956444\n",
      "Surface training t=9592, loss=0.03228394128382206\n",
      "Surface training t=9593, loss=0.05022148974239826\n",
      "Surface training t=9594, loss=0.08519252017140388\n",
      "Surface training t=9595, loss=0.05715499445796013\n",
      "Surface training t=9596, loss=0.05444946512579918\n",
      "Surface training t=9597, loss=0.04275214672088623\n",
      "Surface training t=9598, loss=0.038327381014823914\n",
      "Surface training t=9599, loss=0.043680014088749886\n",
      "Surface training t=9600, loss=0.04741479456424713\n",
      "Surface training t=9601, loss=0.051451221108436584\n",
      "Surface training t=9602, loss=0.050843991339206696\n",
      "Surface training t=9603, loss=0.05559903010725975\n",
      "Surface training t=9604, loss=0.07972249761223793\n",
      "Surface training t=9605, loss=0.05389361083507538\n",
      "Surface training t=9606, loss=0.051591549068689346\n",
      "Surface training t=9607, loss=0.06458930671215057\n",
      "Surface training t=9608, loss=0.09324443340301514\n",
      "Surface training t=9609, loss=0.05589481070637703\n",
      "Surface training t=9610, loss=0.059662291780114174\n",
      "Surface training t=9611, loss=0.08070912957191467\n",
      "Surface training t=9612, loss=0.06537622399628162\n",
      "Surface training t=9613, loss=0.1030251532793045\n",
      "Surface training t=9614, loss=0.09773481264710426\n",
      "Surface training t=9615, loss=0.05376869533210993\n",
      "Surface training t=9616, loss=0.08494728431105614\n",
      "Surface training t=9617, loss=0.07080794870853424\n",
      "Surface training t=9618, loss=0.06408644653856754\n",
      "Surface training t=9619, loss=0.09025547653436661\n",
      "Surface training t=9620, loss=0.05096572823822498\n",
      "Surface training t=9621, loss=0.055591631680727005\n",
      "Surface training t=9622, loss=0.06853910908102989\n",
      "Surface training t=9623, loss=0.07944243773818016\n",
      "Surface training t=9624, loss=0.05409197881817818\n",
      "Surface training t=9625, loss=0.06228103116154671\n",
      "Surface training t=9626, loss=0.04511628299951553\n",
      "Surface training t=9627, loss=0.04267706349492073\n",
      "Surface training t=9628, loss=0.03981582075357437\n",
      "Surface training t=9629, loss=0.0512133352458477\n",
      "Surface training t=9630, loss=0.04367625713348389\n",
      "Surface training t=9631, loss=0.040670245885849\n",
      "Surface training t=9632, loss=0.036961184814572334\n",
      "Surface training t=9633, loss=0.0328025259077549\n",
      "Surface training t=9634, loss=0.024608075618743896\n",
      "Surface training t=9635, loss=0.035683467984199524\n",
      "Surface training t=9636, loss=0.04048912227153778\n",
      "Surface training t=9637, loss=0.033353839069604874\n",
      "Surface training t=9638, loss=0.029496613889932632\n",
      "Surface training t=9639, loss=0.024243250489234924\n",
      "Surface training t=9640, loss=0.0413667056709528\n",
      "Surface training t=9641, loss=0.032224529422819614\n",
      "Surface training t=9642, loss=0.06127243861556053\n",
      "Surface training t=9643, loss=0.0427035391330719\n",
      "Surface training t=9644, loss=0.04325283691287041\n",
      "Surface training t=9645, loss=0.04072938673198223\n",
      "Surface training t=9646, loss=0.039362452924251556\n",
      "Surface training t=9647, loss=0.03235528990626335\n",
      "Surface training t=9648, loss=0.03354926221072674\n",
      "Surface training t=9649, loss=0.02771482802927494\n",
      "Surface training t=9650, loss=0.03448040783405304\n",
      "Surface training t=9651, loss=0.03549465723335743\n",
      "Surface training t=9652, loss=0.027388805523514748\n",
      "Surface training t=9653, loss=0.03026576153934002\n",
      "Surface training t=9654, loss=0.05495193973183632\n",
      "Surface training t=9655, loss=0.04096831753849983\n",
      "Surface training t=9656, loss=0.04998972453176975\n",
      "Surface training t=9657, loss=0.03820373211055994\n",
      "Surface training t=9658, loss=0.04685964994132519\n",
      "Surface training t=9659, loss=0.04174238629639149\n",
      "Surface training t=9660, loss=0.048475438728928566\n",
      "Surface training t=9661, loss=0.09136119484901428\n",
      "Surface training t=9662, loss=0.04767816141247749\n",
      "Surface training t=9663, loss=0.03835471346974373\n",
      "Surface training t=9664, loss=0.03901979140937328\n",
      "Surface training t=9665, loss=0.040290042757987976\n",
      "Surface training t=9666, loss=0.04047124646604061\n",
      "Surface training t=9667, loss=0.04652789793908596\n",
      "Surface training t=9668, loss=0.05165460519492626\n",
      "Surface training t=9669, loss=0.03693537972867489\n",
      "Surface training t=9670, loss=0.03542547672986984\n",
      "Surface training t=9671, loss=0.034970201551914215\n",
      "Surface training t=9672, loss=0.03913707099854946\n",
      "Surface training t=9673, loss=0.03581010177731514\n",
      "Surface training t=9674, loss=0.058096855878829956\n",
      "Surface training t=9675, loss=0.04551977664232254\n",
      "Surface training t=9676, loss=0.04601316899061203\n",
      "Surface training t=9677, loss=0.02752481773495674\n",
      "Surface training t=9678, loss=0.028341968543827534\n",
      "Surface training t=9679, loss=0.021652281284332275\n",
      "Surface training t=9680, loss=0.028540683910250664\n",
      "Surface training t=9681, loss=0.02807280421257019\n",
      "Surface training t=9682, loss=0.026121693663299084\n",
      "Surface training t=9683, loss=0.031979845836758614\n",
      "Surface training t=9684, loss=0.02994145080447197\n",
      "Surface training t=9685, loss=0.02723598200827837\n",
      "Surface training t=9686, loss=0.032264893874526024\n",
      "Surface training t=9687, loss=0.02331255003809929\n",
      "Surface training t=9688, loss=0.027916375547647476\n",
      "Surface training t=9689, loss=0.02188572846353054\n",
      "Surface training t=9690, loss=0.028100019320845604\n",
      "Surface training t=9691, loss=0.03550111688673496\n",
      "Surface training t=9692, loss=0.03787989541888237\n",
      "Surface training t=9693, loss=0.04034296702593565\n",
      "Surface training t=9694, loss=0.042332058772444725\n",
      "Surface training t=9695, loss=0.0580764040350914\n",
      "Surface training t=9696, loss=0.040525149554014206\n",
      "Surface training t=9697, loss=0.04776923172175884\n",
      "Surface training t=9698, loss=0.04752679914236069\n",
      "Surface training t=9699, loss=0.050335297361016273\n",
      "Surface training t=9700, loss=0.04207035154104233\n",
      "Surface training t=9701, loss=0.07913321629166603\n",
      "Surface training t=9702, loss=0.05155370198190212\n",
      "Surface training t=9703, loss=0.06481650471687317\n",
      "Surface training t=9704, loss=0.05674724094569683\n",
      "Surface training t=9705, loss=0.07580525428056717\n",
      "Surface training t=9706, loss=0.04628270212560892\n",
      "Surface training t=9707, loss=0.06056515872478485\n",
      "Surface training t=9708, loss=0.04634849540889263\n",
      "Surface training t=9709, loss=0.07873732224106789\n",
      "Surface training t=9710, loss=0.04775333032011986\n",
      "Surface training t=9711, loss=0.051997408270835876\n",
      "Surface training t=9712, loss=0.04067164473235607\n",
      "Surface training t=9713, loss=0.04588287137448788\n",
      "Surface training t=9714, loss=0.03470141813158989\n",
      "Surface training t=9715, loss=0.040035003796219826\n",
      "Surface training t=9716, loss=0.03483313973993063\n",
      "Surface training t=9717, loss=0.044902531430125237\n",
      "Surface training t=9718, loss=0.05480106733739376\n",
      "Surface training t=9719, loss=0.03802408929914236\n",
      "Surface training t=9720, loss=0.033179871737957\n",
      "Surface training t=9721, loss=0.03341903816908598\n",
      "Surface training t=9722, loss=0.0360405994579196\n",
      "Surface training t=9723, loss=0.032299199141561985\n",
      "Surface training t=9724, loss=0.03686586953699589\n",
      "Surface training t=9725, loss=0.0374327190220356\n",
      "Surface training t=9726, loss=0.035309815779328346\n",
      "Surface training t=9727, loss=0.03177106939256191\n",
      "Surface training t=9728, loss=0.041969183832407\n",
      "Surface training t=9729, loss=0.04386531189084053\n",
      "Surface training t=9730, loss=0.046255506575107574\n",
      "Surface training t=9731, loss=0.04588194936513901\n",
      "Surface training t=9732, loss=0.041171811521053314\n",
      "Surface training t=9733, loss=0.04797145538032055\n",
      "Surface training t=9734, loss=0.05640877224504948\n",
      "Surface training t=9735, loss=0.04319402202963829\n",
      "Surface training t=9736, loss=0.046799033880233765\n",
      "Surface training t=9737, loss=0.049804169684648514\n",
      "Surface training t=9738, loss=0.043122146278619766\n",
      "Surface training t=9739, loss=0.03397925663739443\n",
      "Surface training t=9740, loss=0.03472306951880455\n",
      "Surface training t=9741, loss=0.03452576603740454\n",
      "Surface training t=9742, loss=0.025295660831034184\n",
      "Surface training t=9743, loss=0.024610275402665138\n",
      "Surface training t=9744, loss=0.01982066687196493\n",
      "Surface training t=9745, loss=0.026164928451180458\n",
      "Surface training t=9746, loss=0.021937541663646698\n",
      "Surface training t=9747, loss=0.028930291533470154\n",
      "Surface training t=9748, loss=0.041878306306898594\n",
      "Surface training t=9749, loss=0.06223225221037865\n",
      "Surface training t=9750, loss=0.050932345911860466\n",
      "Surface training t=9751, loss=0.031629906967282295\n",
      "Surface training t=9752, loss=0.04046468436717987\n",
      "Surface training t=9753, loss=0.035214792005717754\n",
      "Surface training t=9754, loss=0.03572136349976063\n",
      "Surface training t=9755, loss=0.032507757656276226\n",
      "Surface training t=9756, loss=0.025383979082107544\n",
      "Surface training t=9757, loss=0.030492953956127167\n",
      "Surface training t=9758, loss=0.028841997496783733\n",
      "Surface training t=9759, loss=0.031140918843448162\n",
      "Surface training t=9760, loss=0.025309761054813862\n",
      "Surface training t=9761, loss=0.02563254628330469\n",
      "Surface training t=9762, loss=0.02107082214206457\n",
      "Surface training t=9763, loss=0.02259235642850399\n",
      "Surface training t=9764, loss=0.03294987324625254\n",
      "Surface training t=9765, loss=0.0250629847869277\n",
      "Surface training t=9766, loss=0.02570464089512825\n",
      "Surface training t=9767, loss=0.0316918920725584\n",
      "Surface training t=9768, loss=0.0221020532771945\n",
      "Surface training t=9769, loss=0.02888836245983839\n",
      "Surface training t=9770, loss=0.025691034272313118\n",
      "Surface training t=9771, loss=0.03012376930564642\n",
      "Surface training t=9772, loss=0.033816058188676834\n",
      "Surface training t=9773, loss=0.034473590552806854\n",
      "Surface training t=9774, loss=0.03275670018047094\n",
      "Surface training t=9775, loss=0.03478014189749956\n",
      "Surface training t=9776, loss=0.04749942384660244\n",
      "Surface training t=9777, loss=0.05500585585832596\n",
      "Surface training t=9778, loss=0.05502730421721935\n",
      "Surface training t=9779, loss=0.041955118998885155\n",
      "Surface training t=9780, loss=0.06417053192853928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=9781, loss=0.046845052391290665\n",
      "Surface training t=9782, loss=0.05558740720152855\n",
      "Surface training t=9783, loss=0.042470917105674744\n",
      "Surface training t=9784, loss=0.04015364404767752\n",
      "Surface training t=9785, loss=0.05125902406871319\n",
      "Surface training t=9786, loss=0.03614345006644726\n",
      "Surface training t=9787, loss=0.034554870799183846\n",
      "Surface training t=9788, loss=0.03435451164841652\n",
      "Surface training t=9789, loss=0.03089738730341196\n",
      "Surface training t=9790, loss=0.031422413885593414\n",
      "Surface training t=9791, loss=0.02586725540459156\n",
      "Surface training t=9792, loss=0.027485317550599575\n",
      "Surface training t=9793, loss=0.028855178505182266\n",
      "Surface training t=9794, loss=0.025477871298789978\n",
      "Surface training t=9795, loss=0.024517958983778954\n",
      "Surface training t=9796, loss=0.03277725912630558\n",
      "Surface training t=9797, loss=0.03995757922530174\n",
      "Surface training t=9798, loss=0.02935598697513342\n",
      "Surface training t=9799, loss=0.04120850935578346\n",
      "Surface training t=9800, loss=0.032350036315619946\n",
      "Surface training t=9801, loss=0.031068935990333557\n",
      "Surface training t=9802, loss=0.029241779819130898\n",
      "Surface training t=9803, loss=0.03024875931441784\n",
      "Surface training t=9804, loss=0.0340093057602644\n",
      "Surface training t=9805, loss=0.0254001310095191\n",
      "Surface training t=9806, loss=0.026202308014035225\n",
      "Surface training t=9807, loss=0.01988877821713686\n",
      "Surface training t=9808, loss=0.019833034835755825\n",
      "Surface training t=9809, loss=0.025605501607060432\n",
      "Surface training t=9810, loss=0.02306707203388214\n",
      "Surface training t=9811, loss=0.04053156264126301\n",
      "Surface training t=9812, loss=0.04044343903660774\n",
      "Surface training t=9813, loss=0.04054510220885277\n",
      "Surface training t=9814, loss=0.03673507645726204\n",
      "Surface training t=9815, loss=0.036589449271559715\n",
      "Surface training t=9816, loss=0.04192679654806852\n",
      "Surface training t=9817, loss=0.042464567348361015\n",
      "Surface training t=9818, loss=0.04389127902686596\n",
      "Surface training t=9819, loss=0.030117040500044823\n",
      "Surface training t=9820, loss=0.029859211295843124\n",
      "Surface training t=9821, loss=0.029658429324626923\n",
      "Surface training t=9822, loss=0.03004886768758297\n",
      "Surface training t=9823, loss=0.027326665818691254\n",
      "Surface training t=9824, loss=0.03422069922089577\n",
      "Surface training t=9825, loss=0.025610674172639847\n",
      "Surface training t=9826, loss=0.04044313542544842\n",
      "Surface training t=9827, loss=0.03133660461753607\n",
      "Surface training t=9828, loss=0.037571960128843784\n",
      "Surface training t=9829, loss=0.03089696727693081\n",
      "Surface training t=9830, loss=0.019786283373832703\n",
      "Surface training t=9831, loss=0.022971102967858315\n",
      "Surface training t=9832, loss=0.02991261798888445\n",
      "Surface training t=9833, loss=0.032658349722623825\n",
      "Surface training t=9834, loss=0.03630463220179081\n",
      "Surface training t=9835, loss=0.029939375817775726\n",
      "Surface training t=9836, loss=0.04860666207969189\n",
      "Surface training t=9837, loss=0.03785592317581177\n",
      "Surface training t=9838, loss=0.06328899972140789\n",
      "Surface training t=9839, loss=0.04503430984914303\n",
      "Surface training t=9840, loss=0.06322731077671051\n",
      "Surface training t=9841, loss=0.032554587349295616\n",
      "Surface training t=9842, loss=0.03244688920676708\n",
      "Surface training t=9843, loss=0.03422770835459232\n",
      "Surface training t=9844, loss=0.025853966362774372\n",
      "Surface training t=9845, loss=0.028578310273587704\n",
      "Surface training t=9846, loss=0.03600117936730385\n",
      "Surface training t=9847, loss=0.03025093674659729\n",
      "Surface training t=9848, loss=0.027431972324848175\n",
      "Surface training t=9849, loss=0.025682665407657623\n",
      "Surface training t=9850, loss=0.02262546494603157\n",
      "Surface training t=9851, loss=0.03106637392193079\n",
      "Surface training t=9852, loss=0.025001823902130127\n",
      "Surface training t=9853, loss=0.03303942456841469\n",
      "Surface training t=9854, loss=0.038126105442643166\n",
      "Surface training t=9855, loss=0.03684781491756439\n",
      "Surface training t=9856, loss=0.050304166972637177\n",
      "Surface training t=9857, loss=0.05062614940106869\n",
      "Surface training t=9858, loss=0.06105678901076317\n",
      "Surface training t=9859, loss=0.06497257575392723\n",
      "Surface training t=9860, loss=0.061710599809885025\n",
      "Surface training t=9861, loss=0.09231666848063469\n",
      "Surface training t=9862, loss=0.09500177577137947\n",
      "Surface training t=9863, loss=0.06341942586004734\n",
      "Surface training t=9864, loss=0.07825279980897903\n",
      "Surface training t=9865, loss=0.05198800191283226\n",
      "Surface training t=9866, loss=0.06309714540839195\n",
      "Surface training t=9867, loss=0.059427034109830856\n",
      "Surface training t=9868, loss=0.062377069145441055\n",
      "Surface training t=9869, loss=0.053951533511281013\n",
      "Surface training t=9870, loss=0.06392696313560009\n",
      "Surface training t=9871, loss=0.05181345343589783\n",
      "Surface training t=9872, loss=0.06595994532108307\n",
      "Surface training t=9873, loss=0.0426073158159852\n",
      "Surface training t=9874, loss=0.05544991046190262\n",
      "Surface training t=9875, loss=0.03630150482058525\n",
      "Surface training t=9876, loss=0.05216534808278084\n",
      "Surface training t=9877, loss=0.05957831256091595\n",
      "Surface training t=9878, loss=0.07713223248720169\n",
      "Surface training t=9879, loss=0.06256452761590481\n",
      "Surface training t=9880, loss=0.06471492163836956\n",
      "Surface training t=9881, loss=0.04116855934262276\n",
      "Surface training t=9882, loss=0.03661922086030245\n",
      "Surface training t=9883, loss=0.033974237740039825\n",
      "Surface training t=9884, loss=0.029743600636720657\n",
      "Surface training t=9885, loss=0.024250414222478867\n",
      "Surface training t=9886, loss=0.028775472193956375\n",
      "Surface training t=9887, loss=0.021571089513599873\n",
      "Surface training t=9888, loss=0.022054464556276798\n",
      "Surface training t=9889, loss=0.0450966227799654\n",
      "Surface training t=9890, loss=0.0736551284790039\n",
      "Surface training t=9891, loss=0.04962374269962311\n",
      "Surface training t=9892, loss=0.054248249158263206\n",
      "Surface training t=9893, loss=0.07569807209074497\n",
      "Surface training t=9894, loss=0.08034750819206238\n",
      "Surface training t=9895, loss=0.053210845217108727\n",
      "Surface training t=9896, loss=0.050148120149970055\n",
      "Surface training t=9897, loss=0.05084865167737007\n",
      "Surface training t=9898, loss=0.055280519649386406\n",
      "Surface training t=9899, loss=0.04295804165303707\n",
      "Surface training t=9900, loss=0.039004847407341\n",
      "Surface training t=9901, loss=0.038425687700510025\n",
      "Surface training t=9902, loss=0.027777206152677536\n",
      "Surface training t=9903, loss=0.03342422563582659\n",
      "Surface training t=9904, loss=0.03936477843672037\n",
      "Surface training t=9905, loss=0.03225699905306101\n",
      "Surface training t=9906, loss=0.028677593916654587\n",
      "Surface training t=9907, loss=0.029800566844642162\n",
      "Surface training t=9908, loss=0.03560225013643503\n",
      "Surface training t=9909, loss=0.029753895476460457\n",
      "Surface training t=9910, loss=0.03147604875266552\n",
      "Surface training t=9911, loss=0.043466974049806595\n",
      "Surface training t=9912, loss=0.02908132318407297\n",
      "Surface training t=9913, loss=0.029013480991125107\n",
      "Surface training t=9914, loss=0.036918558180332184\n",
      "Surface training t=9915, loss=0.041765378788113594\n",
      "Surface training t=9916, loss=0.046316832304000854\n",
      "Surface training t=9917, loss=0.04024007171392441\n",
      "Surface training t=9918, loss=0.02525930292904377\n",
      "Surface training t=9919, loss=0.02582209836691618\n",
      "Surface training t=9920, loss=0.030994855798780918\n",
      "Surface training t=9921, loss=0.04750153794884682\n",
      "Surface training t=9922, loss=0.036150360479950905\n",
      "Surface training t=9923, loss=0.042304592207074165\n",
      "Surface training t=9924, loss=0.03407760430127382\n",
      "Surface training t=9925, loss=0.04683597944676876\n",
      "Surface training t=9926, loss=0.07207256183028221\n",
      "Surface training t=9927, loss=0.046765596605837345\n",
      "Surface training t=9928, loss=0.05672438256442547\n",
      "Surface training t=9929, loss=0.05093419551849365\n",
      "Surface training t=9930, loss=0.1120775155723095\n",
      "Surface training t=9931, loss=0.0649009495973587\n",
      "Surface training t=9932, loss=0.10376643016934395\n",
      "Surface training t=9933, loss=0.07983535900712013\n",
      "Surface training t=9934, loss=0.0602797232568264\n",
      "Surface training t=9935, loss=0.06487612798810005\n",
      "Surface training t=9936, loss=0.05333993211388588\n",
      "Surface training t=9937, loss=0.04502210021018982\n",
      "Surface training t=9938, loss=0.04467012546956539\n",
      "Surface training t=9939, loss=0.04566812328994274\n",
      "Surface training t=9940, loss=0.04065253213047981\n",
      "Surface training t=9941, loss=0.043726781383156776\n",
      "Surface training t=9942, loss=0.03772701323032379\n",
      "Surface training t=9943, loss=0.053089918568730354\n",
      "Surface training t=9944, loss=0.03968043904751539\n",
      "Surface training t=9945, loss=0.044239116832613945\n",
      "Surface training t=9946, loss=0.03249960392713547\n",
      "Surface training t=9947, loss=0.03550666756927967\n",
      "Surface training t=9948, loss=0.03768976218998432\n",
      "Surface training t=9949, loss=0.03947068564593792\n",
      "Surface training t=9950, loss=0.05047670565545559\n",
      "Surface training t=9951, loss=0.048138244077563286\n",
      "Surface training t=9952, loss=0.03888723719865084\n",
      "Surface training t=9953, loss=0.03439189866185188\n",
      "Surface training t=9954, loss=0.06670074164867401\n",
      "Surface training t=9955, loss=0.0484831091016531\n",
      "Surface training t=9956, loss=0.044133247807621956\n",
      "Surface training t=9957, loss=0.04253001883625984\n",
      "Surface training t=9958, loss=0.04961005039513111\n",
      "Surface training t=9959, loss=0.0832429900765419\n",
      "Surface training t=9960, loss=0.06547260843217373\n",
      "Surface training t=9961, loss=0.07965943403542042\n",
      "Surface training t=9962, loss=0.1098501868546009\n",
      "Surface training t=9963, loss=0.058495599776506424\n",
      "Surface training t=9964, loss=0.0457482673227787\n",
      "Surface training t=9965, loss=0.038222795352339745\n",
      "Surface training t=9966, loss=0.03202606085687876\n",
      "Surface training t=9967, loss=0.03426981158554554\n",
      "Surface training t=9968, loss=0.038480695337057114\n",
      "Surface training t=9969, loss=0.03482761047780514\n",
      "Surface training t=9970, loss=0.03078118059784174\n",
      "Surface training t=9971, loss=0.02579465415328741\n",
      "Surface training t=9972, loss=0.03555011749267578\n",
      "Surface training t=9973, loss=0.03186333738267422\n",
      "Surface training t=9974, loss=0.04782022349536419\n",
      "Surface training t=9975, loss=0.03161400742828846\n",
      "Surface training t=9976, loss=0.03836209233850241\n",
      "Surface training t=9977, loss=0.04053671099245548\n",
      "Surface training t=9978, loss=0.03392592817544937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=9979, loss=0.03513842076063156\n",
      "Surface training t=9980, loss=0.02911230083554983\n",
      "Surface training t=9981, loss=0.03213627636432648\n",
      "Surface training t=9982, loss=0.035955335944890976\n",
      "Surface training t=9983, loss=0.026778013445436954\n",
      "Surface training t=9984, loss=0.028628237545490265\n",
      "Surface training t=9985, loss=0.019306113477796316\n",
      "Surface training t=9986, loss=0.021754919551312923\n",
      "Surface training t=9987, loss=0.021722011268138885\n",
      "Surface training t=9988, loss=0.02328298892825842\n",
      "Surface training t=9989, loss=0.03128358907997608\n",
      "Surface training t=9990, loss=0.031944140791893005\n",
      "Surface training t=9991, loss=0.03851511888206005\n",
      "Surface training t=9992, loss=0.045129429548978806\n",
      "Surface training t=9993, loss=0.04487387277185917\n",
      "Surface training t=9994, loss=0.04326272942125797\n",
      "Surface training t=9995, loss=0.0630657747387886\n",
      "Surface training t=9996, loss=0.061392663046717644\n",
      "Surface training t=9997, loss=0.05690637230873108\n",
      "Surface training t=9998, loss=0.06861584447324276\n",
      "Surface training t=9999, loss=0.05559576116502285\n",
      "Surface training t=10000, loss=0.0552041232585907\n",
      "Surface training t=10001, loss=0.053176818415522575\n",
      "Surface training t=10002, loss=0.0668993890285492\n",
      "Surface training t=10003, loss=0.053711721673607826\n",
      "Surface training t=10004, loss=0.05412929505109787\n",
      "Surface training t=10005, loss=0.041481535881757736\n",
      "Surface training t=10006, loss=0.05261689983308315\n",
      "Surface training t=10007, loss=0.05061380937695503\n",
      "Surface training t=10008, loss=0.03995688445866108\n",
      "Surface training t=10009, loss=0.05743413418531418\n",
      "Surface training t=10010, loss=0.06143329478800297\n",
      "Surface training t=10011, loss=0.05623563565313816\n",
      "Surface training t=10012, loss=0.06789541989564896\n",
      "Surface training t=10013, loss=0.11653601378202438\n",
      "Surface training t=10014, loss=0.0687463004142046\n",
      "Surface training t=10015, loss=0.12056971341371536\n",
      "Surface training t=10016, loss=0.06651921849697828\n",
      "Surface training t=10017, loss=0.07269874215126038\n",
      "Surface training t=10018, loss=0.12217768281698227\n",
      "Surface training t=10019, loss=0.08255169168114662\n",
      "Surface training t=10020, loss=0.11834921315312386\n",
      "Surface training t=10021, loss=0.06449250690639019\n",
      "Surface training t=10022, loss=0.07664654217660427\n",
      "Surface training t=10023, loss=0.07400002330541611\n",
      "Surface training t=10024, loss=0.055634185671806335\n",
      "Surface training t=10025, loss=0.049993470311164856\n",
      "Surface training t=10026, loss=0.04627246782183647\n",
      "Surface training t=10027, loss=0.03961518406867981\n",
      "Surface training t=10028, loss=0.05354587361216545\n",
      "Surface training t=10029, loss=0.05192580632865429\n",
      "Surface training t=10030, loss=0.040375834330916405\n",
      "Surface training t=10031, loss=0.05572260916233063\n",
      "Surface training t=10032, loss=0.055518703535199165\n",
      "Surface training t=10033, loss=0.053782498463988304\n",
      "Surface training t=10034, loss=0.05879073403775692\n",
      "Surface training t=10035, loss=0.05371899902820587\n",
      "Surface training t=10036, loss=0.05233234167098999\n",
      "Surface training t=10037, loss=0.04716517589986324\n",
      "Surface training t=10038, loss=0.07891199365258217\n",
      "Surface training t=10039, loss=0.0534086087718606\n",
      "Surface training t=10040, loss=0.053868163377046585\n",
      "Surface training t=10041, loss=0.08119812607765198\n",
      "Surface training t=10042, loss=0.044754695147275925\n",
      "Surface training t=10043, loss=0.04634913243353367\n",
      "Surface training t=10044, loss=0.04866505041718483\n",
      "Surface training t=10045, loss=0.06509912386536598\n",
      "Surface training t=10046, loss=0.049819594249129295\n",
      "Surface training t=10047, loss=0.05708593130111694\n",
      "Surface training t=10048, loss=0.05396900698542595\n",
      "Surface training t=10049, loss=0.061180684715509415\n",
      "Surface training t=10050, loss=0.07677885517477989\n",
      "Surface training t=10051, loss=0.07904712110757828\n",
      "Surface training t=10052, loss=0.054377997294068336\n",
      "Surface training t=10053, loss=0.06409035995602608\n",
      "Surface training t=10054, loss=0.03973081894218922\n",
      "Surface training t=10055, loss=0.046222032979130745\n",
      "Surface training t=10056, loss=0.04702114313840866\n",
      "Surface training t=10057, loss=0.038458364084362984\n",
      "Surface training t=10058, loss=0.0435914508998394\n",
      "Surface training t=10059, loss=0.04407105967402458\n",
      "Surface training t=10060, loss=0.0434817997738719\n",
      "Surface training t=10061, loss=0.06018920801579952\n",
      "Surface training t=10062, loss=0.05618672072887421\n",
      "Surface training t=10063, loss=0.046082017943263054\n",
      "Surface training t=10064, loss=0.042998867109417915\n",
      "Surface training t=10065, loss=0.04944811202585697\n",
      "Surface training t=10066, loss=0.03588935825973749\n",
      "Surface training t=10067, loss=0.03463468700647354\n",
      "Surface training t=10068, loss=0.037223391234874725\n",
      "Surface training t=10069, loss=0.04326380416750908\n",
      "Surface training t=10070, loss=0.030343296006321907\n",
      "Surface training t=10071, loss=0.03187241964042187\n",
      "Surface training t=10072, loss=0.026870728470385075\n",
      "Surface training t=10073, loss=0.028946534730494022\n",
      "Surface training t=10074, loss=0.02675891574472189\n",
      "Surface training t=10075, loss=0.04072811733931303\n",
      "Surface training t=10076, loss=0.045844689942896366\n",
      "Surface training t=10077, loss=0.057266825810074806\n",
      "Surface training t=10078, loss=0.04769870266318321\n",
      "Surface training t=10079, loss=0.07368579134345055\n",
      "Surface training t=10080, loss=0.057704322040081024\n",
      "Surface training t=10081, loss=0.0429528784006834\n",
      "Surface training t=10082, loss=0.04970583878457546\n",
      "Surface training t=10083, loss=0.052121374756097794\n",
      "Surface training t=10084, loss=0.058404453098773956\n",
      "Surface training t=10085, loss=0.04809127189218998\n",
      "Surface training t=10086, loss=0.03895685262978077\n",
      "Surface training t=10087, loss=0.03892005793750286\n",
      "Surface training t=10088, loss=0.05157887563109398\n",
      "Surface training t=10089, loss=0.0396233182400465\n",
      "Surface training t=10090, loss=0.03532646410167217\n",
      "Surface training t=10091, loss=0.038371266797184944\n",
      "Surface training t=10092, loss=0.04098943620920181\n",
      "Surface training t=10093, loss=0.05357351526618004\n",
      "Surface training t=10094, loss=0.03897012397646904\n",
      "Surface training t=10095, loss=0.05055328458547592\n",
      "Surface training t=10096, loss=0.044959332793951035\n",
      "Surface training t=10097, loss=0.06199840269982815\n",
      "Surface training t=10098, loss=0.056280072778463364\n",
      "Surface training t=10099, loss=0.05898223631083965\n",
      "Surface training t=10100, loss=0.03866659291088581\n",
      "Surface training t=10101, loss=0.03914599772542715\n",
      "Surface training t=10102, loss=0.036914730444550514\n",
      "Surface training t=10103, loss=0.03428987227380276\n",
      "Surface training t=10104, loss=0.03709630109369755\n",
      "Surface training t=10105, loss=0.04364946484565735\n",
      "Surface training t=10106, loss=0.03159980196505785\n",
      "Surface training t=10107, loss=0.040154374204576015\n",
      "Surface training t=10108, loss=0.041275457479059696\n",
      "Surface training t=10109, loss=0.05291826277971268\n",
      "Surface training t=10110, loss=0.06508992984890938\n",
      "Surface training t=10111, loss=0.03842878434807062\n",
      "Surface training t=10112, loss=0.044659778475761414\n",
      "Surface training t=10113, loss=0.039658937603235245\n",
      "Surface training t=10114, loss=0.037680793553590775\n",
      "Surface training t=10115, loss=0.03968821745365858\n",
      "Surface training t=10116, loss=0.053207285702228546\n",
      "Surface training t=10117, loss=0.05052236560732126\n",
      "Surface training t=10118, loss=0.040446002036333084\n",
      "Surface training t=10119, loss=0.05578835494816303\n",
      "Surface training t=10120, loss=0.05274380370974541\n",
      "Surface training t=10121, loss=0.05330612789839506\n",
      "Surface training t=10122, loss=0.06245820224285126\n",
      "Surface training t=10123, loss=0.09076216071844101\n",
      "Surface training t=10124, loss=0.06009232625365257\n",
      "Surface training t=10125, loss=0.051950063556432724\n",
      "Surface training t=10126, loss=0.04080612771213055\n",
      "Surface training t=10127, loss=0.061554279178380966\n",
      "Surface training t=10128, loss=0.046287041157484055\n",
      "Surface training t=10129, loss=0.04589936509728432\n",
      "Surface training t=10130, loss=0.0449000708758831\n",
      "Surface training t=10131, loss=0.039347318932414055\n",
      "Surface training t=10132, loss=0.03852219507098198\n",
      "Surface training t=10133, loss=0.048353444784879684\n",
      "Surface training t=10134, loss=0.052006373181939125\n",
      "Surface training t=10135, loss=0.06853253021836281\n",
      "Surface training t=10136, loss=0.055752710439264774\n",
      "Surface training t=10137, loss=0.0884203352034092\n",
      "Surface training t=10138, loss=0.06818754598498344\n",
      "Surface training t=10139, loss=0.048604393377900124\n",
      "Surface training t=10140, loss=0.05408797040581703\n",
      "Surface training t=10141, loss=0.07509317994117737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=10142, loss=0.05164557881653309\n",
      "Surface training t=10143, loss=0.050653401762247086\n",
      "Surface training t=10144, loss=0.05137175880372524\n",
      "Surface training t=10145, loss=0.08287498354911804\n",
      "Surface training t=10146, loss=0.047613443806767464\n",
      "Surface training t=10147, loss=0.04886421002447605\n",
      "Surface training t=10148, loss=0.03512655571103096\n",
      "Surface training t=10149, loss=0.030551722273230553\n",
      "Surface training t=10150, loss=0.03185121342539787\n",
      "Surface training t=10151, loss=0.03168782405555248\n",
      "Surface training t=10152, loss=0.026615488342940807\n",
      "Surface training t=10153, loss=0.024617253802716732\n",
      "Surface training t=10154, loss=0.02414338756352663\n",
      "Surface training t=10155, loss=0.02661123964935541\n",
      "Surface training t=10156, loss=0.03253814112395048\n",
      "Surface training t=10157, loss=0.0468501690775156\n",
      "Surface training t=10158, loss=0.04174484871327877\n",
      "Surface training t=10159, loss=0.039210086688399315\n",
      "Surface training t=10160, loss=0.0551311019808054\n",
      "Surface training t=10161, loss=0.043258825317025185\n",
      "Surface training t=10162, loss=0.040702554397284985\n",
      "Surface training t=10163, loss=0.04365590214729309\n",
      "Surface training t=10164, loss=0.047865571454167366\n",
      "Surface training t=10165, loss=0.04236300848424435\n",
      "Surface training t=10166, loss=0.04419406317174435\n",
      "Surface training t=10167, loss=0.07637339271605015\n",
      "Surface training t=10168, loss=0.04829160496592522\n",
      "Surface training t=10169, loss=0.04910353757441044\n",
      "Surface training t=10170, loss=0.0375076113268733\n",
      "Surface training t=10171, loss=0.03193048760294914\n",
      "Surface training t=10172, loss=0.02487209253013134\n",
      "Surface training t=10173, loss=0.030872889794409275\n",
      "Surface training t=10174, loss=0.027221872471272945\n",
      "Surface training t=10175, loss=0.03397348336875439\n",
      "Surface training t=10176, loss=0.027830010280013084\n",
      "Surface training t=10177, loss=0.028233695775270462\n",
      "Surface training t=10178, loss=0.02258757594972849\n",
      "Surface training t=10179, loss=0.03291437216103077\n",
      "Surface training t=10180, loss=0.026247069239616394\n",
      "Surface training t=10181, loss=0.041936418041586876\n",
      "Surface training t=10182, loss=0.03820056654512882\n",
      "Surface training t=10183, loss=0.0371690820902586\n",
      "Surface training t=10184, loss=0.03853004239499569\n",
      "Surface training t=10185, loss=0.031143338419497013\n",
      "Surface training t=10186, loss=0.03369366005063057\n",
      "Surface training t=10187, loss=0.04160961788147688\n",
      "Surface training t=10188, loss=0.0413852259516716\n",
      "Surface training t=10189, loss=0.04577409662306309\n",
      "Surface training t=10190, loss=0.02826558891683817\n",
      "Surface training t=10191, loss=0.03688550088554621\n",
      "Surface training t=10192, loss=0.03389245644211769\n",
      "Surface training t=10193, loss=0.029560739174485207\n",
      "Surface training t=10194, loss=0.02915916219353676\n",
      "Surface training t=10195, loss=0.03117198497056961\n",
      "Surface training t=10196, loss=0.02744162641465664\n",
      "Surface training t=10197, loss=0.0316119771450758\n",
      "Surface training t=10198, loss=0.02641559299081564\n",
      "Surface training t=10199, loss=0.02468715701252222\n",
      "Surface training t=10200, loss=0.023612082935869694\n",
      "Surface training t=10201, loss=0.030111941508948803\n",
      "Surface training t=10202, loss=0.026073862798511982\n",
      "Surface training t=10203, loss=0.029508890584111214\n",
      "Surface training t=10204, loss=0.030209733173251152\n",
      "Surface training t=10205, loss=0.03423548210412264\n",
      "Surface training t=10206, loss=0.03365482576191425\n",
      "Surface training t=10207, loss=0.030245058238506317\n",
      "Surface training t=10208, loss=0.03064719494432211\n",
      "Surface training t=10209, loss=0.02879456616938114\n",
      "Surface training t=10210, loss=0.033775823190808296\n",
      "Surface training t=10211, loss=0.03809588402509689\n",
      "Surface training t=10212, loss=0.0331625621765852\n",
      "Surface training t=10213, loss=0.03654100559651852\n",
      "Surface training t=10214, loss=0.04219038411974907\n",
      "Surface training t=10215, loss=0.037985507398843765\n",
      "Surface training t=10216, loss=0.038014279678463936\n",
      "Surface training t=10217, loss=0.040300989523530006\n",
      "Surface training t=10218, loss=0.03987761773169041\n",
      "Surface training t=10219, loss=0.03905288502573967\n",
      "Surface training t=10220, loss=0.030323071405291557\n",
      "Surface training t=10221, loss=0.03988369181752205\n",
      "Surface training t=10222, loss=0.04164737742394209\n",
      "Surface training t=10223, loss=0.03798751346766949\n",
      "Surface training t=10224, loss=0.041907090693712234\n",
      "Surface training t=10225, loss=0.04195941239595413\n",
      "Surface training t=10226, loss=0.03529318142682314\n",
      "Surface training t=10227, loss=0.04130350612103939\n",
      "Surface training t=10228, loss=0.034645332023501396\n",
      "Surface training t=10229, loss=0.040404289960861206\n",
      "Surface training t=10230, loss=0.03256712481379509\n",
      "Surface training t=10231, loss=0.03470898699015379\n",
      "Surface training t=10232, loss=0.034676726907491684\n",
      "Surface training t=10233, loss=0.028044359758496284\n",
      "Surface training t=10234, loss=0.03443706128746271\n",
      "Surface training t=10235, loss=0.03371697012335062\n",
      "Surface training t=10236, loss=0.03183699585497379\n",
      "Surface training t=10237, loss=0.040588621981441975\n",
      "Surface training t=10238, loss=0.06975742056965828\n",
      "Surface training t=10239, loss=0.05205198656767607\n",
      "Surface training t=10240, loss=0.05725729092955589\n",
      "Surface training t=10241, loss=0.0764884278178215\n",
      "Surface training t=10242, loss=0.05112484470009804\n",
      "Surface training t=10243, loss=0.05060353875160217\n",
      "Surface training t=10244, loss=0.07504701055586338\n",
      "Surface training t=10245, loss=0.08416920155286789\n",
      "Surface training t=10246, loss=0.06034698523581028\n",
      "Surface training t=10247, loss=0.0864882804453373\n",
      "Surface training t=10248, loss=0.057651400566101074\n",
      "Surface training t=10249, loss=0.0523360799998045\n",
      "Surface training t=10250, loss=0.059643425047397614\n",
      "Surface training t=10251, loss=0.04274960421025753\n",
      "Surface training t=10252, loss=0.05232745595276356\n",
      "Surface training t=10253, loss=0.030609366483986378\n",
      "Surface training t=10254, loss=0.027925084345042706\n",
      "Surface training t=10255, loss=0.03684339299798012\n",
      "Surface training t=10256, loss=0.034067437052726746\n",
      "Surface training t=10257, loss=0.033058520406484604\n",
      "Surface training t=10258, loss=0.030579732730984688\n",
      "Surface training t=10259, loss=0.03044566512107849\n",
      "Surface training t=10260, loss=0.024730922654271126\n",
      "Surface training t=10261, loss=0.035100968554615974\n",
      "Surface training t=10262, loss=0.02316869981586933\n",
      "Surface training t=10263, loss=0.028907266445457935\n",
      "Surface training t=10264, loss=0.02657398022711277\n",
      "Surface training t=10265, loss=0.023579571396112442\n",
      "Surface training t=10266, loss=0.02716299705207348\n",
      "Surface training t=10267, loss=0.022095728665590286\n",
      "Surface training t=10268, loss=0.025254477746784687\n",
      "Surface training t=10269, loss=0.02550618350505829\n",
      "Surface training t=10270, loss=0.035784730687737465\n",
      "Surface training t=10271, loss=0.03809582442045212\n",
      "Surface training t=10272, loss=0.04981626570224762\n",
      "Surface training t=10273, loss=0.03942711278796196\n",
      "Surface training t=10274, loss=0.03831825964152813\n",
      "Surface training t=10275, loss=0.044275565072894096\n",
      "Surface training t=10276, loss=0.05108219012618065\n",
      "Surface training t=10277, loss=0.050042192451655865\n",
      "Surface training t=10278, loss=0.09088393673300743\n",
      "Surface training t=10279, loss=0.06289412453770638\n",
      "Surface training t=10280, loss=0.07390052452683449\n",
      "Surface training t=10281, loss=0.038265518844127655\n",
      "Surface training t=10282, loss=0.06480318680405617\n",
      "Surface training t=10283, loss=0.042924802750349045\n",
      "Surface training t=10284, loss=0.057474978268146515\n",
      "Surface training t=10285, loss=0.04242348298430443\n",
      "Surface training t=10286, loss=0.07943232357501984\n",
      "Surface training t=10287, loss=0.04092517774552107\n",
      "Surface training t=10288, loss=0.0634765774011612\n",
      "Surface training t=10289, loss=0.04998117871582508\n",
      "Surface training t=10290, loss=0.061519548296928406\n",
      "Surface training t=10291, loss=0.039690712466835976\n",
      "Surface training t=10292, loss=0.04949751868844032\n",
      "Surface training t=10293, loss=0.04341285303235054\n",
      "Surface training t=10294, loss=0.04334530420601368\n",
      "Surface training t=10295, loss=0.04037601873278618\n",
      "Surface training t=10296, loss=0.03532003052532673\n",
      "Surface training t=10297, loss=0.041481902822852135\n",
      "Surface training t=10298, loss=0.040257083252072334\n",
      "Surface training t=10299, loss=0.03954309970140457\n",
      "Surface training t=10300, loss=0.03658858500421047\n",
      "Surface training t=10301, loss=0.031401876360177994\n",
      "Surface training t=10302, loss=0.030122479423880577\n",
      "Surface training t=10303, loss=0.03546759206801653\n",
      "Surface training t=10304, loss=0.03861340694129467\n",
      "Surface training t=10305, loss=0.029197081923484802\n",
      "Surface training t=10306, loss=0.023041173815727234\n",
      "Surface training t=10307, loss=0.022154697217047215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=10308, loss=0.022264442406594753\n",
      "Surface training t=10309, loss=0.034806784242391586\n",
      "Surface training t=10310, loss=0.029838894493877888\n",
      "Surface training t=10311, loss=0.01955081894993782\n",
      "Surface training t=10312, loss=0.025854877196252346\n",
      "Surface training t=10313, loss=0.021305694244801998\n",
      "Surface training t=10314, loss=0.019803152419626713\n",
      "Surface training t=10315, loss=0.02099719177931547\n",
      "Surface training t=10316, loss=0.021786095574498177\n",
      "Surface training t=10317, loss=0.031986369751393795\n",
      "Surface training t=10318, loss=0.02350609190762043\n",
      "Surface training t=10319, loss=0.025771548971533775\n",
      "Surface training t=10320, loss=0.022226085886359215\n",
      "Surface training t=10321, loss=0.025358865037560463\n",
      "Surface training t=10322, loss=0.03148622810840607\n",
      "Surface training t=10323, loss=0.042726971209049225\n",
      "Surface training t=10324, loss=0.0317647960036993\n",
      "Surface training t=10325, loss=0.04019859805703163\n",
      "Surface training t=10326, loss=0.041154056787490845\n",
      "Surface training t=10327, loss=0.04610336758196354\n",
      "Surface training t=10328, loss=0.043056756258010864\n",
      "Surface training t=10329, loss=0.0365313496440649\n",
      "Surface training t=10330, loss=0.04904133453965187\n",
      "Surface training t=10331, loss=0.04008457902818918\n",
      "Surface training t=10332, loss=0.039284732192754745\n",
      "Surface training t=10333, loss=0.03795858100056648\n",
      "Surface training t=10334, loss=0.03766319900751114\n",
      "Surface training t=10335, loss=0.04027015343308449\n",
      "Surface training t=10336, loss=0.03712175600230694\n",
      "Surface training t=10337, loss=0.04258379898965359\n",
      "Surface training t=10338, loss=0.038580965250730515\n",
      "Surface training t=10339, loss=0.03690105676651001\n",
      "Surface training t=10340, loss=0.042586660012602806\n",
      "Surface training t=10341, loss=0.040245224721729755\n",
      "Surface training t=10342, loss=0.03662216104567051\n",
      "Surface training t=10343, loss=0.024936582893133163\n",
      "Surface training t=10344, loss=0.02520741242915392\n",
      "Surface training t=10345, loss=0.02643571887165308\n",
      "Surface training t=10346, loss=0.02363696414977312\n",
      "Surface training t=10347, loss=0.02485257387161255\n",
      "Surface training t=10348, loss=0.02928951196372509\n",
      "Surface training t=10349, loss=0.020431715063750744\n",
      "Surface training t=10350, loss=0.03575311228632927\n",
      "Surface training t=10351, loss=0.032945615239441395\n",
      "Surface training t=10352, loss=0.04724055342376232\n",
      "Surface training t=10353, loss=0.06159903481602669\n",
      "Surface training t=10354, loss=0.042982058599591255\n",
      "Surface training t=10355, loss=0.039262499660253525\n",
      "Surface training t=10356, loss=0.0423861239105463\n",
      "Surface training t=10357, loss=0.04813875071704388\n",
      "Surface training t=10358, loss=0.032667020335793495\n",
      "Surface training t=10359, loss=0.0668123159557581\n",
      "Surface training t=10360, loss=0.06442930921912193\n",
      "Surface training t=10361, loss=0.04254647810012102\n",
      "Surface training t=10362, loss=0.048341650515794754\n",
      "Surface training t=10363, loss=0.04355290066450834\n",
      "Surface training t=10364, loss=0.06134597957134247\n",
      "Surface training t=10365, loss=0.04776672273874283\n",
      "Surface training t=10366, loss=0.04063998442143202\n",
      "Surface training t=10367, loss=0.03762879967689514\n",
      "Surface training t=10368, loss=0.036902520805597305\n",
      "Surface training t=10369, loss=0.0381644032895565\n",
      "Surface training t=10370, loss=0.0316401869058609\n",
      "Surface training t=10371, loss=0.025287476368248463\n",
      "Surface training t=10372, loss=0.02629378717392683\n",
      "Surface training t=10373, loss=0.028134833090007305\n",
      "Surface training t=10374, loss=0.01866371463984251\n",
      "Surface training t=10375, loss=0.03391370829194784\n",
      "Surface training t=10376, loss=0.0368014220148325\n",
      "Surface training t=10377, loss=0.040422579273581505\n",
      "Surface training t=10378, loss=0.03712454345077276\n",
      "Surface training t=10379, loss=0.037484840489923954\n",
      "Surface training t=10380, loss=0.032123331911861897\n",
      "Surface training t=10381, loss=0.030061624944210052\n",
      "Surface training t=10382, loss=0.031847625970840454\n",
      "Surface training t=10383, loss=0.03589693270623684\n",
      "Surface training t=10384, loss=0.03807973489165306\n",
      "Surface training t=10385, loss=0.03332462254911661\n",
      "Surface training t=10386, loss=0.03492157720029354\n",
      "Surface training t=10387, loss=0.0414896160364151\n",
      "Surface training t=10388, loss=0.047856248915195465\n",
      "Surface training t=10389, loss=0.043843261897563934\n",
      "Surface training t=10390, loss=0.03877134621143341\n",
      "Surface training t=10391, loss=0.02910782303661108\n",
      "Surface training t=10392, loss=0.031780194491147995\n",
      "Surface training t=10393, loss=0.04119581915438175\n",
      "Surface training t=10394, loss=0.038183968514204025\n",
      "Surface training t=10395, loss=0.026926840655505657\n",
      "Surface training t=10396, loss=0.04479452036321163\n",
      "Surface training t=10397, loss=0.033798808231949806\n",
      "Surface training t=10398, loss=0.05306596681475639\n",
      "Surface training t=10399, loss=0.0466107502579689\n",
      "Surface training t=10400, loss=0.04333961009979248\n",
      "Surface training t=10401, loss=0.03755320329219103\n",
      "Surface training t=10402, loss=0.04678507708013058\n",
      "Surface training t=10403, loss=0.05550185963511467\n",
      "Surface training t=10404, loss=0.04076077602803707\n",
      "Surface training t=10405, loss=0.049694670364260674\n",
      "Surface training t=10406, loss=0.044249244034290314\n",
      "Surface training t=10407, loss=0.040348904207348824\n",
      "Surface training t=10408, loss=0.04561097174882889\n",
      "Surface training t=10409, loss=0.030331257730722427\n",
      "Surface training t=10410, loss=0.032737305387854576\n",
      "Surface training t=10411, loss=0.044342679902911186\n",
      "Surface training t=10412, loss=0.03197923116385937\n",
      "Surface training t=10413, loss=0.0323129678145051\n",
      "Surface training t=10414, loss=0.037368254736065865\n",
      "Surface training t=10415, loss=0.03547900728881359\n",
      "Surface training t=10416, loss=0.03405768610537052\n",
      "Surface training t=10417, loss=0.03793451003730297\n",
      "Surface training t=10418, loss=0.02961980551481247\n",
      "Surface training t=10419, loss=0.02840995229780674\n",
      "Surface training t=10420, loss=0.03329671174287796\n",
      "Surface training t=10421, loss=0.029960574582219124\n",
      "Surface training t=10422, loss=0.02471729926764965\n",
      "Surface training t=10423, loss=0.025833532214164734\n",
      "Surface training t=10424, loss=0.02395201288163662\n",
      "Surface training t=10425, loss=0.025886230170726776\n",
      "Surface training t=10426, loss=0.033580880612134933\n",
      "Surface training t=10427, loss=0.03116112295538187\n",
      "Surface training t=10428, loss=0.033088937401771545\n",
      "Surface training t=10429, loss=0.03170208819210529\n",
      "Surface training t=10430, loss=0.02848398219794035\n",
      "Surface training t=10431, loss=0.033313912339508533\n",
      "Surface training t=10432, loss=0.026817772537469864\n",
      "Surface training t=10433, loss=0.03232817351818085\n",
      "Surface training t=10434, loss=0.024106563068926334\n",
      "Surface training t=10435, loss=0.03315241541713476\n",
      "Surface training t=10436, loss=0.03597105946391821\n",
      "Surface training t=10437, loss=0.028611368499696255\n",
      "Surface training t=10438, loss=0.02814294584095478\n",
      "Surface training t=10439, loss=0.03446531295776367\n",
      "Surface training t=10440, loss=0.02653154917061329\n",
      "Surface training t=10441, loss=0.037221189588308334\n",
      "Surface training t=10442, loss=0.035899532958865166\n",
      "Surface training t=10443, loss=0.050381433218717575\n",
      "Surface training t=10444, loss=0.04819227010011673\n",
      "Surface training t=10445, loss=0.049000538885593414\n",
      "Surface training t=10446, loss=0.04296630993485451\n",
      "Surface training t=10447, loss=0.04176160879433155\n",
      "Surface training t=10448, loss=0.04179989546537399\n",
      "Surface training t=10449, loss=0.06548439897596836\n",
      "Surface training t=10450, loss=0.05287350341677666\n",
      "Surface training t=10451, loss=0.0352574922144413\n",
      "Surface training t=10452, loss=0.039752643555402756\n",
      "Surface training t=10453, loss=0.02926633320748806\n",
      "Surface training t=10454, loss=0.02643778920173645\n",
      "Surface training t=10455, loss=0.03208394628018141\n",
      "Surface training t=10456, loss=0.033109499141573906\n",
      "Surface training t=10457, loss=0.04289858974516392\n",
      "Surface training t=10458, loss=0.0380049217492342\n",
      "Surface training t=10459, loss=0.04852033779025078\n",
      "Surface training t=10460, loss=0.029411365278065205\n",
      "Surface training t=10461, loss=0.028883269988000393\n",
      "Surface training t=10462, loss=0.023066715337336063\n",
      "Surface training t=10463, loss=0.02961540035903454\n",
      "Surface training t=10464, loss=0.0248001953586936\n",
      "Surface training t=10465, loss=0.02685715164989233\n",
      "Surface training t=10466, loss=0.03490010928362608\n",
      "Surface training t=10467, loss=0.0308806411921978\n",
      "Surface training t=10468, loss=0.02617691084742546\n",
      "Surface training t=10469, loss=0.02760316524654627\n",
      "Surface training t=10470, loss=0.028142917901277542\n",
      "Surface training t=10471, loss=0.03181967046111822\n",
      "Surface training t=10472, loss=0.030316168442368507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=10473, loss=0.02667766809463501\n",
      "Surface training t=10474, loss=0.02154628373682499\n",
      "Surface training t=10475, loss=0.023650379851460457\n",
      "Surface training t=10476, loss=0.027034244500100613\n",
      "Surface training t=10477, loss=0.027596281841397285\n",
      "Surface training t=10478, loss=0.03154455218464136\n",
      "Surface training t=10479, loss=0.04870262183248997\n",
      "Surface training t=10480, loss=0.03482109121978283\n",
      "Surface training t=10481, loss=0.026511293835937977\n",
      "Surface training t=10482, loss=0.048429735004901886\n",
      "Surface training t=10483, loss=0.03266201540827751\n",
      "Surface training t=10484, loss=0.0327626820653677\n",
      "Surface training t=10485, loss=0.03288419172167778\n",
      "Surface training t=10486, loss=0.02870188932865858\n",
      "Surface training t=10487, loss=0.03912295773625374\n",
      "Surface training t=10488, loss=0.036443707533180714\n",
      "Surface training t=10489, loss=0.04158764239400625\n",
      "Surface training t=10490, loss=0.04852004162967205\n",
      "Surface training t=10491, loss=0.04033356253057718\n",
      "Surface training t=10492, loss=0.06501326896250248\n",
      "Surface training t=10493, loss=0.07285935804247856\n",
      "Surface training t=10494, loss=0.06226470321416855\n",
      "Surface training t=10495, loss=0.05265854485332966\n",
      "Surface training t=10496, loss=0.08212151005864143\n",
      "Surface training t=10497, loss=0.061180323362350464\n",
      "Surface training t=10498, loss=0.0840926393866539\n",
      "Surface training t=10499, loss=0.05771430395543575\n",
      "Surface training t=10500, loss=0.030833685770630836\n",
      "Surface training t=10501, loss=0.031037844717502594\n",
      "Surface training t=10502, loss=0.028105645440518856\n",
      "Surface training t=10503, loss=0.029551485553383827\n",
      "Surface training t=10504, loss=0.028896297328174114\n",
      "Surface training t=10505, loss=0.0222398042678833\n",
      "Surface training t=10506, loss=0.028602697886526585\n",
      "Surface training t=10507, loss=0.024248333647847176\n",
      "Surface training t=10508, loss=0.03370511904358864\n",
      "Surface training t=10509, loss=0.03950043395161629\n",
      "Surface training t=10510, loss=0.036582913249731064\n",
      "Surface training t=10511, loss=0.04119616933166981\n",
      "Surface training t=10512, loss=0.045269036665558815\n",
      "Surface training t=10513, loss=0.046139344573020935\n",
      "Surface training t=10514, loss=0.03537360206246376\n",
      "Surface training t=10515, loss=0.03275374881923199\n",
      "Surface training t=10516, loss=0.030099579133093357\n",
      "Surface training t=10517, loss=0.029888578690588474\n",
      "Surface training t=10518, loss=0.02668571937829256\n",
      "Surface training t=10519, loss=0.02903863787651062\n",
      "Surface training t=10520, loss=0.027890988625586033\n",
      "Surface training t=10521, loss=0.029017831198871136\n",
      "Surface training t=10522, loss=0.039518747478723526\n",
      "Surface training t=10523, loss=0.031044079922139645\n",
      "Surface training t=10524, loss=0.03953508287668228\n",
      "Surface training t=10525, loss=0.03783324547111988\n",
      "Surface training t=10526, loss=0.03468124195933342\n",
      "Surface training t=10527, loss=0.06134353205561638\n",
      "Surface training t=10528, loss=0.04366491362452507\n",
      "Surface training t=10529, loss=0.029430123046040535\n",
      "Surface training t=10530, loss=0.04767957702279091\n",
      "Surface training t=10531, loss=0.03571981191635132\n",
      "Surface training t=10532, loss=0.028589338064193726\n",
      "Surface training t=10533, loss=0.03470889013260603\n",
      "Surface training t=10534, loss=0.03884916566312313\n",
      "Surface training t=10535, loss=0.06203284673392773\n",
      "Surface training t=10536, loss=0.04863422177731991\n",
      "Surface training t=10537, loss=0.04309943504631519\n",
      "Surface training t=10538, loss=0.04191130958497524\n",
      "Surface training t=10539, loss=0.04331866465508938\n",
      "Surface training t=10540, loss=0.03861136455088854\n",
      "Surface training t=10541, loss=0.0422394834458828\n",
      "Surface training t=10542, loss=0.05153198540210724\n",
      "Surface training t=10543, loss=0.04022700898349285\n",
      "Surface training t=10544, loss=0.05552507936954498\n",
      "Surface training t=10545, loss=0.035093882121145725\n",
      "Surface training t=10546, loss=0.04196321591734886\n",
      "Surface training t=10547, loss=0.03167292661964893\n",
      "Surface training t=10548, loss=0.02422194927930832\n",
      "Surface training t=10549, loss=0.026987510733306408\n",
      "Surface training t=10550, loss=0.034305253997445107\n",
      "Surface training t=10551, loss=0.025505672208964825\n",
      "Surface training t=10552, loss=0.029612621292471886\n",
      "Surface training t=10553, loss=0.023912029340863228\n",
      "Surface training t=10554, loss=0.026658199727535248\n",
      "Surface training t=10555, loss=0.02837461233139038\n",
      "Surface training t=10556, loss=0.02411506511271\n",
      "Surface training t=10557, loss=0.03186775930225849\n",
      "Surface training t=10558, loss=0.02976404409855604\n",
      "Surface training t=10559, loss=0.032113151624798775\n",
      "Surface training t=10560, loss=0.03493677731603384\n",
      "Surface training t=10561, loss=0.020406474359333515\n",
      "Surface training t=10562, loss=0.019114219583570957\n",
      "Surface training t=10563, loss=0.027700438164174557\n",
      "Surface training t=10564, loss=0.030615882948040962\n",
      "Surface training t=10565, loss=0.04279706999659538\n",
      "Surface training t=10566, loss=0.05011170916259289\n",
      "Surface training t=10567, loss=0.04515198152512312\n",
      "Surface training t=10568, loss=0.05194425769150257\n",
      "Surface training t=10569, loss=0.061072420328855515\n",
      "Surface training t=10570, loss=0.0530824288725853\n",
      "Surface training t=10571, loss=0.05412288382649422\n",
      "Surface training t=10572, loss=0.058149661868810654\n",
      "Surface training t=10573, loss=0.07063490524888039\n",
      "Surface training t=10574, loss=0.054543815553188324\n",
      "Surface training t=10575, loss=0.054346151649951935\n",
      "Surface training t=10576, loss=0.07032732293009758\n",
      "Surface training t=10577, loss=0.0407723356038332\n",
      "Surface training t=10578, loss=0.04887453652918339\n",
      "Surface training t=10579, loss=0.03949305787682533\n",
      "Surface training t=10580, loss=0.04183978773653507\n",
      "Surface training t=10581, loss=0.040418267250061035\n",
      "Surface training t=10582, loss=0.03210793901234865\n",
      "Surface training t=10583, loss=0.024333054199814796\n",
      "Surface training t=10584, loss=0.032207902520895004\n",
      "Surface training t=10585, loss=0.04726953059434891\n",
      "Surface training t=10586, loss=0.036697544157505035\n",
      "Surface training t=10587, loss=0.06203989312052727\n",
      "Surface training t=10588, loss=0.02782799955457449\n",
      "Surface training t=10589, loss=0.021162117831408978\n",
      "Surface training t=10590, loss=0.02982545830309391\n",
      "Surface training t=10591, loss=0.02090198453515768\n",
      "Surface training t=10592, loss=0.025443321093916893\n",
      "Surface training t=10593, loss=0.03034942038357258\n",
      "Surface training t=10594, loss=0.035134030506014824\n",
      "Surface training t=10595, loss=0.029423994943499565\n",
      "Surface training t=10596, loss=0.03225875925272703\n",
      "Surface training t=10597, loss=0.030065398663282394\n",
      "Surface training t=10598, loss=0.028548619709908962\n",
      "Surface training t=10599, loss=0.0331017579883337\n",
      "Surface training t=10600, loss=0.032094644382596016\n",
      "Surface training t=10601, loss=0.02923988178372383\n",
      "Surface training t=10602, loss=0.02697175182402134\n",
      "Surface training t=10603, loss=0.02375225070863962\n",
      "Surface training t=10604, loss=0.03451364766806364\n",
      "Surface training t=10605, loss=0.05060939863324165\n",
      "Surface training t=10606, loss=0.03196030855178833\n",
      "Surface training t=10607, loss=0.04487640783190727\n",
      "Surface training t=10608, loss=0.04483645595610142\n",
      "Surface training t=10609, loss=0.03913031332194805\n",
      "Surface training t=10610, loss=0.04011109285056591\n",
      "Surface training t=10611, loss=0.02879066951572895\n",
      "Surface training t=10612, loss=0.03351656720042229\n",
      "Surface training t=10613, loss=0.04685969464480877\n",
      "Surface training t=10614, loss=0.044928304851055145\n",
      "Surface training t=10615, loss=0.055711107328534126\n",
      "Surface training t=10616, loss=0.03667363151907921\n",
      "Surface training t=10617, loss=0.04710390791296959\n",
      "Surface training t=10618, loss=0.044050028547644615\n",
      "Surface training t=10619, loss=0.043288715183734894\n",
      "Surface training t=10620, loss=0.049854788929224014\n",
      "Surface training t=10621, loss=0.036180827766656876\n",
      "Surface training t=10622, loss=0.05145513452589512\n",
      "Surface training t=10623, loss=0.03700250107795\n",
      "Surface training t=10624, loss=0.02766193449497223\n",
      "Surface training t=10625, loss=0.05324385315179825\n",
      "Surface training t=10626, loss=0.035786401480436325\n",
      "Surface training t=10627, loss=0.04900624044239521\n",
      "Surface training t=10628, loss=0.047600459307432175\n",
      "Surface training t=10629, loss=0.04292517155408859\n",
      "Surface training t=10630, loss=0.0377275999635458\n",
      "Surface training t=10631, loss=0.02541211061179638\n",
      "Surface training t=10632, loss=0.027124525979161263\n",
      "Surface training t=10633, loss=0.03742269426584244\n",
      "Surface training t=10634, loss=0.0348898945376277\n",
      "Surface training t=10635, loss=0.02923096902668476\n",
      "Surface training t=10636, loss=0.02872171625494957\n",
      "Surface training t=10637, loss=0.035342889837920666\n",
      "Surface training t=10638, loss=0.03296511620283127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=10639, loss=0.03135202545672655\n",
      "Surface training t=10640, loss=0.0339801786467433\n",
      "Surface training t=10641, loss=0.04048769734799862\n",
      "Surface training t=10642, loss=0.029940308071672916\n",
      "Surface training t=10643, loss=0.02939990535378456\n",
      "Surface training t=10644, loss=0.04992597550153732\n",
      "Surface training t=10645, loss=0.03702995181083679\n",
      "Surface training t=10646, loss=0.04777006804943085\n",
      "Surface training t=10647, loss=0.037252962589263916\n",
      "Surface training t=10648, loss=0.03960291109979153\n",
      "Surface training t=10649, loss=0.03490830771625042\n",
      "Surface training t=10650, loss=0.03681675344705582\n",
      "Surface training t=10651, loss=0.030155791901051998\n",
      "Surface training t=10652, loss=0.04036552272737026\n",
      "Surface training t=10653, loss=0.03737049363553524\n",
      "Surface training t=10654, loss=0.034311854280531406\n",
      "Surface training t=10655, loss=0.038804154843091965\n",
      "Surface training t=10656, loss=0.046590808779001236\n",
      "Surface training t=10657, loss=0.035832544788718224\n",
      "Surface training t=10658, loss=0.044175734743475914\n",
      "Surface training t=10659, loss=0.031154428608715534\n",
      "Surface training t=10660, loss=0.04288890026509762\n",
      "Surface training t=10661, loss=0.03962626866996288\n",
      "Surface training t=10662, loss=0.029372980818152428\n",
      "Surface training t=10663, loss=0.02968711033463478\n",
      "Surface training t=10664, loss=0.024450271390378475\n",
      "Surface training t=10665, loss=0.032492514699697495\n",
      "Surface training t=10666, loss=0.02317952085286379\n",
      "Surface training t=10667, loss=0.03469776175916195\n",
      "Surface training t=10668, loss=0.03171565476804972\n",
      "Surface training t=10669, loss=0.031125047244131565\n",
      "Surface training t=10670, loss=0.051489636301994324\n",
      "Surface training t=10671, loss=0.042039357125759125\n",
      "Surface training t=10672, loss=0.04897236451506615\n",
      "Surface training t=10673, loss=0.04250265099108219\n",
      "Surface training t=10674, loss=0.03919205069541931\n",
      "Surface training t=10675, loss=0.03527502715587616\n",
      "Surface training t=10676, loss=0.03393911197781563\n",
      "Surface training t=10677, loss=0.033614382147789\n",
      "Surface training t=10678, loss=0.021367358043789864\n",
      "Surface training t=10679, loss=0.02760409377515316\n",
      "Surface training t=10680, loss=0.044204797595739365\n",
      "Surface training t=10681, loss=0.03074798360466957\n",
      "Surface training t=10682, loss=0.040142931044101715\n",
      "Surface training t=10683, loss=0.04531336948275566\n",
      "Surface training t=10684, loss=0.053206631913781166\n",
      "Surface training t=10685, loss=0.03316459618508816\n",
      "Surface training t=10686, loss=0.0352976992726326\n",
      "Surface training t=10687, loss=0.040114281699061394\n",
      "Surface training t=10688, loss=0.04951820336282253\n",
      "Surface training t=10689, loss=0.02869750652462244\n",
      "Surface training t=10690, loss=0.035994757898151875\n",
      "Surface training t=10691, loss=0.045843906700611115\n",
      "Surface training t=10692, loss=0.038587117567658424\n",
      "Surface training t=10693, loss=0.04163921996951103\n",
      "Surface training t=10694, loss=0.039264015853405\n",
      "Surface training t=10695, loss=0.02660965546965599\n",
      "Surface training t=10696, loss=0.038325498811900616\n",
      "Surface training t=10697, loss=0.03612651117146015\n",
      "Surface training t=10698, loss=0.023622612468898296\n",
      "Surface training t=10699, loss=0.031218654476106167\n",
      "Surface training t=10700, loss=0.02398365270346403\n",
      "Surface training t=10701, loss=0.031536039896309376\n",
      "Surface training t=10702, loss=0.025497011840343475\n",
      "Surface training t=10703, loss=0.03532562591135502\n",
      "Surface training t=10704, loss=0.05332927033305168\n",
      "Surface training t=10705, loss=0.05114104226231575\n",
      "Surface training t=10706, loss=0.042087944224476814\n",
      "Surface training t=10707, loss=0.040262531489133835\n",
      "Surface training t=10708, loss=0.032077585346996784\n",
      "Surface training t=10709, loss=0.03784048929810524\n",
      "Surface training t=10710, loss=0.04256594181060791\n",
      "Surface training t=10711, loss=0.036507404409348965\n",
      "Surface training t=10712, loss=0.07293251901865005\n",
      "Surface training t=10713, loss=0.05113727040588856\n",
      "Surface training t=10714, loss=0.053180805407464504\n",
      "Surface training t=10715, loss=0.06453780643641949\n",
      "Surface training t=10716, loss=0.07993806898593903\n",
      "Surface training t=10717, loss=0.048759251832962036\n",
      "Surface training t=10718, loss=0.06291973032057285\n",
      "Surface training t=10719, loss=0.084276232868433\n",
      "Surface training t=10720, loss=0.06161842495203018\n",
      "Surface training t=10721, loss=0.06437389925122261\n",
      "Surface training t=10722, loss=0.06360597722232342\n",
      "Surface training t=10723, loss=0.057475678622722626\n",
      "Surface training t=10724, loss=0.0455114021897316\n",
      "Surface training t=10725, loss=0.04760575294494629\n",
      "Surface training t=10726, loss=0.04278850741684437\n",
      "Surface training t=10727, loss=0.05603527091443539\n",
      "Surface training t=10728, loss=0.06108280085027218\n",
      "Surface training t=10729, loss=0.0419567059725523\n",
      "Surface training t=10730, loss=0.048026200383901596\n",
      "Surface training t=10731, loss=0.042280176654458046\n",
      "Surface training t=10732, loss=0.04500989802181721\n",
      "Surface training t=10733, loss=0.0469709113240242\n",
      "Surface training t=10734, loss=0.05965266562998295\n",
      "Surface training t=10735, loss=0.047141071408987045\n",
      "Surface training t=10736, loss=0.040896112099289894\n",
      "Surface training t=10737, loss=0.035519497469067574\n",
      "Surface training t=10738, loss=0.027173236943781376\n",
      "Surface training t=10739, loss=0.04328189417719841\n",
      "Surface training t=10740, loss=0.02451085578650236\n",
      "Surface training t=10741, loss=0.024618791416287422\n",
      "Surface training t=10742, loss=0.02720135822892189\n",
      "Surface training t=10743, loss=0.03090658411383629\n",
      "Surface training t=10744, loss=0.029027962125837803\n",
      "Surface training t=10745, loss=0.03730078227818012\n",
      "Surface training t=10746, loss=0.03638355992734432\n",
      "Surface training t=10747, loss=0.046225761994719505\n",
      "Surface training t=10748, loss=0.05782622471451759\n",
      "Surface training t=10749, loss=0.047923644073307514\n",
      "Surface training t=10750, loss=0.045833111740648746\n",
      "Surface training t=10751, loss=0.04338754154741764\n",
      "Surface training t=10752, loss=0.044852644205093384\n",
      "Surface training t=10753, loss=0.0479472316801548\n",
      "Surface training t=10754, loss=0.05065562576055527\n",
      "Surface training t=10755, loss=0.05485492944717407\n",
      "Surface training t=10756, loss=0.04480411112308502\n",
      "Surface training t=10757, loss=0.06136839650571346\n",
      "Surface training t=10758, loss=0.04196491651237011\n",
      "Surface training t=10759, loss=0.05432366952300072\n",
      "Surface training t=10760, loss=0.03804968576878309\n",
      "Surface training t=10761, loss=0.04943566210567951\n",
      "Surface training t=10762, loss=0.041760873049497604\n",
      "Surface training t=10763, loss=0.03106127493083477\n",
      "Surface training t=10764, loss=0.0354672372341156\n",
      "Surface training t=10765, loss=0.032546939328312874\n",
      "Surface training t=10766, loss=0.030781744979321957\n",
      "Surface training t=10767, loss=0.025138622149825096\n",
      "Surface training t=10768, loss=0.02497603092342615\n",
      "Surface training t=10769, loss=0.028889302164316177\n",
      "Surface training t=10770, loss=0.03411855176091194\n",
      "Surface training t=10771, loss=0.03157641086727381\n",
      "Surface training t=10772, loss=0.02633505128324032\n",
      "Surface training t=10773, loss=0.029445679858326912\n",
      "Surface training t=10774, loss=0.04394670017063618\n",
      "Surface training t=10775, loss=0.056510915979743004\n",
      "Surface training t=10776, loss=0.04696076922118664\n",
      "Surface training t=10777, loss=0.041503679007291794\n",
      "Surface training t=10778, loss=0.03528369218111038\n",
      "Surface training t=10779, loss=0.03949617221951485\n",
      "Surface training t=10780, loss=0.04284738749265671\n",
      "Surface training t=10781, loss=0.061343805864453316\n",
      "Surface training t=10782, loss=0.04981871880590916\n",
      "Surface training t=10783, loss=0.037470946088433266\n",
      "Surface training t=10784, loss=0.03538359887897968\n",
      "Surface training t=10785, loss=0.036222491413354874\n",
      "Surface training t=10786, loss=0.041755372658371925\n",
      "Surface training t=10787, loss=0.04418297205120325\n",
      "Surface training t=10788, loss=0.049460720270872116\n",
      "Surface training t=10789, loss=0.04273960366845131\n",
      "Surface training t=10790, loss=0.058095017448067665\n",
      "Surface training t=10791, loss=0.052130354568362236\n",
      "Surface training t=10792, loss=0.042904166504740715\n",
      "Surface training t=10793, loss=0.052305297926068306\n",
      "Surface training t=10794, loss=0.040412004105746746\n",
      "Surface training t=10795, loss=0.03474356606602669\n",
      "Surface training t=10796, loss=0.030194020830094814\n",
      "Surface training t=10797, loss=0.025345097295939922\n",
      "Surface training t=10798, loss=0.026510796509683132\n",
      "Surface training t=10799, loss=0.031115809455513954\n",
      "Surface training t=10800, loss=0.028955568559467793\n",
      "Surface training t=10801, loss=0.02691280096769333\n",
      "Surface training t=10802, loss=0.020632791332900524\n",
      "Surface training t=10803, loss=0.02119307406246662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=10804, loss=0.02182989288121462\n",
      "Surface training t=10805, loss=0.02125230897217989\n",
      "Surface training t=10806, loss=0.02671710727736354\n",
      "Surface training t=10807, loss=0.026030404958873987\n",
      "Surface training t=10808, loss=0.030275175347924232\n",
      "Surface training t=10809, loss=0.03423696104437113\n",
      "Surface training t=10810, loss=0.031982299871742725\n",
      "Surface training t=10811, loss=0.027277909219264984\n",
      "Surface training t=10812, loss=0.024952255189418793\n",
      "Surface training t=10813, loss=0.028680535964667797\n",
      "Surface training t=10814, loss=0.018084790091961622\n",
      "Surface training t=10815, loss=0.020466486923396587\n",
      "Surface training t=10816, loss=0.022014047019183636\n",
      "Surface training t=10817, loss=0.016064967960119247\n",
      "Surface training t=10818, loss=0.01880833599716425\n",
      "Surface training t=10819, loss=0.027507459744811058\n",
      "Surface training t=10820, loss=0.02903564739972353\n",
      "Surface training t=10821, loss=0.018083248287439346\n",
      "Surface training t=10822, loss=0.02270052209496498\n",
      "Surface training t=10823, loss=0.026085812598466873\n",
      "Surface training t=10824, loss=0.026145382784307003\n",
      "Surface training t=10825, loss=0.03259713388979435\n",
      "Surface training t=10826, loss=0.034948620945215225\n",
      "Surface training t=10827, loss=0.0316650215536356\n",
      "Surface training t=10828, loss=0.03249985817819834\n",
      "Surface training t=10829, loss=0.035868313163518906\n",
      "Surface training t=10830, loss=0.025112698785960674\n",
      "Surface training t=10831, loss=0.03109772317111492\n",
      "Surface training t=10832, loss=0.02637647185474634\n",
      "Surface training t=10833, loss=0.03385895676910877\n",
      "Surface training t=10834, loss=0.028794080018997192\n",
      "Surface training t=10835, loss=0.028488454408943653\n",
      "Surface training t=10836, loss=0.02589571475982666\n",
      "Surface training t=10837, loss=0.02468270342797041\n",
      "Surface training t=10838, loss=0.02362192887812853\n",
      "Surface training t=10839, loss=0.024331655353307724\n",
      "Surface training t=10840, loss=0.02050400897860527\n",
      "Surface training t=10841, loss=0.02247653156518936\n",
      "Surface training t=10842, loss=0.024143471382558346\n",
      "Surface training t=10843, loss=0.019671843387186527\n",
      "Surface training t=10844, loss=0.026409746147692204\n",
      "Surface training t=10845, loss=0.018729599192738533\n",
      "Surface training t=10846, loss=0.02793724089860916\n",
      "Surface training t=10847, loss=0.015902189537882805\n",
      "Surface training t=10848, loss=0.03149175550788641\n",
      "Surface training t=10849, loss=0.03419112414121628\n",
      "Surface training t=10850, loss=0.07493579015135765\n",
      "Surface training t=10851, loss=0.03778894431889057\n",
      "Surface training t=10852, loss=0.04792250506579876\n",
      "Surface training t=10853, loss=0.040699233300983906\n",
      "Surface training t=10854, loss=0.034523578360676765\n",
      "Surface training t=10855, loss=0.03580716345459223\n",
      "Surface training t=10856, loss=0.031964211724698544\n",
      "Surface training t=10857, loss=0.03588065318763256\n",
      "Surface training t=10858, loss=0.04688720218837261\n",
      "Surface training t=10859, loss=0.04282287694513798\n",
      "Surface training t=10860, loss=0.034446234814822674\n",
      "Surface training t=10861, loss=0.03644188493490219\n",
      "Surface training t=10862, loss=0.04461202211678028\n",
      "Surface training t=10863, loss=0.049566082656383514\n",
      "Surface training t=10864, loss=0.06684493832290173\n",
      "Surface training t=10865, loss=0.053939394652843475\n",
      "Surface training t=10866, loss=0.03508090414106846\n",
      "Surface training t=10867, loss=0.03748917952179909\n",
      "Surface training t=10868, loss=0.03492738865315914\n",
      "Surface training t=10869, loss=0.030033606104552746\n",
      "Surface training t=10870, loss=0.023228895850479603\n",
      "Surface training t=10871, loss=0.03226271644234657\n",
      "Surface training t=10872, loss=0.02377146575599909\n",
      "Surface training t=10873, loss=0.022556801326572895\n",
      "Surface training t=10874, loss=0.019782712683081627\n",
      "Surface training t=10875, loss=0.030087538994848728\n",
      "Surface training t=10876, loss=0.03469127602875233\n",
      "Surface training t=10877, loss=0.027045883238315582\n",
      "Surface training t=10878, loss=0.022555246949195862\n",
      "Surface training t=10879, loss=0.02666017133742571\n",
      "Surface training t=10880, loss=0.02448971103876829\n",
      "Surface training t=10881, loss=0.02864906471222639\n",
      "Surface training t=10882, loss=0.03340204246342182\n",
      "Surface training t=10883, loss=0.032036153599619865\n",
      "Surface training t=10884, loss=0.04099954478442669\n",
      "Surface training t=10885, loss=0.038793450221419334\n",
      "Surface training t=10886, loss=0.03989509120583534\n",
      "Surface training t=10887, loss=0.03232881147414446\n",
      "Surface training t=10888, loss=0.04101690463721752\n",
      "Surface training t=10889, loss=0.04260420799255371\n",
      "Surface training t=10890, loss=0.04092409461736679\n",
      "Surface training t=10891, loss=0.04725709743797779\n",
      "Surface training t=10892, loss=0.05788172595202923\n",
      "Surface training t=10893, loss=0.06484103202819824\n",
      "Surface training t=10894, loss=0.05355794541537762\n",
      "Surface training t=10895, loss=0.06289570964872837\n",
      "Surface training t=10896, loss=0.04495074599981308\n",
      "Surface training t=10897, loss=0.05400536023080349\n",
      "Surface training t=10898, loss=0.05126476287841797\n",
      "Surface training t=10899, loss=0.03441243339329958\n",
      "Surface training t=10900, loss=0.0386420302093029\n",
      "Surface training t=10901, loss=0.047665221616625786\n",
      "Surface training t=10902, loss=0.04567788075655699\n",
      "Surface training t=10903, loss=0.051114872097969055\n",
      "Surface training t=10904, loss=0.03995370864868164\n",
      "Surface training t=10905, loss=0.04316811449825764\n",
      "Surface training t=10906, loss=0.03627379983663559\n",
      "Surface training t=10907, loss=0.046600041911005974\n",
      "Surface training t=10908, loss=0.029196422547101974\n",
      "Surface training t=10909, loss=0.036748625338077545\n",
      "Surface training t=10910, loss=0.030622358433902264\n",
      "Surface training t=10911, loss=0.040905365720391273\n",
      "Surface training t=10912, loss=0.04913830757141113\n",
      "Surface training t=10913, loss=0.03931283298879862\n",
      "Surface training t=10914, loss=0.05385892279446125\n",
      "Surface training t=10915, loss=0.04383918642997742\n",
      "Surface training t=10916, loss=0.03481395076960325\n",
      "Surface training t=10917, loss=0.035897212103009224\n",
      "Surface training t=10918, loss=0.03720603883266449\n",
      "Surface training t=10919, loss=0.03117867186665535\n",
      "Surface training t=10920, loss=0.02126583270728588\n",
      "Surface training t=10921, loss=0.02060992456972599\n",
      "Surface training t=10922, loss=0.026676014065742493\n",
      "Surface training t=10923, loss=0.03201455157250166\n",
      "Surface training t=10924, loss=0.03592053893953562\n",
      "Surface training t=10925, loss=0.03377673402428627\n",
      "Surface training t=10926, loss=0.041387793608009815\n",
      "Surface training t=10927, loss=0.045444389805197716\n",
      "Surface training t=10928, loss=0.048120640218257904\n",
      "Surface training t=10929, loss=0.0388901149854064\n",
      "Surface training t=10930, loss=0.03290302585810423\n",
      "Surface training t=10931, loss=0.06329094246029854\n",
      "Surface training t=10932, loss=0.05164899490773678\n",
      "Surface training t=10933, loss=0.0524128582328558\n",
      "Surface training t=10934, loss=0.05253366008400917\n",
      "Surface training t=10935, loss=0.11407863348722458\n",
      "Surface training t=10936, loss=0.059735147282481194\n",
      "Surface training t=10937, loss=0.06119479238986969\n",
      "Surface training t=10938, loss=0.08852678537368774\n",
      "Surface training t=10939, loss=0.056577712297439575\n",
      "Surface training t=10940, loss=0.054726630449295044\n",
      "Surface training t=10941, loss=0.05338568612933159\n",
      "Surface training t=10942, loss=0.05537521652877331\n",
      "Surface training t=10943, loss=0.04852870851755142\n",
      "Surface training t=10944, loss=0.05174817517399788\n",
      "Surface training t=10945, loss=0.05395638011395931\n",
      "Surface training t=10946, loss=0.0720602534711361\n",
      "Surface training t=10947, loss=0.05013006366789341\n",
      "Surface training t=10948, loss=0.06184978596866131\n",
      "Surface training t=10949, loss=0.056979063898324966\n",
      "Surface training t=10950, loss=0.05229860544204712\n",
      "Surface training t=10951, loss=0.044095929712057114\n",
      "Surface training t=10952, loss=0.04782000742852688\n",
      "Surface training t=10953, loss=0.03354764170944691\n",
      "Surface training t=10954, loss=0.06923217698931694\n",
      "Surface training t=10955, loss=0.04503660276532173\n",
      "Surface training t=10956, loss=0.03664322383701801\n",
      "Surface training t=10957, loss=0.0437039639800787\n",
      "Surface training t=10958, loss=0.04055158607661724\n",
      "Surface training t=10959, loss=0.03954434208571911\n",
      "Surface training t=10960, loss=0.03668890707194805\n",
      "Surface training t=10961, loss=0.035437594167888165\n",
      "Surface training t=10962, loss=0.035550051368772984\n",
      "Surface training t=10963, loss=0.03373430483043194\n",
      "Surface training t=10964, loss=0.03351916652172804\n",
      "Surface training t=10965, loss=0.029954131692647934\n",
      "Surface training t=10966, loss=0.038140833377838135\n",
      "Surface training t=10967, loss=0.04954799264669418\n",
      "Surface training t=10968, loss=0.04772351123392582\n",
      "Surface training t=10969, loss=0.04477420449256897\n",
      "Surface training t=10970, loss=0.059024104848504066\n",
      "Surface training t=10971, loss=0.043410396203398705\n",
      "Surface training t=10972, loss=0.04652927629649639\n",
      "Surface training t=10973, loss=0.04433487728238106\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=10974, loss=0.05945497378706932\n",
      "Surface training t=10975, loss=0.05200235918164253\n",
      "Surface training t=10976, loss=0.05681310594081879\n",
      "Surface training t=10977, loss=0.052025919780135155\n",
      "Surface training t=10978, loss=0.04708333872258663\n",
      "Surface training t=10979, loss=0.04061117768287659\n",
      "Surface training t=10980, loss=0.03346690535545349\n",
      "Surface training t=10981, loss=0.04556695744395256\n",
      "Surface training t=10982, loss=0.035431066527962685\n",
      "Surface training t=10983, loss=0.033076317980885506\n",
      "Surface training t=10984, loss=0.04061114601790905\n",
      "Surface training t=10985, loss=0.037828246131539345\n",
      "Surface training t=10986, loss=0.027516289614140987\n",
      "Surface training t=10987, loss=0.03378842584788799\n",
      "Surface training t=10988, loss=0.0381292849779129\n",
      "Surface training t=10989, loss=0.03625165857374668\n",
      "Surface training t=10990, loss=0.0380301745608449\n",
      "Surface training t=10991, loss=0.030732966028153896\n",
      "Surface training t=10992, loss=0.02800859697163105\n",
      "Surface training t=10993, loss=0.020035672932863235\n",
      "Surface training t=10994, loss=0.02927867043763399\n",
      "Surface training t=10995, loss=0.030204166658222675\n",
      "Surface training t=10996, loss=0.03508669603615999\n",
      "Surface training t=10997, loss=0.03449722658842802\n",
      "Surface training t=10998, loss=0.0304669551551342\n",
      "Surface training t=10999, loss=0.027322061359882355\n",
      "Surface training t=11000, loss=0.027827569283545017\n",
      "Surface training t=11001, loss=0.021743142046034336\n",
      "Surface training t=11002, loss=0.03999102767556906\n",
      "Surface training t=11003, loss=0.039857182651758194\n",
      "Surface training t=11004, loss=0.0478806234896183\n",
      "Surface training t=11005, loss=0.04306376725435257\n",
      "Surface training t=11006, loss=0.03313049953430891\n",
      "Surface training t=11007, loss=0.04791368544101715\n",
      "Surface training t=11008, loss=0.058404870331287384\n",
      "Surface training t=11009, loss=0.04683832451701164\n",
      "Surface training t=11010, loss=0.03792624641209841\n",
      "Surface training t=11011, loss=0.046983420848846436\n",
      "Surface training t=11012, loss=0.051636140793561935\n",
      "Surface training t=11013, loss=0.06530560180544853\n",
      "Surface training t=11014, loss=0.04459776729345322\n",
      "Surface training t=11015, loss=0.04017213173210621\n",
      "Surface training t=11016, loss=0.03432430699467659\n",
      "Surface training t=11017, loss=0.02797113172709942\n",
      "Surface training t=11018, loss=0.02991264872252941\n",
      "Surface training t=11019, loss=0.02870272286236286\n",
      "Surface training t=11020, loss=0.03230737894773483\n",
      "Surface training t=11021, loss=0.027703316882252693\n",
      "Surface training t=11022, loss=0.026403306052088737\n",
      "Surface training t=11023, loss=0.02408428303897381\n",
      "Surface training t=11024, loss=0.03139473218470812\n",
      "Surface training t=11025, loss=0.023748946376144886\n",
      "Surface training t=11026, loss=0.03463666792958975\n",
      "Surface training t=11027, loss=0.030866118147969246\n",
      "Surface training t=11028, loss=0.02541498839855194\n",
      "Surface training t=11029, loss=0.03161970805376768\n",
      "Surface training t=11030, loss=0.031546845100820065\n",
      "Surface training t=11031, loss=0.025490702129900455\n",
      "Surface training t=11032, loss=0.030150829814374447\n",
      "Surface training t=11033, loss=0.029916470870375633\n",
      "Surface training t=11034, loss=0.026311070658266544\n",
      "Surface training t=11035, loss=0.023949009366333485\n",
      "Surface training t=11036, loss=0.0292681148275733\n",
      "Surface training t=11037, loss=0.023838608525693417\n",
      "Surface training t=11038, loss=0.03380574844777584\n",
      "Surface training t=11039, loss=0.03385463822633028\n",
      "Surface training t=11040, loss=0.03379620984196663\n",
      "Surface training t=11041, loss=0.04074391722679138\n",
      "Surface training t=11042, loss=0.04474029690027237\n",
      "Surface training t=11043, loss=0.04595031030476093\n",
      "Surface training t=11044, loss=0.0551317073404789\n",
      "Surface training t=11045, loss=0.045329103246331215\n",
      "Surface training t=11046, loss=0.049969074316322803\n",
      "Surface training t=11047, loss=0.047268821857869625\n",
      "Surface training t=11048, loss=0.03787730447947979\n",
      "Surface training t=11049, loss=0.035931944847106934\n",
      "Surface training t=11050, loss=0.03282209113240242\n",
      "Surface training t=11051, loss=0.03450717218220234\n",
      "Surface training t=11052, loss=0.030459990724921227\n",
      "Surface training t=11053, loss=0.030009448528289795\n",
      "Surface training t=11054, loss=0.020903666503727436\n",
      "Surface training t=11055, loss=0.022521440871059895\n",
      "Surface training t=11056, loss=0.0255051264539361\n",
      "Surface training t=11057, loss=0.024085654877126217\n",
      "Surface training t=11058, loss=0.028030390851199627\n",
      "Surface training t=11059, loss=0.03166860342025757\n",
      "Surface training t=11060, loss=0.03955323249101639\n",
      "Surface training t=11061, loss=0.03762551583349705\n",
      "Surface training t=11062, loss=0.02239538636058569\n",
      "Surface training t=11063, loss=0.025066646747291088\n",
      "Surface training t=11064, loss=0.031870643608272076\n",
      "Surface training t=11065, loss=0.03884461522102356\n",
      "Surface training t=11066, loss=0.029834166169166565\n",
      "Surface training t=11067, loss=0.04401626996695995\n",
      "Surface training t=11068, loss=0.038002196699380875\n",
      "Surface training t=11069, loss=0.03849751316010952\n",
      "Surface training t=11070, loss=0.04859440587460995\n",
      "Surface training t=11071, loss=0.04518369771540165\n",
      "Surface training t=11072, loss=0.041935596615076065\n",
      "Surface training t=11073, loss=0.04368293005973101\n",
      "Surface training t=11074, loss=0.03528903238475323\n",
      "Surface training t=11075, loss=0.04118398390710354\n",
      "Surface training t=11076, loss=0.03841305524110794\n",
      "Surface training t=11077, loss=0.03499338775873184\n",
      "Surface training t=11078, loss=0.028427118435502052\n",
      "Surface training t=11079, loss=0.030446866527199745\n",
      "Surface training t=11080, loss=0.02677764929831028\n",
      "Surface training t=11081, loss=0.034860304556787014\n",
      "Surface training t=11082, loss=0.030588660389184952\n",
      "Surface training t=11083, loss=0.03109389916062355\n",
      "Surface training t=11084, loss=0.03279468044638634\n",
      "Surface training t=11085, loss=0.02523223776370287\n",
      "Surface training t=11086, loss=0.03725864551961422\n",
      "Surface training t=11087, loss=0.055247919633984566\n",
      "Surface training t=11088, loss=0.0590790007263422\n",
      "Surface training t=11089, loss=0.07844819873571396\n",
      "Surface training t=11090, loss=0.050701867789030075\n",
      "Surface training t=11091, loss=0.09167860820889473\n",
      "Surface training t=11092, loss=0.07658710703253746\n",
      "Surface training t=11093, loss=0.0604693628847599\n",
      "Surface training t=11094, loss=0.07749302685260773\n",
      "Surface training t=11095, loss=0.07521400414407253\n",
      "Surface training t=11096, loss=0.05018512159585953\n",
      "Surface training t=11097, loss=0.07352230697870255\n",
      "Surface training t=11098, loss=0.04829910025000572\n",
      "Surface training t=11099, loss=0.04891192726790905\n",
      "Surface training t=11100, loss=0.029878496192395687\n",
      "Surface training t=11101, loss=0.029552364721894264\n",
      "Surface training t=11102, loss=0.03145590145140886\n",
      "Surface training t=11103, loss=0.025606281124055386\n",
      "Surface training t=11104, loss=0.030120336450636387\n",
      "Surface training t=11105, loss=0.03794011101126671\n",
      "Surface training t=11106, loss=0.03375796042382717\n",
      "Surface training t=11107, loss=0.03619900904595852\n",
      "Surface training t=11108, loss=0.03288686741143465\n",
      "Surface training t=11109, loss=0.03722661919891834\n",
      "Surface training t=11110, loss=0.03871374577283859\n",
      "Surface training t=11111, loss=0.050140850245952606\n",
      "Surface training t=11112, loss=0.041137512773275375\n",
      "Surface training t=11113, loss=0.04453890398144722\n",
      "Surface training t=11114, loss=0.04394189082086086\n",
      "Surface training t=11115, loss=0.0555911622941494\n",
      "Surface training t=11116, loss=0.04471798986196518\n",
      "Surface training t=11117, loss=0.04619688726961613\n",
      "Surface training t=11118, loss=0.03879383020102978\n",
      "Surface training t=11119, loss=0.04097231291234493\n",
      "Surface training t=11120, loss=0.03403526544570923\n",
      "Surface training t=11121, loss=0.03104157280176878\n",
      "Surface training t=11122, loss=0.032861978746950626\n",
      "Surface training t=11123, loss=0.03418836370110512\n",
      "Surface training t=11124, loss=0.03547750972211361\n",
      "Surface training t=11125, loss=0.030378584749996662\n",
      "Surface training t=11126, loss=0.02843862771987915\n",
      "Surface training t=11127, loss=0.03734388668090105\n",
      "Surface training t=11128, loss=0.03732013702392578\n",
      "Surface training t=11129, loss=0.03904424514621496\n",
      "Surface training t=11130, loss=0.04096238687634468\n",
      "Surface training t=11131, loss=0.03802429139614105\n",
      "Surface training t=11132, loss=0.031755231320858\n",
      "Surface training t=11133, loss=0.04243353195488453\n",
      "Surface training t=11134, loss=0.03971514105796814\n",
      "Surface training t=11135, loss=0.04278046265244484\n",
      "Surface training t=11136, loss=0.036417581140995026\n",
      "Surface training t=11137, loss=0.03403211012482643\n",
      "Surface training t=11138, loss=0.043289193883538246\n",
      "Surface training t=11139, loss=0.036160483956336975\n",
      "Surface training t=11140, loss=0.05868368037045002\n",
      "Surface training t=11141, loss=0.05042780190706253\n",
      "Surface training t=11142, loss=0.04256210010498762\n",
      "Surface training t=11143, loss=0.042487820610404015\n",
      "Surface training t=11144, loss=0.054848119616508484\n",
      "Surface training t=11145, loss=0.05271989293396473\n",
      "Surface training t=11146, loss=0.03656122833490372\n",
      "Surface training t=11147, loss=0.04957849346101284\n",
      "Surface training t=11148, loss=0.028857842087745667\n",
      "Surface training t=11149, loss=0.029366870410740376\n",
      "Surface training t=11150, loss=0.03626571036875248\n",
      "Surface training t=11151, loss=0.027188997715711594\n",
      "Surface training t=11152, loss=0.031911347061395645\n",
      "Surface training t=11153, loss=0.023212797939777374\n",
      "Surface training t=11154, loss=0.022446024231612682\n",
      "Surface training t=11155, loss=0.02678001392632723\n",
      "Surface training t=11156, loss=0.03136262111365795\n",
      "Surface training t=11157, loss=0.03237648215144873\n",
      "Surface training t=11158, loss=0.033757999539375305\n",
      "Surface training t=11159, loss=0.03894772753119469\n",
      "Surface training t=11160, loss=0.03476575389504433\n",
      "Surface training t=11161, loss=0.03078055288642645\n",
      "Surface training t=11162, loss=0.03175841644406319\n",
      "Surface training t=11163, loss=0.031074970960617065\n",
      "Surface training t=11164, loss=0.030265034176409245\n",
      "Surface training t=11165, loss=0.02067168615758419\n",
      "Surface training t=11166, loss=0.03480026964098215\n",
      "Surface training t=11167, loss=0.03629057668149471\n",
      "Surface training t=11168, loss=0.03134308662265539\n",
      "Surface training t=11169, loss=0.03488091751933098\n",
      "Surface training t=11170, loss=0.035907335579395294\n",
      "Surface training t=11171, loss=0.03798791207373142\n",
      "Surface training t=11172, loss=0.044814230874180794\n",
      "Surface training t=11173, loss=0.0315037090331316\n",
      "Surface training t=11174, loss=0.02307728584855795\n",
      "Surface training t=11175, loss=0.023803369142115116\n",
      "Surface training t=11176, loss=0.03433940280228853\n",
      "Surface training t=11177, loss=0.029247989878058434\n",
      "Surface training t=11178, loss=0.03356870822608471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=11179, loss=0.027279495261609554\n",
      "Surface training t=11180, loss=0.028030581772327423\n",
      "Surface training t=11181, loss=0.028657400980591774\n",
      "Surface training t=11182, loss=0.029748951084911823\n",
      "Surface training t=11183, loss=0.025320883840322495\n",
      "Surface training t=11184, loss=0.026166697964072227\n",
      "Surface training t=11185, loss=0.0335750300437212\n",
      "Surface training t=11186, loss=0.0441119447350502\n",
      "Surface training t=11187, loss=0.03149466495960951\n",
      "Surface training t=11188, loss=0.025491079315543175\n",
      "Surface training t=11189, loss=0.038921624422073364\n",
      "Surface training t=11190, loss=0.04754873551428318\n",
      "Surface training t=11191, loss=0.0407584011554718\n",
      "Surface training t=11192, loss=0.04826445318758488\n",
      "Surface training t=11193, loss=0.0325331874191761\n",
      "Surface training t=11194, loss=0.039123523980379105\n",
      "Surface training t=11195, loss=0.029299885034561157\n",
      "Surface training t=11196, loss=0.04828532412648201\n",
      "Surface training t=11197, loss=0.058950796723365784\n",
      "Surface training t=11198, loss=0.03801672346889973\n",
      "Surface training t=11199, loss=0.04234074056148529\n",
      "Surface training t=11200, loss=0.04204142652451992\n",
      "Surface training t=11201, loss=0.035676551051437855\n",
      "Surface training t=11202, loss=0.04663011617958546\n",
      "Surface training t=11203, loss=0.03886612318456173\n",
      "Surface training t=11204, loss=0.0365255456417799\n",
      "Surface training t=11205, loss=0.044707924127578735\n",
      "Surface training t=11206, loss=0.03673810698091984\n",
      "Surface training t=11207, loss=0.033444540575146675\n",
      "Surface training t=11208, loss=0.03689511679112911\n",
      "Surface training t=11209, loss=0.05935061722993851\n",
      "Surface training t=11210, loss=0.04146118275821209\n",
      "Surface training t=11211, loss=0.04929587431252003\n",
      "Surface training t=11212, loss=0.05064902454614639\n",
      "Surface training t=11213, loss=0.041516974568367004\n",
      "Surface training t=11214, loss=0.049255991354584694\n",
      "Surface training t=11215, loss=0.043811606243252754\n",
      "Surface training t=11216, loss=0.03947215527296066\n",
      "Surface training t=11217, loss=0.036072537302970886\n",
      "Surface training t=11218, loss=0.03550896421074867\n",
      "Surface training t=11219, loss=0.026143977418541908\n",
      "Surface training t=11220, loss=0.030080311931669712\n",
      "Surface training t=11221, loss=0.02685660868883133\n",
      "Surface training t=11222, loss=0.032281290739774704\n",
      "Surface training t=11223, loss=0.030510805547237396\n",
      "Surface training t=11224, loss=0.02799601759761572\n",
      "Surface training t=11225, loss=0.024477068334817886\n",
      "Surface training t=11226, loss=0.02313843183219433\n",
      "Surface training t=11227, loss=0.03279389999806881\n",
      "Surface training t=11228, loss=0.02918354794383049\n",
      "Surface training t=11229, loss=0.047030363231897354\n",
      "Surface training t=11230, loss=0.06758537329733372\n",
      "Surface training t=11231, loss=0.05969914235174656\n",
      "Surface training t=11232, loss=0.06634508818387985\n",
      "Surface training t=11233, loss=0.11174072325229645\n",
      "Surface training t=11234, loss=0.0631900392472744\n",
      "Surface training t=11235, loss=0.08032135292887688\n",
      "Surface training t=11236, loss=0.05230497941374779\n",
      "Surface training t=11237, loss=0.04635640233755112\n",
      "Surface training t=11238, loss=0.04554085060954094\n",
      "Surface training t=11239, loss=0.04916191101074219\n",
      "Surface training t=11240, loss=0.04801309481263161\n",
      "Surface training t=11241, loss=0.04400135576725006\n",
      "Surface training t=11242, loss=0.048577992245554924\n",
      "Surface training t=11243, loss=0.04149340093135834\n",
      "Surface training t=11244, loss=0.04501849692314863\n",
      "Surface training t=11245, loss=0.06709949672222137\n",
      "Surface training t=11246, loss=0.06781795620918274\n",
      "Surface training t=11247, loss=0.03678948059678078\n",
      "Surface training t=11248, loss=0.07871599495410919\n",
      "Surface training t=11249, loss=0.04363572411239147\n",
      "Surface training t=11250, loss=0.05564182810485363\n",
      "Surface training t=11251, loss=0.044652379117906094\n",
      "Surface training t=11252, loss=0.07443896681070328\n",
      "Surface training t=11253, loss=0.044677263125777245\n",
      "Surface training t=11254, loss=0.054074399173259735\n",
      "Surface training t=11255, loss=0.036486140452325344\n",
      "Surface training t=11256, loss=0.033536831848323345\n",
      "Surface training t=11257, loss=0.04013626091182232\n",
      "Surface training t=11258, loss=0.034396568313241005\n",
      "Surface training t=11259, loss=0.028277949430048466\n",
      "Surface training t=11260, loss=0.0309640783816576\n",
      "Surface training t=11261, loss=0.036947691813111305\n",
      "Surface training t=11262, loss=0.03506591636687517\n",
      "Surface training t=11263, loss=0.036005325615406036\n",
      "Surface training t=11264, loss=0.023972424678504467\n",
      "Surface training t=11265, loss=0.031052427366375923\n",
      "Surface training t=11266, loss=0.03284656535834074\n",
      "Surface training t=11267, loss=0.06404076889157295\n",
      "Surface training t=11268, loss=0.03149169683456421\n",
      "Surface training t=11269, loss=0.026764744892716408\n",
      "Surface training t=11270, loss=0.025411914102733135\n",
      "Surface training t=11271, loss=0.0339587340131402\n",
      "Surface training t=11272, loss=0.03441932890564203\n",
      "Surface training t=11273, loss=0.04661421291530132\n",
      "Surface training t=11274, loss=0.04266108386218548\n",
      "Surface training t=11275, loss=0.04605872184038162\n",
      "Surface training t=11276, loss=0.03664335235953331\n",
      "Surface training t=11277, loss=0.03199778310954571\n",
      "Surface training t=11278, loss=0.04184915870428085\n",
      "Surface training t=11279, loss=0.03439647704362869\n",
      "Surface training t=11280, loss=0.04253818467259407\n",
      "Surface training t=11281, loss=0.03099072352051735\n",
      "Surface training t=11282, loss=0.02704133652150631\n",
      "Surface training t=11283, loss=0.026169774122536182\n",
      "Surface training t=11284, loss=0.032608529552817345\n",
      "Surface training t=11285, loss=0.047314777970314026\n",
      "Surface training t=11286, loss=0.06177929788827896\n",
      "Surface training t=11287, loss=0.04835661128163338\n",
      "Surface training t=11288, loss=0.052771296352148056\n",
      "Surface training t=11289, loss=0.060540974140167236\n",
      "Surface training t=11290, loss=0.0884426049888134\n",
      "Surface training t=11291, loss=0.05560927093029022\n",
      "Surface training t=11292, loss=0.06633590720593929\n",
      "Surface training t=11293, loss=0.08765051886439323\n",
      "Surface training t=11294, loss=0.055047377943992615\n",
      "Surface training t=11295, loss=0.06388953141868114\n",
      "Surface training t=11296, loss=0.06805517338216305\n",
      "Surface training t=11297, loss=0.05683361738920212\n",
      "Surface training t=11298, loss=0.0633210614323616\n",
      "Surface training t=11299, loss=0.09778452664613724\n",
      "Surface training t=11300, loss=0.06065656617283821\n",
      "Surface training t=11301, loss=0.07381154038012028\n",
      "Surface training t=11302, loss=0.060194313526153564\n",
      "Surface training t=11303, loss=0.04707789421081543\n",
      "Surface training t=11304, loss=0.04721560515463352\n",
      "Surface training t=11305, loss=0.041418058797717094\n",
      "Surface training t=11306, loss=0.04853122681379318\n",
      "Surface training t=11307, loss=0.04897921346127987\n",
      "Surface training t=11308, loss=0.047785211354494095\n",
      "Surface training t=11309, loss=0.044893043115735054\n",
      "Surface training t=11310, loss=0.03343877848237753\n",
      "Surface training t=11311, loss=0.04175255447626114\n",
      "Surface training t=11312, loss=0.05877929553389549\n",
      "Surface training t=11313, loss=0.07927097752690315\n",
      "Surface training t=11314, loss=0.06832899898290634\n",
      "Surface training t=11315, loss=0.05741462856531143\n",
      "Surface training t=11316, loss=0.09328053891658783\n",
      "Surface training t=11317, loss=0.05499231815338135\n",
      "Surface training t=11318, loss=0.05576854385435581\n",
      "Surface training t=11319, loss=0.07904863730072975\n",
      "Surface training t=11320, loss=0.056181471794843674\n",
      "Surface training t=11321, loss=0.07548883929848671\n",
      "Surface training t=11322, loss=0.05188814178109169\n",
      "Surface training t=11323, loss=0.05690220184624195\n",
      "Surface training t=11324, loss=0.04888194799423218\n",
      "Surface training t=11325, loss=0.039660584181547165\n",
      "Surface training t=11326, loss=0.05793063901364803\n",
      "Surface training t=11327, loss=0.06256066262722015\n",
      "Surface training t=11328, loss=0.04962901584804058\n",
      "Surface training t=11329, loss=0.06784894689917564\n",
      "Surface training t=11330, loss=0.05308467335999012\n",
      "Surface training t=11331, loss=0.04506018944084644\n",
      "Surface training t=11332, loss=0.048604099079966545\n",
      "Surface training t=11333, loss=0.04660458490252495\n",
      "Surface training t=11334, loss=0.07890717685222626\n",
      "Surface training t=11335, loss=0.05764361657202244\n",
      "Surface training t=11336, loss=0.08261112868785858\n",
      "Surface training t=11337, loss=0.0744934044778347\n",
      "Surface training t=11338, loss=0.04569068364799023\n",
      "Surface training t=11339, loss=0.05813376605510712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=11340, loss=0.06256229057908058\n",
      "Surface training t=11341, loss=0.04694327898323536\n",
      "Surface training t=11342, loss=0.03773123025894165\n",
      "Surface training t=11343, loss=0.04292144253849983\n",
      "Surface training t=11344, loss=0.029991980642080307\n",
      "Surface training t=11345, loss=0.037470631301403046\n",
      "Surface training t=11346, loss=0.06253634765744209\n",
      "Surface training t=11347, loss=0.05321370251476765\n",
      "Surface training t=11348, loss=0.04693162068724632\n",
      "Surface training t=11349, loss=0.05876714177429676\n",
      "Surface training t=11350, loss=0.06851114518940449\n",
      "Surface training t=11351, loss=0.055448682978749275\n",
      "Surface training t=11352, loss=0.04551924951374531\n",
      "Surface training t=11353, loss=0.06917238980531693\n",
      "Surface training t=11354, loss=0.05489119328558445\n",
      "Surface training t=11355, loss=0.048664407804608345\n",
      "Surface training t=11356, loss=0.048825453966856\n",
      "Surface training t=11357, loss=0.07866957038640976\n",
      "Surface training t=11358, loss=0.05562827177345753\n",
      "Surface training t=11359, loss=0.052012160420417786\n",
      "Surface training t=11360, loss=0.07370662316679955\n",
      "Surface training t=11361, loss=0.05585993081331253\n",
      "Surface training t=11362, loss=0.0663751121610403\n",
      "Surface training t=11363, loss=0.06248384714126587\n",
      "Surface training t=11364, loss=0.039996521547436714\n",
      "Surface training t=11365, loss=0.04774046875536442\n",
      "Surface training t=11366, loss=0.05204523354768753\n",
      "Surface training t=11367, loss=0.05591347627341747\n",
      "Surface training t=11368, loss=0.05518430098891258\n",
      "Surface training t=11369, loss=0.05458247289061546\n",
      "Surface training t=11370, loss=0.05729864165186882\n",
      "Surface training t=11371, loss=0.04424292407929897\n",
      "Surface training t=11372, loss=0.052566759288311005\n",
      "Surface training t=11373, loss=0.03649114724248648\n",
      "Surface training t=11374, loss=0.033453334122896194\n",
      "Surface training t=11375, loss=0.038289519026875496\n",
      "Surface training t=11376, loss=0.028411234728991985\n",
      "Surface training t=11377, loss=0.02105150744318962\n",
      "Surface training t=11378, loss=0.019927309826016426\n",
      "Surface training t=11379, loss=0.027145572006702423\n",
      "Surface training t=11380, loss=0.029146947897970676\n",
      "Surface training t=11381, loss=0.025282910093665123\n",
      "Surface training t=11382, loss=0.026104153133928776\n",
      "Surface training t=11383, loss=0.04017334431409836\n",
      "Surface training t=11384, loss=0.02982643712311983\n",
      "Surface training t=11385, loss=0.02373102307319641\n",
      "Surface training t=11386, loss=0.02644356433302164\n",
      "Surface training t=11387, loss=0.03328059706836939\n",
      "Surface training t=11388, loss=0.03598703909665346\n",
      "Surface training t=11389, loss=0.04435192979872227\n",
      "Surface training t=11390, loss=0.024593709036707878\n",
      "Surface training t=11391, loss=0.025304957292973995\n",
      "Surface training t=11392, loss=0.03976079635322094\n",
      "Surface training t=11393, loss=0.0327102430164814\n",
      "Surface training t=11394, loss=0.04763777367770672\n",
      "Surface training t=11395, loss=0.0444517582654953\n",
      "Surface training t=11396, loss=0.03202732093632221\n",
      "Surface training t=11397, loss=0.03789703082293272\n",
      "Surface training t=11398, loss=0.03166765067726374\n",
      "Surface training t=11399, loss=0.028791015967726707\n",
      "Surface training t=11400, loss=0.03722020983695984\n",
      "Surface training t=11401, loss=0.04445054940879345\n",
      "Surface training t=11402, loss=0.035373494029045105\n",
      "Surface training t=11403, loss=0.04556262120604515\n",
      "Surface training t=11404, loss=0.0360143817961216\n",
      "Surface training t=11405, loss=0.03000570647418499\n",
      "Surface training t=11406, loss=0.03954851068556309\n",
      "Surface training t=11407, loss=0.032313670963048935\n",
      "Surface training t=11408, loss=0.034535398706793785\n",
      "Surface training t=11409, loss=0.040755610913038254\n",
      "Surface training t=11410, loss=0.030053893104195595\n",
      "Surface training t=11411, loss=0.023476533591747284\n",
      "Surface training t=11412, loss=0.024413660168647766\n",
      "Surface training t=11413, loss=0.029000372625887394\n",
      "Surface training t=11414, loss=0.027729563415050507\n",
      "Surface training t=11415, loss=0.02788886148482561\n",
      "Surface training t=11416, loss=0.027876703068614006\n",
      "Surface training t=11417, loss=0.028567031025886536\n",
      "Surface training t=11418, loss=0.030440744012594223\n",
      "Surface training t=11419, loss=0.027196664363145828\n",
      "Surface training t=11420, loss=0.03014492802321911\n",
      "Surface training t=11421, loss=0.03337198495864868\n",
      "Surface training t=11422, loss=0.02525939792394638\n",
      "Surface training t=11423, loss=0.023688621819019318\n",
      "Surface training t=11424, loss=0.025986148044466972\n",
      "Surface training t=11425, loss=0.020750414580106735\n",
      "Surface training t=11426, loss=0.026563572697341442\n",
      "Surface training t=11427, loss=0.0350961284711957\n",
      "Surface training t=11428, loss=0.03670506924390793\n",
      "Surface training t=11429, loss=0.03437509760260582\n",
      "Surface training t=11430, loss=0.04256178345531225\n",
      "Surface training t=11431, loss=0.03898369334638119\n",
      "Surface training t=11432, loss=0.03794073313474655\n",
      "Surface training t=11433, loss=0.03614658396691084\n",
      "Surface training t=11434, loss=0.04332432709634304\n",
      "Surface training t=11435, loss=0.03128670807927847\n",
      "Surface training t=11436, loss=0.02643904834985733\n",
      "Surface training t=11437, loss=0.029639512300491333\n",
      "Surface training t=11438, loss=0.025195869617164135\n",
      "Surface training t=11439, loss=0.02378642838448286\n",
      "Surface training t=11440, loss=0.02968513686209917\n",
      "Surface training t=11441, loss=0.018902769312262535\n",
      "Surface training t=11442, loss=0.023495730943977833\n",
      "Surface training t=11443, loss=0.03091521468013525\n",
      "Surface training t=11444, loss=0.022315435111522675\n",
      "Surface training t=11445, loss=0.03286361135542393\n",
      "Surface training t=11446, loss=0.03266992047429085\n",
      "Surface training t=11447, loss=0.028056658804416656\n",
      "Surface training t=11448, loss=0.03645182214677334\n",
      "Surface training t=11449, loss=0.03195179719477892\n",
      "Surface training t=11450, loss=0.031709291972219944\n",
      "Surface training t=11451, loss=0.028651542961597443\n",
      "Surface training t=11452, loss=0.026986148208379745\n",
      "Surface training t=11453, loss=0.02905412670224905\n",
      "Surface training t=11454, loss=0.029664839617908\n",
      "Surface training t=11455, loss=0.023317349143326283\n",
      "Surface training t=11456, loss=0.02787572704255581\n",
      "Surface training t=11457, loss=0.02278158161789179\n",
      "Surface training t=11458, loss=0.02206432167440653\n",
      "Surface training t=11459, loss=0.030329053290188313\n",
      "Surface training t=11460, loss=0.03464786987751722\n",
      "Surface training t=11461, loss=0.03351390175521374\n",
      "Surface training t=11462, loss=0.03562829364091158\n",
      "Surface training t=11463, loss=0.045165298506617546\n",
      "Surface training t=11464, loss=0.033217272721230984\n",
      "Surface training t=11465, loss=0.031868916004896164\n",
      "Surface training t=11466, loss=0.03502443339675665\n",
      "Surface training t=11467, loss=0.03132994845509529\n",
      "Surface training t=11468, loss=0.06236528977751732\n",
      "Surface training t=11469, loss=0.04158896300941706\n",
      "Surface training t=11470, loss=0.039212413132190704\n",
      "Surface training t=11471, loss=0.03616165462881327\n",
      "Surface training t=11472, loss=0.0428061056882143\n",
      "Surface training t=11473, loss=0.028360892087221146\n",
      "Surface training t=11474, loss=0.03128928691148758\n",
      "Surface training t=11475, loss=0.02004765346646309\n",
      "Surface training t=11476, loss=0.023419302888214588\n",
      "Surface training t=11477, loss=0.02360957581549883\n",
      "Surface training t=11478, loss=0.03260624222457409\n",
      "Surface training t=11479, loss=0.02273140288889408\n",
      "Surface training t=11480, loss=0.03789710812270641\n",
      "Surface training t=11481, loss=0.028918548487126827\n",
      "Surface training t=11482, loss=0.031581347808241844\n",
      "Surface training t=11483, loss=0.027396906167268753\n",
      "Surface training t=11484, loss=0.02698743063956499\n",
      "Surface training t=11485, loss=0.029396462254226208\n",
      "Surface training t=11486, loss=0.03174661286175251\n",
      "Surface training t=11487, loss=0.03600987605750561\n",
      "Surface training t=11488, loss=0.030288215726614\n",
      "Surface training t=11489, loss=0.03378038760274649\n",
      "Surface training t=11490, loss=0.021671529859304428\n",
      "Surface training t=11491, loss=0.024636548943817616\n",
      "Surface training t=11492, loss=0.025365865789353848\n",
      "Surface training t=11493, loss=0.027309481985867023\n",
      "Surface training t=11494, loss=0.026006488129496574\n",
      "Surface training t=11495, loss=0.02561031747609377\n",
      "Surface training t=11496, loss=0.028790229000151157\n",
      "Surface training t=11497, loss=0.03649519011378288\n",
      "Surface training t=11498, loss=0.032257040962576866\n",
      "Surface training t=11499, loss=0.03024490736424923\n",
      "Surface training t=11500, loss=0.03204156272113323\n",
      "Surface training t=11501, loss=0.03214436583220959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=11502, loss=0.03170387912541628\n",
      "Surface training t=11503, loss=0.033174825832247734\n",
      "Surface training t=11504, loss=0.04536721855401993\n",
      "Surface training t=11505, loss=0.02996735367923975\n",
      "Surface training t=11506, loss=0.030127701349556446\n",
      "Surface training t=11507, loss=0.03330632112920284\n",
      "Surface training t=11508, loss=0.022919845767319202\n",
      "Surface training t=11509, loss=0.024318912997841835\n",
      "Surface training t=11510, loss=0.024303052574396133\n",
      "Surface training t=11511, loss=0.023393738083541393\n",
      "Surface training t=11512, loss=0.02820400707423687\n",
      "Surface training t=11513, loss=0.021071147173643112\n",
      "Surface training t=11514, loss=0.025568410754203796\n",
      "Surface training t=11515, loss=0.04192521050572395\n",
      "Surface training t=11516, loss=0.04976092278957367\n",
      "Surface training t=11517, loss=0.05072609521448612\n",
      "Surface training t=11518, loss=0.04664454981684685\n",
      "Surface training t=11519, loss=0.05941338464617729\n",
      "Surface training t=11520, loss=0.06736944615840912\n",
      "Surface training t=11521, loss=0.05368954315781593\n",
      "Surface training t=11522, loss=0.05289642419666052\n",
      "Surface training t=11523, loss=0.06799268163740635\n",
      "Surface training t=11524, loss=0.043144697323441505\n",
      "Surface training t=11525, loss=0.05788404867053032\n",
      "Surface training t=11526, loss=0.04483414348214865\n",
      "Surface training t=11527, loss=0.06708406470716\n",
      "Surface training t=11528, loss=0.06506906822323799\n",
      "Surface training t=11529, loss=0.04283582977950573\n",
      "Surface training t=11530, loss=0.04111601039767265\n",
      "Surface training t=11531, loss=0.04453452676534653\n",
      "Surface training t=11532, loss=0.03851154074072838\n",
      "Surface training t=11533, loss=0.048751574009656906\n",
      "Surface training t=11534, loss=0.03486087918281555\n",
      "Surface training t=11535, loss=0.04658287763595581\n",
      "Surface training t=11536, loss=0.03951745666563511\n",
      "Surface training t=11537, loss=0.032109225168824196\n",
      "Surface training t=11538, loss=0.05288400501012802\n",
      "Surface training t=11539, loss=0.03967327997088432\n",
      "Surface training t=11540, loss=0.02690905425697565\n",
      "Surface training t=11541, loss=0.030057040974497795\n",
      "Surface training t=11542, loss=0.033305300399661064\n",
      "Surface training t=11543, loss=0.03130077663809061\n",
      "Surface training t=11544, loss=0.03529734257608652\n",
      "Surface training t=11545, loss=0.025371787138283253\n",
      "Surface training t=11546, loss=0.03507429827004671\n",
      "Surface training t=11547, loss=0.036204611882567406\n",
      "Surface training t=11548, loss=0.03682188503444195\n",
      "Surface training t=11549, loss=0.026590938679873943\n",
      "Surface training t=11550, loss=0.021938789635896683\n",
      "Surface training t=11551, loss=0.02740021888166666\n",
      "Surface training t=11552, loss=0.03787728399038315\n",
      "Surface training t=11553, loss=0.02863560151308775\n",
      "Surface training t=11554, loss=0.024720363318920135\n",
      "Surface training t=11555, loss=0.03096193727105856\n",
      "Surface training t=11556, loss=0.027121136896312237\n",
      "Surface training t=11557, loss=0.028370208106935024\n",
      "Surface training t=11558, loss=0.02845364809036255\n",
      "Surface training t=11559, loss=0.02884555049240589\n",
      "Surface training t=11560, loss=0.03340292535722256\n",
      "Surface training t=11561, loss=0.04028586111962795\n",
      "Surface training t=11562, loss=0.04454396106302738\n",
      "Surface training t=11563, loss=0.04906126298010349\n",
      "Surface training t=11564, loss=0.052984876558184624\n",
      "Surface training t=11565, loss=0.051243891939520836\n",
      "Surface training t=11566, loss=0.05123339220881462\n",
      "Surface training t=11567, loss=0.049140069633722305\n",
      "Surface training t=11568, loss=0.04827135242521763\n",
      "Surface training t=11569, loss=0.04061258025467396\n",
      "Surface training t=11570, loss=0.044097088277339935\n",
      "Surface training t=11571, loss=0.040903232991695404\n",
      "Surface training t=11572, loss=0.039380330592393875\n",
      "Surface training t=11573, loss=0.04072415642440319\n",
      "Surface training t=11574, loss=0.04228805750608444\n",
      "Surface training t=11575, loss=0.04379066824913025\n",
      "Surface training t=11576, loss=0.04994741827249527\n",
      "Surface training t=11577, loss=0.06445391476154327\n",
      "Surface training t=11578, loss=0.04591444320976734\n",
      "Surface training t=11579, loss=0.05609169043600559\n",
      "Surface training t=11580, loss=0.04500511009246111\n",
      "Surface training t=11581, loss=0.07293420657515526\n",
      "Surface training t=11582, loss=0.05251314211636782\n",
      "Surface training t=11583, loss=0.08014829084277153\n",
      "Surface training t=11584, loss=0.054596398025751114\n",
      "Surface training t=11585, loss=0.0566310603171587\n",
      "Surface training t=11586, loss=0.04834252968430519\n",
      "Surface training t=11587, loss=0.05140722170472145\n",
      "Surface training t=11588, loss=0.061918532475829124\n",
      "Surface training t=11589, loss=0.045350316911935806\n",
      "Surface training t=11590, loss=0.03955306112766266\n",
      "Surface training t=11591, loss=0.036579618230462074\n",
      "Surface training t=11592, loss=0.03780831303447485\n",
      "Surface training t=11593, loss=0.04224136285483837\n",
      "Surface training t=11594, loss=0.028547615744173527\n",
      "Surface training t=11595, loss=0.02276198286563158\n",
      "Surface training t=11596, loss=0.02239386085420847\n",
      "Surface training t=11597, loss=0.02853253297507763\n",
      "Surface training t=11598, loss=0.03124887775629759\n",
      "Surface training t=11599, loss=0.03333522565662861\n",
      "Surface training t=11600, loss=0.027466720901429653\n",
      "Surface training t=11601, loss=0.02491567935794592\n",
      "Surface training t=11602, loss=0.02408661413937807\n",
      "Surface training t=11603, loss=0.03041334729641676\n",
      "Surface training t=11604, loss=0.02568584866821766\n",
      "Surface training t=11605, loss=0.023988324217498302\n",
      "Surface training t=11606, loss=0.02678501047194004\n",
      "Surface training t=11607, loss=0.045428283512592316\n",
      "Surface training t=11608, loss=0.04057093895971775\n",
      "Surface training t=11609, loss=0.046835070475935936\n",
      "Surface training t=11610, loss=0.026525876484811306\n",
      "Surface training t=11611, loss=0.025630032643675804\n",
      "Surface training t=11612, loss=0.04650491289794445\n",
      "Surface training t=11613, loss=0.04262649267911911\n",
      "Surface training t=11614, loss=0.030212951824069023\n",
      "Surface training t=11615, loss=0.028112956322729588\n",
      "Surface training t=11616, loss=0.025277589447796345\n",
      "Surface training t=11617, loss=0.022481445223093033\n",
      "Surface training t=11618, loss=0.02985925506800413\n",
      "Surface training t=11619, loss=0.031314192339777946\n",
      "Surface training t=11620, loss=0.03961034119129181\n",
      "Surface training t=11621, loss=0.05772283300757408\n",
      "Surface training t=11622, loss=0.038216181099414825\n",
      "Surface training t=11623, loss=0.03360092546790838\n",
      "Surface training t=11624, loss=0.028657224029302597\n",
      "Surface training t=11625, loss=0.0218088673427701\n",
      "Surface training t=11626, loss=0.04541107825934887\n",
      "Surface training t=11627, loss=0.02559402957558632\n",
      "Surface training t=11628, loss=0.03176212124526501\n",
      "Surface training t=11629, loss=0.050405802205204964\n",
      "Surface training t=11630, loss=0.04523864574730396\n",
      "Surface training t=11631, loss=0.044979327358305454\n",
      "Surface training t=11632, loss=0.05580543167889118\n",
      "Surface training t=11633, loss=0.03435731213539839\n",
      "Surface training t=11634, loss=0.06205643527209759\n",
      "Surface training t=11635, loss=0.0426715649664402\n",
      "Surface training t=11636, loss=0.05383720062673092\n",
      "Surface training t=11637, loss=0.04109368100762367\n",
      "Surface training t=11638, loss=0.041859615594148636\n",
      "Surface training t=11639, loss=0.0379277765750885\n",
      "Surface training t=11640, loss=0.033468274399638176\n",
      "Surface training t=11641, loss=0.037339527159929276\n",
      "Surface training t=11642, loss=0.03869018889963627\n",
      "Surface training t=11643, loss=0.03940887004137039\n",
      "Surface training t=11644, loss=0.036030106246471405\n",
      "Surface training t=11645, loss=0.04701951704919338\n",
      "Surface training t=11646, loss=0.038424527272582054\n",
      "Surface training t=11647, loss=0.03752836398780346\n",
      "Surface training t=11648, loss=0.03896413929760456\n",
      "Surface training t=11649, loss=0.032001250423491\n",
      "Surface training t=11650, loss=0.03568375017493963\n",
      "Surface training t=11651, loss=0.03907960094511509\n",
      "Surface training t=11652, loss=0.03275182377547026\n",
      "Surface training t=11653, loss=0.026301690377295017\n",
      "Surface training t=11654, loss=0.03163823764771223\n",
      "Surface training t=11655, loss=0.02901165932416916\n",
      "Surface training t=11656, loss=0.02589340414851904\n",
      "Surface training t=11657, loss=0.03368263226002455\n",
      "Surface training t=11658, loss=0.03944507148116827\n",
      "Surface training t=11659, loss=0.03993241209536791\n",
      "Surface training t=11660, loss=0.04466360993683338\n",
      "Surface training t=11661, loss=0.03524382226169109\n",
      "Surface training t=11662, loss=0.03204575926065445\n",
      "Surface training t=11663, loss=0.04505706578493118\n",
      "Surface training t=11664, loss=0.04125002771615982\n",
      "Surface training t=11665, loss=0.08167210221290588\n",
      "Surface training t=11666, loss=0.058838147670030594\n",
      "Surface training t=11667, loss=0.05276590958237648\n",
      "Surface training t=11668, loss=0.05879668891429901\n",
      "Surface training t=11669, loss=0.04493461363017559\n",
      "Surface training t=11670, loss=0.0367509201169014\n",
      "Surface training t=11671, loss=0.04886358045041561\n",
      "Surface training t=11672, loss=0.03158768452703953\n",
      "Surface training t=11673, loss=0.03268328495323658\n",
      "Surface training t=11674, loss=0.03078166116029024\n",
      "Surface training t=11675, loss=0.03655903600156307\n",
      "Surface training t=11676, loss=0.03475003503262997\n",
      "Surface training t=11677, loss=0.03704196214675903\n",
      "Surface training t=11678, loss=0.03588869795203209\n",
      "Surface training t=11679, loss=0.04234583117067814\n",
      "Surface training t=11680, loss=0.0370864300057292\n",
      "Surface training t=11681, loss=0.06291300989687443\n",
      "Surface training t=11682, loss=0.04634012468159199\n",
      "Surface training t=11683, loss=0.047509532421827316\n",
      "Surface training t=11684, loss=0.06746160052716732\n",
      "Surface training t=11685, loss=0.04715622030198574\n",
      "Surface training t=11686, loss=0.04598282277584076\n",
      "Surface training t=11687, loss=0.05240884702652693\n",
      "Surface training t=11688, loss=0.06534083187580109\n",
      "Surface training t=11689, loss=0.05284835025668144\n",
      "Surface training t=11690, loss=0.05310375615954399\n",
      "Surface training t=11691, loss=0.09248173609375954\n",
      "Surface training t=11692, loss=0.05139552615582943\n",
      "Surface training t=11693, loss=0.04832927417010069\n",
      "Surface training t=11694, loss=0.07729126885533333\n",
      "Surface training t=11695, loss=0.04212953895330429\n",
      "Surface training t=11696, loss=0.0372253879904747\n",
      "Surface training t=11697, loss=0.03148388396948576\n",
      "Surface training t=11698, loss=0.038089606910943985\n",
      "Surface training t=11699, loss=0.04093078523874283\n",
      "Surface training t=11700, loss=0.033332811668515205\n",
      "Surface training t=11701, loss=0.055019913241267204\n",
      "Surface training t=11702, loss=0.04515167884528637\n",
      "Surface training t=11703, loss=0.04289364442229271\n",
      "Surface training t=11704, loss=0.033816336654126644\n",
      "Surface training t=11705, loss=0.03223617188632488\n",
      "Surface training t=11706, loss=0.04533064737915993\n",
      "Surface training t=11707, loss=0.03718586079776287\n",
      "Surface training t=11708, loss=0.032066699117422104\n",
      "Surface training t=11709, loss=0.03946681134402752\n",
      "Surface training t=11710, loss=0.04234444722533226\n",
      "Surface training t=11711, loss=0.027915989980101585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=11712, loss=0.042710280045866966\n",
      "Surface training t=11713, loss=0.059450024738907814\n",
      "Surface training t=11714, loss=0.03601620998233557\n",
      "Surface training t=11715, loss=0.03714335151016712\n",
      "Surface training t=11716, loss=0.0394566934555769\n",
      "Surface training t=11717, loss=0.04432694800198078\n",
      "Surface training t=11718, loss=0.04593280889093876\n",
      "Surface training t=11719, loss=0.040461814031004906\n",
      "Surface training t=11720, loss=0.03269823081791401\n",
      "Surface training t=11721, loss=0.03867927007377148\n",
      "Surface training t=11722, loss=0.03909553214907646\n",
      "Surface training t=11723, loss=0.05760508216917515\n",
      "Surface training t=11724, loss=0.04119596257805824\n",
      "Surface training t=11725, loss=0.03418806288391352\n",
      "Surface training t=11726, loss=0.03500501997768879\n",
      "Surface training t=11727, loss=0.03264998272061348\n",
      "Surface training t=11728, loss=0.038870006799697876\n",
      "Surface training t=11729, loss=0.034182919189333916\n",
      "Surface training t=11730, loss=0.03857816196978092\n",
      "Surface training t=11731, loss=0.038939639925956726\n",
      "Surface training t=11732, loss=0.0403689444065094\n",
      "Surface training t=11733, loss=0.03342798538506031\n",
      "Surface training t=11734, loss=0.026877908036112785\n",
      "Surface training t=11735, loss=0.027024108916521072\n",
      "Surface training t=11736, loss=0.03247682377696037\n",
      "Surface training t=11737, loss=0.02894135657697916\n",
      "Surface training t=11738, loss=0.026730922050774097\n",
      "Surface training t=11739, loss=0.02813761681318283\n",
      "Surface training t=11740, loss=0.028149310499429703\n",
      "Surface training t=11741, loss=0.033748809248209\n",
      "Surface training t=11742, loss=0.02832846064120531\n",
      "Surface training t=11743, loss=0.028686609119176865\n",
      "Surface training t=11744, loss=0.029118158854544163\n",
      "Surface training t=11745, loss=0.03953852504491806\n",
      "Surface training t=11746, loss=0.031251478008925915\n",
      "Surface training t=11747, loss=0.018590277060866356\n",
      "Surface training t=11748, loss=0.019722568802535534\n",
      "Surface training t=11749, loss=0.02661329321563244\n",
      "Surface training t=11750, loss=0.026972361840307713\n",
      "Surface training t=11751, loss=0.026411771774291992\n",
      "Surface training t=11752, loss=0.02000356651842594\n",
      "Surface training t=11753, loss=0.0339378472417593\n",
      "Surface training t=11754, loss=0.03933335281908512\n",
      "Surface training t=11755, loss=0.062489401549100876\n",
      "Surface training t=11756, loss=0.04872240871191025\n",
      "Surface training t=11757, loss=0.043097203597426414\n",
      "Surface training t=11758, loss=0.044610852375626564\n",
      "Surface training t=11759, loss=0.036312706768512726\n",
      "Surface training t=11760, loss=0.040036506950855255\n",
      "Surface training t=11761, loss=0.039362385869026184\n",
      "Surface training t=11762, loss=0.052644263952970505\n",
      "Surface training t=11763, loss=0.03690907172858715\n",
      "Surface training t=11764, loss=0.035646623000502586\n",
      "Surface training t=11765, loss=0.028403379023075104\n",
      "Surface training t=11766, loss=0.027390629053115845\n",
      "Surface training t=11767, loss=0.024703106842935085\n",
      "Surface training t=11768, loss=0.019745182245969772\n",
      "Surface training t=11769, loss=0.023372539319097996\n",
      "Surface training t=11770, loss=0.03802109882235527\n",
      "Surface training t=11771, loss=0.02631764393299818\n",
      "Surface training t=11772, loss=0.028489885851740837\n",
      "Surface training t=11773, loss=0.032432932406663895\n",
      "Surface training t=11774, loss=0.03381565771996975\n",
      "Surface training t=11775, loss=0.03024531714618206\n",
      "Surface training t=11776, loss=0.03918503411114216\n",
      "Surface training t=11777, loss=0.04415063001215458\n",
      "Surface training t=11778, loss=0.04837014153599739\n",
      "Surface training t=11779, loss=0.058942120522260666\n",
      "Surface training t=11780, loss=0.059942856431007385\n",
      "Surface training t=11781, loss=0.050897132605314255\n",
      "Surface training t=11782, loss=0.06085510551929474\n",
      "Surface training t=11783, loss=0.09712990000844002\n",
      "Surface training t=11784, loss=0.0664035715162754\n",
      "Surface training t=11785, loss=0.08752421662211418\n",
      "Surface training t=11786, loss=0.06673630885779858\n",
      "Surface training t=11787, loss=0.05662097781896591\n",
      "Surface training t=11788, loss=0.04151040967553854\n",
      "Surface training t=11789, loss=0.058587074279785156\n",
      "Surface training t=11790, loss=0.04146701097488403\n",
      "Surface training t=11791, loss=0.04765838384628296\n",
      "Surface training t=11792, loss=0.05161985941231251\n",
      "Surface training t=11793, loss=0.09685283154249191\n",
      "Surface training t=11794, loss=0.056884244084358215\n",
      "Surface training t=11795, loss=0.07179586961865425\n",
      "Surface training t=11796, loss=0.08105441369116306\n",
      "Surface training t=11797, loss=0.047458354383707047\n",
      "Surface training t=11798, loss=0.06173772178590298\n",
      "Surface training t=11799, loss=0.0710192583501339\n",
      "Surface training t=11800, loss=0.046667516231536865\n",
      "Surface training t=11801, loss=0.05152638256549835\n",
      "Surface training t=11802, loss=0.056810515001416206\n",
      "Surface training t=11803, loss=0.05580170452594757\n",
      "Surface training t=11804, loss=0.06110772117972374\n",
      "Surface training t=11805, loss=0.05620020814239979\n",
      "Surface training t=11806, loss=0.06958477199077606\n",
      "Surface training t=11807, loss=0.04563489370048046\n",
      "Surface training t=11808, loss=0.045899493619799614\n",
      "Surface training t=11809, loss=0.03392665274441242\n",
      "Surface training t=11810, loss=0.05768570117652416\n",
      "Surface training t=11811, loss=0.04254309181123972\n",
      "Surface training t=11812, loss=0.06333212926983833\n",
      "Surface training t=11813, loss=0.05395970493555069\n",
      "Surface training t=11814, loss=0.06233398802578449\n",
      "Surface training t=11815, loss=0.04160250350832939\n",
      "Surface training t=11816, loss=0.06905992701649666\n",
      "Surface training t=11817, loss=0.045347314327955246\n",
      "Surface training t=11818, loss=0.05846361629664898\n",
      "Surface training t=11819, loss=0.03399323206394911\n",
      "Surface training t=11820, loss=0.06280911341309547\n",
      "Surface training t=11821, loss=0.04548719525337219\n",
      "Surface training t=11822, loss=0.05163554660975933\n",
      "Surface training t=11823, loss=0.036590393632650375\n",
      "Surface training t=11824, loss=0.028938758186995983\n",
      "Surface training t=11825, loss=0.020341564901173115\n",
      "Surface training t=11826, loss=0.022313001565635204\n",
      "Surface training t=11827, loss=0.02113288640975952\n",
      "Surface training t=11828, loss=0.024793021380901337\n",
      "Surface training t=11829, loss=0.030347603373229504\n",
      "Surface training t=11830, loss=0.021046328358352184\n",
      "Surface training t=11831, loss=0.02537492662668228\n",
      "Surface training t=11832, loss=0.03845358267426491\n",
      "Surface training t=11833, loss=0.03739157039672136\n",
      "Surface training t=11834, loss=0.0382535457611084\n",
      "Surface training t=11835, loss=0.0375035684555769\n",
      "Surface training t=11836, loss=0.040742721408605576\n",
      "Surface training t=11837, loss=0.0457520242780447\n",
      "Surface training t=11838, loss=0.039035456255078316\n",
      "Surface training t=11839, loss=0.031386133283376694\n",
      "Surface training t=11840, loss=0.028897474519908428\n",
      "Surface training t=11841, loss=0.032395849004387856\n",
      "Surface training t=11842, loss=0.033233392983675\n",
      "Surface training t=11843, loss=0.0368865467607975\n",
      "Surface training t=11844, loss=0.044013443402945995\n",
      "Surface training t=11845, loss=0.05152413621544838\n",
      "Surface training t=11846, loss=0.03672385960817337\n",
      "Surface training t=11847, loss=0.05022099427878857\n",
      "Surface training t=11848, loss=0.041452035307884216\n",
      "Surface training t=11849, loss=0.04211915470659733\n",
      "Surface training t=11850, loss=0.03750629164278507\n",
      "Surface training t=11851, loss=0.03738962858915329\n",
      "Surface training t=11852, loss=0.03923156764358282\n",
      "Surface training t=11853, loss=0.048410991206765175\n",
      "Surface training t=11854, loss=0.05096236243844032\n",
      "Surface training t=11855, loss=0.040320685133337975\n",
      "Surface training t=11856, loss=0.03922831825911999\n",
      "Surface training t=11857, loss=0.03616899251937866\n",
      "Surface training t=11858, loss=0.039154608733952045\n",
      "Surface training t=11859, loss=0.05444716103374958\n",
      "Surface training t=11860, loss=0.060103313997387886\n",
      "Surface training t=11861, loss=0.04100537486374378\n",
      "Surface training t=11862, loss=0.039444051682949066\n",
      "Surface training t=11863, loss=0.033248452469706535\n",
      "Surface training t=11864, loss=0.026825702749192715\n",
      "Surface training t=11865, loss=0.023562786169350147\n",
      "Surface training t=11866, loss=0.023674474097788334\n",
      "Surface training t=11867, loss=0.024487695656716824\n",
      "Surface training t=11868, loss=0.03048374317586422\n",
      "Surface training t=11869, loss=0.024282622151076794\n",
      "Surface training t=11870, loss=0.01561458082869649\n",
      "Surface training t=11871, loss=0.018116841092705727\n",
      "Surface training t=11872, loss=0.022913829423487186\n",
      "Surface training t=11873, loss=0.027563980780541897\n",
      "Surface training t=11874, loss=0.024142264388501644\n",
      "Surface training t=11875, loss=0.029202108271420002\n",
      "Surface training t=11876, loss=0.028353051282465458\n",
      "Surface training t=11877, loss=0.0272831991314888\n",
      "Surface training t=11878, loss=0.028591053560376167\n",
      "Surface training t=11879, loss=0.027296158485114574\n",
      "Surface training t=11880, loss=0.028889921493828297\n",
      "Surface training t=11881, loss=0.025995231233537197\n",
      "Surface training t=11882, loss=0.026066331192851067\n",
      "Surface training t=11883, loss=0.038930151611566544\n",
      "Surface training t=11884, loss=0.034366197884082794\n",
      "Surface training t=11885, loss=0.03795805759727955\n",
      "Surface training t=11886, loss=0.03794713132083416\n",
      "Surface training t=11887, loss=0.02352282963693142\n",
      "Surface training t=11888, loss=0.028623802587389946\n",
      "Surface training t=11889, loss=0.030482043512165546\n",
      "Surface training t=11890, loss=0.028879838064312935\n",
      "Surface training t=11891, loss=0.019379050470888615\n",
      "Surface training t=11892, loss=0.021052013151347637\n",
      "Surface training t=11893, loss=0.024618711322546005\n",
      "Surface training t=11894, loss=0.02348316926509142\n",
      "Surface training t=11895, loss=0.03216656669974327\n",
      "Surface training t=11896, loss=0.03977838531136513\n",
      "Surface training t=11897, loss=0.026810292154550552\n",
      "Surface training t=11898, loss=0.03629898466169834\n",
      "Surface training t=11899, loss=0.03314533643424511\n",
      "Surface training t=11900, loss=0.024602089077234268\n",
      "Surface training t=11901, loss=0.04170208424329758\n",
      "Surface training t=11902, loss=0.035828765481710434\n",
      "Surface training t=11903, loss=0.0396876223385334\n",
      "Surface training t=11904, loss=0.041470425203442574\n",
      "Surface training t=11905, loss=0.04080024175345898\n",
      "Surface training t=11906, loss=0.032098148949444294\n",
      "Surface training t=11907, loss=0.030728877522051334\n",
      "Surface training t=11908, loss=0.025521486066281796\n",
      "Surface training t=11909, loss=0.029481027275323868\n",
      "Surface training t=11910, loss=0.02219635806977749\n",
      "Surface training t=11911, loss=0.02666182443499565\n",
      "Surface training t=11912, loss=0.0254298010841012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=11913, loss=0.024479073472321033\n",
      "Surface training t=11914, loss=0.022586770355701447\n",
      "Surface training t=11915, loss=0.030523259192705154\n",
      "Surface training t=11916, loss=0.023419145494699478\n",
      "Surface training t=11917, loss=0.025082840584218502\n",
      "Surface training t=11918, loss=0.022244537249207497\n",
      "Surface training t=11919, loss=0.025134348310530186\n",
      "Surface training t=11920, loss=0.025856126099824905\n",
      "Surface training t=11921, loss=0.03041317407041788\n",
      "Surface training t=11922, loss=0.03146866615861654\n",
      "Surface training t=11923, loss=0.027355159632861614\n",
      "Surface training t=11924, loss=0.034088971093297005\n",
      "Surface training t=11925, loss=0.025132829323410988\n",
      "Surface training t=11926, loss=0.028605466708540916\n",
      "Surface training t=11927, loss=0.025487948209047318\n",
      "Surface training t=11928, loss=0.030843224376440048\n",
      "Surface training t=11929, loss=0.030605856329202652\n",
      "Surface training t=11930, loss=0.02792974840849638\n",
      "Surface training t=11931, loss=0.03469162434339523\n",
      "Surface training t=11932, loss=0.027879266999661922\n",
      "Surface training t=11933, loss=0.03614568430930376\n",
      "Surface training t=11934, loss=0.04179645888507366\n",
      "Surface training t=11935, loss=0.03864512965083122\n",
      "Surface training t=11936, loss=0.04144022986292839\n",
      "Surface training t=11937, loss=0.04292854480445385\n",
      "Surface training t=11938, loss=0.046908484771847725\n",
      "Surface training t=11939, loss=0.0390881709754467\n",
      "Surface training t=11940, loss=0.04637259989976883\n",
      "Surface training t=11941, loss=0.048568569123744965\n",
      "Surface training t=11942, loss=0.03829941153526306\n",
      "Surface training t=11943, loss=0.03281260095536709\n",
      "Surface training t=11944, loss=0.04183945618569851\n",
      "Surface training t=11945, loss=0.04764053784310818\n",
      "Surface training t=11946, loss=0.03279973194003105\n",
      "Surface training t=11947, loss=0.032145120203495026\n",
      "Surface training t=11948, loss=0.04662793315947056\n",
      "Surface training t=11949, loss=0.052429916337132454\n",
      "Surface training t=11950, loss=0.05874049849808216\n",
      "Surface training t=11951, loss=0.049514392390847206\n",
      "Surface training t=11952, loss=0.04654789716005325\n",
      "Surface training t=11953, loss=0.0351056344807148\n",
      "Surface training t=11954, loss=0.046618254855275154\n",
      "Surface training t=11955, loss=0.03512047603726387\n",
      "Surface training t=11956, loss=0.04241960495710373\n",
      "Surface training t=11957, loss=0.037215931341052055\n",
      "Surface training t=11958, loss=0.024007082916796207\n",
      "Surface training t=11959, loss=0.028022713027894497\n",
      "Surface training t=11960, loss=0.02483697608113289\n",
      "Surface training t=11961, loss=0.02411500457674265\n",
      "Surface training t=11962, loss=0.01712929643690586\n",
      "Surface training t=11963, loss=0.027640314772725105\n",
      "Surface training t=11964, loss=0.026752455160021782\n",
      "Surface training t=11965, loss=0.023896521888673306\n",
      "Surface training t=11966, loss=0.0211500721052289\n",
      "Surface training t=11967, loss=0.024755201302468777\n",
      "Surface training t=11968, loss=0.035270567052066326\n",
      "Surface training t=11969, loss=0.026086894795298576\n",
      "Surface training t=11970, loss=0.031643849797546864\n",
      "Surface training t=11971, loss=0.0259221363812685\n",
      "Surface training t=11972, loss=0.020506305620074272\n",
      "Surface training t=11973, loss=0.027631686069071293\n",
      "Surface training t=11974, loss=0.027916555292904377\n",
      "Surface training t=11975, loss=0.024425984360277653\n",
      "Surface training t=11976, loss=0.031197327189147472\n",
      "Surface training t=11977, loss=0.037420500069856644\n",
      "Surface training t=11978, loss=0.029176395386457443\n",
      "Surface training t=11979, loss=0.02432045992463827\n",
      "Surface training t=11980, loss=0.028457464650273323\n",
      "Surface training t=11981, loss=0.04230962507426739\n",
      "Surface training t=11982, loss=0.03060702420771122\n",
      "Surface training t=11983, loss=0.041616976261138916\n",
      "Surface training t=11984, loss=0.02928481064736843\n",
      "Surface training t=11985, loss=0.027092655189335346\n",
      "Surface training t=11986, loss=0.023578638210892677\n",
      "Surface training t=11987, loss=0.02600237727165222\n",
      "Surface training t=11988, loss=0.02549467608332634\n",
      "Surface training t=11989, loss=0.023537653498351574\n",
      "Surface training t=11990, loss=0.035148163326084614\n",
      "Surface training t=11991, loss=0.025218719616532326\n",
      "Surface training t=11992, loss=0.02755807712674141\n",
      "Surface training t=11993, loss=0.03553532250225544\n",
      "Surface training t=11994, loss=0.026119237765669823\n",
      "Surface training t=11995, loss=0.03161574900150299\n",
      "Surface training t=11996, loss=0.024956670589745045\n",
      "Surface training t=11997, loss=0.042147206142544746\n",
      "Surface training t=11998, loss=0.039715973660349846\n",
      "Surface training t=11999, loss=0.059074776247143745\n",
      "Surface training t=12000, loss=0.05108816921710968\n",
      "Surface training t=12001, loss=0.05090846307575703\n",
      "Surface training t=12002, loss=0.050555091351270676\n",
      "Surface training t=12003, loss=0.05761568807065487\n",
      "Surface training t=12004, loss=0.0468670055270195\n",
      "Surface training t=12005, loss=0.03844892792403698\n",
      "Surface training t=12006, loss=0.03807738609611988\n",
      "Surface training t=12007, loss=0.037056609988212585\n",
      "Surface training t=12008, loss=0.03809552453458309\n",
      "Surface training t=12009, loss=0.042950255796313286\n",
      "Surface training t=12010, loss=0.04138246923685074\n",
      "Surface training t=12011, loss=0.028491671197116375\n",
      "Surface training t=12012, loss=0.03525695577263832\n",
      "Surface training t=12013, loss=0.04803025349974632\n",
      "Surface training t=12014, loss=0.04779152385890484\n",
      "Surface training t=12015, loss=0.030207821168005466\n",
      "Surface training t=12016, loss=0.02234556432813406\n",
      "Surface training t=12017, loss=0.024154536426067352\n",
      "Surface training t=12018, loss=0.022753004450351\n",
      "Surface training t=12019, loss=0.01690408494323492\n",
      "Surface training t=12020, loss=0.026034828275442123\n",
      "Surface training t=12021, loss=0.023687495850026608\n",
      "Surface training t=12022, loss=0.03549807704985142\n",
      "Surface training t=12023, loss=0.0607950147241354\n",
      "Surface training t=12024, loss=0.03371874429285526\n",
      "Surface training t=12025, loss=0.020977308973670006\n",
      "Surface training t=12026, loss=0.021291016601026058\n",
      "Surface training t=12027, loss=0.03014685120433569\n",
      "Surface training t=12028, loss=0.03134159464389086\n",
      "Surface training t=12029, loss=0.037352935411036015\n",
      "Surface training t=12030, loss=0.04715723730623722\n",
      "Surface training t=12031, loss=0.04950089007616043\n",
      "Surface training t=12032, loss=0.0419344287365675\n",
      "Surface training t=12033, loss=0.036140237003564835\n",
      "Surface training t=12034, loss=0.054951706901192665\n",
      "Surface training t=12035, loss=0.04095725156366825\n",
      "Surface training t=12036, loss=0.048979196697473526\n",
      "Surface training t=12037, loss=0.041022032499313354\n",
      "Surface training t=12038, loss=0.05887085758149624\n",
      "Surface training t=12039, loss=0.04949949402362108\n",
      "Surface training t=12040, loss=0.03584039397537708\n",
      "Surface training t=12041, loss=0.0452567283064127\n",
      "Surface training t=12042, loss=0.03475983440876007\n",
      "Surface training t=12043, loss=0.04447989445179701\n",
      "Surface training t=12044, loss=0.03410761058330536\n",
      "Surface training t=12045, loss=0.024056714959442616\n",
      "Surface training t=12046, loss=0.021940256468951702\n",
      "Surface training t=12047, loss=0.026437247171998024\n",
      "Surface training t=12048, loss=0.017051638569682837\n",
      "Surface training t=12049, loss=0.022690112702548504\n",
      "Surface training t=12050, loss=0.02474101260304451\n",
      "Surface training t=12051, loss=0.024713944643735886\n",
      "Surface training t=12052, loss=0.032602181658148766\n",
      "Surface training t=12053, loss=0.035515302792191505\n",
      "Surface training t=12054, loss=0.04431632161140442\n",
      "Surface training t=12055, loss=0.04266455955803394\n",
      "Surface training t=12056, loss=0.02977015357464552\n",
      "Surface training t=12057, loss=0.03403080254793167\n",
      "Surface training t=12058, loss=0.061470191925764084\n",
      "Surface training t=12059, loss=0.038329510018229485\n",
      "Surface training t=12060, loss=0.030999379232525826\n",
      "Surface training t=12061, loss=0.02349769789725542\n",
      "Surface training t=12062, loss=0.026818678714334965\n",
      "Surface training t=12063, loss=0.025723902508616447\n",
      "Surface training t=12064, loss=0.024934529326856136\n",
      "Surface training t=12065, loss=0.029877033084630966\n",
      "Surface training t=12066, loss=0.021634208969771862\n",
      "Surface training t=12067, loss=0.030387088656425476\n",
      "Surface training t=12068, loss=0.037674494087696075\n",
      "Surface training t=12069, loss=0.03150435257703066\n",
      "Surface training t=12070, loss=0.03391789086163044\n",
      "Surface training t=12071, loss=0.0344823244959116\n",
      "Surface training t=12072, loss=0.03706696908921003\n",
      "Surface training t=12073, loss=0.03834998421370983\n",
      "Surface training t=12074, loss=0.03658059611916542\n",
      "Surface training t=12075, loss=0.036243059672415257\n",
      "Surface training t=12076, loss=0.03493156284093857\n",
      "Surface training t=12077, loss=0.03059332352131605\n",
      "Surface training t=12078, loss=0.04676233232021332\n",
      "Surface training t=12079, loss=0.060154855251312256\n",
      "Surface training t=12080, loss=0.044316615909338\n",
      "Surface training t=12081, loss=0.04939717426896095\n",
      "Surface training t=12082, loss=0.03716549649834633\n",
      "Surface training t=12083, loss=0.03715802635997534\n",
      "Surface training t=12084, loss=0.046179864555597305\n",
      "Surface training t=12085, loss=0.034822018817067146\n",
      "Surface training t=12086, loss=0.03576590120792389\n",
      "Surface training t=12087, loss=0.040404120460152626\n",
      "Surface training t=12088, loss=0.04283313266932964\n",
      "Surface training t=12089, loss=0.037737900391221046\n",
      "Surface training t=12090, loss=0.040186962112784386\n",
      "Surface training t=12091, loss=0.03640513867139816\n",
      "Surface training t=12092, loss=0.033597249537706375\n",
      "Surface training t=12093, loss=0.043120430782437325\n",
      "Surface training t=12094, loss=0.028166884556412697\n",
      "Surface training t=12095, loss=0.027841320261359215\n",
      "Surface training t=12096, loss=0.03567872568964958\n",
      "Surface training t=12097, loss=0.03438556380569935\n",
      "Surface training t=12098, loss=0.037495002150535583\n",
      "Surface training t=12099, loss=0.0344496900215745\n",
      "Surface training t=12100, loss=0.0361768938601017\n",
      "Surface training t=12101, loss=0.035073256120085716\n",
      "Surface training t=12102, loss=0.031712282449007034\n",
      "Surface training t=12103, loss=0.02435292210429907\n",
      "Surface training t=12104, loss=0.026026220060884953\n",
      "Surface training t=12105, loss=0.02788361720740795\n",
      "Surface training t=12106, loss=0.04203793406486511\n",
      "Surface training t=12107, loss=0.031435176730155945\n",
      "Surface training t=12108, loss=0.049941105768084526\n",
      "Surface training t=12109, loss=0.03512087091803551\n",
      "Surface training t=12110, loss=0.04106440022587776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=12111, loss=0.037007471546530724\n",
      "Surface training t=12112, loss=0.032898133620619774\n",
      "Surface training t=12113, loss=0.040839679539203644\n",
      "Surface training t=12114, loss=0.03845112584531307\n",
      "Surface training t=12115, loss=0.041061434894800186\n",
      "Surface training t=12116, loss=0.04091397114098072\n",
      "Surface training t=12117, loss=0.053684353828430176\n",
      "Surface training t=12118, loss=0.048611389473080635\n",
      "Surface training t=12119, loss=0.04987514764070511\n",
      "Surface training t=12120, loss=0.04526483453810215\n",
      "Surface training t=12121, loss=0.04402361065149307\n",
      "Surface training t=12122, loss=0.0339730279520154\n",
      "Surface training t=12123, loss=0.02605330105870962\n",
      "Surface training t=12124, loss=0.031103357672691345\n",
      "Surface training t=12125, loss=0.03080659918487072\n",
      "Surface training t=12126, loss=0.024456007406115532\n",
      "Surface training t=12127, loss=0.024383360520005226\n",
      "Surface training t=12128, loss=0.025348429568111897\n",
      "Surface training t=12129, loss=0.024710568599402905\n",
      "Surface training t=12130, loss=0.024061291478574276\n",
      "Surface training t=12131, loss=0.02610040921717882\n",
      "Surface training t=12132, loss=0.019958512857556343\n",
      "Surface training t=12133, loss=0.022709772922098637\n",
      "Surface training t=12134, loss=0.023640542291104794\n",
      "Surface training t=12135, loss=0.02725403942167759\n",
      "Surface training t=12136, loss=0.03382685873657465\n",
      "Surface training t=12137, loss=0.03909685276448727\n",
      "Surface training t=12138, loss=0.030661435797810555\n",
      "Surface training t=12139, loss=0.027002183720469475\n",
      "Surface training t=12140, loss=0.03322327136993408\n",
      "Surface training t=12141, loss=0.035092856734991074\n",
      "Surface training t=12142, loss=0.029008034616708755\n",
      "Surface training t=12143, loss=0.03499991074204445\n",
      "Surface training t=12144, loss=0.03383883275091648\n",
      "Surface training t=12145, loss=0.050344113260507584\n",
      "Surface training t=12146, loss=0.059904562309384346\n",
      "Surface training t=12147, loss=0.047729870304465294\n",
      "Surface training t=12148, loss=0.04066145420074463\n",
      "Surface training t=12149, loss=0.04011754225939512\n",
      "Surface training t=12150, loss=0.07016756013035774\n",
      "Surface training t=12151, loss=0.04739968851208687\n",
      "Surface training t=12152, loss=0.05185139365494251\n",
      "Surface training t=12153, loss=0.05929350294172764\n",
      "Surface training t=12154, loss=0.07301557436585426\n",
      "Surface training t=12155, loss=0.05018150992691517\n",
      "Surface training t=12156, loss=0.03761842008680105\n",
      "Surface training t=12157, loss=0.03623384051024914\n",
      "Surface training t=12158, loss=0.03365989029407501\n",
      "Surface training t=12159, loss=0.04452211409807205\n",
      "Surface training t=12160, loss=0.047641441226005554\n",
      "Surface training t=12161, loss=0.041451595723629\n",
      "Surface training t=12162, loss=0.04067052714526653\n",
      "Surface training t=12163, loss=0.04590313881635666\n",
      "Surface training t=12164, loss=0.029271936044096947\n",
      "Surface training t=12165, loss=0.034286318346858025\n",
      "Surface training t=12166, loss=0.033955106511712074\n",
      "Surface training t=12167, loss=0.025723163969814777\n",
      "Surface training t=12168, loss=0.0226756501942873\n",
      "Surface training t=12169, loss=0.03193962946534157\n",
      "Surface training t=12170, loss=0.033629368990659714\n",
      "Surface training t=12171, loss=0.037915538996458054\n",
      "Surface training t=12172, loss=0.03924525901675224\n",
      "Surface training t=12173, loss=0.04264504462480545\n",
      "Surface training t=12174, loss=0.039355693385005\n",
      "Surface training t=12175, loss=0.04692487046122551\n",
      "Surface training t=12176, loss=0.07367703691124916\n",
      "Surface training t=12177, loss=0.04954355675727129\n",
      "Surface training t=12178, loss=0.07912030257284641\n",
      "Surface training t=12179, loss=0.07403819635510445\n",
      "Surface training t=12180, loss=0.05315248668193817\n",
      "Surface training t=12181, loss=0.05259174853563309\n",
      "Surface training t=12182, loss=0.05815239995718002\n",
      "Surface training t=12183, loss=0.043906548991799355\n",
      "Surface training t=12184, loss=0.05222043301910162\n",
      "Surface training t=12185, loss=0.05395296402275562\n",
      "Surface training t=12186, loss=0.10239441692829132\n",
      "Surface training t=12187, loss=0.061328671872615814\n",
      "Surface training t=12188, loss=0.08320915326476097\n",
      "Surface training t=12189, loss=0.07017089799046516\n",
      "Surface training t=12190, loss=0.05635911226272583\n",
      "Surface training t=12191, loss=0.057990141212940216\n",
      "Surface training t=12192, loss=0.06452091038227081\n",
      "Surface training t=12193, loss=0.048783911392092705\n",
      "Surface training t=12194, loss=0.05460579693317413\n",
      "Surface training t=12195, loss=0.07345719262957573\n",
      "Surface training t=12196, loss=0.04619571752846241\n",
      "Surface training t=12197, loss=0.06568756327033043\n",
      "Surface training t=12198, loss=0.044536199420690536\n",
      "Surface training t=12199, loss=0.038174280896782875\n",
      "Surface training t=12200, loss=0.05386429280042648\n",
      "Surface training t=12201, loss=0.03330687899142504\n",
      "Surface training t=12202, loss=0.040168240666389465\n",
      "Surface training t=12203, loss=0.03232218511402607\n",
      "Surface training t=12204, loss=0.034456390887498856\n",
      "Surface training t=12205, loss=0.034596432000398636\n",
      "Surface training t=12206, loss=0.032150465063750744\n",
      "Surface training t=12207, loss=0.02809292171150446\n",
      "Surface training t=12208, loss=0.021999786607921124\n",
      "Surface training t=12209, loss=0.02089724550023675\n",
      "Surface training t=12210, loss=0.017279747873544693\n",
      "Surface training t=12211, loss=0.019905495457351208\n",
      "Surface training t=12212, loss=0.02249376755207777\n",
      "Surface training t=12213, loss=0.029154776595532894\n",
      "Surface training t=12214, loss=0.028560331091284752\n",
      "Surface training t=12215, loss=0.030690803192555904\n",
      "Surface training t=12216, loss=0.03330918774008751\n",
      "Surface training t=12217, loss=0.04048966243863106\n",
      "Surface training t=12218, loss=0.03280930407345295\n",
      "Surface training t=12219, loss=0.03056272119283676\n",
      "Surface training t=12220, loss=0.03706668969243765\n",
      "Surface training t=12221, loss=0.02541422378271818\n",
      "Surface training t=12222, loss=0.027516783215105534\n",
      "Surface training t=12223, loss=0.02342742495238781\n",
      "Surface training t=12224, loss=0.03442735597491264\n",
      "Surface training t=12225, loss=0.0452938973903656\n",
      "Surface training t=12226, loss=0.06295546889305115\n",
      "Surface training t=12227, loss=0.04449807479977608\n",
      "Surface training t=12228, loss=0.04747413098812103\n",
      "Surface training t=12229, loss=0.07830575853586197\n",
      "Surface training t=12230, loss=0.05528477858752012\n",
      "Surface training t=12231, loss=0.060967886820435524\n",
      "Surface training t=12232, loss=0.060821738094091415\n",
      "Surface training t=12233, loss=0.04590829461812973\n",
      "Surface training t=12234, loss=0.04086492769420147\n",
      "Surface training t=12235, loss=0.04070317838340998\n",
      "Surface training t=12236, loss=0.04327899031341076\n",
      "Surface training t=12237, loss=0.03797830641269684\n",
      "Surface training t=12238, loss=0.037310369312763214\n",
      "Surface training t=12239, loss=0.0259110564365983\n",
      "Surface training t=12240, loss=0.03298737853765488\n",
      "Surface training t=12241, loss=0.03833097033202648\n",
      "Surface training t=12242, loss=0.03670342545956373\n",
      "Surface training t=12243, loss=0.036798639222979546\n",
      "Surface training t=12244, loss=0.04000961408019066\n",
      "Surface training t=12245, loss=0.04224710911512375\n",
      "Surface training t=12246, loss=0.045384978875517845\n",
      "Surface training t=12247, loss=0.04947434179484844\n",
      "Surface training t=12248, loss=0.034140465781092644\n",
      "Surface training t=12249, loss=0.04298540577292442\n",
      "Surface training t=12250, loss=0.0441450159996748\n",
      "Surface training t=12251, loss=0.04507638141512871\n",
      "Surface training t=12252, loss=0.04726887866854668\n",
      "Surface training t=12253, loss=0.04904091916978359\n",
      "Surface training t=12254, loss=0.04761245474219322\n",
      "Surface training t=12255, loss=0.0353304548189044\n",
      "Surface training t=12256, loss=0.031332097947597504\n",
      "Surface training t=12257, loss=0.030754108913242817\n",
      "Surface training t=12258, loss=0.029574728570878506\n",
      "Surface training t=12259, loss=0.03295351378619671\n",
      "Surface training t=12260, loss=0.027574210427701473\n",
      "Surface training t=12261, loss=0.02995105180889368\n",
      "Surface training t=12262, loss=0.035181015729904175\n",
      "Surface training t=12263, loss=0.0395625326782465\n",
      "Surface training t=12264, loss=0.03515406604856253\n",
      "Surface training t=12265, loss=0.03214822802692652\n",
      "Surface training t=12266, loss=0.03897990845143795\n",
      "Surface training t=12267, loss=0.060614803805947304\n",
      "Surface training t=12268, loss=0.03279111161828041\n",
      "Surface training t=12269, loss=0.032153707928955555\n",
      "Surface training t=12270, loss=0.031688581220805645\n",
      "Surface training t=12271, loss=0.03491240553557873\n",
      "Surface training t=12272, loss=0.03325882367789745\n",
      "Surface training t=12273, loss=0.04662672337144613\n",
      "Surface training t=12274, loss=0.045000696554780006\n",
      "Surface training t=12275, loss=0.04660049080848694\n",
      "Surface training t=12276, loss=0.03375657834112644\n",
      "Surface training t=12277, loss=0.03249399643391371\n",
      "Surface training t=12278, loss=0.025863327085971832\n",
      "Surface training t=12279, loss=0.025530322454869747\n",
      "Surface training t=12280, loss=0.02701669931411743\n",
      "Surface training t=12281, loss=0.0396725507453084\n",
      "Surface training t=12282, loss=0.039444515481591225\n",
      "Surface training t=12283, loss=0.0447534266859293\n",
      "Surface training t=12284, loss=0.06707309186458588\n",
      "Surface training t=12285, loss=0.05481950379908085\n",
      "Surface training t=12286, loss=0.05433288961648941\n",
      "Surface training t=12287, loss=0.061667799949645996\n",
      "Surface training t=12288, loss=0.06331018172204494\n",
      "Surface training t=12289, loss=0.047184910625219345\n",
      "Surface training t=12290, loss=0.04370244778692722\n",
      "Surface training t=12291, loss=0.047854579985141754\n",
      "Surface training t=12292, loss=0.033164625987410545\n",
      "Surface training t=12293, loss=0.04592656344175339\n",
      "Surface training t=12294, loss=0.03629598766565323\n",
      "Surface training t=12295, loss=0.0476359948515892\n",
      "Surface training t=12296, loss=0.04543114826083183\n",
      "Surface training t=12297, loss=0.03920242004096508\n",
      "Surface training t=12298, loss=0.05390145629644394\n",
      "Surface training t=12299, loss=0.04284699633717537\n",
      "Surface training t=12300, loss=0.04292485862970352\n",
      "Surface training t=12301, loss=0.033260936848819256\n",
      "Surface training t=12302, loss=0.06619828194379807\n",
      "Surface training t=12303, loss=0.03957704361528158\n",
      "Surface training t=12304, loss=0.04773146100342274\n",
      "Surface training t=12305, loss=0.034961861558258533\n",
      "Surface training t=12306, loss=0.059373585507273674\n",
      "Surface training t=12307, loss=0.04168887995183468\n",
      "Surface training t=12308, loss=0.06191146932542324\n",
      "Surface training t=12309, loss=0.03222079947590828\n",
      "Surface training t=12310, loss=0.057285599410533905\n",
      "Surface training t=12311, loss=0.03623204305768013\n",
      "Surface training t=12312, loss=0.03542507067322731\n",
      "Surface training t=12313, loss=0.04847865737974644\n",
      "Surface training t=12314, loss=0.03427486214786768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=12315, loss=0.0307550011202693\n",
      "Surface training t=12316, loss=0.0297624496743083\n",
      "Surface training t=12317, loss=0.02910312358289957\n",
      "Surface training t=12318, loss=0.03625021409243345\n",
      "Surface training t=12319, loss=0.028089764527976513\n",
      "Surface training t=12320, loss=0.03050406277179718\n",
      "Surface training t=12321, loss=0.04387149214744568\n",
      "Surface training t=12322, loss=0.03292839974164963\n",
      "Surface training t=12323, loss=0.04163946956396103\n",
      "Surface training t=12324, loss=0.03223246615380049\n",
      "Surface training t=12325, loss=0.029054217040538788\n",
      "Surface training t=12326, loss=0.041308676823973656\n",
      "Surface training t=12327, loss=0.02895650640130043\n",
      "Surface training t=12328, loss=0.026150323450565338\n",
      "Surface training t=12329, loss=0.032121023163199425\n",
      "Surface training t=12330, loss=0.026767240837216377\n",
      "Surface training t=12331, loss=0.03386533632874489\n",
      "Surface training t=12332, loss=0.029870194382965565\n",
      "Surface training t=12333, loss=0.03341282531619072\n",
      "Surface training t=12334, loss=0.03170268144458532\n",
      "Surface training t=12335, loss=0.030471906065940857\n",
      "Surface training t=12336, loss=0.03328107111155987\n",
      "Surface training t=12337, loss=0.024066049605607986\n",
      "Surface training t=12338, loss=0.029861589893698692\n",
      "Surface training t=12339, loss=0.037479473277926445\n",
      "Surface training t=12340, loss=0.03648386709392071\n",
      "Surface training t=12341, loss=0.041501984000205994\n",
      "Surface training t=12342, loss=0.038972143083810806\n",
      "Surface training t=12343, loss=0.04902007803320885\n",
      "Surface training t=12344, loss=0.036414528265595436\n",
      "Surface training t=12345, loss=0.02573335636407137\n",
      "Surface training t=12346, loss=0.03449399396777153\n",
      "Surface training t=12347, loss=0.03783908113837242\n",
      "Surface training t=12348, loss=0.030347472056746483\n",
      "Surface training t=12349, loss=0.04165704548358917\n",
      "Surface training t=12350, loss=0.030283947475254536\n",
      "Surface training t=12351, loss=0.028151477687060833\n",
      "Surface training t=12352, loss=0.03033613134175539\n",
      "Surface training t=12353, loss=0.025388221256434917\n",
      "Surface training t=12354, loss=0.021570741198956966\n",
      "Surface training t=12355, loss=0.025420840829610825\n",
      "Surface training t=12356, loss=0.027328559197485447\n",
      "Surface training t=12357, loss=0.029171286150813103\n",
      "Surface training t=12358, loss=0.026507234200835228\n",
      "Surface training t=12359, loss=0.02876550890505314\n",
      "Surface training t=12360, loss=0.02527738455682993\n",
      "Surface training t=12361, loss=0.054150836542248726\n",
      "Surface training t=12362, loss=0.05209682136774063\n",
      "Surface training t=12363, loss=0.04664291813969612\n",
      "Surface training t=12364, loss=0.060673924162983894\n",
      "Surface training t=12365, loss=0.0508884321898222\n",
      "Surface training t=12366, loss=0.042436081916093826\n",
      "Surface training t=12367, loss=0.03594537451863289\n",
      "Surface training t=12368, loss=0.039263274520635605\n",
      "Surface training t=12369, loss=0.0389140285551548\n",
      "Surface training t=12370, loss=0.0338668180629611\n",
      "Surface training t=12371, loss=0.03609903156757355\n",
      "Surface training t=12372, loss=0.039681414142251015\n",
      "Surface training t=12373, loss=0.03984042815864086\n",
      "Surface training t=12374, loss=0.03488069958984852\n",
      "Surface training t=12375, loss=0.03721787501126528\n",
      "Surface training t=12376, loss=0.049157969653606415\n",
      "Surface training t=12377, loss=0.06566046550869942\n",
      "Surface training t=12378, loss=0.047817543148994446\n",
      "Surface training t=12379, loss=0.03259846195578575\n",
      "Surface training t=12380, loss=0.03434528410434723\n",
      "Surface training t=12381, loss=0.058143822476267815\n",
      "Surface training t=12382, loss=0.041471198201179504\n",
      "Surface training t=12383, loss=0.03324577305465937\n",
      "Surface training t=12384, loss=0.03375734016299248\n",
      "Surface training t=12385, loss=0.02942838706076145\n",
      "Surface training t=12386, loss=0.04114348068833351\n",
      "Surface training t=12387, loss=0.04669257625937462\n",
      "Surface training t=12388, loss=0.043368831276893616\n",
      "Surface training t=12389, loss=0.04010738618671894\n",
      "Surface training t=12390, loss=0.03111635148525238\n",
      "Surface training t=12391, loss=0.042583195492625237\n",
      "Surface training t=12392, loss=0.04220614396035671\n",
      "Surface training t=12393, loss=0.0483523104339838\n",
      "Surface training t=12394, loss=0.04000884108245373\n",
      "Surface training t=12395, loss=0.037326108664274216\n",
      "Surface training t=12396, loss=0.04083302430808544\n",
      "Surface training t=12397, loss=0.035160805098712444\n",
      "Surface training t=12398, loss=0.044528404250741005\n",
      "Surface training t=12399, loss=0.05363406799733639\n",
      "Surface training t=12400, loss=0.052103957161307335\n",
      "Surface training t=12401, loss=0.03989861160516739\n",
      "Surface training t=12402, loss=0.05139268934726715\n",
      "Surface training t=12403, loss=0.038501420989632607\n",
      "Surface training t=12404, loss=0.036485252901911736\n",
      "Surface training t=12405, loss=0.046270204707980156\n",
      "Surface training t=12406, loss=0.04334612004458904\n",
      "Surface training t=12407, loss=0.038897816091775894\n",
      "Surface training t=12408, loss=0.0339969415217638\n",
      "Surface training t=12409, loss=0.04783933237195015\n",
      "Surface training t=12410, loss=0.042502932250499725\n",
      "Surface training t=12411, loss=0.049552736803889275\n",
      "Surface training t=12412, loss=0.03815857321023941\n",
      "Surface training t=12413, loss=0.022598404437303543\n",
      "Surface training t=12414, loss=0.035013667307794094\n",
      "Surface training t=12415, loss=0.0321880467236042\n",
      "Surface training t=12416, loss=0.021890236996114254\n",
      "Surface training t=12417, loss=0.025628372095525265\n",
      "Surface training t=12418, loss=0.025558643974363804\n",
      "Surface training t=12419, loss=0.022304379381239414\n",
      "Surface training t=12420, loss=0.02769464999437332\n",
      "Surface training t=12421, loss=0.03487162105739117\n",
      "Surface training t=12422, loss=0.04258210398256779\n",
      "Surface training t=12423, loss=0.04410388134419918\n",
      "Surface training t=12424, loss=0.04106734599918127\n",
      "Surface training t=12425, loss=0.037924304604530334\n",
      "Surface training t=12426, loss=0.031661126762628555\n",
      "Surface training t=12427, loss=0.0269319424405694\n",
      "Surface training t=12428, loss=0.0252620130777359\n",
      "Surface training t=12429, loss=0.027663614600896835\n",
      "Surface training t=12430, loss=0.03306430298835039\n",
      "Surface training t=12431, loss=0.038645800203084946\n",
      "Surface training t=12432, loss=0.040374502539634705\n",
      "Surface training t=12433, loss=0.03784971125423908\n",
      "Surface training t=12434, loss=0.03613691218197346\n",
      "Surface training t=12435, loss=0.04575464501976967\n",
      "Surface training t=12436, loss=0.031555941328406334\n",
      "Surface training t=12437, loss=0.0294362623244524\n",
      "Surface training t=12438, loss=0.04308623820543289\n",
      "Surface training t=12439, loss=0.03514753095805645\n",
      "Surface training t=12440, loss=0.02634300384670496\n",
      "Surface training t=12441, loss=0.03594284504652023\n",
      "Surface training t=12442, loss=0.02193571161478758\n",
      "Surface training t=12443, loss=0.022031779401004314\n",
      "Surface training t=12444, loss=0.028183589689433575\n",
      "Surface training t=12445, loss=0.024068632163107395\n",
      "Surface training t=12446, loss=0.02180934976786375\n",
      "Surface training t=12447, loss=0.023470106534659863\n",
      "Surface training t=12448, loss=0.018896136432886124\n",
      "Surface training t=12449, loss=0.02519854437559843\n",
      "Surface training t=12450, loss=0.030125231482088566\n",
      "Surface training t=12451, loss=0.0200284281745553\n",
      "Surface training t=12452, loss=0.020025339908897877\n",
      "Surface training t=12453, loss=0.023052812553942204\n",
      "Surface training t=12454, loss=0.02437381912022829\n",
      "Surface training t=12455, loss=0.027679042890667915\n",
      "Surface training t=12456, loss=0.04177156277000904\n",
      "Surface training t=12457, loss=0.032172445207834244\n",
      "Surface training t=12458, loss=0.029403138905763626\n",
      "Surface training t=12459, loss=0.03759072162210941\n",
      "Surface training t=12460, loss=0.025828486308455467\n",
      "Surface training t=12461, loss=0.02563746925443411\n",
      "Surface training t=12462, loss=0.02583381999284029\n",
      "Surface training t=12463, loss=0.016022298019379377\n",
      "Surface training t=12464, loss=0.025163503363728523\n",
      "Surface training t=12465, loss=0.02889763657003641\n",
      "Surface training t=12466, loss=0.02952899131923914\n",
      "Surface training t=12467, loss=0.03673544153571129\n",
      "Surface training t=12468, loss=0.027346517890691757\n",
      "Surface training t=12469, loss=0.028958319686353207\n",
      "Surface training t=12470, loss=0.02937144972383976\n",
      "Surface training t=12471, loss=0.0346825011074543\n",
      "Surface training t=12472, loss=0.03155255317687988\n",
      "Surface training t=12473, loss=0.0433941762894392\n",
      "Surface training t=12474, loss=0.06435202993452549\n",
      "Surface training t=12475, loss=0.047598617151379585\n",
      "Surface training t=12476, loss=0.04141384735703468\n",
      "Surface training t=12477, loss=0.037118266336619854\n",
      "Surface training t=12478, loss=0.036446463316679\n",
      "Surface training t=12479, loss=0.027417007833719254\n",
      "Surface training t=12480, loss=0.022952372208237648\n",
      "Surface training t=12481, loss=0.02932611759752035\n",
      "Surface training t=12482, loss=0.027636060491204262\n",
      "Surface training t=12483, loss=0.024426245130598545\n",
      "Surface training t=12484, loss=0.02286856807768345\n",
      "Surface training t=12485, loss=0.03129067551344633\n",
      "Surface training t=12486, loss=0.03055106569081545\n",
      "Surface training t=12487, loss=0.034928640350699425\n",
      "Surface training t=12488, loss=0.03477586526423693\n",
      "Surface training t=12489, loss=0.02739572525024414\n",
      "Surface training t=12490, loss=0.031231679022312164\n",
      "Surface training t=12491, loss=0.030121146701276302\n",
      "Surface training t=12492, loss=0.031001383438706398\n",
      "Surface training t=12493, loss=0.02997505571693182\n",
      "Surface training t=12494, loss=0.02236081939190626\n",
      "Surface training t=12495, loss=0.02397724613547325\n",
      "Surface training t=12496, loss=0.02969940844923258\n",
      "Surface training t=12497, loss=0.027963164262473583\n",
      "Surface training t=12498, loss=0.028571483679115772\n",
      "Surface training t=12499, loss=0.048927852883934975\n",
      "Surface training t=12500, loss=0.06664520688354969\n",
      "Surface training t=12501, loss=0.035515252500772476\n",
      "Surface training t=12502, loss=0.045330485329031944\n",
      "Surface training t=12503, loss=0.04083961062133312\n",
      "Surface training t=12504, loss=0.026494687423110008\n",
      "Surface training t=12505, loss=0.026822548359632492\n",
      "Surface training t=12506, loss=0.021882263012230396\n",
      "Surface training t=12507, loss=0.024263864383101463\n",
      "Surface training t=12508, loss=0.05112306773662567\n",
      "Surface training t=12509, loss=0.039732856675982475\n",
      "Surface training t=12510, loss=0.04112856090068817\n",
      "Surface training t=12511, loss=0.03204690292477608\n",
      "Surface training t=12512, loss=0.034335522912442684\n",
      "Surface training t=12513, loss=0.03790389187633991\n",
      "Surface training t=12514, loss=0.029478294774889946\n",
      "Surface training t=12515, loss=0.02931332029402256\n",
      "Surface training t=12516, loss=0.03048960492014885\n",
      "Surface training t=12517, loss=0.04300360754132271\n",
      "Surface training t=12518, loss=0.03975447826087475\n",
      "Surface training t=12519, loss=0.04911000654101372\n",
      "Surface training t=12520, loss=0.039726609364151955\n",
      "Surface training t=12521, loss=0.04066724330186844\n",
      "Surface training t=12522, loss=0.0376146100461483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=12523, loss=0.03374988678842783\n",
      "Surface training t=12524, loss=0.040968263521790504\n",
      "Surface training t=12525, loss=0.03870909661054611\n",
      "Surface training t=12526, loss=0.0413967315107584\n",
      "Surface training t=12527, loss=0.06196424178779125\n",
      "Surface training t=12528, loss=0.04351839516311884\n",
      "Surface training t=12529, loss=0.046451106667518616\n",
      "Surface training t=12530, loss=0.037822870537638664\n",
      "Surface training t=12531, loss=0.04206354729831219\n",
      "Surface training t=12532, loss=0.036365268751978874\n",
      "Surface training t=12533, loss=0.040509020909667015\n",
      "Surface training t=12534, loss=0.042407382279634476\n",
      "Surface training t=12535, loss=0.03541755676269531\n",
      "Surface training t=12536, loss=0.03127353452146053\n",
      "Surface training t=12537, loss=0.03553757630288601\n",
      "Surface training t=12538, loss=0.03150209505110979\n",
      "Surface training t=12539, loss=0.025195199064910412\n",
      "Surface training t=12540, loss=0.029726143926382065\n",
      "Surface training t=12541, loss=0.03773801028728485\n",
      "Surface training t=12542, loss=0.033774154260754585\n",
      "Surface training t=12543, loss=0.0371088944375515\n",
      "Surface training t=12544, loss=0.0290329959243536\n",
      "Surface training t=12545, loss=0.03410298749804497\n",
      "Surface training t=12546, loss=0.03134855069220066\n",
      "Surface training t=12547, loss=0.031023260205984116\n",
      "Surface training t=12548, loss=0.03658402897417545\n",
      "Surface training t=12549, loss=0.03592725656926632\n",
      "Surface training t=12550, loss=0.02953312173485756\n",
      "Surface training t=12551, loss=0.022582314908504486\n",
      "Surface training t=12552, loss=0.022588558495044708\n",
      "Surface training t=12553, loss=0.024413978680968285\n",
      "Surface training t=12554, loss=0.02026695292443037\n",
      "Surface training t=12555, loss=0.028112187050282955\n",
      "Surface training t=12556, loss=0.029882878065109253\n",
      "Surface training t=12557, loss=0.03112637158483267\n",
      "Surface training t=12558, loss=0.03616599831730127\n",
      "Surface training t=12559, loss=0.03361402824521065\n",
      "Surface training t=12560, loss=0.0323417317122221\n",
      "Surface training t=12561, loss=0.023634073790162802\n",
      "Surface training t=12562, loss=0.02438289113342762\n",
      "Surface training t=12563, loss=0.030083397403359413\n",
      "Surface training t=12564, loss=0.022414586506783962\n",
      "Surface training t=12565, loss=0.02444557100534439\n",
      "Surface training t=12566, loss=0.03381575830280781\n",
      "Surface training t=12567, loss=0.04576731659471989\n",
      "Surface training t=12568, loss=0.055227797478437424\n",
      "Surface training t=12569, loss=0.04534855950623751\n",
      "Surface training t=12570, loss=0.03639372996985912\n",
      "Surface training t=12571, loss=0.03859425708651543\n",
      "Surface training t=12572, loss=0.03654628060758114\n",
      "Surface training t=12573, loss=0.044207748025655746\n",
      "Surface training t=12574, loss=0.0512180645018816\n",
      "Surface training t=12575, loss=0.04513257555663586\n",
      "Surface training t=12576, loss=0.03598496224731207\n",
      "Surface training t=12577, loss=0.04245874471962452\n",
      "Surface training t=12578, loss=0.042511165142059326\n",
      "Surface training t=12579, loss=0.03865979425609112\n",
      "Surface training t=12580, loss=0.02969285286962986\n",
      "Surface training t=12581, loss=0.028835758566856384\n",
      "Surface training t=12582, loss=0.04352663829922676\n",
      "Surface training t=12583, loss=0.05364221706986427\n",
      "Surface training t=12584, loss=0.04210119787603617\n",
      "Surface training t=12585, loss=0.051009807735681534\n",
      "Surface training t=12586, loss=0.03223956469446421\n",
      "Surface training t=12587, loss=0.03588624205440283\n",
      "Surface training t=12588, loss=0.029344000853598118\n",
      "Surface training t=12589, loss=0.02428731881082058\n",
      "Surface training t=12590, loss=0.022011288441717625\n",
      "Surface training t=12591, loss=0.024353510700166225\n",
      "Surface training t=12592, loss=0.02213140483945608\n",
      "Surface training t=12593, loss=0.022972709499299526\n",
      "Surface training t=12594, loss=0.024489725939929485\n",
      "Surface training t=12595, loss=0.029534795321524143\n",
      "Surface training t=12596, loss=0.024364352226257324\n",
      "Surface training t=12597, loss=0.03166716545820236\n",
      "Surface training t=12598, loss=0.030162072740495205\n",
      "Surface training t=12599, loss=0.05450162850320339\n",
      "Surface training t=12600, loss=0.04791868291795254\n",
      "Surface training t=12601, loss=0.04394618235528469\n",
      "Surface training t=12602, loss=0.0390064837411046\n",
      "Surface training t=12603, loss=0.029503120109438896\n",
      "Surface training t=12604, loss=0.026524067390710115\n",
      "Surface training t=12605, loss=0.04482063464820385\n",
      "Surface training t=12606, loss=0.05348505266010761\n",
      "Surface training t=12607, loss=0.04643702507019043\n",
      "Surface training t=12608, loss=0.04149938654154539\n",
      "Surface training t=12609, loss=0.03652122151106596\n",
      "Surface training t=12610, loss=0.03704649955034256\n",
      "Surface training t=12611, loss=0.036000609397888184\n",
      "Surface training t=12612, loss=0.03177305404096842\n",
      "Surface training t=12613, loss=0.033635541796684265\n",
      "Surface training t=12614, loss=0.030326695181429386\n",
      "Surface training t=12615, loss=0.03214239701628685\n",
      "Surface training t=12616, loss=0.030502045527100563\n",
      "Surface training t=12617, loss=0.029486642219126225\n",
      "Surface training t=12618, loss=0.02584332786500454\n",
      "Surface training t=12619, loss=0.024635495617985725\n",
      "Surface training t=12620, loss=0.023668345995247364\n",
      "Surface training t=12621, loss=0.024934873916208744\n",
      "Surface training t=12622, loss=0.020412937738001347\n",
      "Surface training t=12623, loss=0.027638712897896767\n",
      "Surface training t=12624, loss=0.030845319852232933\n",
      "Surface training t=12625, loss=0.025496291927993298\n",
      "Surface training t=12626, loss=0.024041786789894104\n",
      "Surface training t=12627, loss=0.02968570403754711\n",
      "Surface training t=12628, loss=0.02412980981171131\n",
      "Surface training t=12629, loss=0.028071893379092216\n",
      "Surface training t=12630, loss=0.0212761415168643\n",
      "Surface training t=12631, loss=0.026440116576850414\n",
      "Surface training t=12632, loss=0.02343176305294037\n",
      "Surface training t=12633, loss=0.028699757531285286\n",
      "Surface training t=12634, loss=0.021444959565997124\n",
      "Surface training t=12635, loss=0.030039538629353046\n",
      "Surface training t=12636, loss=0.02292733173817396\n",
      "Surface training t=12637, loss=0.040384840220212936\n",
      "Surface training t=12638, loss=0.027198879048228264\n",
      "Surface training t=12639, loss=0.02319871261715889\n",
      "Surface training t=12640, loss=0.035411857068538666\n",
      "Surface training t=12641, loss=0.03351289965212345\n",
      "Surface training t=12642, loss=0.030780330300331116\n",
      "Surface training t=12643, loss=0.03684269078075886\n",
      "Surface training t=12644, loss=0.025805816985666752\n",
      "Surface training t=12645, loss=0.0346350884065032\n",
      "Surface training t=12646, loss=0.03551922086626291\n",
      "Surface training t=12647, loss=0.037260448560118675\n",
      "Surface training t=12648, loss=0.03436823934316635\n",
      "Surface training t=12649, loss=0.036384688690304756\n",
      "Surface training t=12650, loss=0.029358016327023506\n",
      "Surface training t=12651, loss=0.022075654938817024\n",
      "Surface training t=12652, loss=0.030153940431773663\n",
      "Surface training t=12653, loss=0.030443718656897545\n",
      "Surface training t=12654, loss=0.02808194886893034\n",
      "Surface training t=12655, loss=0.026378728449344635\n",
      "Surface training t=12656, loss=0.024979290552437305\n",
      "Surface training t=12657, loss=0.03364719916135073\n",
      "Surface training t=12658, loss=0.030383278615772724\n",
      "Surface training t=12659, loss=0.028650233522057533\n",
      "Surface training t=12660, loss=0.02472253330051899\n",
      "Surface training t=12661, loss=0.02971371728926897\n",
      "Surface training t=12662, loss=0.03053274191915989\n",
      "Surface training t=12663, loss=0.03170124441385269\n",
      "Surface training t=12664, loss=0.03133795037865639\n",
      "Surface training t=12665, loss=0.028705520555377007\n",
      "Surface training t=12666, loss=0.02534610964357853\n",
      "Surface training t=12667, loss=0.022823640145361423\n",
      "Surface training t=12668, loss=0.0373164638876915\n",
      "Surface training t=12669, loss=0.02596130035817623\n",
      "Surface training t=12670, loss=0.0287037156522274\n",
      "Surface training t=12671, loss=0.03869685344398022\n",
      "Surface training t=12672, loss=0.07193487510085106\n",
      "Surface training t=12673, loss=0.04328374192118645\n",
      "Surface training t=12674, loss=0.048019527457654476\n",
      "Surface training t=12675, loss=0.03122294880449772\n",
      "Surface training t=12676, loss=0.030860701575875282\n",
      "Surface training t=12677, loss=0.033256168477237225\n",
      "Surface training t=12678, loss=0.04481305368244648\n",
      "Surface training t=12679, loss=0.06484838016331196\n",
      "Surface training t=12680, loss=0.043581776320934296\n",
      "Surface training t=12681, loss=0.04646441340446472\n",
      "Surface training t=12682, loss=0.04971126466989517\n",
      "Surface training t=12683, loss=0.05646168440580368\n",
      "Surface training t=12684, loss=0.042463308200240135\n",
      "Surface training t=12685, loss=0.06406467594206333\n",
      "Surface training t=12686, loss=0.048808543011546135\n",
      "Surface training t=12687, loss=0.04218308813869953\n",
      "Surface training t=12688, loss=0.04014523420482874\n",
      "Surface training t=12689, loss=0.04217124730348587\n",
      "Surface training t=12690, loss=0.04178512655198574\n",
      "Surface training t=12691, loss=0.04702062904834747\n",
      "Surface training t=12692, loss=0.03467560838907957\n",
      "Surface training t=12693, loss=0.048152774572372437\n",
      "Surface training t=12694, loss=0.0420637559145689\n",
      "Surface training t=12695, loss=0.027353437151759863\n",
      "Surface training t=12696, loss=0.03396095894277096\n",
      "Surface training t=12697, loss=0.026145605370402336\n",
      "Surface training t=12698, loss=0.035054128617048264\n",
      "Surface training t=12699, loss=0.03624633885920048\n",
      "Surface training t=12700, loss=0.043553391471505165\n",
      "Surface training t=12701, loss=0.03804426081478596\n",
      "Surface training t=12702, loss=0.0416065463796258\n",
      "Surface training t=12703, loss=0.0417763190343976\n",
      "Surface training t=12704, loss=0.04366789758205414\n",
      "Surface training t=12705, loss=0.046121543273329735\n",
      "Surface training t=12706, loss=0.05966959893703461\n",
      "Surface training t=12707, loss=0.05321529507637024\n",
      "Surface training t=12708, loss=0.0506173400208354\n",
      "Surface training t=12709, loss=0.03577084559947252\n",
      "Surface training t=12710, loss=0.05267391540110111\n",
      "Surface training t=12711, loss=0.048758864402770996\n",
      "Surface training t=12712, loss=0.035982681438326836\n",
      "Surface training t=12713, loss=0.03918511979281902\n",
      "Surface training t=12714, loss=0.033500898629426956\n",
      "Surface training t=12715, loss=0.03454674407839775\n",
      "Surface training t=12716, loss=0.046728529036045074\n",
      "Surface training t=12717, loss=0.05030934512615204\n",
      "Surface training t=12718, loss=0.03225363604724407\n",
      "Surface training t=12719, loss=0.032221498899161816\n",
      "Surface training t=12720, loss=0.032265499234199524\n",
      "Surface training t=12721, loss=0.026868635788559914\n",
      "Surface training t=12722, loss=0.03532964363694191\n",
      "Surface training t=12723, loss=0.02956327050924301\n",
      "Surface training t=12724, loss=0.025911983102560043\n",
      "Surface training t=12725, loss=0.02425792720168829\n",
      "Surface training t=12726, loss=0.023449840024113655\n",
      "Surface training t=12727, loss=0.0235116146504879\n",
      "Surface training t=12728, loss=0.029255371540784836\n",
      "Surface training t=12729, loss=0.026481017470359802\n",
      "Surface training t=12730, loss=0.0228462852537632\n",
      "Surface training t=12731, loss=0.024614551104605198\n",
      "Surface training t=12732, loss=0.026444964110851288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=12733, loss=0.02895941585302353\n",
      "Surface training t=12734, loss=0.02385057881474495\n",
      "Surface training t=12735, loss=0.029258674941956997\n",
      "Surface training t=12736, loss=0.02244978304952383\n",
      "Surface training t=12737, loss=0.026740011759102345\n",
      "Surface training t=12738, loss=0.0251816650852561\n",
      "Surface training t=12739, loss=0.025284024886786938\n",
      "Surface training t=12740, loss=0.024090062826871872\n",
      "Surface training t=12741, loss=0.02036570105701685\n",
      "Surface training t=12742, loss=0.0239832466468215\n",
      "Surface training t=12743, loss=0.023039570078253746\n",
      "Surface training t=12744, loss=0.025804128497838974\n",
      "Surface training t=12745, loss=0.02196631021797657\n",
      "Surface training t=12746, loss=0.022302857600152493\n",
      "Surface training t=12747, loss=0.021173407323658466\n",
      "Surface training t=12748, loss=0.02419914212077856\n",
      "Surface training t=12749, loss=0.02324540074914694\n",
      "Surface training t=12750, loss=0.043722955510020256\n",
      "Surface training t=12751, loss=0.038001591339707375\n",
      "Surface training t=12752, loss=0.04955192096531391\n",
      "Surface training t=12753, loss=0.045260834507644176\n",
      "Surface training t=12754, loss=0.05701776407659054\n",
      "Surface training t=12755, loss=0.06563825905323029\n",
      "Surface training t=12756, loss=0.0640685111284256\n",
      "Surface training t=12757, loss=0.052767686545848846\n",
      "Surface training t=12758, loss=0.03145773150026798\n",
      "Surface training t=12759, loss=0.027406752109527588\n",
      "Surface training t=12760, loss=0.030252818018198013\n",
      "Surface training t=12761, loss=0.03586040996015072\n",
      "Surface training t=12762, loss=0.022184470668435097\n",
      "Surface training t=12763, loss=0.026029841974377632\n",
      "Surface training t=12764, loss=0.034932542592287064\n",
      "Surface training t=12765, loss=0.039803074672818184\n",
      "Surface training t=12766, loss=0.04633809067308903\n",
      "Surface training t=12767, loss=0.03066475223749876\n",
      "Surface training t=12768, loss=0.03098794724792242\n",
      "Surface training t=12769, loss=0.03156651183962822\n",
      "Surface training t=12770, loss=0.04313039034605026\n",
      "Surface training t=12771, loss=0.036038193851709366\n",
      "Surface training t=12772, loss=0.03756721317768097\n",
      "Surface training t=12773, loss=0.036184435710310936\n",
      "Surface training t=12774, loss=0.031406993977725506\n",
      "Surface training t=12775, loss=0.03152712155133486\n",
      "Surface training t=12776, loss=0.025786619633436203\n",
      "Surface training t=12777, loss=0.029468336142599583\n",
      "Surface training t=12778, loss=0.028349418193101883\n",
      "Surface training t=12779, loss=0.024465045891702175\n",
      "Surface training t=12780, loss=0.026689736172556877\n",
      "Surface training t=12781, loss=0.03355008736252785\n",
      "Surface training t=12782, loss=0.028100259602069855\n",
      "Surface training t=12783, loss=0.025250894017517567\n",
      "Surface training t=12784, loss=0.025342145934700966\n",
      "Surface training t=12785, loss=0.029932547360658646\n",
      "Surface training t=12786, loss=0.03493888862431049\n",
      "Surface training t=12787, loss=0.04535766877233982\n",
      "Surface training t=12788, loss=0.04755914956331253\n",
      "Surface training t=12789, loss=0.037951333448290825\n",
      "Surface training t=12790, loss=0.04934683069586754\n",
      "Surface training t=12791, loss=0.03311514854431152\n",
      "Surface training t=12792, loss=0.04357445426285267\n",
      "Surface training t=12793, loss=0.03545102849602699\n",
      "Surface training t=12794, loss=0.03315863665193319\n",
      "Surface training t=12795, loss=0.03758750110864639\n",
      "Surface training t=12796, loss=0.03806482162326574\n",
      "Surface training t=12797, loss=0.036660898476839066\n",
      "Surface training t=12798, loss=0.04889974370598793\n",
      "Surface training t=12799, loss=0.04527716897428036\n",
      "Surface training t=12800, loss=0.04645830579102039\n",
      "Surface training t=12801, loss=0.03846263699233532\n",
      "Surface training t=12802, loss=0.042175681330263615\n",
      "Surface training t=12803, loss=0.06064038537442684\n",
      "Surface training t=12804, loss=0.03897765651345253\n",
      "Surface training t=12805, loss=0.039825478568673134\n",
      "Surface training t=12806, loss=0.041902185417711735\n",
      "Surface training t=12807, loss=0.03079801145941019\n",
      "Surface training t=12808, loss=0.028725759126245975\n",
      "Surface training t=12809, loss=0.031378187239170074\n",
      "Surface training t=12810, loss=0.045784955844283104\n",
      "Surface training t=12811, loss=0.038096558302640915\n",
      "Surface training t=12812, loss=0.03380271978676319\n",
      "Surface training t=12813, loss=0.043328458443284035\n",
      "Surface training t=12814, loss=0.0389320682734251\n",
      "Surface training t=12815, loss=0.04728436656296253\n",
      "Surface training t=12816, loss=0.04348711669445038\n",
      "Surface training t=12817, loss=0.02586857322603464\n",
      "Surface training t=12818, loss=0.03144106641411781\n",
      "Surface training t=12819, loss=0.032272694632411\n",
      "Surface training t=12820, loss=0.06215372122824192\n",
      "Surface training t=12821, loss=0.04291365295648575\n",
      "Surface training t=12822, loss=0.04627910256385803\n",
      "Surface training t=12823, loss=0.06605391576886177\n",
      "Surface training t=12824, loss=0.07152386382222176\n",
      "Surface training t=12825, loss=0.05674307234585285\n",
      "Surface training t=12826, loss=0.06740626692771912\n",
      "Surface training t=12827, loss=0.08011832647025585\n",
      "Surface training t=12828, loss=0.05487864464521408\n",
      "Surface training t=12829, loss=0.06464558839797974\n",
      "Surface training t=12830, loss=0.06022876501083374\n",
      "Surface training t=12831, loss=0.04320141393691301\n",
      "Surface training t=12832, loss=0.044495840556919575\n",
      "Surface training t=12833, loss=0.053583456203341484\n",
      "Surface training t=12834, loss=0.05005522631108761\n",
      "Surface training t=12835, loss=0.03349096328020096\n",
      "Surface training t=12836, loss=0.04462946951389313\n",
      "Surface training t=12837, loss=0.04368213564157486\n",
      "Surface training t=12838, loss=0.04281758517026901\n",
      "Surface training t=12839, loss=0.07890687882900238\n",
      "Surface training t=12840, loss=0.055708544328808784\n",
      "Surface training t=12841, loss=0.06973679177463055\n",
      "Surface training t=12842, loss=0.07285220921039581\n",
      "Surface training t=12843, loss=0.05188530869781971\n",
      "Surface training t=12844, loss=0.057526519522070885\n",
      "Surface training t=12845, loss=0.06486906670033932\n",
      "Surface training t=12846, loss=0.06019067019224167\n",
      "Surface training t=12847, loss=0.05363023839890957\n",
      "Surface training t=12848, loss=0.05816779285669327\n",
      "Surface training t=12849, loss=0.04220285266637802\n",
      "Surface training t=12850, loss=0.03870931826531887\n",
      "Surface training t=12851, loss=0.034430958330631256\n",
      "Surface training t=12852, loss=0.03155195154249668\n",
      "Surface training t=12853, loss=0.03425139095634222\n",
      "Surface training t=12854, loss=0.027262595482170582\n",
      "Surface training t=12855, loss=0.0333690382540226\n",
      "Surface training t=12856, loss=0.03341144509613514\n",
      "Surface training t=12857, loss=0.029957816004753113\n",
      "Surface training t=12858, loss=0.03650061972439289\n",
      "Surface training t=12859, loss=0.02435737382620573\n",
      "Surface training t=12860, loss=0.02811670582741499\n",
      "Surface training t=12861, loss=0.031581914983689785\n",
      "Surface training t=12862, loss=0.03216755390167236\n",
      "Surface training t=12863, loss=0.05025964789092541\n",
      "Surface training t=12864, loss=0.037523459643125534\n",
      "Surface training t=12865, loss=0.04179211147129536\n",
      "Surface training t=12866, loss=0.04158705100417137\n",
      "Surface training t=12867, loss=0.03224436193704605\n",
      "Surface training t=12868, loss=0.02852331381291151\n",
      "Surface training t=12869, loss=0.02625813614577055\n",
      "Surface training t=12870, loss=0.030834607779979706\n",
      "Surface training t=12871, loss=0.02839143853634596\n",
      "Surface training t=12872, loss=0.025040989741683006\n",
      "Surface training t=12873, loss=0.02627640124410391\n",
      "Surface training t=12874, loss=0.01948972325772047\n",
      "Surface training t=12875, loss=0.03237503953278065\n",
      "Surface training t=12876, loss=0.021721191704273224\n",
      "Surface training t=12877, loss=0.02244043443351984\n",
      "Surface training t=12878, loss=0.029711468145251274\n",
      "Surface training t=12879, loss=0.026971684768795967\n",
      "Surface training t=12880, loss=0.02742708846926689\n",
      "Surface training t=12881, loss=0.02991531975567341\n",
      "Surface training t=12882, loss=0.03562591038644314\n",
      "Surface training t=12883, loss=0.027365872636437416\n",
      "Surface training t=12884, loss=0.02712245937436819\n",
      "Surface training t=12885, loss=0.02576166857033968\n",
      "Surface training t=12886, loss=0.019509969279170036\n",
      "Surface training t=12887, loss=0.02155737578868866\n",
      "Surface training t=12888, loss=0.02151814615353942\n",
      "Surface training t=12889, loss=0.025936413556337357\n",
      "Surface training t=12890, loss=0.024388207122683525\n",
      "Surface training t=12891, loss=0.030331443063914776\n",
      "Surface training t=12892, loss=0.024926558136940002\n",
      "Surface training t=12893, loss=0.023719491437077522\n",
      "Surface training t=12894, loss=0.026840098202228546\n",
      "Surface training t=12895, loss=0.03515952546149492\n",
      "Surface training t=12896, loss=0.04060668684542179\n",
      "Surface training t=12897, loss=0.03381726332008839\n",
      "Surface training t=12898, loss=0.03129245433956385\n",
      "Surface training t=12899, loss=0.031527645885944366\n",
      "Surface training t=12900, loss=0.025844857096672058\n",
      "Surface training t=12901, loss=0.03899970091879368\n",
      "Surface training t=12902, loss=0.03362388722598553\n",
      "Surface training t=12903, loss=0.03550342097878456\n",
      "Surface training t=12904, loss=0.026300758123397827\n",
      "Surface training t=12905, loss=0.03522667847573757\n",
      "Surface training t=12906, loss=0.04116174019873142\n",
      "Surface training t=12907, loss=0.03426314704120159\n",
      "Surface training t=12908, loss=0.03823879361152649\n",
      "Surface training t=12909, loss=0.026166772469878197\n",
      "Surface training t=12910, loss=0.038934407755732536\n",
      "Surface training t=12911, loss=0.03293926082551479\n",
      "Surface training t=12912, loss=0.05271921865642071\n",
      "Surface training t=12913, loss=0.03710128366947174\n",
      "Surface training t=12914, loss=0.028379669412970543\n",
      "Surface training t=12915, loss=0.029064747504889965\n",
      "Surface training t=12916, loss=0.024029807187616825\n",
      "Surface training t=12917, loss=0.020664514042437077\n",
      "Surface training t=12918, loss=0.02711308654397726\n",
      "Surface training t=12919, loss=0.022874537855386734\n",
      "Surface training t=12920, loss=0.017523811198771\n",
      "Surface training t=12921, loss=0.019993934780359268\n",
      "Surface training t=12922, loss=0.0221833111718297\n",
      "Surface training t=12923, loss=0.028528181836009026\n",
      "Surface training t=12924, loss=0.02366340160369873\n",
      "Surface training t=12925, loss=0.025996758602559566\n",
      "Surface training t=12926, loss=0.041501207277178764\n",
      "Surface training t=12927, loss=0.02642722614109516\n",
      "Surface training t=12928, loss=0.032791998237371445\n",
      "Surface training t=12929, loss=0.06796632334589958\n",
      "Surface training t=12930, loss=0.049577346071600914\n",
      "Surface training t=12931, loss=0.058866485953330994\n",
      "Surface training t=12932, loss=0.062003109604120255\n",
      "Surface training t=12933, loss=0.03969566151499748\n",
      "Surface training t=12934, loss=0.040357427671551704\n",
      "Surface training t=12935, loss=0.05319191887974739\n",
      "Surface training t=12936, loss=0.04542095772922039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=12937, loss=0.03759945183992386\n",
      "Surface training t=12938, loss=0.03120519034564495\n",
      "Surface training t=12939, loss=0.0325512383133173\n",
      "Surface training t=12940, loss=0.04713527485728264\n",
      "Surface training t=12941, loss=0.026954458095133305\n",
      "Surface training t=12942, loss=0.026915020309388638\n",
      "Surface training t=12943, loss=0.02496248483657837\n",
      "Surface training t=12944, loss=0.026695091277360916\n",
      "Surface training t=12945, loss=0.029625224880874157\n",
      "Surface training t=12946, loss=0.03039347007870674\n",
      "Surface training t=12947, loss=0.03372214548289776\n",
      "Surface training t=12948, loss=0.03768894448876381\n",
      "Surface training t=12949, loss=0.0320248631760478\n",
      "Surface training t=12950, loss=0.035303590819239616\n",
      "Surface training t=12951, loss=0.04353743977844715\n",
      "Surface training t=12952, loss=0.028704657219350338\n",
      "Surface training t=12953, loss=0.03125103656202555\n",
      "Surface training t=12954, loss=0.030002658255398273\n",
      "Surface training t=12955, loss=0.031636917032301426\n",
      "Surface training t=12956, loss=0.025851035490632057\n",
      "Surface training t=12957, loss=0.03102942928671837\n",
      "Surface training t=12958, loss=0.03582260571420193\n",
      "Surface training t=12959, loss=0.026846607215702534\n",
      "Surface training t=12960, loss=0.030001377686858177\n",
      "Surface training t=12961, loss=0.0365713257342577\n",
      "Surface training t=12962, loss=0.03837752714753151\n",
      "Surface training t=12963, loss=0.026802093721926212\n",
      "Surface training t=12964, loss=0.03370943106710911\n",
      "Surface training t=12965, loss=0.023912283591926098\n",
      "Surface training t=12966, loss=0.023684502579271793\n",
      "Surface training t=12967, loss=0.03319756966084242\n",
      "Surface training t=12968, loss=0.037629712373018265\n",
      "Surface training t=12969, loss=0.05401916056871414\n",
      "Surface training t=12970, loss=0.038350651040673256\n",
      "Surface training t=12971, loss=0.04193607717752457\n",
      "Surface training t=12972, loss=0.03553636372089386\n",
      "Surface training t=12973, loss=0.037417213432490826\n",
      "Surface training t=12974, loss=0.04227953590452671\n",
      "Surface training t=12975, loss=0.035678278654813766\n",
      "Surface training t=12976, loss=0.03556466102600098\n",
      "Surface training t=12977, loss=0.029706822708249092\n",
      "Surface training t=12978, loss=0.037251269444823265\n",
      "Surface training t=12979, loss=0.03227265924215317\n",
      "Surface training t=12980, loss=0.023925927467644215\n",
      "Surface training t=12981, loss=0.03932215832173824\n",
      "Surface training t=12982, loss=0.035780603997409344\n",
      "Surface training t=12983, loss=0.040049636736512184\n",
      "Surface training t=12984, loss=0.028577404096722603\n",
      "Surface training t=12985, loss=0.01969436276704073\n",
      "Surface training t=12986, loss=0.027553413063287735\n",
      "Surface training t=12987, loss=0.028576195240020752\n",
      "Surface training t=12988, loss=0.0423391442745924\n",
      "Surface training t=12989, loss=0.026653125882148743\n",
      "Surface training t=12990, loss=0.027229255996644497\n",
      "Surface training t=12991, loss=0.026928297244012356\n",
      "Surface training t=12992, loss=0.0178820937871933\n",
      "Surface training t=12993, loss=0.028527667745947838\n",
      "Surface training t=12994, loss=0.02550415974110365\n",
      "Surface training t=12995, loss=0.02979962434619665\n",
      "Surface training t=12996, loss=0.027226404286921024\n",
      "Surface training t=12997, loss=0.02890910767018795\n",
      "Surface training t=12998, loss=0.0217955419793725\n",
      "Surface training t=12999, loss=0.04646047204732895\n",
      "Surface training t=13000, loss=0.06247512996196747\n",
      "Surface training t=13001, loss=0.040555449202656746\n",
      "Surface training t=13002, loss=0.03587806411087513\n",
      "Surface training t=13003, loss=0.03875535726547241\n",
      "Surface training t=13004, loss=0.036702755838632584\n",
      "Surface training t=13005, loss=0.03367178700864315\n",
      "Surface training t=13006, loss=0.03926830179989338\n",
      "Surface training t=13007, loss=0.04652806464582682\n",
      "Surface training t=13008, loss=0.04215431958436966\n",
      "Surface training t=13009, loss=0.034574586898088455\n",
      "Surface training t=13010, loss=0.053463298827409744\n",
      "Surface training t=13011, loss=0.038142165169119835\n",
      "Surface training t=13012, loss=0.04515182785689831\n",
      "Surface training t=13013, loss=0.037538403645157814\n",
      "Surface training t=13014, loss=0.03907699044793844\n",
      "Surface training t=13015, loss=0.0480265561491251\n",
      "Surface training t=13016, loss=0.03439171612262726\n",
      "Surface training t=13017, loss=0.03214010316878557\n",
      "Surface training t=13018, loss=0.031463487073779106\n",
      "Surface training t=13019, loss=0.025771364569664\n",
      "Surface training t=13020, loss=0.031350573524832726\n",
      "Surface training t=13021, loss=0.03338848799467087\n",
      "Surface training t=13022, loss=0.06486430391669273\n",
      "Surface training t=13023, loss=0.06259636022150517\n",
      "Surface training t=13024, loss=0.050767041742801666\n",
      "Surface training t=13025, loss=0.042697688564658165\n",
      "Surface training t=13026, loss=0.05310326628386974\n",
      "Surface training t=13027, loss=0.04527805186808109\n",
      "Surface training t=13028, loss=0.03310857433825731\n",
      "Surface training t=13029, loss=0.03205409552901983\n",
      "Surface training t=13030, loss=0.0369822159409523\n",
      "Surface training t=13031, loss=0.033395237289369106\n",
      "Surface training t=13032, loss=0.039588455110788345\n",
      "Surface training t=13033, loss=0.04040242172777653\n",
      "Surface training t=13034, loss=0.03174043260514736\n",
      "Surface training t=13035, loss=0.04600308649241924\n",
      "Surface training t=13036, loss=0.04667971283197403\n",
      "Surface training t=13037, loss=0.056863198056817055\n",
      "Surface training t=13038, loss=0.03934541158378124\n",
      "Surface training t=13039, loss=0.035132719203829765\n",
      "Surface training t=13040, loss=0.03661501780152321\n",
      "Surface training t=13041, loss=0.03412683680653572\n",
      "Surface training t=13042, loss=0.034749219194054604\n",
      "Surface training t=13043, loss=0.033796786330640316\n",
      "Surface training t=13044, loss=0.0403138492256403\n",
      "Surface training t=13045, loss=0.034671892412006855\n",
      "Surface training t=13046, loss=0.0313531206920743\n",
      "Surface training t=13047, loss=0.028476163744926453\n",
      "Surface training t=13048, loss=0.03462154604494572\n",
      "Surface training t=13049, loss=0.031878044828772545\n",
      "Surface training t=13050, loss=0.031049164943397045\n",
      "Surface training t=13051, loss=0.031278328970074654\n",
      "Surface training t=13052, loss=0.027315703220665455\n",
      "Surface training t=13053, loss=0.026880992576479912\n",
      "Surface training t=13054, loss=0.03447831980884075\n",
      "Surface training t=13055, loss=0.026129341684281826\n",
      "Surface training t=13056, loss=0.03779280558228493\n",
      "Surface training t=13057, loss=0.04037124291062355\n",
      "Surface training t=13058, loss=0.04361516423523426\n",
      "Surface training t=13059, loss=0.028018778190016747\n",
      "Surface training t=13060, loss=0.03676337189972401\n",
      "Surface training t=13061, loss=0.055894188582897186\n",
      "Surface training t=13062, loss=0.0412024836987257\n",
      "Surface training t=13063, loss=0.034534052945673466\n",
      "Surface training t=13064, loss=0.03561723418533802\n",
      "Surface training t=13065, loss=0.025265446864068508\n",
      "Surface training t=13066, loss=0.03590496815741062\n",
      "Surface training t=13067, loss=0.04210568219423294\n",
      "Surface training t=13068, loss=0.03211169131100178\n",
      "Surface training t=13069, loss=0.03368198499083519\n",
      "Surface training t=13070, loss=0.03173708450049162\n",
      "Surface training t=13071, loss=0.02248454838991165\n",
      "Surface training t=13072, loss=0.03423318266868591\n",
      "Surface training t=13073, loss=0.03565453737974167\n",
      "Surface training t=13074, loss=0.0376374963670969\n",
      "Surface training t=13075, loss=0.03896243590861559\n",
      "Surface training t=13076, loss=0.0362981827929616\n",
      "Surface training t=13077, loss=0.03920341655611992\n",
      "Surface training t=13078, loss=0.03632012754678726\n",
      "Surface training t=13079, loss=0.022232109680771828\n",
      "Surface training t=13080, loss=0.024956475012004375\n",
      "Surface training t=13081, loss=0.022664159536361694\n",
      "Surface training t=13082, loss=0.03712012059986591\n",
      "Surface training t=13083, loss=0.026537617668509483\n",
      "Surface training t=13084, loss=0.03138981573283672\n",
      "Surface training t=13085, loss=0.030710740014910698\n",
      "Surface training t=13086, loss=0.02973413933068514\n",
      "Surface training t=13087, loss=0.023732224479317665\n",
      "Surface training t=13088, loss=0.029806483536958694\n",
      "Surface training t=13089, loss=0.020667454227805138\n",
      "Surface training t=13090, loss=0.02911293227225542\n",
      "Surface training t=13091, loss=0.03245797008275986\n",
      "Surface training t=13092, loss=0.024775519967079163\n",
      "Surface training t=13093, loss=0.020967003889381886\n",
      "Surface training t=13094, loss=0.025541704148054123\n",
      "Surface training t=13095, loss=0.02465834002941847\n",
      "Surface training t=13096, loss=0.02171671949326992\n",
      "Surface training t=13097, loss=0.018125249072909355\n",
      "Surface training t=13098, loss=0.019680003635585308\n",
      "Surface training t=13099, loss=0.027384787797927856\n",
      "Surface training t=13100, loss=0.02186893206089735\n",
      "Surface training t=13101, loss=0.03183907736092806\n",
      "Surface training t=13102, loss=0.024255347438156605\n",
      "Surface training t=13103, loss=0.022755061276257038\n",
      "Surface training t=13104, loss=0.019303131848573685\n",
      "Surface training t=13105, loss=0.021795816719532013\n",
      "Surface training t=13106, loss=0.024258017539978027\n",
      "Surface training t=13107, loss=0.022775334306061268\n",
      "Surface training t=13108, loss=0.022396661341190338\n",
      "Surface training t=13109, loss=0.027630014345049858\n",
      "Surface training t=13110, loss=0.02131350338459015\n",
      "Surface training t=13111, loss=0.030290245078504086\n",
      "Surface training t=13112, loss=0.027113737538456917\n",
      "Surface training t=13113, loss=0.024436648003757\n",
      "Surface training t=13114, loss=0.02346398588269949\n",
      "Surface training t=13115, loss=0.034336439333856106\n",
      "Surface training t=13116, loss=0.03791845962405205\n",
      "Surface training t=13117, loss=0.03164512664079666\n",
      "Surface training t=13118, loss=0.023255223408341408\n",
      "Surface training t=13119, loss=0.01845420617610216\n",
      "Surface training t=13120, loss=0.02377550769597292\n",
      "Surface training t=13121, loss=0.018645373173058033\n",
      "Surface training t=13122, loss=0.021365645341575146\n",
      "Surface training t=13123, loss=0.020584354177117348\n",
      "Surface training t=13124, loss=0.02166743203997612\n",
      "Surface training t=13125, loss=0.022814194671809673\n",
      "Surface training t=13126, loss=0.02063207421451807\n",
      "Surface training t=13127, loss=0.031294578686356544\n",
      "Surface training t=13128, loss=0.02908538281917572\n",
      "Surface training t=13129, loss=0.044410793110728264\n",
      "Surface training t=13130, loss=0.05201845243573189\n",
      "Surface training t=13131, loss=0.028376024216413498\n",
      "Surface training t=13132, loss=0.035422769375145435\n",
      "Surface training t=13133, loss=0.04152658395469189\n",
      "Surface training t=13134, loss=0.0381087576970458\n",
      "Surface training t=13135, loss=0.04048936255276203\n",
      "Surface training t=13136, loss=0.04348170757293701\n",
      "Surface training t=13137, loss=0.04859109967947006\n",
      "Surface training t=13138, loss=0.04251220263540745\n",
      "Surface training t=13139, loss=0.032989030703902245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=13140, loss=0.030755172483623028\n",
      "Surface training t=13141, loss=0.03271886520087719\n",
      "Surface training t=13142, loss=0.03191564232110977\n",
      "Surface training t=13143, loss=0.03565588593482971\n",
      "Surface training t=13144, loss=0.04440913535654545\n",
      "Surface training t=13145, loss=0.04285404086112976\n",
      "Surface training t=13146, loss=0.04937610402703285\n",
      "Surface training t=13147, loss=0.03998059220612049\n",
      "Surface training t=13148, loss=0.03833664767444134\n",
      "Surface training t=13149, loss=0.05293609760701656\n",
      "Surface training t=13150, loss=0.046569064259529114\n",
      "Surface training t=13151, loss=0.04776366055011749\n",
      "Surface training t=13152, loss=0.0452838521450758\n",
      "Surface training t=13153, loss=0.0326506057754159\n",
      "Surface training t=13154, loss=0.03261694312095642\n",
      "Surface training t=13155, loss=0.02529417723417282\n",
      "Surface training t=13156, loss=0.022358537651598454\n",
      "Surface training t=13157, loss=0.0261832345277071\n",
      "Surface training t=13158, loss=0.023879623040556908\n",
      "Surface training t=13159, loss=0.021850804798305035\n",
      "Surface training t=13160, loss=0.027492565102875233\n",
      "Surface training t=13161, loss=0.0253919567912817\n",
      "Surface training t=13162, loss=0.023540135473012924\n",
      "Surface training t=13163, loss=0.02854303363710642\n",
      "Surface training t=13164, loss=0.026682169176638126\n",
      "Surface training t=13165, loss=0.0303575387224555\n",
      "Surface training t=13166, loss=0.03238758910447359\n",
      "Surface training t=13167, loss=0.03541390039026737\n",
      "Surface training t=13168, loss=0.0355474129319191\n",
      "Surface training t=13169, loss=0.03724136762320995\n",
      "Surface training t=13170, loss=0.03557371813803911\n",
      "Surface training t=13171, loss=0.030929027125239372\n",
      "Surface training t=13172, loss=0.023721495643258095\n",
      "Surface training t=13173, loss=0.02615263406187296\n",
      "Surface training t=13174, loss=0.033940695226192474\n",
      "Surface training t=13175, loss=0.031162803061306477\n",
      "Surface training t=13176, loss=0.0316926185041666\n",
      "Surface training t=13177, loss=0.0252276249229908\n",
      "Surface training t=13178, loss=0.028431325685232878\n",
      "Surface training t=13179, loss=0.05380859598517418\n",
      "Surface training t=13180, loss=0.041204145178198814\n",
      "Surface training t=13181, loss=0.05100619979202747\n",
      "Surface training t=13182, loss=0.04776899889111519\n",
      "Surface training t=13183, loss=0.05201100558042526\n",
      "Surface training t=13184, loss=0.03608025424182415\n",
      "Surface training t=13185, loss=0.0359322726726532\n",
      "Surface training t=13186, loss=0.03192650340497494\n",
      "Surface training t=13187, loss=0.021111009642481804\n",
      "Surface training t=13188, loss=0.025892185978591442\n",
      "Surface training t=13189, loss=0.022759676910936832\n",
      "Surface training t=13190, loss=0.03363489732146263\n",
      "Surface training t=13191, loss=0.029009437188506126\n",
      "Surface training t=13192, loss=0.03334416262805462\n",
      "Surface training t=13193, loss=0.02989826165139675\n",
      "Surface training t=13194, loss=0.04302610643208027\n",
      "Surface training t=13195, loss=0.028690416365861893\n",
      "Surface training t=13196, loss=0.030446458607912064\n",
      "Surface training t=13197, loss=0.04058699682354927\n",
      "Surface training t=13198, loss=0.025280578061938286\n",
      "Surface training t=13199, loss=0.02462568413466215\n",
      "Surface training t=13200, loss=0.023877804167568684\n",
      "Surface training t=13201, loss=0.027414090000092983\n",
      "Surface training t=13202, loss=0.02760378923267126\n",
      "Surface training t=13203, loss=0.02407244499772787\n",
      "Surface training t=13204, loss=0.01966097392141819\n",
      "Surface training t=13205, loss=0.025915328413248062\n",
      "Surface training t=13206, loss=0.05709011293947697\n",
      "Surface training t=13207, loss=0.03749885316938162\n",
      "Surface training t=13208, loss=0.0321098193526268\n",
      "Surface training t=13209, loss=0.03695846814662218\n",
      "Surface training t=13210, loss=0.033803828060626984\n",
      "Surface training t=13211, loss=0.030063653364777565\n",
      "Surface training t=13212, loss=0.03057339321821928\n",
      "Surface training t=13213, loss=0.03569398634135723\n",
      "Surface training t=13214, loss=0.02317270915955305\n",
      "Surface training t=13215, loss=0.02584466990083456\n",
      "Surface training t=13216, loss=0.031099400483071804\n",
      "Surface training t=13217, loss=0.02800037059932947\n",
      "Surface training t=13218, loss=0.021212213672697544\n",
      "Surface training t=13219, loss=0.030868010595440865\n",
      "Surface training t=13220, loss=0.03240925818681717\n",
      "Surface training t=13221, loss=0.02695943508297205\n",
      "Surface training t=13222, loss=0.028267057612538338\n",
      "Surface training t=13223, loss=0.04068116471171379\n",
      "Surface training t=13224, loss=0.040110135450959206\n",
      "Surface training t=13225, loss=0.036841969937086105\n",
      "Surface training t=13226, loss=0.03980214521288872\n",
      "Surface training t=13227, loss=0.034958794713020325\n",
      "Surface training t=13228, loss=0.029437405988574028\n",
      "Surface training t=13229, loss=0.05072193592786789\n",
      "Surface training t=13230, loss=0.0407419353723526\n",
      "Surface training t=13231, loss=0.03815297316759825\n",
      "Surface training t=13232, loss=0.04403350688517094\n",
      "Surface training t=13233, loss=0.0668304804712534\n",
      "Surface training t=13234, loss=0.0522059453651309\n",
      "Surface training t=13235, loss=0.04355744644999504\n",
      "Surface training t=13236, loss=0.03543333616107702\n",
      "Surface training t=13237, loss=0.032716031186282635\n",
      "Surface training t=13238, loss=0.02948114648461342\n",
      "Surface training t=13239, loss=0.022342249751091003\n",
      "Surface training t=13240, loss=0.030116818845272064\n",
      "Surface training t=13241, loss=0.03134599979966879\n",
      "Surface training t=13242, loss=0.02615312859416008\n",
      "Surface training t=13243, loss=0.02640643808990717\n",
      "Surface training t=13244, loss=0.030820250511169434\n",
      "Surface training t=13245, loss=0.026396669447422028\n",
      "Surface training t=13246, loss=0.03221677616238594\n",
      "Surface training t=13247, loss=0.026386447250843048\n",
      "Surface training t=13248, loss=0.029523540288209915\n",
      "Surface training t=13249, loss=0.029313411563634872\n",
      "Surface training t=13250, loss=0.023079991340637207\n",
      "Surface training t=13251, loss=0.023291954770684242\n",
      "Surface training t=13252, loss=0.03023392055183649\n",
      "Surface training t=13253, loss=0.03548006806522608\n",
      "Surface training t=13254, loss=0.02913620136678219\n",
      "Surface training t=13255, loss=0.024456326849758625\n",
      "Surface training t=13256, loss=0.027598612010478973\n",
      "Surface training t=13257, loss=0.028523088432848454\n",
      "Surface training t=13258, loss=0.028538277372717857\n",
      "Surface training t=13259, loss=0.029444013722240925\n",
      "Surface training t=13260, loss=0.02129646809771657\n",
      "Surface training t=13261, loss=0.028308806009590626\n",
      "Surface training t=13262, loss=0.021699687466025352\n",
      "Surface training t=13263, loss=0.023157857358455658\n",
      "Surface training t=13264, loss=0.020704765804111958\n",
      "Surface training t=13265, loss=0.01838820055127144\n",
      "Surface training t=13266, loss=0.030919483862817287\n",
      "Surface training t=13267, loss=0.029548930935561657\n",
      "Surface training t=13268, loss=0.03839504532516003\n",
      "Surface training t=13269, loss=0.05321299284696579\n",
      "Surface training t=13270, loss=0.040899863466620445\n",
      "Surface training t=13271, loss=0.03341785632073879\n",
      "Surface training t=13272, loss=0.03756161779165268\n",
      "Surface training t=13273, loss=0.04107978194952011\n",
      "Surface training t=13274, loss=0.03590773046016693\n",
      "Surface training t=13275, loss=0.02574065327644348\n",
      "Surface training t=13276, loss=0.0196257708594203\n",
      "Surface training t=13277, loss=0.03294816892594099\n",
      "Surface training t=13278, loss=0.023368467576801777\n",
      "Surface training t=13279, loss=0.027603106573224068\n",
      "Surface training t=13280, loss=0.026795029640197754\n",
      "Surface training t=13281, loss=0.019053340889513493\n",
      "Surface training t=13282, loss=0.02442438341677189\n",
      "Surface training t=13283, loss=0.02057526120916009\n",
      "Surface training t=13284, loss=0.02810668107122183\n",
      "Surface training t=13285, loss=0.01954023167490959\n",
      "Surface training t=13286, loss=0.027568683959543705\n",
      "Surface training t=13287, loss=0.03088922705501318\n",
      "Surface training t=13288, loss=0.02864093706011772\n",
      "Surface training t=13289, loss=0.022801442071795464\n",
      "Surface training t=13290, loss=0.024954847060143948\n",
      "Surface training t=13291, loss=0.02493817824870348\n",
      "Surface training t=13292, loss=0.025921731255948544\n",
      "Surface training t=13293, loss=0.03447457682341337\n",
      "Surface training t=13294, loss=0.023792053572833538\n",
      "Surface training t=13295, loss=0.02114262245595455\n",
      "Surface training t=13296, loss=0.024289097636938095\n",
      "Surface training t=13297, loss=0.024184104055166245\n",
      "Surface training t=13298, loss=0.022819701582193375\n",
      "Surface training t=13299, loss=0.023230482824146748\n",
      "Surface training t=13300, loss=0.017358051147311926\n",
      "Surface training t=13301, loss=0.0237370477989316\n",
      "Surface training t=13302, loss=0.025676033459603786\n",
      "Surface training t=13303, loss=0.02917230688035488\n",
      "Surface training t=13304, loss=0.02630426175892353\n",
      "Surface training t=13305, loss=0.022027943283319473\n",
      "Surface training t=13306, loss=0.03389264550060034\n",
      "Surface training t=13307, loss=0.020056037232279778\n",
      "Surface training t=13308, loss=0.025970148853957653\n",
      "Surface training t=13309, loss=0.02612423151731491\n",
      "Surface training t=13310, loss=0.027448073029518127\n",
      "Surface training t=13311, loss=0.029855528846383095\n",
      "Surface training t=13312, loss=0.029554075561463833\n",
      "Surface training t=13313, loss=0.026147959753870964\n",
      "Surface training t=13314, loss=0.02801382727921009\n",
      "Surface training t=13315, loss=0.0234195152297616\n",
      "Surface training t=13316, loss=0.029201499186456203\n",
      "Surface training t=13317, loss=0.03877091594040394\n",
      "Surface training t=13318, loss=0.03446736000478268\n",
      "Surface training t=13319, loss=0.03201467916369438\n",
      "Surface training t=13320, loss=0.03941269684582949\n",
      "Surface training t=13321, loss=0.05054258182644844\n",
      "Surface training t=13322, loss=0.05125639587640762\n",
      "Surface training t=13323, loss=0.04326598346233368\n",
      "Surface training t=13324, loss=0.039940088987350464\n",
      "Surface training t=13325, loss=0.03497816435992718\n",
      "Surface training t=13326, loss=0.03323958069086075\n",
      "Surface training t=13327, loss=0.050704292953014374\n",
      "Surface training t=13328, loss=0.03522414527833462\n",
      "Surface training t=13329, loss=0.05169903486967087\n",
      "Surface training t=13330, loss=0.043468842282891273\n",
      "Surface training t=13331, loss=0.03463749494403601\n",
      "Surface training t=13332, loss=0.03378703445196152\n",
      "Surface training t=13333, loss=0.037045592442154884\n",
      "Surface training t=13334, loss=0.033416357822716236\n",
      "Surface training t=13335, loss=0.04072560369968414\n",
      "Surface training t=13336, loss=0.03764105495065451\n",
      "Surface training t=13337, loss=0.03166958596557379\n",
      "Surface training t=13338, loss=0.028475471772253513\n",
      "Surface training t=13339, loss=0.02740578632801771\n",
      "Surface training t=13340, loss=0.03598308563232422\n",
      "Surface training t=13341, loss=0.027875647880136967\n",
      "Surface training t=13342, loss=0.028137684799730778\n",
      "Surface training t=13343, loss=0.02603574562817812\n",
      "Surface training t=13344, loss=0.03052180726081133\n",
      "Surface training t=13345, loss=0.03060889709740877\n",
      "Surface training t=13346, loss=0.02308744378387928\n",
      "Surface training t=13347, loss=0.025776811875402927\n",
      "Surface training t=13348, loss=0.034743234515190125\n",
      "Surface training t=13349, loss=0.0332215242087841\n",
      "Surface training t=13350, loss=0.02688067127019167\n",
      "Surface training t=13351, loss=0.03382257837802172\n",
      "Surface training t=13352, loss=0.05588741786777973\n",
      "Surface training t=13353, loss=0.03704954870045185\n",
      "Surface training t=13354, loss=0.03236485831439495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=13355, loss=0.025831490755081177\n",
      "Surface training t=13356, loss=0.04408236965537071\n",
      "Surface training t=13357, loss=0.05459732748568058\n",
      "Surface training t=13358, loss=0.03744134958833456\n",
      "Surface training t=13359, loss=0.03496847953647375\n",
      "Surface training t=13360, loss=0.041187845170497894\n",
      "Surface training t=13361, loss=0.03899003937840462\n",
      "Surface training t=13362, loss=0.048442309722304344\n",
      "Surface training t=13363, loss=0.05767356418073177\n",
      "Surface training t=13364, loss=0.03781318571418524\n",
      "Surface training t=13365, loss=0.04255819506943226\n",
      "Surface training t=13366, loss=0.038129210472106934\n",
      "Surface training t=13367, loss=0.03814986068755388\n",
      "Surface training t=13368, loss=0.043230876326560974\n",
      "Surface training t=13369, loss=0.03444709815084934\n",
      "Surface training t=13370, loss=0.03142274171113968\n",
      "Surface training t=13371, loss=0.031854258850216866\n",
      "Surface training t=13372, loss=0.02909219916909933\n",
      "Surface training t=13373, loss=0.02671030629426241\n",
      "Surface training t=13374, loss=0.022286307066679\n",
      "Surface training t=13375, loss=0.030213394202291965\n",
      "Surface training t=13376, loss=0.03254033997654915\n",
      "Surface training t=13377, loss=0.035529399290680885\n",
      "Surface training t=13378, loss=0.046122562140226364\n",
      "Surface training t=13379, loss=0.055921098217368126\n",
      "Surface training t=13380, loss=0.06260284036397934\n",
      "Surface training t=13381, loss=0.05420893616974354\n",
      "Surface training t=13382, loss=0.037870047613978386\n",
      "Surface training t=13383, loss=0.05038562789559364\n",
      "Surface training t=13384, loss=0.07175816968083382\n",
      "Surface training t=13385, loss=0.04392783157527447\n",
      "Surface training t=13386, loss=0.04052066057920456\n",
      "Surface training t=13387, loss=0.035290103405714035\n",
      "Surface training t=13388, loss=0.03542622551321983\n",
      "Surface training t=13389, loss=0.03153691999614239\n",
      "Surface training t=13390, loss=0.031358351930975914\n",
      "Surface training t=13391, loss=0.024362795054912567\n",
      "Surface training t=13392, loss=0.024365455843508244\n",
      "Surface training t=13393, loss=0.021024607121944427\n",
      "Surface training t=13394, loss=0.024338260293006897\n",
      "Surface training t=13395, loss=0.017171730753034353\n",
      "Surface training t=13396, loss=0.03112044557929039\n",
      "Surface training t=13397, loss=0.020433926954865456\n",
      "Surface training t=13398, loss=0.025699924677610397\n",
      "Surface training t=13399, loss=0.0293455608189106\n",
      "Surface training t=13400, loss=0.031105011701583862\n",
      "Surface training t=13401, loss=0.023150808177888393\n",
      "Surface training t=13402, loss=0.023151599802076817\n",
      "Surface training t=13403, loss=0.026155692525207996\n",
      "Surface training t=13404, loss=0.028017389588057995\n",
      "Surface training t=13405, loss=0.023169303312897682\n",
      "Surface training t=13406, loss=0.03464372083544731\n",
      "Surface training t=13407, loss=0.027093468233942986\n",
      "Surface training t=13408, loss=0.029781198129057884\n",
      "Surface training t=13409, loss=0.021037147380411625\n",
      "Surface training t=13410, loss=0.02217130735516548\n",
      "Surface training t=13411, loss=0.020581657998263836\n",
      "Surface training t=13412, loss=0.024556628428399563\n",
      "Surface training t=13413, loss=0.02392416726797819\n",
      "Surface training t=13414, loss=0.0166275417432189\n",
      "Surface training t=13415, loss=0.025117063894867897\n",
      "Surface training t=13416, loss=0.02043813094496727\n",
      "Surface training t=13417, loss=0.029694552533328533\n",
      "Surface training t=13418, loss=0.02835425455123186\n",
      "Surface training t=13419, loss=0.02611473575234413\n",
      "Surface training t=13420, loss=0.027905032970011234\n",
      "Surface training t=13421, loss=0.030955523252487183\n",
      "Surface training t=13422, loss=0.021043083630502224\n",
      "Surface training t=13423, loss=0.023958735167980194\n",
      "Surface training t=13424, loss=0.018287915736436844\n",
      "Surface training t=13425, loss=0.02325811330229044\n",
      "Surface training t=13426, loss=0.020622420124709606\n",
      "Surface training t=13427, loss=0.02148738782852888\n",
      "Surface training t=13428, loss=0.022606232203543186\n",
      "Surface training t=13429, loss=0.03290373366326094\n",
      "Surface training t=13430, loss=0.024639016948640347\n",
      "Surface training t=13431, loss=0.023957940749824047\n",
      "Surface training t=13432, loss=0.02734319493174553\n",
      "Surface training t=13433, loss=0.04351292923092842\n",
      "Surface training t=13434, loss=0.04417327046394348\n",
      "Surface training t=13435, loss=0.05905386246740818\n",
      "Surface training t=13436, loss=0.05684293806552887\n",
      "Surface training t=13437, loss=0.09432240575551987\n",
      "Surface training t=13438, loss=0.06563552469015121\n",
      "Surface training t=13439, loss=0.06415420025587082\n",
      "Surface training t=13440, loss=0.08274256065487862\n",
      "Surface training t=13441, loss=0.04523441381752491\n",
      "Surface training t=13442, loss=0.08141232281923294\n",
      "Surface training t=13443, loss=0.06971266865730286\n",
      "Surface training t=13444, loss=0.08815896511077881\n",
      "Surface training t=13445, loss=0.0990188904106617\n",
      "Surface training t=13446, loss=0.0520663857460022\n",
      "Surface training t=13447, loss=0.0885562151670456\n",
      "Surface training t=13448, loss=0.04594932869076729\n",
      "Surface training t=13449, loss=0.05572107620537281\n",
      "Surface training t=13450, loss=0.03785883076488972\n",
      "Surface training t=13451, loss=0.04053938575088978\n",
      "Surface training t=13452, loss=0.03539748024195433\n",
      "Surface training t=13453, loss=0.040800461545586586\n",
      "Surface training t=13454, loss=0.024428170174360275\n",
      "Surface training t=13455, loss=0.036851128563284874\n",
      "Surface training t=13456, loss=0.030653072521090508\n",
      "Surface training t=13457, loss=0.027813425287604332\n",
      "Surface training t=13458, loss=0.03262612596154213\n",
      "Surface training t=13459, loss=0.030306929722428322\n",
      "Surface training t=13460, loss=0.04519438184797764\n",
      "Surface training t=13461, loss=0.04132441431283951\n",
      "Surface training t=13462, loss=0.039038754999637604\n",
      "Surface training t=13463, loss=0.043821532279253006\n",
      "Surface training t=13464, loss=0.05882840417325497\n",
      "Surface training t=13465, loss=0.0416253749281168\n",
      "Surface training t=13466, loss=0.03749268688261509\n",
      "Surface training t=13467, loss=0.037561711855232716\n",
      "Surface training t=13468, loss=0.05207606963813305\n",
      "Surface training t=13469, loss=0.033577630296349525\n",
      "Surface training t=13470, loss=0.02598831243813038\n",
      "Surface training t=13471, loss=0.023380454629659653\n",
      "Surface training t=13472, loss=0.029285987839102745\n",
      "Surface training t=13473, loss=0.01917431317269802\n",
      "Surface training t=13474, loss=0.0296226991340518\n",
      "Surface training t=13475, loss=0.023130636662244797\n",
      "Surface training t=13476, loss=0.02676136139780283\n",
      "Surface training t=13477, loss=0.01942539121955633\n",
      "Surface training t=13478, loss=0.02484385296702385\n",
      "Surface training t=13479, loss=0.03176394011825323\n",
      "Surface training t=13480, loss=0.03813982289284468\n",
      "Surface training t=13481, loss=0.0426653828471899\n",
      "Surface training t=13482, loss=0.029764887876808643\n",
      "Surface training t=13483, loss=0.032163300551474094\n",
      "Surface training t=13484, loss=0.04277687892317772\n",
      "Surface training t=13485, loss=0.031459259800612926\n",
      "Surface training t=13486, loss=0.0419267974793911\n",
      "Surface training t=13487, loss=0.03667612001299858\n",
      "Surface training t=13488, loss=0.0244693411514163\n",
      "Surface training t=13489, loss=0.03177603520452976\n",
      "Surface training t=13490, loss=0.030006862245500088\n",
      "Surface training t=13491, loss=0.02552403975278139\n",
      "Surface training t=13492, loss=0.02480954769998789\n",
      "Surface training t=13493, loss=0.025111472234129906\n",
      "Surface training t=13494, loss=0.028411779552698135\n",
      "Surface training t=13495, loss=0.02470836229622364\n",
      "Surface training t=13496, loss=0.03493656124919653\n",
      "Surface training t=13497, loss=0.030016057193279266\n",
      "Surface training t=13498, loss=0.026453974656760693\n",
      "Surface training t=13499, loss=0.031366290524601936\n",
      "Surface training t=13500, loss=0.029426343739032745\n",
      "Surface training t=13501, loss=0.026344556361436844\n",
      "Surface training t=13502, loss=0.019479808397591114\n",
      "Surface training t=13503, loss=0.022079293616116047\n",
      "Surface training t=13504, loss=0.029120846651494503\n",
      "Surface training t=13505, loss=0.02580858487635851\n",
      "Surface training t=13506, loss=0.019130330998450518\n",
      "Surface training t=13507, loss=0.029863239265978336\n",
      "Surface training t=13508, loss=0.020986584946513176\n",
      "Surface training t=13509, loss=0.02206859551370144\n",
      "Surface training t=13510, loss=0.02602058369666338\n",
      "Surface training t=13511, loss=0.02816634811460972\n",
      "Surface training t=13512, loss=0.02116852719336748\n",
      "Surface training t=13513, loss=0.02715421561151743\n",
      "Surface training t=13514, loss=0.03242765553295612\n",
      "Surface training t=13515, loss=0.031973086297512054\n",
      "Surface training t=13516, loss=0.03133998438715935\n",
      "Surface training t=13517, loss=0.025711712427437305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=13518, loss=0.030752241611480713\n",
      "Surface training t=13519, loss=0.02677399292588234\n",
      "Surface training t=13520, loss=0.023830723017454147\n",
      "Surface training t=13521, loss=0.03704587742686272\n",
      "Surface training t=13522, loss=0.026322556659579277\n",
      "Surface training t=13523, loss=0.025705717504024506\n",
      "Surface training t=13524, loss=0.030072779394686222\n",
      "Surface training t=13525, loss=0.02798325102776289\n",
      "Surface training t=13526, loss=0.031204232946038246\n",
      "Surface training t=13527, loss=0.049903204664587975\n",
      "Surface training t=13528, loss=0.0479301493614912\n",
      "Surface training t=13529, loss=0.04562784358859062\n",
      "Surface training t=13530, loss=0.0455595962703228\n",
      "Surface training t=13531, loss=0.06513757072389126\n",
      "Surface training t=13532, loss=0.05577579699456692\n",
      "Surface training t=13533, loss=0.05094816908240318\n",
      "Surface training t=13534, loss=0.07328453101217747\n",
      "Surface training t=13535, loss=0.07139960303902626\n",
      "Surface training t=13536, loss=0.05046548508107662\n",
      "Surface training t=13537, loss=0.04479053430259228\n",
      "Surface training t=13538, loss=0.02953462488949299\n",
      "Surface training t=13539, loss=0.03954462707042694\n",
      "Surface training t=13540, loss=0.05893164686858654\n",
      "Surface training t=13541, loss=0.04040362127125263\n",
      "Surface training t=13542, loss=0.036608412861824036\n",
      "Surface training t=13543, loss=0.030768781900405884\n",
      "Surface training t=13544, loss=0.032179510686546564\n",
      "Surface training t=13545, loss=0.03785110451281071\n",
      "Surface training t=13546, loss=0.0397062823176384\n",
      "Surface training t=13547, loss=0.04893328156322241\n",
      "Surface training t=13548, loss=0.08802817016839981\n",
      "Surface training t=13549, loss=0.04895629175007343\n",
      "Surface training t=13550, loss=0.05132327042520046\n",
      "Surface training t=13551, loss=0.07051629573106766\n",
      "Surface training t=13552, loss=0.055207167752087116\n",
      "Surface training t=13553, loss=0.09131139144301414\n",
      "Surface training t=13554, loss=0.062214478850364685\n",
      "Surface training t=13555, loss=0.047154100611805916\n",
      "Surface training t=13556, loss=0.07411043904721737\n",
      "Surface training t=13557, loss=0.050374044105410576\n",
      "Surface training t=13558, loss=0.04900207370519638\n",
      "Surface training t=13559, loss=0.0686299093067646\n",
      "Surface training t=13560, loss=0.07784422300755978\n",
      "Surface training t=13561, loss=0.05071650631725788\n",
      "Surface training t=13562, loss=0.06626936979591846\n",
      "Surface training t=13563, loss=0.05508064851164818\n",
      "Surface training t=13564, loss=0.04328935779631138\n",
      "Surface training t=13565, loss=0.03868862986564636\n",
      "Surface training t=13566, loss=0.03814706578850746\n",
      "Surface training t=13567, loss=0.04806769825518131\n",
      "Surface training t=13568, loss=0.04551318101584911\n",
      "Surface training t=13569, loss=0.0324285002425313\n",
      "Surface training t=13570, loss=0.032641688361763954\n",
      "Surface training t=13571, loss=0.04256872087717056\n",
      "Surface training t=13572, loss=0.02177216950803995\n",
      "Surface training t=13573, loss=0.028662330470979214\n",
      "Surface training t=13574, loss=0.027877159416675568\n",
      "Surface training t=13575, loss=0.02921524178236723\n",
      "Surface training t=13576, loss=0.027451666072010994\n",
      "Surface training t=13577, loss=0.03148265182971954\n",
      "Surface training t=13578, loss=0.030471818521618843\n",
      "Surface training t=13579, loss=0.02398832980543375\n",
      "Surface training t=13580, loss=0.022982081398367882\n",
      "Surface training t=13581, loss=0.022720509208738804\n",
      "Surface training t=13582, loss=0.02349369414150715\n",
      "Surface training t=13583, loss=0.027935141697525978\n",
      "Surface training t=13584, loss=0.03393544256687164\n",
      "Surface training t=13585, loss=0.02820676565170288\n",
      "Surface training t=13586, loss=0.034770441241562366\n",
      "Surface training t=13587, loss=0.035018108785152435\n",
      "Surface training t=13588, loss=0.029843056574463844\n",
      "Surface training t=13589, loss=0.022766231559216976\n",
      "Surface training t=13590, loss=0.020521407015621662\n",
      "Surface training t=13591, loss=0.024243305437266827\n",
      "Surface training t=13592, loss=0.02552900742739439\n",
      "Surface training t=13593, loss=0.023956842720508575\n",
      "Surface training t=13594, loss=0.026413234882056713\n",
      "Surface training t=13595, loss=0.030750960111618042\n",
      "Surface training t=13596, loss=0.02245223894715309\n",
      "Surface training t=13597, loss=0.02442886494100094\n",
      "Surface training t=13598, loss=0.026520426385104656\n",
      "Surface training t=13599, loss=0.02598494663834572\n",
      "Surface training t=13600, loss=0.026696300134062767\n",
      "Surface training t=13601, loss=0.02874172292649746\n",
      "Surface training t=13602, loss=0.021104389801621437\n",
      "Surface training t=13603, loss=0.021284710615873337\n",
      "Surface training t=13604, loss=0.017526629380881786\n",
      "Surface training t=13605, loss=0.030296118929982185\n",
      "Surface training t=13606, loss=0.02400288637727499\n",
      "Surface training t=13607, loss=0.024436457082629204\n",
      "Surface training t=13608, loss=0.028912583366036415\n",
      "Surface training t=13609, loss=0.0316728912293911\n",
      "Surface training t=13610, loss=0.04587055183947086\n",
      "Surface training t=13611, loss=0.03609801456332207\n",
      "Surface training t=13612, loss=0.03709189407527447\n",
      "Surface training t=13613, loss=0.023607973009347916\n",
      "Surface training t=13614, loss=0.021816186606884003\n",
      "Surface training t=13615, loss=0.02629732433706522\n",
      "Surface training t=13616, loss=0.034403277561068535\n",
      "Surface training t=13617, loss=0.027884860523045063\n",
      "Surface training t=13618, loss=0.025573777966201305\n",
      "Surface training t=13619, loss=0.02756652981042862\n",
      "Surface training t=13620, loss=0.03311421629041433\n",
      "Surface training t=13621, loss=0.04262384958565235\n",
      "Surface training t=13622, loss=0.034912330098450184\n",
      "Surface training t=13623, loss=0.03001951053738594\n",
      "Surface training t=13624, loss=0.05123268626630306\n",
      "Surface training t=13625, loss=0.040472254157066345\n",
      "Surface training t=13626, loss=0.0508038979023695\n",
      "Surface training t=13627, loss=0.03705091215670109\n",
      "Surface training t=13628, loss=0.04229382611811161\n",
      "Surface training t=13629, loss=0.039461562409996986\n",
      "Surface training t=13630, loss=0.03258123621344566\n",
      "Surface training t=13631, loss=0.026847326196730137\n",
      "Surface training t=13632, loss=0.02617811132222414\n",
      "Surface training t=13633, loss=0.027549415826797485\n",
      "Surface training t=13634, loss=0.027042366564273834\n",
      "Surface training t=13635, loss=0.039224918000400066\n",
      "Surface training t=13636, loss=0.06662718206644058\n",
      "Surface training t=13637, loss=0.0349974911659956\n",
      "Surface training t=13638, loss=0.0317644951865077\n",
      "Surface training t=13639, loss=0.028879426419734955\n",
      "Surface training t=13640, loss=0.026229238137602806\n",
      "Surface training t=13641, loss=0.040742507204413414\n",
      "Surface training t=13642, loss=0.026871128007769585\n",
      "Surface training t=13643, loss=0.028582189232110977\n",
      "Surface training t=13644, loss=0.025127566419541836\n",
      "Surface training t=13645, loss=0.022749189287424088\n",
      "Surface training t=13646, loss=0.02376926876604557\n",
      "Surface training t=13647, loss=0.019676110707223415\n",
      "Surface training t=13648, loss=0.025941853411495686\n",
      "Surface training t=13649, loss=0.03228142857551575\n",
      "Surface training t=13650, loss=0.025973031297326088\n",
      "Surface training t=13651, loss=0.03129950165748596\n",
      "Surface training t=13652, loss=0.029434295371174812\n",
      "Surface training t=13653, loss=0.034036193042993546\n",
      "Surface training t=13654, loss=0.02737365011125803\n",
      "Surface training t=13655, loss=0.02553747594356537\n",
      "Surface training t=13656, loss=0.03006730880588293\n",
      "Surface training t=13657, loss=0.03256915509700775\n",
      "Surface training t=13658, loss=0.028129149228334427\n",
      "Surface training t=13659, loss=0.034916577860713005\n",
      "Surface training t=13660, loss=0.029969953931868076\n",
      "Surface training t=13661, loss=0.02590967994183302\n",
      "Surface training t=13662, loss=0.0290373545140028\n",
      "Surface training t=13663, loss=0.045254066586494446\n",
      "Surface training t=13664, loss=0.036404386162757874\n",
      "Surface training t=13665, loss=0.04090743884444237\n",
      "Surface training t=13666, loss=0.03577021509408951\n",
      "Surface training t=13667, loss=0.032351765781641006\n",
      "Surface training t=13668, loss=0.03380280639976263\n",
      "Surface training t=13669, loss=0.0409651305526495\n",
      "Surface training t=13670, loss=0.03541397303342819\n",
      "Surface training t=13671, loss=0.05544855259358883\n",
      "Surface training t=13672, loss=0.0405390989035368\n",
      "Surface training t=13673, loss=0.0483675841242075\n",
      "Surface training t=13674, loss=0.03733862563967705\n",
      "Surface training t=13675, loss=0.04473778232932091\n",
      "Surface training t=13676, loss=0.032873209565877914\n",
      "Surface training t=13677, loss=0.044532520696520805\n",
      "Surface training t=13678, loss=0.03939325921237469\n",
      "Surface training t=13679, loss=0.0488689336925745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=13680, loss=0.035058142617344856\n",
      "Surface training t=13681, loss=0.050320571288466454\n",
      "Surface training t=13682, loss=0.05106909014284611\n",
      "Surface training t=13683, loss=0.04492449201643467\n",
      "Surface training t=13684, loss=0.04398754145950079\n",
      "Surface training t=13685, loss=0.043321847915649414\n",
      "Surface training t=13686, loss=0.03576299175620079\n",
      "Surface training t=13687, loss=0.02741298731416464\n",
      "Surface training t=13688, loss=0.017697406001389027\n",
      "Surface training t=13689, loss=0.023853265680372715\n",
      "Surface training t=13690, loss=0.026923280209302902\n",
      "Surface training t=13691, loss=0.03260529134422541\n",
      "Surface training t=13692, loss=0.031213819980621338\n",
      "Surface training t=13693, loss=0.023400092497467995\n",
      "Surface training t=13694, loss=0.018320227973163128\n",
      "Surface training t=13695, loss=0.022298621013760567\n",
      "Surface training t=13696, loss=0.026345164515078068\n",
      "Surface training t=13697, loss=0.03130707889795303\n",
      "Surface training t=13698, loss=0.03038412518799305\n",
      "Surface training t=13699, loss=0.030631663277745247\n",
      "Surface training t=13700, loss=0.02550343330949545\n",
      "Surface training t=13701, loss=0.041496239602565765\n",
      "Surface training t=13702, loss=0.07912062481045723\n",
      "Surface training t=13703, loss=0.05707823857665062\n",
      "Surface training t=13704, loss=0.06800554320216179\n",
      "Surface training t=13705, loss=0.03960997797548771\n",
      "Surface training t=13706, loss=0.04531977325677872\n",
      "Surface training t=13707, loss=0.037031546235084534\n",
      "Surface training t=13708, loss=0.042595844715833664\n",
      "Surface training t=13709, loss=0.029174418188631535\n",
      "Surface training t=13710, loss=0.025830697268247604\n",
      "Surface training t=13711, loss=0.027527940459549427\n",
      "Surface training t=13712, loss=0.028807434253394604\n",
      "Surface training t=13713, loss=0.03337698057293892\n",
      "Surface training t=13714, loss=0.027910327538847923\n",
      "Surface training t=13715, loss=0.04129071347415447\n",
      "Surface training t=13716, loss=0.044920334592461586\n",
      "Surface training t=13717, loss=0.033017722889781\n",
      "Surface training t=13718, loss=0.048458484932780266\n",
      "Surface training t=13719, loss=0.02985450066626072\n",
      "Surface training t=13720, loss=0.03026145976036787\n",
      "Surface training t=13721, loss=0.026964307762682438\n",
      "Surface training t=13722, loss=0.030839458107948303\n",
      "Surface training t=13723, loss=0.03325008973479271\n",
      "Surface training t=13724, loss=0.02406882680952549\n",
      "Surface training t=13725, loss=0.021766262128949165\n",
      "Surface training t=13726, loss=0.022182834334671497\n",
      "Surface training t=13727, loss=0.028143126517534256\n",
      "Surface training t=13728, loss=0.026948620565235615\n",
      "Surface training t=13729, loss=0.03236984368413687\n",
      "Surface training t=13730, loss=0.037994951009750366\n",
      "Surface training t=13731, loss=0.03848057612776756\n",
      "Surface training t=13732, loss=0.027746668085455894\n",
      "Surface training t=13733, loss=0.036629337817430496\n",
      "Surface training t=13734, loss=0.04607364535331726\n",
      "Surface training t=13735, loss=0.032217610627412796\n",
      "Surface training t=13736, loss=0.034621357917785645\n",
      "Surface training t=13737, loss=0.03816848434507847\n",
      "Surface training t=13738, loss=0.03477117791771889\n",
      "Surface training t=13739, loss=0.04628395102918148\n",
      "Surface training t=13740, loss=0.03775099106132984\n",
      "Surface training t=13741, loss=0.04510080628097057\n",
      "Surface training t=13742, loss=0.035445380955934525\n",
      "Surface training t=13743, loss=0.06869718059897423\n",
      "Surface training t=13744, loss=0.0516985896974802\n",
      "Surface training t=13745, loss=0.05176177993416786\n",
      "Surface training t=13746, loss=0.03517747297883034\n",
      "Surface training t=13747, loss=0.025274316780269146\n",
      "Surface training t=13748, loss=0.028947845101356506\n",
      "Surface training t=13749, loss=0.03149519767612219\n",
      "Surface training t=13750, loss=0.03892180509865284\n",
      "Surface training t=13751, loss=0.04331008717417717\n",
      "Surface training t=13752, loss=0.03875005058944225\n",
      "Surface training t=13753, loss=0.0396821815520525\n",
      "Surface training t=13754, loss=0.027012092992663383\n",
      "Surface training t=13755, loss=0.025020881555974483\n",
      "Surface training t=13756, loss=0.028232164680957794\n",
      "Surface training t=13757, loss=0.026464851573109627\n",
      "Surface training t=13758, loss=0.027621672488749027\n",
      "Surface training t=13759, loss=0.03782094735652208\n",
      "Surface training t=13760, loss=0.04053059034049511\n",
      "Surface training t=13761, loss=0.03895486146211624\n",
      "Surface training t=13762, loss=0.03456123452633619\n",
      "Surface training t=13763, loss=0.042221637442708015\n",
      "Surface training t=13764, loss=0.03358268924057484\n",
      "Surface training t=13765, loss=0.04642964527010918\n",
      "Surface training t=13766, loss=0.042129768058657646\n",
      "Surface training t=13767, loss=0.04963900335133076\n",
      "Surface training t=13768, loss=0.04307186231017113\n",
      "Surface training t=13769, loss=0.058963555842638016\n",
      "Surface training t=13770, loss=0.04455437511205673\n",
      "Surface training t=13771, loss=0.047041233628988266\n",
      "Surface training t=13772, loss=0.03908955305814743\n",
      "Surface training t=13773, loss=0.03628655709326267\n",
      "Surface training t=13774, loss=0.03723317012190819\n",
      "Surface training t=13775, loss=0.03339399676769972\n",
      "Surface training t=13776, loss=0.03589748311787844\n",
      "Surface training t=13777, loss=0.031657833606004715\n",
      "Surface training t=13778, loss=0.02364045660942793\n",
      "Surface training t=13779, loss=0.02708675805479288\n",
      "Surface training t=13780, loss=0.021561358124017715\n",
      "Surface training t=13781, loss=0.03464043140411377\n",
      "Surface training t=13782, loss=0.03792363032698631\n",
      "Surface training t=13783, loss=0.03122500516474247\n",
      "Surface training t=13784, loss=0.029089692048728466\n",
      "Surface training t=13785, loss=0.029548855498433113\n",
      "Surface training t=13786, loss=0.03054555505514145\n",
      "Surface training t=13787, loss=0.03268733713775873\n",
      "Surface training t=13788, loss=0.037015726789832115\n",
      "Surface training t=13789, loss=0.029391629621386528\n",
      "Surface training t=13790, loss=0.021397276781499386\n",
      "Surface training t=13791, loss=0.03289364371448755\n",
      "Surface training t=13792, loss=0.029160174541175365\n",
      "Surface training t=13793, loss=0.046421829611063004\n",
      "Surface training t=13794, loss=0.03704557940363884\n",
      "Surface training t=13795, loss=0.03644762374460697\n",
      "Surface training t=13796, loss=0.03908371739089489\n",
      "Surface training t=13797, loss=0.03552538435906172\n",
      "Surface training t=13798, loss=0.045979006215929985\n",
      "Surface training t=13799, loss=0.03807699307799339\n",
      "Surface training t=13800, loss=0.027968275360763073\n",
      "Surface training t=13801, loss=0.03290417045354843\n",
      "Surface training t=13802, loss=0.033240366727113724\n",
      "Surface training t=13803, loss=0.025287526659667492\n",
      "Surface training t=13804, loss=0.025961047038435936\n",
      "Surface training t=13805, loss=0.023016619496047497\n",
      "Surface training t=13806, loss=0.024936504662036896\n",
      "Surface training t=13807, loss=0.023420248180627823\n",
      "Surface training t=13808, loss=0.025403444655239582\n",
      "Surface training t=13809, loss=0.027910687029361725\n",
      "Surface training t=13810, loss=0.03776142746210098\n",
      "Surface training t=13811, loss=0.03330831602215767\n",
      "Surface training t=13812, loss=0.04392717964947224\n",
      "Surface training t=13813, loss=0.03582381084561348\n",
      "Surface training t=13814, loss=0.037391955964267254\n",
      "Surface training t=13815, loss=0.03313238173723221\n",
      "Surface training t=13816, loss=0.022728124633431435\n",
      "Surface training t=13817, loss=0.03211703896522522\n",
      "Surface training t=13818, loss=0.025624689646065235\n",
      "Surface training t=13819, loss=0.031063036993145943\n",
      "Surface training t=13820, loss=0.027043686248362064\n",
      "Surface training t=13821, loss=0.027126572094857693\n",
      "Surface training t=13822, loss=0.027426972053945065\n",
      "Surface training t=13823, loss=0.028704051859676838\n",
      "Surface training t=13824, loss=0.03808103688061237\n",
      "Surface training t=13825, loss=0.02679811790585518\n",
      "Surface training t=13826, loss=0.032298882491886616\n",
      "Surface training t=13827, loss=0.03934377245604992\n",
      "Surface training t=13828, loss=0.036663029342889786\n",
      "Surface training t=13829, loss=0.03485299088060856\n",
      "Surface training t=13830, loss=0.03411778621375561\n",
      "Surface training t=13831, loss=0.03380439430475235\n",
      "Surface training t=13832, loss=0.03219862561672926\n",
      "Surface training t=13833, loss=0.03320223093032837\n",
      "Surface training t=13834, loss=0.02818630449473858\n",
      "Surface training t=13835, loss=0.030796555802226067\n",
      "Surface training t=13836, loss=0.04733473062515259\n",
      "Surface training t=13837, loss=0.03497024439275265\n",
      "Surface training t=13838, loss=0.037559812888503075\n",
      "Surface training t=13839, loss=0.03472265228629112\n",
      "Surface training t=13840, loss=0.03346306178718805\n",
      "Surface training t=13841, loss=0.034632375463843346\n",
      "Surface training t=13842, loss=0.028555214405059814\n",
      "Surface training t=13843, loss=0.0296970522031188\n",
      "Surface training t=13844, loss=0.02328332420438528\n",
      "Surface training t=13845, loss=0.03342358209192753\n",
      "Surface training t=13846, loss=0.032446492463350296\n",
      "Surface training t=13847, loss=0.031702710315585136\n",
      "Surface training t=13848, loss=0.03567204810678959\n",
      "Surface training t=13849, loss=0.04670288786292076\n",
      "Surface training t=13850, loss=0.037232255563139915\n",
      "Surface training t=13851, loss=0.042843712493777275\n",
      "Surface training t=13852, loss=0.02977658621966839\n",
      "Surface training t=13853, loss=0.036573600955307484\n",
      "Surface training t=13854, loss=0.03172707837074995\n",
      "Surface training t=13855, loss=0.04087887145578861\n",
      "Surface training t=13856, loss=0.03214677982032299\n",
      "Surface training t=13857, loss=0.04390549473464489\n",
      "Surface training t=13858, loss=0.03186095505952835\n",
      "Surface training t=13859, loss=0.03484022431075573\n",
      "Surface training t=13860, loss=0.03314511850476265\n",
      "Surface training t=13861, loss=0.024791962467134\n",
      "Surface training t=13862, loss=0.034013972617685795\n",
      "Surface training t=13863, loss=0.04565126821398735\n",
      "Surface training t=13864, loss=0.03495749272406101\n",
      "Surface training t=13865, loss=0.061427218839526176\n",
      "Surface training t=13866, loss=0.03881211765110493\n",
      "Surface training t=13867, loss=0.04048933833837509\n",
      "Surface training t=13868, loss=0.030162548646330833\n",
      "Surface training t=13869, loss=0.028792472556233406\n",
      "Surface training t=13870, loss=0.030753156170248985\n",
      "Surface training t=13871, loss=0.02894261945039034\n",
      "Surface training t=13872, loss=0.03171855956315994\n",
      "Surface training t=13873, loss=0.02866060845553875\n",
      "Surface training t=13874, loss=0.035161230713129044\n",
      "Surface training t=13875, loss=0.018753504380583763\n",
      "Surface training t=13876, loss=0.038643984124064445\n",
      "Surface training t=13877, loss=0.026464339345693588\n",
      "Surface training t=13878, loss=0.02277358714491129\n",
      "Surface training t=13879, loss=0.033624665811657906\n",
      "Surface training t=13880, loss=0.024394266307353973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=13881, loss=0.03170290030539036\n",
      "Surface training t=13882, loss=0.019982087425887585\n",
      "Surface training t=13883, loss=0.025132297538220882\n",
      "Surface training t=13884, loss=0.03237181901931763\n",
      "Surface training t=13885, loss=0.038944728672504425\n",
      "Surface training t=13886, loss=0.02878305409103632\n",
      "Surface training t=13887, loss=0.02949683368206024\n",
      "Surface training t=13888, loss=0.06278765015304089\n",
      "Surface training t=13889, loss=0.037193652242422104\n",
      "Surface training t=13890, loss=0.04100907780230045\n",
      "Surface training t=13891, loss=0.0408175615593791\n",
      "Surface training t=13892, loss=0.05768631771206856\n",
      "Surface training t=13893, loss=0.04213433898985386\n",
      "Surface training t=13894, loss=0.04765648394823074\n",
      "Surface training t=13895, loss=0.03168647829443216\n",
      "Surface training t=13896, loss=0.03291793167591095\n",
      "Surface training t=13897, loss=0.03260558098554611\n",
      "Surface training t=13898, loss=0.026996382512152195\n",
      "Surface training t=13899, loss=0.028568378649652004\n",
      "Surface training t=13900, loss=0.03485804796218872\n",
      "Surface training t=13901, loss=0.03323475271463394\n",
      "Surface training t=13902, loss=0.03354397974908352\n",
      "Surface training t=13903, loss=0.03266902267932892\n",
      "Surface training t=13904, loss=0.0452854298055172\n",
      "Surface training t=13905, loss=0.033993011340498924\n",
      "Surface training t=13906, loss=0.03562996722757816\n",
      "Surface training t=13907, loss=0.04995770752429962\n",
      "Surface training t=13908, loss=0.035723255947232246\n",
      "Surface training t=13909, loss=0.0412422139197588\n",
      "Surface training t=13910, loss=0.038536581210792065\n",
      "Surface training t=13911, loss=0.0462891086935997\n",
      "Surface training t=13912, loss=0.061115844175219536\n",
      "Surface training t=13913, loss=0.046859873458743095\n",
      "Surface training t=13914, loss=0.057494815438985825\n",
      "Surface training t=13915, loss=0.10486815497279167\n",
      "Surface training t=13916, loss=0.05757317692041397\n",
      "Surface training t=13917, loss=0.059166254475712776\n",
      "Surface training t=13918, loss=0.05441868677735329\n",
      "Surface training t=13919, loss=0.042638376355171204\n",
      "Surface training t=13920, loss=0.04097859375178814\n",
      "Surface training t=13921, loss=0.039321502670645714\n",
      "Surface training t=13922, loss=0.0299471328034997\n",
      "Surface training t=13923, loss=0.03535163588821888\n",
      "Surface training t=13924, loss=0.03882916457951069\n",
      "Surface training t=13925, loss=0.03385992348194122\n",
      "Surface training t=13926, loss=0.033029383048415184\n",
      "Surface training t=13927, loss=0.02628587931394577\n",
      "Surface training t=13928, loss=0.028305859304964542\n",
      "Surface training t=13929, loss=0.034717812202870846\n",
      "Surface training t=13930, loss=0.03318816516548395\n",
      "Surface training t=13931, loss=0.027814985252916813\n",
      "Surface training t=13932, loss=0.026328569278120995\n",
      "Surface training t=13933, loss=0.055738743394613266\n",
      "Surface training t=13934, loss=0.04414654150605202\n",
      "Surface training t=13935, loss=0.04903707280755043\n",
      "Surface training t=13936, loss=0.04113193042576313\n",
      "Surface training t=13937, loss=0.059512294828891754\n",
      "Surface training t=13938, loss=0.03790280595421791\n",
      "Surface training t=13939, loss=0.049221914261579514\n",
      "Surface training t=13940, loss=0.05573011189699173\n",
      "Surface training t=13941, loss=0.037261126562952995\n",
      "Surface training t=13942, loss=0.044721854850649834\n",
      "Surface training t=13943, loss=0.039092994295060635\n",
      "Surface training t=13944, loss=0.06203925982117653\n",
      "Surface training t=13945, loss=0.050775811076164246\n",
      "Surface training t=13946, loss=0.04897141829133034\n",
      "Surface training t=13947, loss=0.04582871124148369\n",
      "Surface training t=13948, loss=0.03820059075951576\n",
      "Surface training t=13949, loss=0.04057523235678673\n",
      "Surface training t=13950, loss=0.04056461714208126\n",
      "Surface training t=13951, loss=0.05124921724200249\n",
      "Surface training t=13952, loss=0.05372558534145355\n",
      "Surface training t=13953, loss=0.03926356043666601\n",
      "Surface training t=13954, loss=0.04776381142437458\n",
      "Surface training t=13955, loss=0.05452452972531319\n",
      "Surface training t=13956, loss=0.04329737089574337\n",
      "Surface training t=13957, loss=0.03732382971793413\n",
      "Surface training t=13958, loss=0.04008391872048378\n",
      "Surface training t=13959, loss=0.05152638256549835\n",
      "Surface training t=13960, loss=0.03835897333920002\n",
      "Surface training t=13961, loss=0.0652043242007494\n",
      "Surface training t=13962, loss=0.03525172919034958\n",
      "Surface training t=13963, loss=0.062271783128380775\n",
      "Surface training t=13964, loss=0.03908103518188\n",
      "Surface training t=13965, loss=0.041570551693439484\n",
      "Surface training t=13966, loss=0.04006312135607004\n",
      "Surface training t=13967, loss=0.030207660049200058\n",
      "Surface training t=13968, loss=0.0440187007188797\n",
      "Surface training t=13969, loss=0.04451700486242771\n",
      "Surface training t=13970, loss=0.06584057584404945\n",
      "Surface training t=13971, loss=0.04039647988975048\n",
      "Surface training t=13972, loss=0.04368837736546993\n",
      "Surface training t=13973, loss=0.03249570168554783\n",
      "Surface training t=13974, loss=0.040850816294550896\n",
      "Surface training t=13975, loss=0.03143572621047497\n",
      "Surface training t=13976, loss=0.03328951261937618\n",
      "Surface training t=13977, loss=0.03317007515579462\n",
      "Surface training t=13978, loss=0.02618809975683689\n",
      "Surface training t=13979, loss=0.0418217908591032\n",
      "Surface training t=13980, loss=0.03700936771929264\n",
      "Surface training t=13981, loss=0.03226904012262821\n",
      "Surface training t=13982, loss=0.03596014529466629\n",
      "Surface training t=13983, loss=0.045430270954966545\n",
      "Surface training t=13984, loss=0.04362824372947216\n",
      "Surface training t=13985, loss=0.038909947499632835\n",
      "Surface training t=13986, loss=0.026897506788372993\n",
      "Surface training t=13987, loss=0.03707763087004423\n",
      "Surface training t=13988, loss=0.036192938685417175\n",
      "Surface training t=13989, loss=0.03568931017071009\n",
      "Surface training t=13990, loss=0.03813168592751026\n",
      "Surface training t=13991, loss=0.031909133307635784\n",
      "Surface training t=13992, loss=0.028609858825802803\n",
      "Surface training t=13993, loss=0.030209532007575035\n",
      "Surface training t=13994, loss=0.019936423748731613\n",
      "Surface training t=13995, loss=0.022698037326335907\n",
      "Surface training t=13996, loss=0.023371636867523193\n",
      "Surface training t=13997, loss=0.02231160644441843\n",
      "Surface training t=13998, loss=0.01963573321700096\n",
      "Surface training t=13999, loss=0.033583417534828186\n",
      "Surface training t=14000, loss=0.03069987241178751\n",
      "Surface training t=14001, loss=0.024740119464695454\n",
      "Surface training t=14002, loss=0.02175561711192131\n",
      "Surface training t=14003, loss=0.029643571004271507\n",
      "Surface training t=14004, loss=0.032085100188851357\n",
      "Surface training t=14005, loss=0.04411038011312485\n",
      "Surface training t=14006, loss=0.036934467032551765\n",
      "Surface training t=14007, loss=0.0459744967520237\n",
      "Surface training t=14008, loss=0.043235257267951965\n",
      "Surface training t=14009, loss=0.0459454171359539\n",
      "Surface training t=14010, loss=0.04989313706755638\n",
      "Surface training t=14011, loss=0.058528393507003784\n",
      "Surface training t=14012, loss=0.04586777649819851\n",
      "Surface training t=14013, loss=0.048678627237677574\n",
      "Surface training t=14014, loss=0.037265677005052567\n",
      "Surface training t=14015, loss=0.040040286257863045\n",
      "Surface training t=14016, loss=0.0308665931224823\n",
      "Surface training t=14017, loss=0.027615398168563843\n",
      "Surface training t=14018, loss=0.022743048146367073\n",
      "Surface training t=14019, loss=0.02495643123984337\n",
      "Surface training t=14020, loss=0.02624350506812334\n",
      "Surface training t=14021, loss=0.023789101280272007\n",
      "Surface training t=14022, loss=0.026327721774578094\n",
      "Surface training t=14023, loss=0.03587584849447012\n",
      "Surface training t=14024, loss=0.0391263123601675\n",
      "Surface training t=14025, loss=0.033950189128518105\n",
      "Surface training t=14026, loss=0.04593292437493801\n",
      "Surface training t=14027, loss=0.03574276156723499\n",
      "Surface training t=14028, loss=0.03730632737278938\n",
      "Surface training t=14029, loss=0.03691742941737175\n",
      "Surface training t=14030, loss=0.037794992327690125\n",
      "Surface training t=14031, loss=0.02206866256892681\n",
      "Surface training t=14032, loss=0.022516359575092793\n",
      "Surface training t=14033, loss=0.028422742150723934\n",
      "Surface training t=14034, loss=0.03044013772159815\n",
      "Surface training t=14035, loss=0.03227666858583689\n",
      "Surface training t=14036, loss=0.03084433823823929\n",
      "Surface training t=14037, loss=0.03428014740347862\n",
      "Surface training t=14038, loss=0.03684201464056969\n",
      "Surface training t=14039, loss=0.03357839398086071\n",
      "Surface training t=14040, loss=0.03149075619876385\n",
      "Surface training t=14041, loss=0.02645300794392824\n",
      "Surface training t=14042, loss=0.03354785591363907\n",
      "Surface training t=14043, loss=0.036290492862463\n",
      "Surface training t=14044, loss=0.028359590098261833\n",
      "Surface training t=14045, loss=0.04512536898255348\n",
      "Surface training t=14046, loss=0.02943381853401661\n",
      "Surface training t=14047, loss=0.026087645441293716\n",
      "Surface training t=14048, loss=0.026883614249527454\n",
      "Surface training t=14049, loss=0.025202888995409012\n",
      "Surface training t=14050, loss=0.022787603549659252\n",
      "Surface training t=14051, loss=0.02184968162328005\n",
      "Surface training t=14052, loss=0.021278949454426765\n",
      "Surface training t=14053, loss=0.0285725686699152\n",
      "Surface training t=14054, loss=0.023968116380274296\n",
      "Surface training t=14055, loss=0.02491540927439928\n",
      "Surface training t=14056, loss=0.03010235633701086\n",
      "Surface training t=14057, loss=0.028145084157586098\n",
      "Surface training t=14058, loss=0.02708241529762745\n",
      "Surface training t=14059, loss=0.03249694500118494\n",
      "Surface training t=14060, loss=0.027966078370809555\n",
      "Surface training t=14061, loss=0.025882226414978504\n",
      "Surface training t=14062, loss=0.03631077706813812\n",
      "Surface training t=14063, loss=0.04206275939941406\n",
      "Surface training t=14064, loss=0.03368195705115795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=14065, loss=0.03715007472783327\n",
      "Surface training t=14066, loss=0.051301661878824234\n",
      "Surface training t=14067, loss=0.04401724599301815\n",
      "Surface training t=14068, loss=0.04076375253498554\n",
      "Surface training t=14069, loss=0.043109314516186714\n",
      "Surface training t=14070, loss=0.037499270401895046\n",
      "Surface training t=14071, loss=0.04016214609146118\n",
      "Surface training t=14072, loss=0.04590750113129616\n",
      "Surface training t=14073, loss=0.038069961592555046\n",
      "Surface training t=14074, loss=0.038946542888879776\n",
      "Surface training t=14075, loss=0.04146208614110947\n",
      "Surface training t=14076, loss=0.028354693204164505\n",
      "Surface training t=14077, loss=0.01925351470708847\n",
      "Surface training t=14078, loss=0.02107424195855856\n",
      "Surface training t=14079, loss=0.018464609049260616\n",
      "Surface training t=14080, loss=0.023811811581254005\n",
      "Surface training t=14081, loss=0.027906635776162148\n",
      "Surface training t=14082, loss=0.03195985686033964\n",
      "Surface training t=14083, loss=0.02789708785712719\n",
      "Surface training t=14084, loss=0.030856279656291008\n",
      "Surface training t=14085, loss=0.0251094289124012\n",
      "Surface training t=14086, loss=0.026697661727666855\n",
      "Surface training t=14087, loss=0.031418646685779095\n",
      "Surface training t=14088, loss=0.04209708794951439\n",
      "Surface training t=14089, loss=0.03416530508548021\n",
      "Surface training t=14090, loss=0.03652207553386688\n",
      "Surface training t=14091, loss=0.02940494194626808\n",
      "Surface training t=14092, loss=0.027754864655435085\n",
      "Surface training t=14093, loss=0.03165122680366039\n",
      "Surface training t=14094, loss=0.024599775671958923\n",
      "Surface training t=14095, loss=0.030601492151618004\n",
      "Surface training t=14096, loss=0.024874483235180378\n",
      "Surface training t=14097, loss=0.028951654210686684\n",
      "Surface training t=14098, loss=0.02421568799763918\n",
      "Surface training t=14099, loss=0.019984296523034573\n",
      "Surface training t=14100, loss=0.02053492423146963\n",
      "Surface training t=14101, loss=0.02611595019698143\n",
      "Surface training t=14102, loss=0.026885760948061943\n",
      "Surface training t=14103, loss=0.05188866704702377\n",
      "Surface training t=14104, loss=0.043164629489183426\n",
      "Surface training t=14105, loss=0.048489767126739025\n",
      "Surface training t=14106, loss=0.05466601625084877\n",
      "Surface training t=14107, loss=0.1038956418633461\n",
      "Surface training t=14108, loss=0.061707235872745514\n",
      "Surface training t=14109, loss=0.06075960956513882\n",
      "Surface training t=14110, loss=0.06556646153330803\n",
      "Surface training t=14111, loss=0.04635104164481163\n",
      "Surface training t=14112, loss=0.03296418022364378\n",
      "Surface training t=14113, loss=0.02756384201347828\n",
      "Surface training t=14114, loss=0.026911658234894276\n",
      "Surface training t=14115, loss=0.028926950879395008\n",
      "Surface training t=14116, loss=0.03179418295621872\n",
      "Surface training t=14117, loss=0.041769228875637054\n",
      "Surface training t=14118, loss=0.054894085973501205\n",
      "Surface training t=14119, loss=0.0386265330016613\n",
      "Surface training t=14120, loss=0.0403472688049078\n",
      "Surface training t=14121, loss=0.027336392551660538\n",
      "Surface training t=14122, loss=0.036509525030851364\n",
      "Surface training t=14123, loss=0.03149740491062403\n",
      "Surface training t=14124, loss=0.03386266343295574\n",
      "Surface training t=14125, loss=0.024687965400516987\n",
      "Surface training t=14126, loss=0.031081688590347767\n",
      "Surface training t=14127, loss=0.03283830173313618\n",
      "Surface training t=14128, loss=0.026328327134251595\n",
      "Surface training t=14129, loss=0.03194093145430088\n",
      "Surface training t=14130, loss=0.034837573766708374\n",
      "Surface training t=14131, loss=0.03714960813522339\n",
      "Surface training t=14132, loss=0.04256419837474823\n",
      "Surface training t=14133, loss=0.03767057694494724\n",
      "Surface training t=14134, loss=0.038227710872888565\n",
      "Surface training t=14135, loss=0.036960795521736145\n",
      "Surface training t=14136, loss=0.04383591562509537\n",
      "Surface training t=14137, loss=0.03142302017658949\n",
      "Surface training t=14138, loss=0.028691603802144527\n",
      "Surface training t=14139, loss=0.03722143918275833\n",
      "Surface training t=14140, loss=0.03470178321003914\n",
      "Surface training t=14141, loss=0.028161906637251377\n",
      "Surface training t=14142, loss=0.038097078911960125\n",
      "Surface training t=14143, loss=0.03717733174562454\n",
      "Surface training t=14144, loss=0.027453850023448467\n",
      "Surface training t=14145, loss=0.028696014545857906\n",
      "Surface training t=14146, loss=0.02999188844114542\n",
      "Surface training t=14147, loss=0.031998785212635994\n",
      "Surface training t=14148, loss=0.04727570526301861\n",
      "Surface training t=14149, loss=0.054830703884363174\n",
      "Surface training t=14150, loss=0.038632805459201336\n",
      "Surface training t=14151, loss=0.03854517079889774\n",
      "Surface training t=14152, loss=0.03124251402914524\n",
      "Surface training t=14153, loss=0.03064655140042305\n",
      "Surface training t=14154, loss=0.044662075117230415\n",
      "Surface training t=14155, loss=0.03070718329399824\n",
      "Surface training t=14156, loss=0.03526462335139513\n",
      "Surface training t=14157, loss=0.04347432404756546\n",
      "Surface training t=14158, loss=0.026141086593270302\n",
      "Surface training t=14159, loss=0.023531770333647728\n",
      "Surface training t=14160, loss=0.035041820257902145\n",
      "Surface training t=14161, loss=0.027734420262277126\n",
      "Surface training t=14162, loss=0.03481232561171055\n",
      "Surface training t=14163, loss=0.03762086667120457\n",
      "Surface training t=14164, loss=0.03508457541465759\n",
      "Surface training t=14165, loss=0.03630257956683636\n",
      "Surface training t=14166, loss=0.02933566737920046\n",
      "Surface training t=14167, loss=0.03155849315226078\n",
      "Surface training t=14168, loss=0.03594425693154335\n",
      "Surface training t=14169, loss=0.029411688446998596\n",
      "Surface training t=14170, loss=0.02549551986157894\n",
      "Surface training t=14171, loss=0.017479277215898037\n",
      "Surface training t=14172, loss=0.023816023021936417\n",
      "Surface training t=14173, loss=0.024538605473935604\n",
      "Surface training t=14174, loss=0.020739826373755932\n",
      "Surface training t=14175, loss=0.023124187719076872\n",
      "Surface training t=14176, loss=0.017545084469020367\n",
      "Surface training t=14177, loss=0.024723154492676258\n",
      "Surface training t=14178, loss=0.026234318502247334\n",
      "Surface training t=14179, loss=0.028748388402163982\n",
      "Surface training t=14180, loss=0.04089026898145676\n",
      "Surface training t=14181, loss=0.03432234562933445\n",
      "Surface training t=14182, loss=0.043195560574531555\n",
      "Surface training t=14183, loss=0.02980712614953518\n",
      "Surface training t=14184, loss=0.03459140658378601\n",
      "Surface training t=14185, loss=0.0327260997146368\n",
      "Surface training t=14186, loss=0.023164255544543266\n",
      "Surface training t=14187, loss=0.03142717853188515\n",
      "Surface training t=14188, loss=0.035093722864985466\n",
      "Surface training t=14189, loss=0.028523748740553856\n",
      "Surface training t=14190, loss=0.03826352767646313\n",
      "Surface training t=14191, loss=0.038318103179335594\n",
      "Surface training t=14192, loss=0.054002705961465836\n",
      "Surface training t=14193, loss=0.04396700672805309\n",
      "Surface training t=14194, loss=0.04989572614431381\n",
      "Surface training t=14195, loss=0.03941944241523743\n",
      "Surface training t=14196, loss=0.06973053514957428\n",
      "Surface training t=14197, loss=0.050623999908566475\n",
      "Surface training t=14198, loss=0.0527593158185482\n",
      "Surface training t=14199, loss=0.05693576671183109\n",
      "Surface training t=14200, loss=0.051904693245887756\n",
      "Surface training t=14201, loss=0.04269547201693058\n",
      "Surface training t=14202, loss=0.05762689560651779\n",
      "Surface training t=14203, loss=0.04182312451303005\n",
      "Surface training t=14204, loss=0.0384923592209816\n",
      "Surface training t=14205, loss=0.05105765536427498\n",
      "Surface training t=14206, loss=0.03712433576583862\n",
      "Surface training t=14207, loss=0.0348972212523222\n",
      "Surface training t=14208, loss=0.03300912119448185\n",
      "Surface training t=14209, loss=0.02276457380503416\n",
      "Surface training t=14210, loss=0.023698249831795692\n",
      "Surface training t=14211, loss=0.026478917337954044\n",
      "Surface training t=14212, loss=0.022409302182495594\n",
      "Surface training t=14213, loss=0.02075917460024357\n",
      "Surface training t=14214, loss=0.022379054687917233\n",
      "Surface training t=14215, loss=0.025274978019297123\n",
      "Surface training t=14216, loss=0.025970667600631714\n",
      "Surface training t=14217, loss=0.026712768711149693\n",
      "Surface training t=14218, loss=0.028509952127933502\n",
      "Surface training t=14219, loss=0.021827984135597944\n",
      "Surface training t=14220, loss=0.028964375145733356\n",
      "Surface training t=14221, loss=0.027979865670204163\n",
      "Surface training t=14222, loss=0.021672634407877922\n",
      "Surface training t=14223, loss=0.03766068257391453\n",
      "Surface training t=14224, loss=0.037372919730842113\n",
      "Surface training t=14225, loss=0.035315765999257565\n",
      "Surface training t=14226, loss=0.03523034788668156\n",
      "Surface training t=14227, loss=0.031051822006702423\n",
      "Surface training t=14228, loss=0.060096338391304016\n",
      "Surface training t=14229, loss=0.04522351175546646\n",
      "Surface training t=14230, loss=0.051581814885139465\n",
      "Surface training t=14231, loss=0.032771484926342964\n",
      "Surface training t=14232, loss=0.06588295474648476\n",
      "Surface training t=14233, loss=0.05577551946043968\n",
      "Surface training t=14234, loss=0.045776788145303726\n",
      "Surface training t=14235, loss=0.04865499585866928\n",
      "Surface training t=14236, loss=0.06908482313156128\n",
      "Surface training t=14237, loss=0.04961125925183296\n",
      "Surface training t=14238, loss=0.06640628352761269\n",
      "Surface training t=14239, loss=0.06372785940766335\n",
      "Surface training t=14240, loss=0.05485174059867859\n",
      "Surface training t=14241, loss=0.08077820017933846\n",
      "Surface training t=14242, loss=0.043106790632009506\n",
      "Surface training t=14243, loss=0.05267234891653061\n",
      "Surface training t=14244, loss=0.03193186782300472\n",
      "Surface training t=14245, loss=0.04920491203665733\n",
      "Surface training t=14246, loss=0.035450092516839504\n",
      "Surface training t=14247, loss=0.049904268234968185\n",
      "Surface training t=14248, loss=0.03862507734447718\n",
      "Surface training t=14249, loss=0.044321225956082344\n",
      "Surface training t=14250, loss=0.03357073664665222\n",
      "Surface training t=14251, loss=0.03947561141103506\n",
      "Surface training t=14252, loss=0.0498114675283432\n",
      "Surface training t=14253, loss=0.04274086467921734\n",
      "Surface training t=14254, loss=0.036288049072027206\n",
      "Surface training t=14255, loss=0.035929882898926735\n",
      "Surface training t=14256, loss=0.034542473033070564\n",
      "Surface training t=14257, loss=0.03031568042933941\n",
      "Surface training t=14258, loss=0.0332356933504343\n",
      "Surface training t=14259, loss=0.030625914223492146\n",
      "Surface training t=14260, loss=0.038620345294475555\n",
      "Surface training t=14261, loss=0.03912381827831268\n",
      "Surface training t=14262, loss=0.03571716882288456\n",
      "Surface training t=14263, loss=0.033486814238131046\n",
      "Surface training t=14264, loss=0.030178561806678772\n",
      "Surface training t=14265, loss=0.0367218554019928\n",
      "Surface training t=14266, loss=0.03466452471911907\n",
      "Surface training t=14267, loss=0.04012872278690338\n",
      "Surface training t=14268, loss=0.03203495591878891\n",
      "Surface training t=14269, loss=0.025652294978499413\n",
      "Surface training t=14270, loss=0.03086458519101143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=14271, loss=0.023012550547719002\n",
      "Surface training t=14272, loss=0.02589265536516905\n",
      "Surface training t=14273, loss=0.0284084165468812\n",
      "Surface training t=14274, loss=0.033787138760089874\n",
      "Surface training t=14275, loss=0.023780410178005695\n",
      "Surface training t=14276, loss=0.026029150001704693\n",
      "Surface training t=14277, loss=0.024253438226878643\n",
      "Surface training t=14278, loss=0.029156140983104706\n",
      "Surface training t=14279, loss=0.04621439427137375\n",
      "Surface training t=14280, loss=0.06488944590091705\n",
      "Surface training t=14281, loss=0.05181817524135113\n",
      "Surface training t=14282, loss=0.0640740804374218\n",
      "Surface training t=14283, loss=0.061257705092430115\n",
      "Surface training t=14284, loss=0.049651408568024635\n",
      "Surface training t=14285, loss=0.05148984678089619\n",
      "Surface training t=14286, loss=0.10417207330465317\n",
      "Surface training t=14287, loss=0.060180800035595894\n",
      "Surface training t=14288, loss=0.06328889541327953\n",
      "Surface training t=14289, loss=0.06253402680158615\n",
      "Surface training t=14290, loss=0.044863779097795486\n",
      "Surface training t=14291, loss=0.050561362877488136\n",
      "Surface training t=14292, loss=0.0449625626206398\n",
      "Surface training t=14293, loss=0.04091520234942436\n",
      "Surface training t=14294, loss=0.03973735682666302\n",
      "Surface training t=14295, loss=0.03636149875819683\n",
      "Surface training t=14296, loss=0.04566664807498455\n",
      "Surface training t=14297, loss=0.040746187791228294\n",
      "Surface training t=14298, loss=0.038649410009384155\n",
      "Surface training t=14299, loss=0.03806559182703495\n",
      "Surface training t=14300, loss=0.028164786286652088\n",
      "Surface training t=14301, loss=0.03171217814087868\n",
      "Surface training t=14302, loss=0.034757078625261784\n",
      "Surface training t=14303, loss=0.03087129071354866\n",
      "Surface training t=14304, loss=0.030222147703170776\n",
      "Surface training t=14305, loss=0.03541441913694143\n",
      "Surface training t=14306, loss=0.042020680382847786\n",
      "Surface training t=14307, loss=0.024460253305733204\n",
      "Surface training t=14308, loss=0.038594139739871025\n",
      "Surface training t=14309, loss=0.039691269397735596\n",
      "Surface training t=14310, loss=0.06284867599606514\n",
      "Surface training t=14311, loss=0.030371051281690598\n",
      "Surface training t=14312, loss=0.02372363954782486\n",
      "Surface training t=14313, loss=0.02937956526875496\n",
      "Surface training t=14314, loss=0.043087903410196304\n",
      "Surface training t=14315, loss=0.034419638104736805\n",
      "Surface training t=14316, loss=0.030744980089366436\n",
      "Surface training t=14317, loss=0.026823433116078377\n",
      "Surface training t=14318, loss=0.027856333181262016\n",
      "Surface training t=14319, loss=0.024686760269105434\n",
      "Surface training t=14320, loss=0.01998529490083456\n",
      "Surface training t=14321, loss=0.01764159183949232\n",
      "Surface training t=14322, loss=0.028927762061357498\n",
      "Surface training t=14323, loss=0.02590646781027317\n",
      "Surface training t=14324, loss=0.029021148569881916\n",
      "Surface training t=14325, loss=0.035558778792619705\n",
      "Surface training t=14326, loss=0.027370997704565525\n",
      "Surface training t=14327, loss=0.026001199148595333\n",
      "Surface training t=14328, loss=0.035801636055111885\n",
      "Surface training t=14329, loss=0.030812932178378105\n",
      "Surface training t=14330, loss=0.035912283696234226\n",
      "Surface training t=14331, loss=0.03651457838714123\n",
      "Surface training t=14332, loss=0.03338473662734032\n",
      "Surface training t=14333, loss=0.0308114280924201\n",
      "Surface training t=14334, loss=0.034058813005685806\n",
      "Surface training t=14335, loss=0.039456648752093315\n",
      "Surface training t=14336, loss=0.0389491468667984\n",
      "Surface training t=14337, loss=0.034711653366684914\n",
      "Surface training t=14338, loss=0.03779999166727066\n",
      "Surface training t=14339, loss=0.030622989870607853\n",
      "Surface training t=14340, loss=0.03025469183921814\n",
      "Surface training t=14341, loss=0.02924729697406292\n",
      "Surface training t=14342, loss=0.040672944858670235\n",
      "Surface training t=14343, loss=0.043746042996644974\n",
      "Surface training t=14344, loss=0.028860379941761494\n",
      "Surface training t=14345, loss=0.03333492670208216\n",
      "Surface training t=14346, loss=0.04075673781335354\n",
      "Surface training t=14347, loss=0.030497372150421143\n",
      "Surface training t=14348, loss=0.0413789302110672\n",
      "Surface training t=14349, loss=0.03955327905714512\n",
      "Surface training t=14350, loss=0.03242386877536774\n",
      "Surface training t=14351, loss=0.04013572260737419\n",
      "Surface training t=14352, loss=0.02504448313266039\n",
      "Surface training t=14353, loss=0.03345884941518307\n",
      "Surface training t=14354, loss=0.03811902552843094\n",
      "Surface training t=14355, loss=0.026274679228663445\n",
      "Surface training t=14356, loss=0.0219234861433506\n",
      "Surface training t=14357, loss=0.044810738414525986\n",
      "Surface training t=14358, loss=0.026311487890779972\n",
      "Surface training t=14359, loss=0.03590661846101284\n",
      "Surface training t=14360, loss=0.06125040724873543\n",
      "Surface training t=14361, loss=0.04784688726067543\n",
      "Surface training t=14362, loss=0.04271807707846165\n",
      "Surface training t=14363, loss=0.050846802070736885\n",
      "Surface training t=14364, loss=0.059207817539572716\n",
      "Surface training t=14365, loss=0.044809965416789055\n",
      "Surface training t=14366, loss=0.04974792338907719\n",
      "Surface training t=14367, loss=0.06716062128543854\n",
      "Surface training t=14368, loss=0.04584789276123047\n",
      "Surface training t=14369, loss=0.047899773344397545\n",
      "Surface training t=14370, loss=0.03336813859641552\n",
      "Surface training t=14371, loss=0.03372153080999851\n",
      "Surface training t=14372, loss=0.029622619040310383\n",
      "Surface training t=14373, loss=0.024487094953656197\n",
      "Surface training t=14374, loss=0.0371838454157114\n",
      "Surface training t=14375, loss=0.03318409714847803\n",
      "Surface training t=14376, loss=0.033566540107131004\n",
      "Surface training t=14377, loss=0.033369798213243484\n",
      "Surface training t=14378, loss=0.03026823326945305\n",
      "Surface training t=14379, loss=0.025199757888913155\n",
      "Surface training t=14380, loss=0.025297102518379688\n",
      "Surface training t=14381, loss=0.02235732041299343\n",
      "Surface training t=14382, loss=0.02896038256585598\n",
      "Surface training t=14383, loss=0.03466391749680042\n",
      "Surface training t=14384, loss=0.030549753457307816\n",
      "Surface training t=14385, loss=0.03047199919819832\n",
      "Surface training t=14386, loss=0.02987744379788637\n",
      "Surface training t=14387, loss=0.03154538944363594\n",
      "Surface training t=14388, loss=0.036453818902373314\n",
      "Surface training t=14389, loss=0.034520833753049374\n",
      "Surface training t=14390, loss=0.03267333097755909\n",
      "Surface training t=14391, loss=0.03816030826419592\n",
      "Surface training t=14392, loss=0.032291047275066376\n",
      "Surface training t=14393, loss=0.029100869782269\n",
      "Surface training t=14394, loss=0.026438836939632893\n",
      "Surface training t=14395, loss=0.0264720031991601\n",
      "Surface training t=14396, loss=0.0201083580031991\n",
      "Surface training t=14397, loss=0.028278682380914688\n",
      "Surface training t=14398, loss=0.03425589669495821\n",
      "Surface training t=14399, loss=0.046913446858525276\n",
      "Surface training t=14400, loss=0.04258785769343376\n",
      "Surface training t=14401, loss=0.04412650689482689\n",
      "Surface training t=14402, loss=0.037135498598217964\n",
      "Surface training t=14403, loss=0.034913625568151474\n",
      "Surface training t=14404, loss=0.04253789409995079\n",
      "Surface training t=14405, loss=0.045373786240816116\n",
      "Surface training t=14406, loss=0.05486079305410385\n",
      "Surface training t=14407, loss=0.06607267260551453\n",
      "Surface training t=14408, loss=0.04717965982854366\n",
      "Surface training t=14409, loss=0.061130622401833534\n",
      "Surface training t=14410, loss=0.030773463658988476\n",
      "Surface training t=14411, loss=0.038778072223067284\n",
      "Surface training t=14412, loss=0.0362280635163188\n",
      "Surface training t=14413, loss=0.02607795875519514\n",
      "Surface training t=14414, loss=0.023434409871697426\n",
      "Surface training t=14415, loss=0.02959115244448185\n",
      "Surface training t=14416, loss=0.03362779226154089\n",
      "Surface training t=14417, loss=0.03334695287048817\n",
      "Surface training t=14418, loss=0.02827569004148245\n",
      "Surface training t=14419, loss=0.03607737924903631\n",
      "Surface training t=14420, loss=0.036455441266298294\n",
      "Surface training t=14421, loss=0.044826941564679146\n",
      "Surface training t=14422, loss=0.030452998355031013\n",
      "Surface training t=14423, loss=0.023294128477573395\n",
      "Surface training t=14424, loss=0.03706016764044762\n",
      "Surface training t=14425, loss=0.04092387109994888\n",
      "Surface training t=14426, loss=0.03545206133276224\n",
      "Surface training t=14427, loss=0.03201553598046303\n",
      "Surface training t=14428, loss=0.028004865162074566\n",
      "Surface training t=14429, loss=0.030954129993915558\n",
      "Surface training t=14430, loss=0.034384939819574356\n",
      "Surface training t=14431, loss=0.037647122517228127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=14432, loss=0.034594496712088585\n",
      "Surface training t=14433, loss=0.027344845235347748\n",
      "Surface training t=14434, loss=0.02690762933343649\n",
      "Surface training t=14435, loss=0.02673687320202589\n",
      "Surface training t=14436, loss=0.03103051520884037\n",
      "Surface training t=14437, loss=0.022277021780610085\n",
      "Surface training t=14438, loss=0.022525496315211058\n",
      "Surface training t=14439, loss=0.016671743243932724\n",
      "Surface training t=14440, loss=0.02257512602955103\n",
      "Surface training t=14441, loss=0.021442673169076443\n",
      "Surface training t=14442, loss=0.025309008546173573\n",
      "Surface training t=14443, loss=0.02005670592188835\n",
      "Surface training t=14444, loss=0.018066341057419777\n",
      "Surface training t=14445, loss=0.025242699310183525\n",
      "Surface training t=14446, loss=0.025137332268059254\n",
      "Surface training t=14447, loss=0.02511262334883213\n",
      "Surface training t=14448, loss=0.027627043426036835\n",
      "Surface training t=14449, loss=0.03321895096451044\n",
      "Surface training t=14450, loss=0.034608226269483566\n",
      "Surface training t=14451, loss=0.036935484036803246\n",
      "Surface training t=14452, loss=0.038375288248062134\n",
      "Surface training t=14453, loss=0.02933731535449624\n",
      "Surface training t=14454, loss=0.03573846071958542\n",
      "Surface training t=14455, loss=0.04110333137214184\n",
      "Surface training t=14456, loss=0.045455725863575935\n",
      "Surface training t=14457, loss=0.039492092095315456\n",
      "Surface training t=14458, loss=0.03648689389228821\n",
      "Surface training t=14459, loss=0.03600358963012695\n",
      "Surface training t=14460, loss=0.026458842679858208\n",
      "Surface training t=14461, loss=0.0503835529088974\n",
      "Surface training t=14462, loss=0.041917646303772926\n",
      "Surface training t=14463, loss=0.03947584703564644\n",
      "Surface training t=14464, loss=0.038445133715867996\n",
      "Surface training t=14465, loss=0.040258483961224556\n",
      "Surface training t=14466, loss=0.06593122705817223\n",
      "Surface training t=14467, loss=0.04169527254998684\n",
      "Surface training t=14468, loss=0.046218391507864\n",
      "Surface training t=14469, loss=0.10070542991161346\n",
      "Surface training t=14470, loss=0.05783892795443535\n",
      "Surface training t=14471, loss=0.08702217787504196\n",
      "Surface training t=14472, loss=0.05617010034620762\n",
      "Surface training t=14473, loss=0.05276033841073513\n",
      "Surface training t=14474, loss=0.0568408016115427\n",
      "Surface training t=14475, loss=0.03922050725668669\n",
      "Surface training t=14476, loss=0.03959671873599291\n",
      "Surface training t=14477, loss=0.05064762756228447\n",
      "Surface training t=14478, loss=0.047700922936201096\n",
      "Surface training t=14479, loss=0.03452697861939669\n",
      "Surface training t=14480, loss=0.03522396273910999\n",
      "Surface training t=14481, loss=0.0317825973033905\n",
      "Surface training t=14482, loss=0.02572279144078493\n",
      "Surface training t=14483, loss=0.02959488518536091\n",
      "Surface training t=14484, loss=0.0266581978648901\n",
      "Surface training t=14485, loss=0.03739857114851475\n",
      "Surface training t=14486, loss=0.02934996411204338\n",
      "Surface training t=14487, loss=0.04430007189512253\n",
      "Surface training t=14488, loss=0.04168540798127651\n",
      "Surface training t=14489, loss=0.03549339063465595\n",
      "Surface training t=14490, loss=0.026673873886466026\n",
      "Surface training t=14491, loss=0.026365550234913826\n",
      "Surface training t=14492, loss=0.01993488520383835\n",
      "Surface training t=14493, loss=0.026733380742371082\n",
      "Surface training t=14494, loss=0.02118724025785923\n",
      "Surface training t=14495, loss=0.024947425350546837\n",
      "Surface training t=14496, loss=0.01885572774335742\n",
      "Surface training t=14497, loss=0.024971971288323402\n",
      "Surface training t=14498, loss=0.021464979276061058\n",
      "Surface training t=14499, loss=0.021834378130733967\n",
      "Surface training t=14500, loss=0.021400523371994495\n",
      "Surface training t=14501, loss=0.018310116603970528\n",
      "Surface training t=14502, loss=0.02797598112374544\n",
      "Surface training t=14503, loss=0.026091418229043484\n",
      "Surface training t=14504, loss=0.02086326666176319\n",
      "Surface training t=14505, loss=0.022968475706875324\n",
      "Surface training t=14506, loss=0.02301607746630907\n",
      "Surface training t=14507, loss=0.021863451227545738\n",
      "Surface training t=14508, loss=0.027314461767673492\n",
      "Surface training t=14509, loss=0.026953990571200848\n",
      "Surface training t=14510, loss=0.03244287893176079\n",
      "Surface training t=14511, loss=0.0260021286085248\n",
      "Surface training t=14512, loss=0.027927525341510773\n",
      "Surface training t=14513, loss=0.027057438157498837\n",
      "Surface training t=14514, loss=0.031065702438354492\n",
      "Surface training t=14515, loss=0.03986556176096201\n",
      "Surface training t=14516, loss=0.044974185526371\n",
      "Surface training t=14517, loss=0.040236372500658035\n",
      "Surface training t=14518, loss=0.031912392005324364\n",
      "Surface training t=14519, loss=0.02919139340519905\n",
      "Surface training t=14520, loss=0.026713427156209946\n",
      "Surface training t=14521, loss=0.02822030708193779\n",
      "Surface training t=14522, loss=0.03529102634638548\n",
      "Surface training t=14523, loss=0.02590423170477152\n",
      "Surface training t=14524, loss=0.020462526008486748\n",
      "Surface training t=14525, loss=0.028597218915820122\n",
      "Surface training t=14526, loss=0.030006535351276398\n",
      "Surface training t=14527, loss=0.029571411199867725\n",
      "Surface training t=14528, loss=0.0397662203758955\n",
      "Surface training t=14529, loss=0.04621439427137375\n",
      "Surface training t=14530, loss=0.056739725172519684\n",
      "Surface training t=14531, loss=0.03623206913471222\n",
      "Surface training t=14532, loss=0.03946557641029358\n",
      "Surface training t=14533, loss=0.04382668621838093\n",
      "Surface training t=14534, loss=0.038754912093281746\n",
      "Surface training t=14535, loss=0.04097096063196659\n",
      "Surface training t=14536, loss=0.04314905405044556\n",
      "Surface training t=14537, loss=0.04759722016751766\n",
      "Surface training t=14538, loss=0.0611510556191206\n",
      "Surface training t=14539, loss=0.06025143153965473\n",
      "Surface training t=14540, loss=0.04737558215856552\n",
      "Surface training t=14541, loss=0.04949330352246761\n",
      "Surface training t=14542, loss=0.0855475664138794\n",
      "Surface training t=14543, loss=0.056252263486385345\n",
      "Surface training t=14544, loss=0.07185852155089378\n",
      "Surface training t=14545, loss=0.0644020140171051\n",
      "Surface training t=14546, loss=0.04862675070762634\n",
      "Surface training t=14547, loss=0.0702507458627224\n",
      "Surface training t=14548, loss=0.04406085889786482\n",
      "Surface training t=14549, loss=0.04176292475312948\n",
      "Surface training t=14550, loss=0.048207519575953484\n",
      "Surface training t=14551, loss=0.026883866637945175\n",
      "Surface training t=14552, loss=0.03271685168147087\n",
      "Surface training t=14553, loss=0.03039118554443121\n",
      "Surface training t=14554, loss=0.020520287565886974\n",
      "Surface training t=14555, loss=0.025708685629069805\n",
      "Surface training t=14556, loss=0.03413883689790964\n",
      "Surface training t=14557, loss=0.027001998387277126\n",
      "Surface training t=14558, loss=0.023057020734995604\n",
      "Surface training t=14559, loss=0.05650327354669571\n",
      "Surface training t=14560, loss=0.03901625890284777\n",
      "Surface training t=14561, loss=0.04889889806509018\n",
      "Surface training t=14562, loss=0.038545302115380764\n",
      "Surface training t=14563, loss=0.07054547965526581\n",
      "Surface training t=14564, loss=0.04422052949666977\n",
      "Surface training t=14565, loss=0.043371979147195816\n",
      "Surface training t=14566, loss=0.03209813218563795\n",
      "Surface training t=14567, loss=0.01973497960716486\n",
      "Surface training t=14568, loss=0.03646285645663738\n",
      "Surface training t=14569, loss=0.026312327943742275\n",
      "Surface training t=14570, loss=0.031505304388701916\n",
      "Surface training t=14571, loss=0.03674857318401337\n",
      "Surface training t=14572, loss=0.026914574205875397\n",
      "Surface training t=14573, loss=0.026312263682484627\n",
      "Surface training t=14574, loss=0.03876025602221489\n",
      "Surface training t=14575, loss=0.03402486443519592\n",
      "Surface training t=14576, loss=0.03505958616733551\n",
      "Surface training t=14577, loss=0.030448351986706257\n",
      "Surface training t=14578, loss=0.02798634674400091\n",
      "Surface training t=14579, loss=0.03280625492334366\n",
      "Surface training t=14580, loss=0.0363532193005085\n",
      "Surface training t=14581, loss=0.031180025078356266\n",
      "Surface training t=14582, loss=0.03216549567878246\n",
      "Surface training t=14583, loss=0.028763309121131897\n",
      "Surface training t=14584, loss=0.030585838481783867\n",
      "Surface training t=14585, loss=0.0346829928457737\n",
      "Surface training t=14586, loss=0.03128585126250982\n",
      "Surface training t=14587, loss=0.04311365634202957\n",
      "Surface training t=14588, loss=0.04062799643725157\n",
      "Surface training t=14589, loss=0.04325109161436558\n",
      "Surface training t=14590, loss=0.043051473796367645\n",
      "Surface training t=14591, loss=0.044292956590652466\n",
      "Surface training t=14592, loss=0.04264703020453453\n",
      "Surface training t=14593, loss=0.03858240507543087\n",
      "Surface training t=14594, loss=0.03576001152396202\n",
      "Surface training t=14595, loss=0.05923020839691162\n",
      "Surface training t=14596, loss=0.03706434741616249\n",
      "Surface training t=14597, loss=0.041552841663360596\n",
      "Surface training t=14598, loss=0.0428941547870636\n",
      "Surface training t=14599, loss=0.03254920057952404\n",
      "Surface training t=14600, loss=0.040896205231547356\n",
      "Surface training t=14601, loss=0.03387158829718828\n",
      "Surface training t=14602, loss=0.03261195681989193\n",
      "Surface training t=14603, loss=0.03216651640832424\n",
      "Surface training t=14604, loss=0.03343428857624531\n",
      "Surface training t=14605, loss=0.04082923009991646\n",
      "Surface training t=14606, loss=0.039016611874103546\n",
      "Surface training t=14607, loss=0.03423462063074112\n",
      "Surface training t=14608, loss=0.022729715332388878\n",
      "Surface training t=14609, loss=0.029512574896216393\n",
      "Surface training t=14610, loss=0.02383282408118248\n",
      "Surface training t=14611, loss=0.025753258727490902\n",
      "Surface training t=14612, loss=0.022100877948105335\n",
      "Surface training t=14613, loss=0.020688053220510483\n",
      "Surface training t=14614, loss=0.031163454055786133\n",
      "Surface training t=14615, loss=0.043468913063406944\n",
      "Surface training t=14616, loss=0.03976544924080372\n",
      "Surface training t=14617, loss=0.03969067335128784\n",
      "Surface training t=14618, loss=0.038922784850001335\n",
      "Surface training t=14619, loss=0.03531617857515812\n",
      "Surface training t=14620, loss=0.039198155514895916\n",
      "Surface training t=14621, loss=0.04432293772697449\n",
      "Surface training t=14622, loss=0.046233417466282845\n",
      "Surface training t=14623, loss=0.03115019015967846\n",
      "Surface training t=14624, loss=0.038441259413957596\n",
      "Surface training t=14625, loss=0.030296615324914455\n",
      "Surface training t=14626, loss=0.02605722937732935\n",
      "Surface training t=14627, loss=0.029321451671421528\n",
      "Surface training t=14628, loss=0.0388207733631134\n",
      "Surface training t=14629, loss=0.03453186899423599\n",
      "Surface training t=14630, loss=0.02530717197805643\n",
      "Surface training t=14631, loss=0.02877061255276203\n",
      "Surface training t=14632, loss=0.027729149907827377\n",
      "Surface training t=14633, loss=0.025210097432136536\n",
      "Surface training t=14634, loss=0.035908980295062065\n",
      "Surface training t=14635, loss=0.03154575917869806\n",
      "Surface training t=14636, loss=0.044033026322722435\n",
      "Surface training t=14637, loss=0.04752436280250549\n",
      "Surface training t=14638, loss=0.029444048181176186\n",
      "Surface training t=14639, loss=0.033436316065490246\n",
      "Surface training t=14640, loss=0.029069450683891773\n",
      "Surface training t=14641, loss=0.02082943171262741\n",
      "Surface training t=14642, loss=0.022585125640034676\n",
      "Surface training t=14643, loss=0.024985208176076412\n",
      "Surface training t=14644, loss=0.02236458659172058\n",
      "Surface training t=14645, loss=0.021095208823680878\n",
      "Surface training t=14646, loss=0.023037821054458618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=14647, loss=0.030790502205491066\n",
      "Surface training t=14648, loss=0.030235628597438335\n",
      "Surface training t=14649, loss=0.035025062039494514\n",
      "Surface training t=14650, loss=0.04224836453795433\n",
      "Surface training t=14651, loss=0.037880903109908104\n",
      "Surface training t=14652, loss=0.03134852182120085\n",
      "Surface training t=14653, loss=0.03216871619224548\n",
      "Surface training t=14654, loss=0.0355074517428875\n",
      "Surface training t=14655, loss=0.05457836017012596\n",
      "Surface training t=14656, loss=0.06318793073296547\n",
      "Surface training t=14657, loss=0.049047477543354034\n",
      "Surface training t=14658, loss=0.07264153100550175\n",
      "Surface training t=14659, loss=0.043684015050530434\n",
      "Surface training t=14660, loss=0.03937288839370012\n",
      "Surface training t=14661, loss=0.03968206886202097\n",
      "Surface training t=14662, loss=0.027336928993463516\n",
      "Surface training t=14663, loss=0.026614677160978317\n",
      "Surface training t=14664, loss=0.02583096269518137\n",
      "Surface training t=14665, loss=0.0270165977999568\n",
      "Surface training t=14666, loss=0.02949741017073393\n",
      "Surface training t=14667, loss=0.03295387513935566\n",
      "Surface training t=14668, loss=0.03174677211791277\n",
      "Surface training t=14669, loss=0.024831589311361313\n",
      "Surface training t=14670, loss=0.030772159807384014\n",
      "Surface training t=14671, loss=0.030150648206472397\n",
      "Surface training t=14672, loss=0.03017403930425644\n",
      "Surface training t=14673, loss=0.022388567216694355\n",
      "Surface training t=14674, loss=0.02405296266078949\n",
      "Surface training t=14675, loss=0.02085775788873434\n",
      "Surface training t=14676, loss=0.029062965884804726\n",
      "Surface training t=14677, loss=0.03765677474439144\n",
      "Surface training t=14678, loss=0.03009692393243313\n",
      "Surface training t=14679, loss=0.047008002176880836\n",
      "Surface training t=14680, loss=0.036413442343473434\n",
      "Surface training t=14681, loss=0.03860749676823616\n",
      "Surface training t=14682, loss=0.032888004556298256\n",
      "Surface training t=14683, loss=0.03560022357851267\n",
      "Surface training t=14684, loss=0.039425572380423546\n",
      "Surface training t=14685, loss=0.040300311520695686\n",
      "Surface training t=14686, loss=0.031003671698272228\n",
      "Surface training t=14687, loss=0.029408561065793037\n",
      "Surface training t=14688, loss=0.026715420186519623\n",
      "Surface training t=14689, loss=0.032217228785157204\n",
      "Surface training t=14690, loss=0.04425251483917236\n",
      "Surface training t=14691, loss=0.028979790396988392\n",
      "Surface training t=14692, loss=0.02002234337851405\n",
      "Surface training t=14693, loss=0.01899200165644288\n",
      "Surface training t=14694, loss=0.017029283568263054\n",
      "Surface training t=14695, loss=0.017360825091600418\n",
      "Surface training t=14696, loss=0.02491142973303795\n",
      "Surface training t=14697, loss=0.01901335036382079\n",
      "Surface training t=14698, loss=0.02180231362581253\n",
      "Surface training t=14699, loss=0.017870577052235603\n",
      "Surface training t=14700, loss=0.023037197068333626\n",
      "Surface training t=14701, loss=0.021835094317793846\n",
      "Surface training t=14702, loss=0.02774345502257347\n",
      "Surface training t=14703, loss=0.024554288014769554\n",
      "Surface training t=14704, loss=0.02858489193022251\n",
      "Surface training t=14705, loss=0.029411046765744686\n",
      "Surface training t=14706, loss=0.03146505355834961\n",
      "Surface training t=14707, loss=0.038954295217990875\n",
      "Surface training t=14708, loss=0.03797786869108677\n",
      "Surface training t=14709, loss=0.04058322124183178\n",
      "Surface training t=14710, loss=0.03413465712219477\n",
      "Surface training t=14711, loss=0.026638170704245567\n",
      "Surface training t=14712, loss=0.022735940292477608\n",
      "Surface training t=14713, loss=0.03054061532020569\n",
      "Surface training t=14714, loss=0.024008406326174736\n",
      "Surface training t=14715, loss=0.02375233918428421\n",
      "Surface training t=14716, loss=0.027510536834597588\n",
      "Surface training t=14717, loss=0.022900204174220562\n",
      "Surface training t=14718, loss=0.019546416588127613\n",
      "Surface training t=14719, loss=0.021224599331617355\n",
      "Surface training t=14720, loss=0.02486582938581705\n",
      "Surface training t=14721, loss=0.034919507801532745\n",
      "Surface training t=14722, loss=0.04531195014715195\n",
      "Surface training t=14723, loss=0.04703045729547739\n",
      "Surface training t=14724, loss=0.0415448434650898\n",
      "Surface training t=14725, loss=0.04278997704386711\n",
      "Surface training t=14726, loss=0.03605066239833832\n",
      "Surface training t=14727, loss=0.027237174101173878\n",
      "Surface training t=14728, loss=0.031683717854321\n",
      "Surface training t=14729, loss=0.029679981991648674\n",
      "Surface training t=14730, loss=0.03612198680639267\n",
      "Surface training t=14731, loss=0.037239582277834415\n",
      "Surface training t=14732, loss=0.03319677896797657\n",
      "Surface training t=14733, loss=0.031959060579538345\n",
      "Surface training t=14734, loss=0.03383590281009674\n",
      "Surface training t=14735, loss=0.03275960311293602\n",
      "Surface training t=14736, loss=0.05081087723374367\n",
      "Surface training t=14737, loss=0.0373365031555295\n",
      "Surface training t=14738, loss=0.045803558081388474\n",
      "Surface training t=14739, loss=0.03139459155499935\n",
      "Surface training t=14740, loss=0.034484583884477615\n",
      "Surface training t=14741, loss=0.03156307153403759\n",
      "Surface training t=14742, loss=0.029448915272951126\n",
      "Surface training t=14743, loss=0.029547596350312233\n",
      "Surface training t=14744, loss=0.03971519693732262\n",
      "Surface training t=14745, loss=0.03553934581577778\n",
      "Surface training t=14746, loss=0.05420858971774578\n",
      "Surface training t=14747, loss=0.028970837593078613\n",
      "Surface training t=14748, loss=0.036434208042919636\n",
      "Surface training t=14749, loss=0.04098181053996086\n",
      "Surface training t=14750, loss=0.03720193635672331\n",
      "Surface training t=14751, loss=0.03758283704519272\n",
      "Surface training t=14752, loss=0.028045176528394222\n",
      "Surface training t=14753, loss=0.04377066157758236\n",
      "Surface training t=14754, loss=0.03980712220072746\n",
      "Surface training t=14755, loss=0.031235179863870144\n",
      "Surface training t=14756, loss=0.033427963964641094\n",
      "Surface training t=14757, loss=0.03697965852916241\n",
      "Surface training t=14758, loss=0.04677066393196583\n",
      "Surface training t=14759, loss=0.03896172158420086\n",
      "Surface training t=14760, loss=0.02833027858287096\n",
      "Surface training t=14761, loss=0.030347044579684734\n",
      "Surface training t=14762, loss=0.025543554686009884\n",
      "Surface training t=14763, loss=0.02868015132844448\n",
      "Surface training t=14764, loss=0.03015940450131893\n",
      "Surface training t=14765, loss=0.0282584922388196\n",
      "Surface training t=14766, loss=0.028454345650970936\n",
      "Surface training t=14767, loss=0.027065057307481766\n",
      "Surface training t=14768, loss=0.031190217472612858\n",
      "Surface training t=14769, loss=0.031790840439498425\n",
      "Surface training t=14770, loss=0.028010668233036995\n",
      "Surface training t=14771, loss=0.025015609338879585\n",
      "Surface training t=14772, loss=0.017526039853692055\n",
      "Surface training t=14773, loss=0.018713164143264294\n",
      "Surface training t=14774, loss=0.021724597550928593\n",
      "Surface training t=14775, loss=0.024292951449751854\n",
      "Surface training t=14776, loss=0.01885318197309971\n",
      "Surface training t=14777, loss=0.02075230050832033\n",
      "Surface training t=14778, loss=0.022893519140779972\n",
      "Surface training t=14779, loss=0.025885568000376225\n",
      "Surface training t=14780, loss=0.027670307084918022\n",
      "Surface training t=14781, loss=0.020160396583378315\n",
      "Surface training t=14782, loss=0.027111073955893517\n",
      "Surface training t=14783, loss=0.02764908503741026\n",
      "Surface training t=14784, loss=0.032979000359773636\n",
      "Surface training t=14785, loss=0.025374198332428932\n",
      "Surface training t=14786, loss=0.02324309851974249\n",
      "Surface training t=14787, loss=0.019592080265283585\n",
      "Surface training t=14788, loss=0.021181391552090645\n",
      "Surface training t=14789, loss=0.024914919398725033\n",
      "Surface training t=14790, loss=0.029047873802483082\n",
      "Surface training t=14791, loss=0.020766916684806347\n",
      "Surface training t=14792, loss=0.022528184577822685\n",
      "Surface training t=14793, loss=0.017069464549422264\n",
      "Surface training t=14794, loss=0.0207609455101192\n",
      "Surface training t=14795, loss=0.02234857715666294\n",
      "Surface training t=14796, loss=0.02326724585145712\n",
      "Surface training t=14797, loss=0.026257596909999847\n",
      "Surface training t=14798, loss=0.03443365916609764\n",
      "Surface training t=14799, loss=0.03334043733775616\n",
      "Surface training t=14800, loss=0.026318836957216263\n",
      "Surface training t=14801, loss=0.022576837800443172\n",
      "Surface training t=14802, loss=0.019845161586999893\n",
      "Surface training t=14803, loss=0.02001618780195713\n",
      "Surface training t=14804, loss=0.02692379616200924\n",
      "Surface training t=14805, loss=0.024166779592633247\n",
      "Surface training t=14806, loss=0.022196709644049406\n",
      "Surface training t=14807, loss=0.04143448360264301\n",
      "Surface training t=14808, loss=0.030840329825878143\n",
      "Surface training t=14809, loss=0.044484635815024376\n",
      "Surface training t=14810, loss=0.03708674293011427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=14811, loss=0.03461820725351572\n",
      "Surface training t=14812, loss=0.0714222714304924\n",
      "Surface training t=14813, loss=0.051839519292116165\n",
      "Surface training t=14814, loss=0.07042700238525867\n",
      "Surface training t=14815, loss=0.05704209767282009\n",
      "Surface training t=14816, loss=0.050358790904283524\n",
      "Surface training t=14817, loss=0.07651129737496376\n",
      "Surface training t=14818, loss=0.04312440566718578\n",
      "Surface training t=14819, loss=0.060027822852134705\n",
      "Surface training t=14820, loss=0.04641519859433174\n",
      "Surface training t=14821, loss=0.060072725638747215\n",
      "Surface training t=14822, loss=0.05240422673523426\n",
      "Surface training t=14823, loss=0.05202162079513073\n",
      "Surface training t=14824, loss=0.03927704505622387\n",
      "Surface training t=14825, loss=0.044966939836740494\n",
      "Surface training t=14826, loss=0.04414377175271511\n",
      "Surface training t=14827, loss=0.03893780708312988\n",
      "Surface training t=14828, loss=0.05810593254864216\n",
      "Surface training t=14829, loss=0.04475197568535805\n",
      "Surface training t=14830, loss=0.04094741307199001\n",
      "Surface training t=14831, loss=0.05159301869571209\n",
      "Surface training t=14832, loss=0.06393280811607838\n",
      "Surface training t=14833, loss=0.04481532610952854\n",
      "Surface training t=14834, loss=0.04728267714381218\n",
      "Surface training t=14835, loss=0.06111092120409012\n",
      "Surface training t=14836, loss=0.04479987360537052\n",
      "Surface training t=14837, loss=0.03720259107649326\n",
      "Surface training t=14838, loss=0.029525652527809143\n",
      "Surface training t=14839, loss=0.034794121980667114\n",
      "Surface training t=14840, loss=0.03038083389401436\n",
      "Surface training t=14841, loss=0.02216207655146718\n",
      "Surface training t=14842, loss=0.0222352659329772\n",
      "Surface training t=14843, loss=0.025169952772557735\n",
      "Surface training t=14844, loss=0.02540847286581993\n",
      "Surface training t=14845, loss=0.022720327600836754\n",
      "Surface training t=14846, loss=0.02928626537322998\n",
      "Surface training t=14847, loss=0.025493310764431953\n",
      "Surface training t=14848, loss=0.020718250423669815\n",
      "Surface training t=14849, loss=0.026504913344979286\n",
      "Surface training t=14850, loss=0.027267961762845516\n",
      "Surface training t=14851, loss=0.04407843388617039\n",
      "Surface training t=14852, loss=0.022735510021448135\n",
      "Surface training t=14853, loss=0.026456650346517563\n",
      "Surface training t=14854, loss=0.03276762552559376\n",
      "Surface training t=14855, loss=0.031022622250020504\n",
      "Surface training t=14856, loss=0.02696584351360798\n",
      "Surface training t=14857, loss=0.05499829351902008\n",
      "Surface training t=14858, loss=0.04908584617078304\n",
      "Surface training t=14859, loss=0.03934415057301521\n",
      "Surface training t=14860, loss=0.033466508612036705\n",
      "Surface training t=14861, loss=0.03024081140756607\n",
      "Surface training t=14862, loss=0.029530229978263378\n",
      "Surface training t=14863, loss=0.047477228567004204\n",
      "Surface training t=14864, loss=0.05001882463693619\n",
      "Surface training t=14865, loss=0.04491589684039354\n",
      "Surface training t=14866, loss=0.04784647561609745\n",
      "Surface training t=14867, loss=0.05440368130803108\n",
      "Surface training t=14868, loss=0.046669915318489075\n",
      "Surface training t=14869, loss=0.038807233795523643\n",
      "Surface training t=14870, loss=0.034582775086164474\n",
      "Surface training t=14871, loss=0.042741917073726654\n",
      "Surface training t=14872, loss=0.029893139377236366\n",
      "Surface training t=14873, loss=0.039271388202905655\n",
      "Surface training t=14874, loss=0.030129238963127136\n",
      "Surface training t=14875, loss=0.028353605419397354\n",
      "Surface training t=14876, loss=0.031021030619740486\n",
      "Surface training t=14877, loss=0.02644565049558878\n",
      "Surface training t=14878, loss=0.027708424255251884\n",
      "Surface training t=14879, loss=0.021678715012967587\n",
      "Surface training t=14880, loss=0.02133149653673172\n",
      "Surface training t=14881, loss=0.016151142306625843\n",
      "Surface training t=14882, loss=0.024267311207950115\n",
      "Surface training t=14883, loss=0.0237449137493968\n",
      "Surface training t=14884, loss=0.029681353829801083\n",
      "Surface training t=14885, loss=0.036926064640283585\n",
      "Surface training t=14886, loss=0.03759586811065674\n",
      "Surface training t=14887, loss=0.037416426464915276\n",
      "Surface training t=14888, loss=0.03517606668174267\n",
      "Surface training t=14889, loss=0.03646175004541874\n",
      "Surface training t=14890, loss=0.03373667877167463\n",
      "Surface training t=14891, loss=0.02578025683760643\n",
      "Surface training t=14892, loss=0.031856922432780266\n",
      "Surface training t=14893, loss=0.03257541637867689\n",
      "Surface training t=14894, loss=0.032198451459407806\n",
      "Surface training t=14895, loss=0.037250662222504616\n",
      "Surface training t=14896, loss=0.029604660347104073\n",
      "Surface training t=14897, loss=0.048388609662652016\n",
      "Surface training t=14898, loss=0.03771193511784077\n",
      "Surface training t=14899, loss=0.03756230976432562\n",
      "Surface training t=14900, loss=0.044977810233831406\n",
      "Surface training t=14901, loss=0.04035212658345699\n",
      "Surface training t=14902, loss=0.03862352296710014\n",
      "Surface training t=14903, loss=0.03472587279975414\n",
      "Surface training t=14904, loss=0.03791241720318794\n",
      "Surface training t=14905, loss=0.0438633281737566\n",
      "Surface training t=14906, loss=0.04833095520734787\n",
      "Surface training t=14907, loss=0.05820138566195965\n",
      "Surface training t=14908, loss=0.04235684685409069\n",
      "Surface training t=14909, loss=0.03537747077643871\n",
      "Surface training t=14910, loss=0.030656966380774975\n",
      "Surface training t=14911, loss=0.022597920149564743\n",
      "Surface training t=14912, loss=0.026238396763801575\n",
      "Surface training t=14913, loss=0.026418662630021572\n",
      "Surface training t=14914, loss=0.029482011683285236\n",
      "Surface training t=14915, loss=0.027490461245179176\n",
      "Surface training t=14916, loss=0.024658330716192722\n",
      "Surface training t=14917, loss=0.023109283298254013\n",
      "Surface training t=14918, loss=0.03708342835307121\n",
      "Surface training t=14919, loss=0.02940687444061041\n",
      "Surface training t=14920, loss=0.02795390971004963\n",
      "Surface training t=14921, loss=0.024564845487475395\n",
      "Surface training t=14922, loss=0.02080015279352665\n",
      "Surface training t=14923, loss=0.020203428342938423\n",
      "Surface training t=14924, loss=0.02398399729281664\n",
      "Surface training t=14925, loss=0.032088056206703186\n",
      "Surface training t=14926, loss=0.02939747367054224\n",
      "Surface training t=14927, loss=0.0240129129961133\n",
      "Surface training t=14928, loss=0.0209078686311841\n",
      "Surface training t=14929, loss=0.019591777585446835\n",
      "Surface training t=14930, loss=0.021213104017078876\n",
      "Surface training t=14931, loss=0.02247515320777893\n",
      "Surface training t=14932, loss=0.024536553770303726\n",
      "Surface training t=14933, loss=0.022672190330922604\n",
      "Surface training t=14934, loss=0.01628392841666937\n",
      "Surface training t=14935, loss=0.02616934757679701\n",
      "Surface training t=14936, loss=0.03901160508394241\n",
      "Surface training t=14937, loss=0.04927893355488777\n",
      "Surface training t=14938, loss=0.03357283677905798\n",
      "Surface training t=14939, loss=0.03038155660033226\n",
      "Surface training t=14940, loss=0.03120441921055317\n",
      "Surface training t=14941, loss=0.03806396760046482\n",
      "Surface training t=14942, loss=0.03839998412877321\n",
      "Surface training t=14943, loss=0.028905510902404785\n",
      "Surface training t=14944, loss=0.02982563991099596\n",
      "Surface training t=14945, loss=0.04795660637319088\n",
      "Surface training t=14946, loss=0.06529025733470917\n",
      "Surface training t=14947, loss=0.05051240138709545\n",
      "Surface training t=14948, loss=0.06377586163580418\n",
      "Surface training t=14949, loss=0.05951935984194279\n",
      "Surface training t=14950, loss=0.060124997049570084\n",
      "Surface training t=14951, loss=0.04706851579248905\n",
      "Surface training t=14952, loss=0.049412135034799576\n",
      "Surface training t=14953, loss=0.045802166685462\n",
      "Surface training t=14954, loss=0.04013565741479397\n",
      "Surface training t=14955, loss=0.051699692383408546\n",
      "Surface training t=14956, loss=0.04192976653575897\n",
      "Surface training t=14957, loss=0.035728223621845245\n",
      "Surface training t=14958, loss=0.033735780976712704\n",
      "Surface training t=14959, loss=0.03273976594209671\n",
      "Surface training t=14960, loss=0.027272884733974934\n",
      "Surface training t=14961, loss=0.02219737507402897\n",
      "Surface training t=14962, loss=0.019547978416085243\n",
      "Surface training t=14963, loss=0.0231284461915493\n",
      "Surface training t=14964, loss=0.024099870584905148\n",
      "Surface training t=14965, loss=0.028719897381961346\n",
      "Surface training t=14966, loss=0.026288467459380627\n",
      "Surface training t=14967, loss=0.024433079175651073\n",
      "Surface training t=14968, loss=0.020517664030194283\n",
      "Surface training t=14969, loss=0.02209391538053751\n",
      "Surface training t=14970, loss=0.03798668086528778\n",
      "Surface training t=14971, loss=0.03527834080159664\n",
      "Surface training t=14972, loss=0.03267460409551859\n",
      "Surface training t=14973, loss=0.03992060758173466\n",
      "Surface training t=14974, loss=0.035689364187419415\n",
      "Surface training t=14975, loss=0.04112738184630871\n",
      "Surface training t=14976, loss=0.04470241069793701\n",
      "Surface training t=14977, loss=0.05426301434636116\n",
      "Surface training t=14978, loss=0.0325406389310956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=14979, loss=0.030154483392834663\n",
      "Surface training t=14980, loss=0.038306355476379395\n",
      "Surface training t=14981, loss=0.03475421015173197\n",
      "Surface training t=14982, loss=0.03340405412018299\n",
      "Surface training t=14983, loss=0.029585043899714947\n",
      "Surface training t=14984, loss=0.036237264052033424\n",
      "Surface training t=14985, loss=0.043970199301838875\n",
      "Surface training t=14986, loss=0.03379361890256405\n",
      "Surface training t=14987, loss=0.058759186416864395\n",
      "Surface training t=14988, loss=0.032475823536515236\n",
      "Surface training t=14989, loss=0.03688844479620457\n",
      "Surface training t=14990, loss=0.03402273915708065\n",
      "Surface training t=14991, loss=0.037831103429198265\n",
      "Surface training t=14992, loss=0.03988667204976082\n",
      "Surface training t=14993, loss=0.03787502646446228\n",
      "Surface training t=14994, loss=0.037396226078271866\n",
      "Surface training t=14995, loss=0.027577885426580906\n",
      "Surface training t=14996, loss=0.024708544835448265\n",
      "Surface training t=14997, loss=0.030634109862148762\n",
      "Surface training t=14998, loss=0.02557909395545721\n",
      "Surface training t=14999, loss=0.028318684548139572\n",
      "Surface training t=15000, loss=0.035872168838977814\n",
      "Surface training t=15001, loss=0.021843967959284782\n",
      "Surface training t=15002, loss=0.020537002943456173\n",
      "Surface training t=15003, loss=0.028834285214543343\n",
      "Surface training t=15004, loss=0.027310844510793686\n",
      "Surface training t=15005, loss=0.03763979580253363\n",
      "Surface training t=15006, loss=0.03889341000467539\n",
      "Surface training t=15007, loss=0.038733034394681454\n",
      "Surface training t=15008, loss=0.03505484201014042\n",
      "Surface training t=15009, loss=0.029126303270459175\n",
      "Surface training t=15010, loss=0.024851874448359013\n",
      "Surface training t=15011, loss=0.02406773716211319\n",
      "Surface training t=15012, loss=0.025246864184737206\n",
      "Surface training t=15013, loss=0.025350390002131462\n",
      "Surface training t=15014, loss=0.022890202701091766\n",
      "Surface training t=15015, loss=0.026039733551442623\n",
      "Surface training t=15016, loss=0.028854187577962875\n",
      "Surface training t=15017, loss=0.030506672337651253\n",
      "Surface training t=15018, loss=0.031903572380542755\n",
      "Surface training t=15019, loss=0.030222252942621708\n",
      "Surface training t=15020, loss=0.02879152912646532\n",
      "Surface training t=15021, loss=0.022390280850231647\n",
      "Surface training t=15022, loss=0.029756231233477592\n",
      "Surface training t=15023, loss=0.025926836766302586\n",
      "Surface training t=15024, loss=0.021105157677084208\n",
      "Surface training t=15025, loss=0.02641029842197895\n",
      "Surface training t=15026, loss=0.02876809611916542\n",
      "Surface training t=15027, loss=0.026555745862424374\n",
      "Surface training t=15028, loss=0.031376615166664124\n",
      "Surface training t=15029, loss=0.028607498854398727\n",
      "Surface training t=15030, loss=0.03642373904585838\n",
      "Surface training t=15031, loss=0.04230774752795696\n",
      "Surface training t=15032, loss=0.0354207344353199\n",
      "Surface training t=15033, loss=0.042456965893507004\n",
      "Surface training t=15034, loss=0.037848230451345444\n",
      "Surface training t=15035, loss=0.02906200662255287\n",
      "Surface training t=15036, loss=0.029597804881632328\n",
      "Surface training t=15037, loss=0.02652245294302702\n",
      "Surface training t=15038, loss=0.024859524331986904\n",
      "Surface training t=15039, loss=0.02321698982268572\n",
      "Surface training t=15040, loss=0.017046797089278698\n",
      "Surface training t=15041, loss=0.019387600012123585\n",
      "Surface training t=15042, loss=0.022272989153862\n",
      "Surface training t=15043, loss=0.01897173747420311\n",
      "Surface training t=15044, loss=0.026109798811376095\n",
      "Surface training t=15045, loss=0.01792672462761402\n",
      "Surface training t=15046, loss=0.021791782695800066\n",
      "Surface training t=15047, loss=0.035009680315852165\n",
      "Surface training t=15048, loss=0.028726722113788128\n",
      "Surface training t=15049, loss=0.04067746177315712\n",
      "Surface training t=15050, loss=0.0571853406727314\n",
      "Surface training t=15051, loss=0.03335770592093468\n",
      "Surface training t=15052, loss=0.03090020827949047\n",
      "Surface training t=15053, loss=0.03840817138552666\n",
      "Surface training t=15054, loss=0.04157673195004463\n",
      "Surface training t=15055, loss=0.03095510322600603\n",
      "Surface training t=15056, loss=0.0467566829174757\n",
      "Surface training t=15057, loss=0.034106602892279625\n",
      "Surface training t=15058, loss=0.037356168031692505\n",
      "Surface training t=15059, loss=0.032756232656538486\n",
      "Surface training t=15060, loss=0.03270841855555773\n",
      "Surface training t=15061, loss=0.031180775724351406\n",
      "Surface training t=15062, loss=0.03857925906777382\n",
      "Surface training t=15063, loss=0.043291373178362846\n",
      "Surface training t=15064, loss=0.03936300054192543\n",
      "Surface training t=15065, loss=0.023632358759641647\n",
      "Surface training t=15066, loss=0.02654614206403494\n",
      "Surface training t=15067, loss=0.03094937466084957\n",
      "Surface training t=15068, loss=0.04275118000805378\n",
      "Surface training t=15069, loss=0.04324040561914444\n",
      "Surface training t=15070, loss=0.03128260839730501\n",
      "Surface training t=15071, loss=0.035753706470131874\n",
      "Surface training t=15072, loss=0.03572102636098862\n",
      "Surface training t=15073, loss=0.03953784145414829\n",
      "Surface training t=15074, loss=0.03190483897924423\n",
      "Surface training t=15075, loss=0.023926792666316032\n",
      "Surface training t=15076, loss=0.025533176958560944\n",
      "Surface training t=15077, loss=0.02758217044174671\n",
      "Surface training t=15078, loss=0.020210150629281998\n",
      "Surface training t=15079, loss=0.022624392993748188\n",
      "Surface training t=15080, loss=0.026112692430615425\n",
      "Surface training t=15081, loss=0.023042311891913414\n",
      "Surface training t=15082, loss=0.023256690241396427\n",
      "Surface training t=15083, loss=0.03921345993876457\n",
      "Surface training t=15084, loss=0.03160989470779896\n",
      "Surface training t=15085, loss=0.038107577711343765\n",
      "Surface training t=15086, loss=0.04736359976232052\n",
      "Surface training t=15087, loss=0.04325043223798275\n",
      "Surface training t=15088, loss=0.03668496757745743\n",
      "Surface training t=15089, loss=0.03139086626470089\n",
      "Surface training t=15090, loss=0.03127029538154602\n",
      "Surface training t=15091, loss=0.03858558461070061\n",
      "Surface training t=15092, loss=0.04557598754763603\n",
      "Surface training t=15093, loss=0.038238679990172386\n",
      "Surface training t=15094, loss=0.04293639771640301\n",
      "Surface training t=15095, loss=0.036249094642698765\n",
      "Surface training t=15096, loss=0.03465061914175749\n",
      "Surface training t=15097, loss=0.04929704591631889\n",
      "Surface training t=15098, loss=0.0317152738571167\n",
      "Surface training t=15099, loss=0.03229132667183876\n",
      "Surface training t=15100, loss=0.023047572001814842\n",
      "Surface training t=15101, loss=0.03435772471129894\n",
      "Surface training t=15102, loss=0.020357532426714897\n",
      "Surface training t=15103, loss=0.023405645042657852\n",
      "Surface training t=15104, loss=0.022531109862029552\n",
      "Surface training t=15105, loss=0.024169535376131535\n",
      "Surface training t=15106, loss=0.025872550904750824\n",
      "Surface training t=15107, loss=0.02155851572751999\n",
      "Surface training t=15108, loss=0.028785265050828457\n",
      "Surface training t=15109, loss=0.026066862046718597\n",
      "Surface training t=15110, loss=0.036949075758457184\n",
      "Surface training t=15111, loss=0.04245085082948208\n",
      "Surface training t=15112, loss=0.03791217878460884\n",
      "Surface training t=15113, loss=0.0314197214320302\n",
      "Surface training t=15114, loss=0.03676407225430012\n",
      "Surface training t=15115, loss=0.024505620822310448\n",
      "Surface training t=15116, loss=0.030171263962984085\n",
      "Surface training t=15117, loss=0.03244854137301445\n",
      "Surface training t=15118, loss=0.0314783938229084\n",
      "Surface training t=15119, loss=0.029688320122659206\n",
      "Surface training t=15120, loss=0.037624698132276535\n",
      "Surface training t=15121, loss=0.03360296320170164\n",
      "Surface training t=15122, loss=0.024783072993159294\n",
      "Surface training t=15123, loss=0.02455913368612528\n",
      "Surface training t=15124, loss=0.02298442553728819\n",
      "Surface training t=15125, loss=0.03401149809360504\n",
      "Surface training t=15126, loss=0.029489918611943722\n",
      "Surface training t=15127, loss=0.0231455834582448\n",
      "Surface training t=15128, loss=0.04484408348798752\n",
      "Surface training t=15129, loss=0.04525492712855339\n",
      "Surface training t=15130, loss=0.02879390586167574\n",
      "Surface training t=15131, loss=0.03544835653156042\n",
      "Surface training t=15132, loss=0.03576475288718939\n",
      "Surface training t=15133, loss=0.03383990749716759\n",
      "Surface training t=15134, loss=0.03099407535046339\n",
      "Surface training t=15135, loss=0.03017563559114933\n",
      "Surface training t=15136, loss=0.02414026390761137\n",
      "Surface training t=15137, loss=0.021268337033689022\n",
      "Surface training t=15138, loss=0.025412194430828094\n",
      "Surface training t=15139, loss=0.020321286283433437\n",
      "Surface training t=15140, loss=0.020250589586794376\n",
      "Surface training t=15141, loss=0.01946160662919283\n",
      "Surface training t=15142, loss=0.022019676864147186\n",
      "Surface training t=15143, loss=0.027998490259051323\n",
      "Surface training t=15144, loss=0.021237329579889774\n",
      "Surface training t=15145, loss=0.026991084218025208\n",
      "Surface training t=15146, loss=0.029815114103257656\n",
      "Surface training t=15147, loss=0.036661023274064064\n",
      "Surface training t=15148, loss=0.03510045353323221\n",
      "Surface training t=15149, loss=0.03552866820245981\n",
      "Surface training t=15150, loss=0.033400509506464005\n",
      "Surface training t=15151, loss=0.031026553362607956\n",
      "Surface training t=15152, loss=0.036613866686820984\n",
      "Surface training t=15153, loss=0.02529510110616684\n",
      "Surface training t=15154, loss=0.03137247357517481\n",
      "Surface training t=15155, loss=0.025029560551047325\n",
      "Surface training t=15156, loss=0.034587765112519264\n",
      "Surface training t=15157, loss=0.02968159131705761\n",
      "Surface training t=15158, loss=0.02357185073196888\n",
      "Surface training t=15159, loss=0.02335439156740904\n",
      "Surface training t=15160, loss=0.029448282904922962\n",
      "Surface training t=15161, loss=0.022786548361182213\n",
      "Surface training t=15162, loss=0.025141184218227863\n",
      "Surface training t=15163, loss=0.03565248288214207\n",
      "Surface training t=15164, loss=0.04121347889304161\n",
      "Surface training t=15165, loss=0.024807706475257874\n",
      "Surface training t=15166, loss=0.03632932901382446\n",
      "Surface training t=15167, loss=0.023955239914357662\n",
      "Surface training t=15168, loss=0.03843092918395996\n",
      "Surface training t=15169, loss=0.0333851370960474\n",
      "Surface training t=15170, loss=0.02741834055632353\n",
      "Surface training t=15171, loss=0.03315651975572109\n",
      "Surface training t=15172, loss=0.048325708135962486\n",
      "Surface training t=15173, loss=0.0323941595852375\n",
      "Surface training t=15174, loss=0.04125814884901047\n",
      "Surface training t=15175, loss=0.04856242425739765\n",
      "Surface training t=15176, loss=0.05409959889948368\n",
      "Surface training t=15177, loss=0.044672444462776184\n",
      "Surface training t=15178, loss=0.03969264030456543\n",
      "Surface training t=15179, loss=0.03425964340567589\n",
      "Surface training t=15180, loss=0.037174616008996964\n",
      "Surface training t=15181, loss=0.03571275994181633\n",
      "Surface training t=15182, loss=0.03779183514416218\n",
      "Surface training t=15183, loss=0.030110825784504414\n",
      "Surface training t=15184, loss=0.02526071108877659\n",
      "Surface training t=15185, loss=0.02471269480884075\n",
      "Surface training t=15186, loss=0.019655758515000343\n",
      "Surface training t=15187, loss=0.020947673358023167\n",
      "Surface training t=15188, loss=0.025733912363648415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=15189, loss=0.04071275144815445\n",
      "Surface training t=15190, loss=0.03455535601824522\n",
      "Surface training t=15191, loss=0.03663475066423416\n",
      "Surface training t=15192, loss=0.03549319785088301\n",
      "Surface training t=15193, loss=0.02710307016968727\n",
      "Surface training t=15194, loss=0.037617145106196404\n",
      "Surface training t=15195, loss=0.04118681885302067\n",
      "Surface training t=15196, loss=0.03838116489350796\n",
      "Surface training t=15197, loss=0.051619360223412514\n",
      "Surface training t=15198, loss=0.029988822527229786\n",
      "Surface training t=15199, loss=0.039311593398451805\n",
      "Surface training t=15200, loss=0.02894795872271061\n",
      "Surface training t=15201, loss=0.02887051459401846\n",
      "Surface training t=15202, loss=0.029929548501968384\n",
      "Surface training t=15203, loss=0.022359635680913925\n",
      "Surface training t=15204, loss=0.02519870176911354\n",
      "Surface training t=15205, loss=0.03346828743815422\n",
      "Surface training t=15206, loss=0.028022761456668377\n",
      "Surface training t=15207, loss=0.027102001011371613\n",
      "Surface training t=15208, loss=0.05546630918979645\n",
      "Surface training t=15209, loss=0.03813370130956173\n",
      "Surface training t=15210, loss=0.04593355022370815\n",
      "Surface training t=15211, loss=0.04206651262938976\n",
      "Surface training t=15212, loss=0.0433470644056797\n",
      "Surface training t=15213, loss=0.0414297254756093\n",
      "Surface training t=15214, loss=0.039000075310468674\n",
      "Surface training t=15215, loss=0.04195425100624561\n",
      "Surface training t=15216, loss=0.06019884906709194\n",
      "Surface training t=15217, loss=0.039081012830138206\n",
      "Surface training t=15218, loss=0.04934836830943823\n",
      "Surface training t=15219, loss=0.04572309926152229\n",
      "Surface training t=15220, loss=0.05005377158522606\n",
      "Surface training t=15221, loss=0.03944585286080837\n",
      "Surface training t=15222, loss=0.06808904372155666\n",
      "Surface training t=15223, loss=0.039072586223483086\n",
      "Surface training t=15224, loss=0.04163086786866188\n",
      "Surface training t=15225, loss=0.03834014758467674\n",
      "Surface training t=15226, loss=0.031206531450152397\n",
      "Surface training t=15227, loss=0.041853344067931175\n",
      "Surface training t=15228, loss=0.039036212489008904\n",
      "Surface training t=15229, loss=0.029719128273427486\n",
      "Surface training t=15230, loss=0.03454127907752991\n",
      "Surface training t=15231, loss=0.028098193928599358\n",
      "Surface training t=15232, loss=0.031557921320199966\n",
      "Surface training t=15233, loss=0.051622333005070686\n",
      "Surface training t=15234, loss=0.03959731571376324\n",
      "Surface training t=15235, loss=0.042023804038763046\n",
      "Surface training t=15236, loss=0.06575890630483627\n",
      "Surface training t=15237, loss=0.046087417751550674\n",
      "Surface training t=15238, loss=0.039937831461429596\n",
      "Surface training t=15239, loss=0.03871818818151951\n",
      "Surface training t=15240, loss=0.05489284172654152\n",
      "Surface training t=15241, loss=0.046274442225694656\n",
      "Surface training t=15242, loss=0.0393533892929554\n",
      "Surface training t=15243, loss=0.04820125363767147\n",
      "Surface training t=15244, loss=0.042128026485443115\n",
      "Surface training t=15245, loss=0.031062286347150803\n",
      "Surface training t=15246, loss=0.03387299180030823\n",
      "Surface training t=15247, loss=0.0425557978451252\n",
      "Surface training t=15248, loss=0.047223107889294624\n",
      "Surface training t=15249, loss=0.03792184218764305\n",
      "Surface training t=15250, loss=0.03543564211577177\n",
      "Surface training t=15251, loss=0.02811131626367569\n",
      "Surface training t=15252, loss=0.027047925628721714\n",
      "Surface training t=15253, loss=0.027216549031436443\n",
      "Surface training t=15254, loss=0.03343955706804991\n",
      "Surface training t=15255, loss=0.031179186888039112\n",
      "Surface training t=15256, loss=0.03171716816723347\n",
      "Surface training t=15257, loss=0.03131342027336359\n",
      "Surface training t=15258, loss=0.0294580589979887\n",
      "Surface training t=15259, loss=0.02232828550040722\n",
      "Surface training t=15260, loss=0.028821103274822235\n",
      "Surface training t=15261, loss=0.034532505087554455\n",
      "Surface training t=15262, loss=0.03942881524562836\n",
      "Surface training t=15263, loss=0.031231829896569252\n",
      "Surface training t=15264, loss=0.03210196923464537\n",
      "Surface training t=15265, loss=0.02366002555936575\n",
      "Surface training t=15266, loss=0.02381425816565752\n",
      "Surface training t=15267, loss=0.02262010332196951\n",
      "Surface training t=15268, loss=0.027956493198871613\n",
      "Surface training t=15269, loss=0.022159294225275517\n",
      "Surface training t=15270, loss=0.01981926243752241\n",
      "Surface training t=15271, loss=0.021706247702240944\n",
      "Surface training t=15272, loss=0.03033525589853525\n",
      "Surface training t=15273, loss=0.04455200769007206\n",
      "Surface training t=15274, loss=0.040818155743181705\n",
      "Surface training t=15275, loss=0.051128849387168884\n",
      "Surface training t=15276, loss=0.05040697939693928\n",
      "Surface training t=15277, loss=0.047275424003601074\n",
      "Surface training t=15278, loss=0.03766264766454697\n",
      "Surface training t=15279, loss=0.047301555052399635\n",
      "Surface training t=15280, loss=0.03742195200175047\n",
      "Surface training t=15281, loss=0.048134759068489075\n",
      "Surface training t=15282, loss=0.04551492538303137\n",
      "Surface training t=15283, loss=0.06094200350344181\n",
      "Surface training t=15284, loss=0.04433344304561615\n",
      "Surface training t=15285, loss=0.06094921752810478\n",
      "Surface training t=15286, loss=0.03937242738902569\n",
      "Surface training t=15287, loss=0.045765455812215805\n",
      "Surface training t=15288, loss=0.03392449487000704\n",
      "Surface training t=15289, loss=0.0332789346575737\n",
      "Surface training t=15290, loss=0.027786923572421074\n",
      "Surface training t=15291, loss=0.035349106416106224\n",
      "Surface training t=15292, loss=0.022494551725685596\n",
      "Surface training t=15293, loss=0.017357012256979942\n",
      "Surface training t=15294, loss=0.02249153610318899\n",
      "Surface training t=15295, loss=0.02370939403772354\n",
      "Surface training t=15296, loss=0.024958311580121517\n",
      "Surface training t=15297, loss=0.022533100098371506\n",
      "Surface training t=15298, loss=0.02431580889970064\n",
      "Surface training t=15299, loss=0.03136770613491535\n",
      "Surface training t=15300, loss=0.03160075657069683\n",
      "Surface training t=15301, loss=0.020031972788274288\n",
      "Surface training t=15302, loss=0.02000970020890236\n",
      "Surface training t=15303, loss=0.029584025964140892\n",
      "Surface training t=15304, loss=0.04356475733220577\n",
      "Surface training t=15305, loss=0.03254327829927206\n",
      "Surface training t=15306, loss=0.037643685936927795\n",
      "Surface training t=15307, loss=0.03524326719343662\n",
      "Surface training t=15308, loss=0.03929763752967119\n",
      "Surface training t=15309, loss=0.031166622415184975\n",
      "Surface training t=15310, loss=0.032771652564406395\n",
      "Surface training t=15311, loss=0.04259094223380089\n",
      "Surface training t=15312, loss=0.023737926967442036\n",
      "Surface training t=15313, loss=0.03349723108112812\n",
      "Surface training t=15314, loss=0.03550969623029232\n",
      "Surface training t=15315, loss=0.03307926468551159\n",
      "Surface training t=15316, loss=0.03476143628358841\n",
      "Surface training t=15317, loss=0.02934842463582754\n",
      "Surface training t=15318, loss=0.02812312263995409\n",
      "Surface training t=15319, loss=0.03105069510638714\n",
      "Surface training t=15320, loss=0.027601461857557297\n",
      "Surface training t=15321, loss=0.03150874376296997\n",
      "Surface training t=15322, loss=0.03230824135243893\n",
      "Surface training t=15323, loss=0.024539240635931492\n",
      "Surface training t=15324, loss=0.02166091650724411\n",
      "Surface training t=15325, loss=0.0265837162733078\n",
      "Surface training t=15326, loss=0.025830180384218693\n",
      "Surface training t=15327, loss=0.026578246615827084\n",
      "Surface training t=15328, loss=0.025950287468731403\n",
      "Surface training t=15329, loss=0.03822942264378071\n",
      "Surface training t=15330, loss=0.03195448126643896\n",
      "Surface training t=15331, loss=0.02652145829051733\n",
      "Surface training t=15332, loss=0.027157222852110863\n",
      "Surface training t=15333, loss=0.01807170920073986\n",
      "Surface training t=15334, loss=0.01905650831758976\n",
      "Surface training t=15335, loss=0.023626035079360008\n",
      "Surface training t=15336, loss=0.024403647519648075\n",
      "Surface training t=15337, loss=0.017880717292428017\n",
      "Surface training t=15338, loss=0.02027930412441492\n",
      "Surface training t=15339, loss=0.019680820405483246\n",
      "Surface training t=15340, loss=0.019655439537018538\n",
      "Surface training t=15341, loss=0.020965982228517532\n",
      "Surface training t=15342, loss=0.02701718918979168\n",
      "Surface training t=15343, loss=0.02578413486480713\n",
      "Surface training t=15344, loss=0.017125731334090233\n",
      "Surface training t=15345, loss=0.016260885633528233\n",
      "Surface training t=15346, loss=0.0142126539722085\n",
      "Surface training t=15347, loss=0.024075536988675594\n",
      "Surface training t=15348, loss=0.025937851518392563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=15349, loss=0.02416436094790697\n",
      "Surface training t=15350, loss=0.025236506946384907\n",
      "Surface training t=15351, loss=0.015507987700402737\n",
      "Surface training t=15352, loss=0.02109680138528347\n",
      "Surface training t=15353, loss=0.02733113430440426\n",
      "Surface training t=15354, loss=0.03207891806960106\n",
      "Surface training t=15355, loss=0.03197050280869007\n",
      "Surface training t=15356, loss=0.03566392604261637\n",
      "Surface training t=15357, loss=0.03284095413982868\n",
      "Surface training t=15358, loss=0.031293464824557304\n",
      "Surface training t=15359, loss=0.026997439563274384\n",
      "Surface training t=15360, loss=0.038122598081827164\n",
      "Surface training t=15361, loss=0.05421802029013634\n",
      "Surface training t=15362, loss=0.04469728656113148\n",
      "Surface training t=15363, loss=0.06290527060627937\n",
      "Surface training t=15364, loss=0.04565666988492012\n",
      "Surface training t=15365, loss=0.046674976125359535\n",
      "Surface training t=15366, loss=0.04143163003027439\n",
      "Surface training t=15367, loss=0.034794473089277744\n",
      "Surface training t=15368, loss=0.027465468272566795\n",
      "Surface training t=15369, loss=0.03175725694745779\n",
      "Surface training t=15370, loss=0.03312279004603624\n",
      "Surface training t=15371, loss=0.030424601398408413\n",
      "Surface training t=15372, loss=0.032782590948045254\n",
      "Surface training t=15373, loss=0.03522895276546478\n",
      "Surface training t=15374, loss=0.04493056237697601\n",
      "Surface training t=15375, loss=0.07506505772471428\n",
      "Surface training t=15376, loss=0.05234985798597336\n",
      "Surface training t=15377, loss=0.08397665992379189\n",
      "Surface training t=15378, loss=0.055520445108413696\n",
      "Surface training t=15379, loss=0.04888099804520607\n",
      "Surface training t=15380, loss=0.07031509466469288\n",
      "Surface training t=15381, loss=0.03769771009683609\n",
      "Surface training t=15382, loss=0.03293406963348389\n",
      "Surface training t=15383, loss=0.039748094975948334\n",
      "Surface training t=15384, loss=0.03569369576871395\n",
      "Surface training t=15385, loss=0.03434084914624691\n",
      "Surface training t=15386, loss=0.03518327884376049\n",
      "Surface training t=15387, loss=0.031870948150753975\n",
      "Surface training t=15388, loss=0.03158479183912277\n",
      "Surface training t=15389, loss=0.030750352889299393\n",
      "Surface training t=15390, loss=0.02449152059853077\n",
      "Surface training t=15391, loss=0.03664942178875208\n",
      "Surface training t=15392, loss=0.023616917431354523\n",
      "Surface training t=15393, loss=0.026852981187403202\n",
      "Surface training t=15394, loss=0.027906293980777264\n",
      "Surface training t=15395, loss=0.025705430656671524\n",
      "Surface training t=15396, loss=0.03971292823553085\n",
      "Surface training t=15397, loss=0.03134367987513542\n",
      "Surface training t=15398, loss=0.026576191186904907\n",
      "Surface training t=15399, loss=0.03695118613541126\n",
      "Surface training t=15400, loss=0.04554547369480133\n",
      "Surface training t=15401, loss=0.03771987184882164\n",
      "Surface training t=15402, loss=0.021064667962491512\n",
      "Surface training t=15403, loss=0.02616407722234726\n",
      "Surface training t=15404, loss=0.027872154489159584\n",
      "Surface training t=15405, loss=0.02815447933971882\n",
      "Surface training t=15406, loss=0.025316506624221802\n",
      "Surface training t=15407, loss=0.03376915864646435\n",
      "Surface training t=15408, loss=0.025631199590861797\n",
      "Surface training t=15409, loss=0.02418704330921173\n",
      "Surface training t=15410, loss=0.024145156145095825\n",
      "Surface training t=15411, loss=0.03291966952383518\n",
      "Surface training t=15412, loss=0.025993015617132187\n",
      "Surface training t=15413, loss=0.054848309606313705\n",
      "Surface training t=15414, loss=0.04414381645619869\n",
      "Surface training t=15415, loss=0.045070938766002655\n",
      "Surface training t=15416, loss=0.030003498308360577\n",
      "Surface training t=15417, loss=0.035712956450879574\n",
      "Surface training t=15418, loss=0.049563098698854446\n",
      "Surface training t=15419, loss=0.04185549356043339\n",
      "Surface training t=15420, loss=0.041443632915616035\n",
      "Surface training t=15421, loss=0.033710477873682976\n",
      "Surface training t=15422, loss=0.0485635232180357\n",
      "Surface training t=15423, loss=0.04300372302532196\n",
      "Surface training t=15424, loss=0.04833174869418144\n",
      "Surface training t=15425, loss=0.062309928238391876\n",
      "Surface training t=15426, loss=0.03285157214850187\n",
      "Surface training t=15427, loss=0.03558841161429882\n",
      "Surface training t=15428, loss=0.0329244714230299\n",
      "Surface training t=15429, loss=0.02749159187078476\n",
      "Surface training t=15430, loss=0.02597676869481802\n",
      "Surface training t=15431, loss=0.029104292392730713\n",
      "Surface training t=15432, loss=0.03578292764723301\n",
      "Surface training t=15433, loss=0.024387413635849953\n",
      "Surface training t=15434, loss=0.027576557360589504\n",
      "Surface training t=15435, loss=0.027365023270249367\n",
      "Surface training t=15436, loss=0.021193822845816612\n",
      "Surface training t=15437, loss=0.02726319432258606\n",
      "Surface training t=15438, loss=0.03134919237345457\n",
      "Surface training t=15439, loss=0.027537078596651554\n",
      "Surface training t=15440, loss=0.023281918838620186\n",
      "Surface training t=15441, loss=0.0259825699031353\n",
      "Surface training t=15442, loss=0.03391887806355953\n",
      "Surface training t=15443, loss=0.03148360084742308\n",
      "Surface training t=15444, loss=0.02999679371714592\n",
      "Surface training t=15445, loss=0.040974076837301254\n",
      "Surface training t=15446, loss=0.02861373219639063\n",
      "Surface training t=15447, loss=0.03354723937809467\n",
      "Surface training t=15448, loss=0.035148030146956444\n",
      "Surface training t=15449, loss=0.04287026263773441\n",
      "Surface training t=15450, loss=0.0577937588095665\n",
      "Surface training t=15451, loss=0.04480113461613655\n",
      "Surface training t=15452, loss=0.06753470003604889\n",
      "Surface training t=15453, loss=0.05906166322529316\n",
      "Surface training t=15454, loss=0.06100947968661785\n",
      "Surface training t=15455, loss=0.0505671501159668\n",
      "Surface training t=15456, loss=0.04942736774682999\n",
      "Surface training t=15457, loss=0.03852817416191101\n",
      "Surface training t=15458, loss=0.045357661321759224\n",
      "Surface training t=15459, loss=0.029058239422738552\n",
      "Surface training t=15460, loss=0.024453267455101013\n",
      "Surface training t=15461, loss=0.028780890628695488\n",
      "Surface training t=15462, loss=0.025208679027855396\n",
      "Surface training t=15463, loss=0.025833027437329292\n",
      "Surface training t=15464, loss=0.0247125206515193\n",
      "Surface training t=15465, loss=0.020214191637933254\n",
      "Surface training t=15466, loss=0.021512201987206936\n",
      "Surface training t=15467, loss=0.021020712330937386\n",
      "Surface training t=15468, loss=0.021337871439754963\n",
      "Surface training t=15469, loss=0.018021277152001858\n",
      "Surface training t=15470, loss=0.04186508618295193\n",
      "Surface training t=15471, loss=0.03175138495862484\n",
      "Surface training t=15472, loss=0.04575423337519169\n",
      "Surface training t=15473, loss=0.03752464707940817\n",
      "Surface training t=15474, loss=0.03318133298307657\n",
      "Surface training t=15475, loss=0.032470593228936195\n",
      "Surface training t=15476, loss=0.027591840364038944\n",
      "Surface training t=15477, loss=0.02767145447432995\n",
      "Surface training t=15478, loss=0.037202777341008186\n",
      "Surface training t=15479, loss=0.023704933933913708\n",
      "Surface training t=15480, loss=0.027558297850191593\n",
      "Surface training t=15481, loss=0.02856991533190012\n",
      "Surface training t=15482, loss=0.025517815724015236\n",
      "Surface training t=15483, loss=0.027482083067297935\n",
      "Surface training t=15484, loss=0.01952300500124693\n",
      "Surface training t=15485, loss=0.021849369630217552\n",
      "Surface training t=15486, loss=0.02296392433345318\n",
      "Surface training t=15487, loss=0.01990081463009119\n",
      "Surface training t=15488, loss=0.017353837378323078\n",
      "Surface training t=15489, loss=0.017746607773005962\n",
      "Surface training t=15490, loss=0.02001695241779089\n",
      "Surface training t=15491, loss=0.023239722475409508\n",
      "Surface training t=15492, loss=0.0204480467364192\n",
      "Surface training t=15493, loss=0.020392264239490032\n",
      "Surface training t=15494, loss=0.025009791366755962\n",
      "Surface training t=15495, loss=0.03015989065170288\n",
      "Surface training t=15496, loss=0.02288348600268364\n",
      "Surface training t=15497, loss=0.02614655252546072\n",
      "Surface training t=15498, loss=0.027360012754797935\n",
      "Surface training t=15499, loss=0.03302668686956167\n",
      "Surface training t=15500, loss=0.02805164735764265\n",
      "Surface training t=15501, loss=0.02438394259661436\n",
      "Surface training t=15502, loss=0.031398115679621696\n",
      "Surface training t=15503, loss=0.032447868026793\n",
      "Surface training t=15504, loss=0.027962549589574337\n",
      "Surface training t=15505, loss=0.030859134159982204\n",
      "Surface training t=15506, loss=0.034785935655236244\n",
      "Surface training t=15507, loss=0.02496548742055893\n",
      "Surface training t=15508, loss=0.02885294985026121\n",
      "Surface training t=15509, loss=0.023092525079846382\n",
      "Surface training t=15510, loss=0.029985088855028152\n",
      "Surface training t=15511, loss=0.02438183967024088\n",
      "Surface training t=15512, loss=0.026176735758781433\n",
      "Surface training t=15513, loss=0.017918616998940706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=15514, loss=0.024211694486439228\n",
      "Surface training t=15515, loss=0.022384281270205975\n",
      "Surface training t=15516, loss=0.02868500631302595\n",
      "Surface training t=15517, loss=0.02128252200782299\n",
      "Surface training t=15518, loss=0.026249799877405167\n",
      "Surface training t=15519, loss=0.0195238059386611\n",
      "Surface training t=15520, loss=0.02688614185899496\n",
      "Surface training t=15521, loss=0.0188873540610075\n",
      "Surface training t=15522, loss=0.02385317161679268\n",
      "Surface training t=15523, loss=0.021115087904036045\n",
      "Surface training t=15524, loss=0.021195690147578716\n",
      "Surface training t=15525, loss=0.019770794548094273\n",
      "Surface training t=15526, loss=0.025441205129027367\n",
      "Surface training t=15527, loss=0.021201717667281628\n",
      "Surface training t=15528, loss=0.030664476566016674\n",
      "Surface training t=15529, loss=0.023585334420204163\n",
      "Surface training t=15530, loss=0.02453522849828005\n",
      "Surface training t=15531, loss=0.03102372493594885\n",
      "Surface training t=15532, loss=0.020865999162197113\n",
      "Surface training t=15533, loss=0.0208194674924016\n",
      "Surface training t=15534, loss=0.024144482798874378\n",
      "Surface training t=15535, loss=0.022643910720944405\n",
      "Surface training t=15536, loss=0.024614524096250534\n",
      "Surface training t=15537, loss=0.028061901219189167\n",
      "Surface training t=15538, loss=0.022330619394779205\n",
      "Surface training t=15539, loss=0.019890643656253815\n",
      "Surface training t=15540, loss=0.01886215340346098\n",
      "Surface training t=15541, loss=0.018754911608994007\n",
      "Surface training t=15542, loss=0.01802636031061411\n",
      "Surface training t=15543, loss=0.020047493278980255\n",
      "Surface training t=15544, loss=0.02153055462986231\n",
      "Surface training t=15545, loss=0.025078846141695976\n",
      "Surface training t=15546, loss=0.022728512063622475\n",
      "Surface training t=15547, loss=0.018027614802122116\n",
      "Surface training t=15548, loss=0.0254152650013566\n",
      "Surface training t=15549, loss=0.017299401573836803\n",
      "Surface training t=15550, loss=0.024010013788938522\n",
      "Surface training t=15551, loss=0.02939168270677328\n",
      "Surface training t=15552, loss=0.034618351608514786\n",
      "Surface training t=15553, loss=0.030536637641489506\n",
      "Surface training t=15554, loss=0.03616218362003565\n",
      "Surface training t=15555, loss=0.0392384622246027\n",
      "Surface training t=15556, loss=0.042323850095272064\n",
      "Surface training t=15557, loss=0.046909451484680176\n",
      "Surface training t=15558, loss=0.044432684779167175\n",
      "Surface training t=15559, loss=0.039827995002269745\n",
      "Surface training t=15560, loss=0.037901872768998146\n",
      "Surface training t=15561, loss=0.04546169936656952\n",
      "Surface training t=15562, loss=0.04757666774094105\n",
      "Surface training t=15563, loss=0.03426308836787939\n",
      "Surface training t=15564, loss=0.029896091669797897\n",
      "Surface training t=15565, loss=0.03173602279275656\n",
      "Surface training t=15566, loss=0.03949769400060177\n",
      "Surface training t=15567, loss=0.03631558362394571\n",
      "Surface training t=15568, loss=0.03747112303972244\n",
      "Surface training t=15569, loss=0.04918411932885647\n",
      "Surface training t=15570, loss=0.0355228828266263\n",
      "Surface training t=15571, loss=0.038060709834098816\n",
      "Surface training t=15572, loss=0.034195429645478725\n",
      "Surface training t=15573, loss=0.03304165229201317\n",
      "Surface training t=15574, loss=0.031914543360471725\n",
      "Surface training t=15575, loss=0.026478610932826996\n",
      "Surface training t=15576, loss=0.03096064832061529\n",
      "Surface training t=15577, loss=0.03717488422989845\n",
      "Surface training t=15578, loss=0.028942499309778214\n",
      "Surface training t=15579, loss=0.02751602977514267\n",
      "Surface training t=15580, loss=0.03315145242959261\n",
      "Surface training t=15581, loss=0.04174906946718693\n",
      "Surface training t=15582, loss=0.03571513667702675\n",
      "Surface training t=15583, loss=0.03578466922044754\n",
      "Surface training t=15584, loss=0.031730953603982925\n",
      "Surface training t=15585, loss=0.02728702686727047\n",
      "Surface training t=15586, loss=0.025094651617109776\n",
      "Surface training t=15587, loss=0.03362848423421383\n",
      "Surface training t=15588, loss=0.02650786004960537\n",
      "Surface training t=15589, loss=0.03254857752472162\n",
      "Surface training t=15590, loss=0.038107750937342644\n",
      "Surface training t=15591, loss=0.02718375436961651\n",
      "Surface training t=15592, loss=0.030176742933690548\n",
      "Surface training t=15593, loss=0.03784124739468098\n",
      "Surface training t=15594, loss=0.028204871341586113\n",
      "Surface training t=15595, loss=0.026469222269952297\n",
      "Surface training t=15596, loss=0.03916606493294239\n",
      "Surface training t=15597, loss=0.029900791123509407\n",
      "Surface training t=15598, loss=0.030290115624666214\n",
      "Surface training t=15599, loss=0.02383136097341776\n",
      "Surface training t=15600, loss=0.022955958731472492\n",
      "Surface training t=15601, loss=0.02294343803077936\n",
      "Surface training t=15602, loss=0.018649865873157978\n",
      "Surface training t=15603, loss=0.016872776672244072\n",
      "Surface training t=15604, loss=0.023253572173416615\n",
      "Surface training t=15605, loss=0.02542850375175476\n",
      "Surface training t=15606, loss=0.028610344976186752\n",
      "Surface training t=15607, loss=0.03114071860909462\n",
      "Surface training t=15608, loss=0.03521896153688431\n",
      "Surface training t=15609, loss=0.03737238235771656\n",
      "Surface training t=15610, loss=0.03153856284916401\n",
      "Surface training t=15611, loss=0.03318065591156483\n",
      "Surface training t=15612, loss=0.027213262394070625\n",
      "Surface training t=15613, loss=0.029233530163764954\n",
      "Surface training t=15614, loss=0.027691401541233063\n",
      "Surface training t=15615, loss=0.03516964986920357\n",
      "Surface training t=15616, loss=0.029642436653375626\n",
      "Surface training t=15617, loss=0.0332756107673049\n",
      "Surface training t=15618, loss=0.036674099043011665\n",
      "Surface training t=15619, loss=0.03099394589662552\n",
      "Surface training t=15620, loss=0.02849547192454338\n",
      "Surface training t=15621, loss=0.026272589340806007\n",
      "Surface training t=15622, loss=0.016142306849360466\n",
      "Surface training t=15623, loss=0.029153630137443542\n",
      "Surface training t=15624, loss=0.03670460171997547\n",
      "Surface training t=15625, loss=0.04553364776074886\n",
      "Surface training t=15626, loss=0.04084263928234577\n",
      "Surface training t=15627, loss=0.03691013716161251\n",
      "Surface training t=15628, loss=0.03231557738035917\n",
      "Surface training t=15629, loss=0.023903843015432358\n",
      "Surface training t=15630, loss=0.02045456040650606\n",
      "Surface training t=15631, loss=0.02794811874628067\n",
      "Surface training t=15632, loss=0.029256063513457775\n",
      "Surface training t=15633, loss=0.03413252532482147\n",
      "Surface training t=15634, loss=0.03475208953022957\n",
      "Surface training t=15635, loss=0.021107859909534454\n",
      "Surface training t=15636, loss=0.036854350939393044\n",
      "Surface training t=15637, loss=0.036046240478754044\n",
      "Surface training t=15638, loss=0.024917740374803543\n",
      "Surface training t=15639, loss=0.023158395662903786\n",
      "Surface training t=15640, loss=0.033392686396837234\n",
      "Surface training t=15641, loss=0.026266107335686684\n",
      "Surface training t=15642, loss=0.02387668751180172\n",
      "Surface training t=15643, loss=0.029869982972741127\n",
      "Surface training t=15644, loss=0.03270428627729416\n",
      "Surface training t=15645, loss=0.038445739075541496\n",
      "Surface training t=15646, loss=0.023693589493632317\n",
      "Surface training t=15647, loss=0.03318726643919945\n",
      "Surface training t=15648, loss=0.0300632668659091\n",
      "Surface training t=15649, loss=0.03037348948419094\n",
      "Surface training t=15650, loss=0.038376014679670334\n",
      "Surface training t=15651, loss=0.026665964163839817\n",
      "Surface training t=15652, loss=0.03278893604874611\n",
      "Surface training t=15653, loss=0.028511458076536655\n",
      "Surface training t=15654, loss=0.056950643658638\n",
      "Surface training t=15655, loss=0.03492315672338009\n",
      "Surface training t=15656, loss=0.030175406485795975\n",
      "Surface training t=15657, loss=0.03374594636261463\n",
      "Surface training t=15658, loss=0.03967026714235544\n",
      "Surface training t=15659, loss=0.04287850111722946\n",
      "Surface training t=15660, loss=0.04228784330189228\n",
      "Surface training t=15661, loss=0.03260484617203474\n",
      "Surface training t=15662, loss=0.033429672941565514\n",
      "Surface training t=15663, loss=0.037964699789881706\n",
      "Surface training t=15664, loss=0.036006041802465916\n",
      "Surface training t=15665, loss=0.06184297613799572\n",
      "Surface training t=15666, loss=0.03635288029909134\n",
      "Surface training t=15667, loss=0.03427094966173172\n",
      "Surface training t=15668, loss=0.03086489997804165\n",
      "Surface training t=15669, loss=0.026690850034356117\n",
      "Surface training t=15670, loss=0.026976278983056545\n",
      "Surface training t=15671, loss=0.023570767603814602\n",
      "Surface training t=15672, loss=0.02251159306615591\n",
      "Surface training t=15673, loss=0.02154921554028988\n",
      "Surface training t=15674, loss=0.025123849511146545\n",
      "Surface training t=15675, loss=0.03247045632451773\n",
      "Surface training t=15676, loss=0.0421422328799963\n",
      "Surface training t=15677, loss=0.04844948649406433\n",
      "Surface training t=15678, loss=0.039688197895884514\n",
      "Surface training t=15679, loss=0.04933101683855057\n",
      "Surface training t=15680, loss=0.045863986015319824\n",
      "Surface training t=15681, loss=0.045191871002316475\n",
      "Surface training t=15682, loss=0.0406420286744833\n",
      "Surface training t=15683, loss=0.03794400952756405\n",
      "Surface training t=15684, loss=0.031433602795004845\n",
      "Surface training t=15685, loss=0.03165876027196646\n",
      "Surface training t=15686, loss=0.030771028250455856\n",
      "Surface training t=15687, loss=0.047853583469986916\n",
      "Surface training t=15688, loss=0.030790260061621666\n",
      "Surface training t=15689, loss=0.03249947726726532\n",
      "Surface training t=15690, loss=0.03166940901428461\n",
      "Surface training t=15691, loss=0.02608345542103052\n",
      "Surface training t=15692, loss=0.023070515133440495\n",
      "Surface training t=15693, loss=0.018668957520276308\n",
      "Surface training t=15694, loss=0.03579794615507126\n",
      "Surface training t=15695, loss=0.030850306153297424\n",
      "Surface training t=15696, loss=0.03931304067373276\n",
      "Surface training t=15697, loss=0.03082057647407055\n",
      "Surface training t=15698, loss=0.032640241086483\n",
      "Surface training t=15699, loss=0.03503426909446716\n",
      "Surface training t=15700, loss=0.04307582601904869\n",
      "Surface training t=15701, loss=0.0357164591550827\n",
      "Surface training t=15702, loss=0.05634156987071037\n",
      "Surface training t=15703, loss=0.04069438762962818\n",
      "Surface training t=15704, loss=0.03789801523089409\n",
      "Surface training t=15705, loss=0.03587358724325895\n",
      "Surface training t=15706, loss=0.03695694636553526\n",
      "Surface training t=15707, loss=0.042252397164702415\n",
      "Surface training t=15708, loss=0.034611888229846954\n",
      "Surface training t=15709, loss=0.04153137095272541\n",
      "Surface training t=15710, loss=0.03830880671739578\n",
      "Surface training t=15711, loss=0.031980348750948906\n",
      "Surface training t=15712, loss=0.0377805233001709\n",
      "Surface training t=15713, loss=0.026146222837269306\n",
      "Surface training t=15714, loss=0.038316212594509125\n",
      "Surface training t=15715, loss=0.027355393394827843\n",
      "Surface training t=15716, loss=0.024930600076913834\n",
      "Surface training t=15717, loss=0.030017894692718983\n",
      "Surface training t=15718, loss=0.029213073663413525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=15719, loss=0.022966885939240456\n",
      "Surface training t=15720, loss=0.025824337266385555\n",
      "Surface training t=15721, loss=0.041364554315805435\n",
      "Surface training t=15722, loss=0.025270920246839523\n",
      "Surface training t=15723, loss=0.029362354427576065\n",
      "Surface training t=15724, loss=0.044282468035817146\n",
      "Surface training t=15725, loss=0.02759317308664322\n",
      "Surface training t=15726, loss=0.030459209345281124\n",
      "Surface training t=15727, loss=0.03619024716317654\n",
      "Surface training t=15728, loss=0.028363638557493687\n",
      "Surface training t=15729, loss=0.03812399320304394\n",
      "Surface training t=15730, loss=0.028976453468203545\n",
      "Surface training t=15731, loss=0.02400352992117405\n",
      "Surface training t=15732, loss=0.03707408159971237\n",
      "Surface training t=15733, loss=0.03212850447744131\n",
      "Surface training t=15734, loss=0.027920091524720192\n",
      "Surface training t=15735, loss=0.027365966700017452\n",
      "Surface training t=15736, loss=0.021671374328434467\n",
      "Surface training t=15737, loss=0.01865849643945694\n",
      "Surface training t=15738, loss=0.01889218483120203\n",
      "Surface training t=15739, loss=0.01709859073162079\n",
      "Surface training t=15740, loss=0.017727931030094624\n",
      "Surface training t=15741, loss=0.019578556530177593\n",
      "Surface training t=15742, loss=0.0204637898132205\n",
      "Surface training t=15743, loss=0.01892266795039177\n",
      "Surface training t=15744, loss=0.02066379226744175\n",
      "Surface training t=15745, loss=0.024406843818724155\n",
      "Surface training t=15746, loss=0.025370892137289047\n",
      "Surface training t=15747, loss=0.026127428747713566\n",
      "Surface training t=15748, loss=0.014717877842485905\n",
      "Surface training t=15749, loss=0.020405221730470657\n",
      "Surface training t=15750, loss=0.021323764696717262\n",
      "Surface training t=15751, loss=0.0203507412225008\n",
      "Surface training t=15752, loss=0.023691777139902115\n",
      "Surface training t=15753, loss=0.025418154895305634\n",
      "Surface training t=15754, loss=0.02696719393134117\n",
      "Surface training t=15755, loss=0.02677479200065136\n",
      "Surface training t=15756, loss=0.03069592732936144\n",
      "Surface training t=15757, loss=0.02797526028007269\n",
      "Surface training t=15758, loss=0.023672141134738922\n",
      "Surface training t=15759, loss=0.028870154172182083\n",
      "Surface training t=15760, loss=0.03052039910107851\n",
      "Surface training t=15761, loss=0.026792348362505436\n",
      "Surface training t=15762, loss=0.05081101693212986\n",
      "Surface training t=15763, loss=0.04384349659085274\n",
      "Surface training t=15764, loss=0.04946546256542206\n",
      "Surface training t=15765, loss=0.03128632716834545\n",
      "Surface training t=15766, loss=0.031805465929210186\n",
      "Surface training t=15767, loss=0.03939596749842167\n",
      "Surface training t=15768, loss=0.02789849042892456\n",
      "Surface training t=15769, loss=0.03110644780099392\n",
      "Surface training t=15770, loss=0.02618452813476324\n",
      "Surface training t=15771, loss=0.02657841145992279\n",
      "Surface training t=15772, loss=0.02599995583295822\n",
      "Surface training t=15773, loss=0.018650048412382603\n",
      "Surface training t=15774, loss=0.03937584348022938\n",
      "Surface training t=15775, loss=0.03137415274977684\n",
      "Surface training t=15776, loss=0.044606706127524376\n",
      "Surface training t=15777, loss=0.03421878628432751\n",
      "Surface training t=15778, loss=0.034990094602108\n",
      "Surface training t=15779, loss=0.034325988963246346\n",
      "Surface training t=15780, loss=0.039339903742074966\n",
      "Surface training t=15781, loss=0.0386742502450943\n",
      "Surface training t=15782, loss=0.0491860955953598\n",
      "Surface training t=15783, loss=0.04571058601140976\n",
      "Surface training t=15784, loss=0.03862587735056877\n",
      "Surface training t=15785, loss=0.03415527194738388\n",
      "Surface training t=15786, loss=0.030937418341636658\n",
      "Surface training t=15787, loss=0.04097258113324642\n",
      "Surface training t=15788, loss=0.028917303308844566\n",
      "Surface training t=15789, loss=0.03164204675704241\n",
      "Surface training t=15790, loss=0.029517545364797115\n",
      "Surface training t=15791, loss=0.02808926161378622\n",
      "Surface training t=15792, loss=0.02355615422129631\n",
      "Surface training t=15793, loss=0.026319251395761967\n",
      "Surface training t=15794, loss=0.028821466490626335\n",
      "Surface training t=15795, loss=0.032861300744116306\n",
      "Surface training t=15796, loss=0.027084503322839737\n",
      "Surface training t=15797, loss=0.023651795461773872\n",
      "Surface training t=15798, loss=0.020556146278977394\n",
      "Surface training t=15799, loss=0.024355669505894184\n",
      "Surface training t=15800, loss=0.020613281056284904\n",
      "Surface training t=15801, loss=0.02462968695908785\n",
      "Surface training t=15802, loss=0.022248380817472935\n",
      "Surface training t=15803, loss=0.019836250692605972\n",
      "Surface training t=15804, loss=0.032396506518125534\n",
      "Surface training t=15805, loss=0.029139386489987373\n",
      "Surface training t=15806, loss=0.02213421557098627\n",
      "Surface training t=15807, loss=0.04499134607613087\n",
      "Surface training t=15808, loss=0.05819929763674736\n",
      "Surface training t=15809, loss=0.04100774694234133\n",
      "Surface training t=15810, loss=0.038033630698919296\n",
      "Surface training t=15811, loss=0.061162879690527916\n",
      "Surface training t=15812, loss=0.06086342968046665\n",
      "Surface training t=15813, loss=0.04741383343935013\n",
      "Surface training t=15814, loss=0.05630430579185486\n",
      "Surface training t=15815, loss=0.05782277137041092\n",
      "Surface training t=15816, loss=0.04812300205230713\n",
      "Surface training t=15817, loss=0.04781578667461872\n",
      "Surface training t=15818, loss=0.0342218354344368\n",
      "Surface training t=15819, loss=0.04122980125248432\n",
      "Surface training t=15820, loss=0.042227379977703094\n",
      "Surface training t=15821, loss=0.040089123882353306\n",
      "Surface training t=15822, loss=0.04154660925269127\n",
      "Surface training t=15823, loss=0.06751811876893044\n",
      "Surface training t=15824, loss=0.04809625819325447\n",
      "Surface training t=15825, loss=0.04420839063823223\n",
      "Surface training t=15826, loss=0.05823804624378681\n",
      "Surface training t=15827, loss=0.04540478251874447\n",
      "Surface training t=15828, loss=0.040117572993040085\n",
      "Surface training t=15829, loss=0.033829337917268276\n",
      "Surface training t=15830, loss=0.031047048047184944\n",
      "Surface training t=15831, loss=0.028113112784922123\n",
      "Surface training t=15832, loss=0.028952203691005707\n",
      "Surface training t=15833, loss=0.03614202607423067\n",
      "Surface training t=15834, loss=0.032952685840427876\n",
      "Surface training t=15835, loss=0.036961160600185394\n",
      "Surface training t=15836, loss=0.04478790983557701\n",
      "Surface training t=15837, loss=0.05377568118274212\n",
      "Surface training t=15838, loss=0.036940788850188255\n",
      "Surface training t=15839, loss=0.03139446675777435\n",
      "Surface training t=15840, loss=0.03294371720403433\n",
      "Surface training t=15841, loss=0.02980584930628538\n",
      "Surface training t=15842, loss=0.028524961322546005\n",
      "Surface training t=15843, loss=0.024851161986589432\n",
      "Surface training t=15844, loss=0.02792180236428976\n",
      "Surface training t=15845, loss=0.029614772647619247\n",
      "Surface training t=15846, loss=0.029156731441617012\n",
      "Surface training t=15847, loss=0.030926935374736786\n",
      "Surface training t=15848, loss=0.034397853538393974\n",
      "Surface training t=15849, loss=0.025825909338891506\n",
      "Surface training t=15850, loss=0.03505315072834492\n",
      "Surface training t=15851, loss=0.028788385912775993\n",
      "Surface training t=15852, loss=0.02509764675050974\n",
      "Surface training t=15853, loss=0.02837403118610382\n",
      "Surface training t=15854, loss=0.019765996374189854\n",
      "Surface training t=15855, loss=0.023956499062478542\n",
      "Surface training t=15856, loss=0.02514677122235298\n",
      "Surface training t=15857, loss=0.026123923249542713\n",
      "Surface training t=15858, loss=0.023922253400087357\n",
      "Surface training t=15859, loss=0.029606235213577747\n",
      "Surface training t=15860, loss=0.028010642156004906\n",
      "Surface training t=15861, loss=0.037012615241110325\n",
      "Surface training t=15862, loss=0.03816164471209049\n",
      "Surface training t=15863, loss=0.025579281151294708\n",
      "Surface training t=15864, loss=0.023591966368258\n",
      "Surface training t=15865, loss=0.025687054730951786\n",
      "Surface training t=15866, loss=0.025973121635615826\n",
      "Surface training t=15867, loss=0.04008401930332184\n",
      "Surface training t=15868, loss=0.025275906547904015\n",
      "Surface training t=15869, loss=0.02748546190559864\n",
      "Surface training t=15870, loss=0.02781838458031416\n",
      "Surface training t=15871, loss=0.032606106251478195\n",
      "Surface training t=15872, loss=0.028908276930451393\n",
      "Surface training t=15873, loss=0.03388374112546444\n",
      "Surface training t=15874, loss=0.03419164847582579\n",
      "Surface training t=15875, loss=0.02495912928134203\n",
      "Surface training t=15876, loss=0.027575630694627762\n",
      "Surface training t=15877, loss=0.028937743976712227\n",
      "Surface training t=15878, loss=0.03310317825525999\n",
      "Surface training t=15879, loss=0.031962063163518906\n",
      "Surface training t=15880, loss=0.04642953537404537\n",
      "Surface training t=15881, loss=0.03439009469002485\n",
      "Surface training t=15882, loss=0.02708382625132799\n",
      "Surface training t=15883, loss=0.034241147339344025\n",
      "Surface training t=15884, loss=0.027898739092051983\n",
      "Surface training t=15885, loss=0.027324694208800793\n",
      "Surface training t=15886, loss=0.02448712196201086\n",
      "Surface training t=15887, loss=0.02239732164889574\n",
      "Surface training t=15888, loss=0.02495412714779377\n",
      "Surface training t=15889, loss=0.025289087556302547\n",
      "Surface training t=15890, loss=0.02763022854924202\n",
      "Surface training t=15891, loss=0.024051327258348465\n",
      "Surface training t=15892, loss=0.020743178203701973\n",
      "Surface training t=15893, loss=0.02023844886571169\n",
      "Surface training t=15894, loss=0.017116709612309933\n",
      "Surface training t=15895, loss=0.026315434835851192\n",
      "Surface training t=15896, loss=0.017716918140649796\n",
      "Surface training t=15897, loss=0.01998956222087145\n",
      "Surface training t=15898, loss=0.021794346161186695\n",
      "Surface training t=15899, loss=0.02272704616189003\n",
      "Surface training t=15900, loss=0.032862789928913116\n",
      "Surface training t=15901, loss=0.027852349914610386\n",
      "Surface training t=15902, loss=0.0326939607039094\n",
      "Surface training t=15903, loss=0.0283629409968853\n",
      "Surface training t=15904, loss=0.042816074565052986\n",
      "Surface training t=15905, loss=0.03959418274462223\n",
      "Surface training t=15906, loss=0.047533201053738594\n",
      "Surface training t=15907, loss=0.03892657905817032\n",
      "Surface training t=15908, loss=0.04215353913605213\n",
      "Surface training t=15909, loss=0.035921262577176094\n",
      "Surface training t=15910, loss=0.026204152964055538\n",
      "Surface training t=15911, loss=0.02870149165391922\n",
      "Surface training t=15912, loss=0.025919771753251553\n",
      "Surface training t=15913, loss=0.02411738410592079\n",
      "Surface training t=15914, loss=0.023363140411674976\n",
      "Surface training t=15915, loss=0.027116266079246998\n",
      "Surface training t=15916, loss=0.03039968479424715\n",
      "Surface training t=15917, loss=0.02259747963398695\n",
      "Surface training t=15918, loss=0.019122297875583172\n",
      "Surface training t=15919, loss=0.026863425970077515\n",
      "Surface training t=15920, loss=0.02401718031615019\n",
      "Surface training t=15921, loss=0.02401942852884531\n",
      "Surface training t=15922, loss=0.0224861279129982\n",
      "Surface training t=15923, loss=0.0207931911572814\n",
      "Surface training t=15924, loss=0.03285770211368799\n",
      "Surface training t=15925, loss=0.030570974573493004\n",
      "Surface training t=15926, loss=0.028778540901839733\n",
      "Surface training t=15927, loss=0.032938034273684025\n",
      "Surface training t=15928, loss=0.03085289429873228\n",
      "Surface training t=15929, loss=0.026128031313419342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=15930, loss=0.02488014753907919\n",
      "Surface training t=15931, loss=0.027758879587054253\n",
      "Surface training t=15932, loss=0.02780450414866209\n",
      "Surface training t=15933, loss=0.035170797258615494\n",
      "Surface training t=15934, loss=0.030254239216446877\n",
      "Surface training t=15935, loss=0.02743374276906252\n",
      "Surface training t=15936, loss=0.0350512471050024\n",
      "Surface training t=15937, loss=0.03870112635195255\n",
      "Surface training t=15938, loss=0.03133537992835045\n",
      "Surface training t=15939, loss=0.030126744881272316\n",
      "Surface training t=15940, loss=0.025047970935702324\n",
      "Surface training t=15941, loss=0.024470741860568523\n",
      "Surface training t=15942, loss=0.029740934260189533\n",
      "Surface training t=15943, loss=0.021259120665490627\n",
      "Surface training t=15944, loss=0.039716100320219994\n",
      "Surface training t=15945, loss=0.03332815505564213\n",
      "Surface training t=15946, loss=0.02914919052273035\n",
      "Surface training t=15947, loss=0.045731544494628906\n",
      "Surface training t=15948, loss=0.040084488689899445\n",
      "Surface training t=15949, loss=0.044205646961927414\n",
      "Surface training t=15950, loss=0.03402782045304775\n",
      "Surface training t=15951, loss=0.0363750234246254\n",
      "Surface training t=15952, loss=0.022162682376801968\n",
      "Surface training t=15953, loss=0.022677174769341946\n",
      "Surface training t=15954, loss=0.021678253076970577\n",
      "Surface training t=15955, loss=0.02234515082091093\n",
      "Surface training t=15956, loss=0.02086067944765091\n",
      "Surface training t=15957, loss=0.016339070163667202\n",
      "Surface training t=15958, loss=0.02245946042239666\n",
      "Surface training t=15959, loss=0.01888938806951046\n",
      "Surface training t=15960, loss=0.020546289160847664\n",
      "Surface training t=15961, loss=0.02753981575369835\n",
      "Surface training t=15962, loss=0.022168569266796112\n",
      "Surface training t=15963, loss=0.023563960567116737\n",
      "Surface training t=15964, loss=0.026290981099009514\n",
      "Surface training t=15965, loss=0.03692127205431461\n",
      "Surface training t=15966, loss=0.02385303657501936\n",
      "Surface training t=15967, loss=0.021740932017564774\n",
      "Surface training t=15968, loss=0.02365689631551504\n",
      "Surface training t=15969, loss=0.024819666519761086\n",
      "Surface training t=15970, loss=0.02175415214151144\n",
      "Surface training t=15971, loss=0.022811430506408215\n",
      "Surface training t=15972, loss=0.03585507534444332\n",
      "Surface training t=15973, loss=0.034321089275181293\n",
      "Surface training t=15974, loss=0.04013654217123985\n",
      "Surface training t=15975, loss=0.02503922674804926\n",
      "Surface training t=15976, loss=0.029342525638639927\n",
      "Surface training t=15977, loss=0.02069673128426075\n",
      "Surface training t=15978, loss=0.028819488361477852\n",
      "Surface training t=15979, loss=0.033446721732616425\n",
      "Surface training t=15980, loss=0.048242220655083656\n",
      "Surface training t=15981, loss=0.04875280614942312\n",
      "Surface training t=15982, loss=0.05031518079340458\n",
      "Surface training t=15983, loss=0.07290169037878513\n",
      "Surface training t=15984, loss=0.05632774718105793\n",
      "Surface training t=15985, loss=0.06741839833557606\n",
      "Surface training t=15986, loss=0.07462773658335209\n",
      "Surface training t=15987, loss=0.04808584600687027\n",
      "Surface training t=15988, loss=0.05640951730310917\n",
      "Surface training t=15989, loss=0.06306791678071022\n",
      "Surface training t=15990, loss=0.04438118264079094\n",
      "Surface training t=15991, loss=0.05683475732803345\n",
      "Surface training t=15992, loss=0.0610501766204834\n",
      "Surface training t=15993, loss=0.04991211835294962\n",
      "Surface training t=15994, loss=0.08789429068565369\n",
      "Surface training t=15995, loss=0.04977790825068951\n",
      "Surface training t=15996, loss=0.04216937534511089\n",
      "Surface training t=15997, loss=0.04152443911880255\n",
      "Surface training t=15998, loss=0.06258906237781048\n",
      "Surface training t=15999, loss=0.04159589111804962\n",
      "Surface training t=16000, loss=0.04414280317723751\n",
      "Surface training t=16001, loss=0.061214471235871315\n",
      "Surface training t=16002, loss=0.06564925611019135\n",
      "Surface training t=16003, loss=0.06103505939245224\n",
      "Surface training t=16004, loss=0.059368645772337914\n",
      "Surface training t=16005, loss=0.03867531195282936\n",
      "Surface training t=16006, loss=0.0452308040112257\n",
      "Surface training t=16007, loss=0.031734032556414604\n",
      "Surface training t=16008, loss=0.02699464652687311\n",
      "Surface training t=16009, loss=0.0380979236215353\n",
      "Surface training t=16010, loss=0.024552849121391773\n",
      "Surface training t=16011, loss=0.029047736898064613\n",
      "Surface training t=16012, loss=0.029621897265315056\n",
      "Surface training t=16013, loss=0.02565467171370983\n",
      "Surface training t=16014, loss=0.02868795581161976\n",
      "Surface training t=16015, loss=0.02549120970070362\n",
      "Surface training t=16016, loss=0.021681216545403004\n",
      "Surface training t=16017, loss=0.023916435427963734\n",
      "Surface training t=16018, loss=0.020113643258810043\n",
      "Surface training t=16019, loss=0.029279636219143867\n",
      "Surface training t=16020, loss=0.03378977719694376\n",
      "Surface training t=16021, loss=0.031064974144101143\n",
      "Surface training t=16022, loss=0.03484161105006933\n",
      "Surface training t=16023, loss=0.036618880927562714\n",
      "Surface training t=16024, loss=0.025171538814902306\n",
      "Surface training t=16025, loss=0.023700186982750893\n",
      "Surface training t=16026, loss=0.027556772343814373\n",
      "Surface training t=16027, loss=0.028946226462721825\n",
      "Surface training t=16028, loss=0.03169481083750725\n",
      "Surface training t=16029, loss=0.025737255811691284\n",
      "Surface training t=16030, loss=0.036737969145178795\n",
      "Surface training t=16031, loss=0.02784332912415266\n",
      "Surface training t=16032, loss=0.021402316633611917\n",
      "Surface training t=16033, loss=0.02726352959871292\n",
      "Surface training t=16034, loss=0.03264736197888851\n",
      "Surface training t=16035, loss=0.021715521812438965\n",
      "Surface training t=16036, loss=0.020827163942158222\n",
      "Surface training t=16037, loss=0.031945718452334404\n",
      "Surface training t=16038, loss=0.022448400035500526\n",
      "Surface training t=16039, loss=0.02402605675160885\n",
      "Surface training t=16040, loss=0.024053932167589664\n",
      "Surface training t=16041, loss=0.02416621334850788\n",
      "Surface training t=16042, loss=0.017988025210797787\n",
      "Surface training t=16043, loss=0.032368121668696404\n",
      "Surface training t=16044, loss=0.027846205979585648\n",
      "Surface training t=16045, loss=0.031165478751063347\n",
      "Surface training t=16046, loss=0.030591626651585102\n",
      "Surface training t=16047, loss=0.03316420875489712\n",
      "Surface training t=16048, loss=0.030993499793112278\n",
      "Surface training t=16049, loss=0.04691632464528084\n",
      "Surface training t=16050, loss=0.033746019937098026\n",
      "Surface training t=16051, loss=0.03352724481374025\n",
      "Surface training t=16052, loss=0.022794504649937153\n",
      "Surface training t=16053, loss=0.02444312535226345\n",
      "Surface training t=16054, loss=0.026347477920353413\n",
      "Surface training t=16055, loss=0.037716373801231384\n",
      "Surface training t=16056, loss=0.029644961468875408\n",
      "Surface training t=16057, loss=0.03695983625948429\n",
      "Surface training t=16058, loss=0.02509856317192316\n",
      "Surface training t=16059, loss=0.020435066893696785\n",
      "Surface training t=16060, loss=0.02069389447569847\n",
      "Surface training t=16061, loss=0.03999643586575985\n",
      "Surface training t=16062, loss=0.0337011544033885\n",
      "Surface training t=16063, loss=0.029693705961108208\n",
      "Surface training t=16064, loss=0.026087770238518715\n",
      "Surface training t=16065, loss=0.025738087482750416\n",
      "Surface training t=16066, loss=0.03819381445646286\n",
      "Surface training t=16067, loss=0.03777011297643185\n",
      "Surface training t=16068, loss=0.042517900466918945\n",
      "Surface training t=16069, loss=0.03941089008003473\n",
      "Surface training t=16070, loss=0.038866691291332245\n",
      "Surface training t=16071, loss=0.02927551604807377\n",
      "Surface training t=16072, loss=0.028436478227376938\n",
      "Surface training t=16073, loss=0.03192823100835085\n",
      "Surface training t=16074, loss=0.02396235056221485\n",
      "Surface training t=16075, loss=0.02357561932876706\n",
      "Surface training t=16076, loss=0.01679617539048195\n",
      "Surface training t=16077, loss=0.017037534154951572\n",
      "Surface training t=16078, loss=0.021189359948039055\n",
      "Surface training t=16079, loss=0.01839878037571907\n",
      "Surface training t=16080, loss=0.022084389813244343\n",
      "Surface training t=16081, loss=0.02477658074349165\n",
      "Surface training t=16082, loss=0.024190410040318966\n",
      "Surface training t=16083, loss=0.028342743404209614\n",
      "Surface training t=16084, loss=0.040196461603045464\n",
      "Surface training t=16085, loss=0.03735689911991358\n",
      "Surface training t=16086, loss=0.06726585328578949\n",
      "Surface training t=16087, loss=0.04134915117174387\n",
      "Surface training t=16088, loss=0.04060852527618408\n",
      "Surface training t=16089, loss=0.032046690583229065\n",
      "Surface training t=16090, loss=0.03726394288241863\n",
      "Surface training t=16091, loss=0.03383242338895798\n",
      "Surface training t=16092, loss=0.04272215627133846\n",
      "Surface training t=16093, loss=0.036128233186900616\n",
      "Surface training t=16094, loss=0.03991597332060337\n",
      "Surface training t=16095, loss=0.04046559892594814\n",
      "Surface training t=16096, loss=0.04301628842949867\n",
      "Surface training t=16097, loss=0.05548678711056709\n",
      "Surface training t=16098, loss=0.04411087557673454\n",
      "Surface training t=16099, loss=0.04613960161805153\n",
      "Surface training t=16100, loss=0.05242433212697506\n",
      "Surface training t=16101, loss=0.05405939556658268\n",
      "Surface training t=16102, loss=0.038655007258057594\n",
      "Surface training t=16103, loss=0.04077538847923279\n",
      "Surface training t=16104, loss=0.03560328669846058\n",
      "Surface training t=16105, loss=0.02771997544914484\n",
      "Surface training t=16106, loss=0.03337400034070015\n",
      "Surface training t=16107, loss=0.028065742924809456\n",
      "Surface training t=16108, loss=0.020280742086470127\n",
      "Surface training t=16109, loss=0.022607408463954926\n",
      "Surface training t=16110, loss=0.026034782640635967\n",
      "Surface training t=16111, loss=0.022899307310581207\n",
      "Surface training t=16112, loss=0.02386600151658058\n",
      "Surface training t=16113, loss=0.02963197883218527\n",
      "Surface training t=16114, loss=0.025495252572000027\n",
      "Surface training t=16115, loss=0.04166559502482414\n",
      "Surface training t=16116, loss=0.04950735718011856\n",
      "Surface training t=16117, loss=0.03293870948255062\n",
      "Surface training t=16118, loss=0.0359977874904871\n",
      "Surface training t=16119, loss=0.0328375780954957\n",
      "Surface training t=16120, loss=0.027188386768102646\n",
      "Surface training t=16121, loss=0.03803757764399052\n",
      "Surface training t=16122, loss=0.029870386235415936\n",
      "Surface training t=16123, loss=0.024349058978259563\n",
      "Surface training t=16124, loss=0.027843660674989223\n",
      "Surface training t=16125, loss=0.0295416247099638\n",
      "Surface training t=16126, loss=0.030907703563570976\n",
      "Surface training t=16127, loss=0.02148399781435728\n",
      "Surface training t=16128, loss=0.023663410916924477\n",
      "Surface training t=16129, loss=0.029586516320705414\n",
      "Surface training t=16130, loss=0.030400780960917473\n",
      "Surface training t=16131, loss=0.025986580178141594\n",
      "Surface training t=16132, loss=0.041925569996237755\n",
      "Surface training t=16133, loss=0.02725112345069647\n",
      "Surface training t=16134, loss=0.03369414061307907\n",
      "Surface training t=16135, loss=0.0305866077542305\n",
      "Surface training t=16136, loss=0.025991201400756836\n",
      "Surface training t=16137, loss=0.024930794723331928\n",
      "Surface training t=16138, loss=0.02915574051439762\n",
      "Surface training t=16139, loss=0.026432378217577934\n",
      "Surface training t=16140, loss=0.033471858128905296\n",
      "Surface training t=16141, loss=0.03119182586669922\n",
      "Surface training t=16142, loss=0.02982445526868105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=16143, loss=0.024021940305829048\n",
      "Surface training t=16144, loss=0.03326742071658373\n",
      "Surface training t=16145, loss=0.029899579472839832\n",
      "Surface training t=16146, loss=0.029256219044327736\n",
      "Surface training t=16147, loss=0.02498300652951002\n",
      "Surface training t=16148, loss=0.02536193560808897\n",
      "Surface training t=16149, loss=0.025866812095046043\n",
      "Surface training t=16150, loss=0.033450646325945854\n",
      "Surface training t=16151, loss=0.03291573282331228\n",
      "Surface training t=16152, loss=0.033883203752338886\n",
      "Surface training t=16153, loss=0.030197005718946457\n",
      "Surface training t=16154, loss=0.03303850628435612\n",
      "Surface training t=16155, loss=0.038406072184443474\n",
      "Surface training t=16156, loss=0.033296141773462296\n",
      "Surface training t=16157, loss=0.04295185208320618\n",
      "Surface training t=16158, loss=0.04157756827771664\n",
      "Surface training t=16159, loss=0.034087625332176685\n",
      "Surface training t=16160, loss=0.048594314604997635\n",
      "Surface training t=16161, loss=0.037968989461660385\n",
      "Surface training t=16162, loss=0.046196091920137405\n",
      "Surface training t=16163, loss=0.031422064639627934\n",
      "Surface training t=16164, loss=0.05147118307650089\n",
      "Surface training t=16165, loss=0.0509542990475893\n",
      "Surface training t=16166, loss=0.06941617280244827\n",
      "Surface training t=16167, loss=0.04586916044354439\n",
      "Surface training t=16168, loss=0.05685681663453579\n",
      "Surface training t=16169, loss=0.04645639657974243\n",
      "Surface training t=16170, loss=0.04948693513870239\n",
      "Surface training t=16171, loss=0.03944068402051926\n",
      "Surface training t=16172, loss=0.04915842041373253\n",
      "Surface training t=16173, loss=0.043433777987957\n",
      "Surface training t=16174, loss=0.037024788558483124\n",
      "Surface training t=16175, loss=0.025087697431445122\n",
      "Surface training t=16176, loss=0.02863250393420458\n",
      "Surface training t=16177, loss=0.02717140782624483\n",
      "Surface training t=16178, loss=0.034452859312295914\n",
      "Surface training t=16179, loss=0.031388552859425545\n",
      "Surface training t=16180, loss=0.027923667803406715\n",
      "Surface training t=16181, loss=0.03827837482094765\n",
      "Surface training t=16182, loss=0.024654727429151535\n",
      "Surface training t=16183, loss=0.02474312763661146\n",
      "Surface training t=16184, loss=0.03100588358938694\n",
      "Surface training t=16185, loss=0.02916189655661583\n",
      "Surface training t=16186, loss=0.032858360558748245\n",
      "Surface training t=16187, loss=0.0333445193246007\n",
      "Surface training t=16188, loss=0.03241423051804304\n",
      "Surface training t=16189, loss=0.03300662338733673\n",
      "Surface training t=16190, loss=0.027826488949358463\n",
      "Surface training t=16191, loss=0.031926026567816734\n",
      "Surface training t=16192, loss=0.031130761839449406\n",
      "Surface training t=16193, loss=0.028623849153518677\n",
      "Surface training t=16194, loss=0.021459725685417652\n",
      "Surface training t=16195, loss=0.024372299201786518\n",
      "Surface training t=16196, loss=0.030791189521551132\n",
      "Surface training t=16197, loss=0.02742127515375614\n",
      "Surface training t=16198, loss=0.028633992187678814\n",
      "Surface training t=16199, loss=0.028700856491923332\n",
      "Surface training t=16200, loss=0.03153786528855562\n",
      "Surface training t=16201, loss=0.026249714195728302\n",
      "Surface training t=16202, loss=0.039607252925634384\n",
      "Surface training t=16203, loss=0.025426399894058704\n",
      "Surface training t=16204, loss=0.025263466872274876\n",
      "Surface training t=16205, loss=0.032185016199946404\n",
      "Surface training t=16206, loss=0.030556950718164444\n",
      "Surface training t=16207, loss=0.024375958368182182\n",
      "Surface training t=16208, loss=0.020555729046463966\n",
      "Surface training t=16209, loss=0.026593961752951145\n",
      "Surface training t=16210, loss=0.029937617480754852\n",
      "Surface training t=16211, loss=0.028554954566061497\n",
      "Surface training t=16212, loss=0.021209669299423695\n",
      "Surface training t=16213, loss=0.02732306346297264\n",
      "Surface training t=16214, loss=0.02750734705477953\n",
      "Surface training t=16215, loss=0.022616303525865078\n",
      "Surface training t=16216, loss=0.02152147237211466\n",
      "Surface training t=16217, loss=0.023656263016164303\n",
      "Surface training t=16218, loss=0.021837467327713966\n",
      "Surface training t=16219, loss=0.01877992507070303\n",
      "Surface training t=16220, loss=0.02890403289347887\n",
      "Surface training t=16221, loss=0.025018448941409588\n",
      "Surface training t=16222, loss=0.021733202040195465\n",
      "Surface training t=16223, loss=0.03215754218399525\n",
      "Surface training t=16224, loss=0.027107037603855133\n",
      "Surface training t=16225, loss=0.037088869139552116\n",
      "Surface training t=16226, loss=0.051128579303622246\n",
      "Surface training t=16227, loss=0.03647992666810751\n",
      "Surface training t=16228, loss=0.03599393926560879\n",
      "Surface training t=16229, loss=0.031462738290429115\n",
      "Surface training t=16230, loss=0.03405530005693436\n",
      "Surface training t=16231, loss=0.03251473046839237\n",
      "Surface training t=16232, loss=0.032032933086156845\n",
      "Surface training t=16233, loss=0.0290200337767601\n",
      "Surface training t=16234, loss=0.02821226231753826\n",
      "Surface training t=16235, loss=0.024600641801953316\n",
      "Surface training t=16236, loss=0.03264179639518261\n",
      "Surface training t=16237, loss=0.024240930564701557\n",
      "Surface training t=16238, loss=0.03314676508307457\n",
      "Surface training t=16239, loss=0.02809679601341486\n",
      "Surface training t=16240, loss=0.02846828941255808\n",
      "Surface training t=16241, loss=0.030958084389567375\n",
      "Surface training t=16242, loss=0.02668245229870081\n",
      "Surface training t=16243, loss=0.03363128378987312\n",
      "Surface training t=16244, loss=0.03257635608315468\n",
      "Surface training t=16245, loss=0.0322352284565568\n",
      "Surface training t=16246, loss=0.033296553418040276\n",
      "Surface training t=16247, loss=0.0496938768774271\n",
      "Surface training t=16248, loss=0.033117749728262424\n",
      "Surface training t=16249, loss=0.03527725860476494\n",
      "Surface training t=16250, loss=0.027071748860180378\n",
      "Surface training t=16251, loss=0.02533910982310772\n",
      "Surface training t=16252, loss=0.023051176220178604\n",
      "Surface training t=16253, loss=0.02768153790384531\n",
      "Surface training t=16254, loss=0.030108816921710968\n",
      "Surface training t=16255, loss=0.02648960892111063\n",
      "Surface training t=16256, loss=0.029697073623538017\n",
      "Surface training t=16257, loss=0.028977200388908386\n",
      "Surface training t=16258, loss=0.026747181080281734\n",
      "Surface training t=16259, loss=0.03106937650591135\n",
      "Surface training t=16260, loss=0.03159098979085684\n",
      "Surface training t=16261, loss=0.0281910989433527\n",
      "Surface training t=16262, loss=0.022576906718313694\n",
      "Surface training t=16263, loss=0.026606284081935883\n",
      "Surface training t=16264, loss=0.021364853717386723\n",
      "Surface training t=16265, loss=0.028858800418674946\n",
      "Surface training t=16266, loss=0.02797351498156786\n",
      "Surface training t=16267, loss=0.03572166711091995\n",
      "Surface training t=16268, loss=0.033232953399419785\n",
      "Surface training t=16269, loss=0.03367687202990055\n",
      "Surface training t=16270, loss=0.030975385569036007\n",
      "Surface training t=16271, loss=0.03478212282061577\n",
      "Surface training t=16272, loss=0.038020575419068336\n",
      "Surface training t=16273, loss=0.03434906993061304\n",
      "Surface training t=16274, loss=0.06035969965159893\n",
      "Surface training t=16275, loss=0.03481170907616615\n",
      "Surface training t=16276, loss=0.03930443711578846\n",
      "Surface training t=16277, loss=0.0399505952373147\n",
      "Surface training t=16278, loss=0.03412921354174614\n",
      "Surface training t=16279, loss=0.03225524444133043\n",
      "Surface training t=16280, loss=0.028758938424289227\n",
      "Surface training t=16281, loss=0.03098296094685793\n",
      "Surface training t=16282, loss=0.03170999325811863\n",
      "Surface training t=16283, loss=0.02097463607788086\n",
      "Surface training t=16284, loss=0.023277183063328266\n",
      "Surface training t=16285, loss=0.03539160639047623\n",
      "Surface training t=16286, loss=0.030003413558006287\n",
      "Surface training t=16287, loss=0.026159174740314484\n",
      "Surface training t=16288, loss=0.03302362188696861\n",
      "Surface training t=16289, loss=0.028905004262924194\n",
      "Surface training t=16290, loss=0.024563239887356758\n",
      "Surface training t=16291, loss=0.03851102292537689\n",
      "Surface training t=16292, loss=0.02605109754949808\n",
      "Surface training t=16293, loss=0.027053920552134514\n",
      "Surface training t=16294, loss=0.028357340022921562\n",
      "Surface training t=16295, loss=0.03522411361336708\n",
      "Surface training t=16296, loss=0.030817179940640926\n",
      "Surface training t=16297, loss=0.042829493060708046\n",
      "Surface training t=16298, loss=0.030011242255568504\n",
      "Surface training t=16299, loss=0.04391833022236824\n",
      "Surface training t=16300, loss=0.029529932886362076\n",
      "Surface training t=16301, loss=0.02597809676080942\n",
      "Surface training t=16302, loss=0.036336785182356834\n",
      "Surface training t=16303, loss=0.024504405446350574\n",
      "Surface training t=16304, loss=0.021612543612718582\n",
      "Surface training t=16305, loss=0.02212686836719513\n",
      "Surface training t=16306, loss=0.018925901502370834\n",
      "Surface training t=16307, loss=0.02112884446978569\n",
      "Surface training t=16308, loss=0.01744119543582201\n",
      "Surface training t=16309, loss=0.024576197378337383\n",
      "Surface training t=16310, loss=0.025272206403315067\n",
      "Surface training t=16311, loss=0.03153500705957413\n",
      "Surface training t=16312, loss=0.03253350034356117\n",
      "Surface training t=16313, loss=0.034479920752346516\n",
      "Surface training t=16314, loss=0.03059413842856884\n",
      "Surface training t=16315, loss=0.030550582334399223\n",
      "Surface training t=16316, loss=0.03448196966201067\n",
      "Surface training t=16317, loss=0.03355031553655863\n",
      "Surface training t=16318, loss=0.029606172814965248\n",
      "Surface training t=16319, loss=0.027694009244441986\n",
      "Surface training t=16320, loss=0.03217262029647827\n",
      "Surface training t=16321, loss=0.02738112211227417\n",
      "Surface training t=16322, loss=0.015456508845090866\n",
      "Surface training t=16323, loss=0.022221856750547886\n",
      "Surface training t=16324, loss=0.020299552008509636\n",
      "Surface training t=16325, loss=0.02341039478778839\n",
      "Surface training t=16326, loss=0.023495597764849663\n",
      "Surface training t=16327, loss=0.03101063333451748\n",
      "Surface training t=16328, loss=0.039049070328474045\n",
      "Surface training t=16329, loss=0.040386682376265526\n",
      "Surface training t=16330, loss=0.03403173200786114\n",
      "Surface training t=16331, loss=0.025266694836318493\n",
      "Surface training t=16332, loss=0.04168766736984253\n",
      "Surface training t=16333, loss=0.02479181718081236\n",
      "Surface training t=16334, loss=0.03288749419152737\n",
      "Surface training t=16335, loss=0.02688744757324457\n",
      "Surface training t=16336, loss=0.028200454078614712\n",
      "Surface training t=16337, loss=0.029928063042461872\n",
      "Surface training t=16338, loss=0.024505929090082645\n",
      "Surface training t=16339, loss=0.03249953221529722\n",
      "Surface training t=16340, loss=0.027235706336796284\n",
      "Surface training t=16341, loss=0.019878790713846684\n",
      "Surface training t=16342, loss=0.023275550454854965\n",
      "Surface training t=16343, loss=0.02950507216155529\n",
      "Surface training t=16344, loss=0.037332212552428246\n",
      "Surface training t=16345, loss=0.030597446486353874\n",
      "Surface training t=16346, loss=0.026634210720658302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=16347, loss=0.03721117973327637\n",
      "Surface training t=16348, loss=0.03439240995794535\n",
      "Surface training t=16349, loss=0.029073627665638924\n",
      "Surface training t=16350, loss=0.03849571384489536\n",
      "Surface training t=16351, loss=0.03687123954296112\n",
      "Surface training t=16352, loss=0.05170613154768944\n",
      "Surface training t=16353, loss=0.0333812702447176\n",
      "Surface training t=16354, loss=0.029347753152251244\n",
      "Surface training t=16355, loss=0.030077188275754452\n",
      "Surface training t=16356, loss=0.023744781501591206\n",
      "Surface training t=16357, loss=0.022419105283915997\n",
      "Surface training t=16358, loss=0.021360909566283226\n",
      "Surface training t=16359, loss=0.021223225630819798\n",
      "Surface training t=16360, loss=0.023297586478292942\n",
      "Surface training t=16361, loss=0.024020826444029808\n",
      "Surface training t=16362, loss=0.022629148326814175\n",
      "Surface training t=16363, loss=0.018031935207545757\n",
      "Surface training t=16364, loss=0.03287927061319351\n",
      "Surface training t=16365, loss=0.050177060067653656\n",
      "Surface training t=16366, loss=0.04376015719026327\n",
      "Surface training t=16367, loss=0.03503922559320927\n",
      "Surface training t=16368, loss=0.030214587226510048\n",
      "Surface training t=16369, loss=0.03384470194578171\n",
      "Surface training t=16370, loss=0.036637745797634125\n",
      "Surface training t=16371, loss=0.038258129730820656\n",
      "Surface training t=16372, loss=0.036613017320632935\n",
      "Surface training t=16373, loss=0.024837682023644447\n",
      "Surface training t=16374, loss=0.020841455552726984\n",
      "Surface training t=16375, loss=0.02388780191540718\n",
      "Surface training t=16376, loss=0.026244078762829304\n",
      "Surface training t=16377, loss=0.03405486326664686\n",
      "Surface training t=16378, loss=0.042210400104522705\n",
      "Surface training t=16379, loss=0.042180100455880165\n",
      "Surface training t=16380, loss=0.04044973663985729\n",
      "Surface training t=16381, loss=0.038200486451387405\n",
      "Surface training t=16382, loss=0.03775922209024429\n",
      "Surface training t=16383, loss=0.04090875759720802\n",
      "Surface training t=16384, loss=0.04075113497674465\n",
      "Surface training t=16385, loss=0.044157253578305244\n",
      "Surface training t=16386, loss=0.04010230302810669\n",
      "Surface training t=16387, loss=0.030727820470929146\n",
      "Surface training t=16388, loss=0.025078821927309036\n",
      "Surface training t=16389, loss=0.03890167735517025\n",
      "Surface training t=16390, loss=0.05731850117444992\n",
      "Surface training t=16391, loss=0.047247941605746746\n",
      "Surface training t=16392, loss=0.062213629484176636\n",
      "Surface training t=16393, loss=0.07997951284050941\n",
      "Surface training t=16394, loss=0.04977695271372795\n",
      "Surface training t=16395, loss=0.06608748622238636\n",
      "Surface training t=16396, loss=0.045349977910518646\n",
      "Surface training t=16397, loss=0.04622978717088699\n",
      "Surface training t=16398, loss=0.038768148981034756\n",
      "Surface training t=16399, loss=0.03213715832680464\n",
      "Surface training t=16400, loss=0.032948799431324005\n",
      "Surface training t=16401, loss=0.03840129263699055\n",
      "Surface training t=16402, loss=0.030043775215744972\n",
      "Surface training t=16403, loss=0.029009776189923286\n",
      "Surface training t=16404, loss=0.028352311812341213\n",
      "Surface training t=16405, loss=0.02766704186797142\n",
      "Surface training t=16406, loss=0.023214426822960377\n",
      "Surface training t=16407, loss=0.026928922161459923\n",
      "Surface training t=16408, loss=0.022875133901834488\n",
      "Surface training t=16409, loss=0.024067655205726624\n",
      "Surface training t=16410, loss=0.030926394276320934\n",
      "Surface training t=16411, loss=0.041408419609069824\n",
      "Surface training t=16412, loss=0.028223571367561817\n",
      "Surface training t=16413, loss=0.03595435991883278\n",
      "Surface training t=16414, loss=0.021946237422525883\n",
      "Surface training t=16415, loss=0.03397160954773426\n",
      "Surface training t=16416, loss=0.0427042692899704\n",
      "Surface training t=16417, loss=0.03216221369802952\n",
      "Surface training t=16418, loss=0.03006597887724638\n",
      "Surface training t=16419, loss=0.028279822319746017\n",
      "Surface training t=16420, loss=0.02932578045874834\n",
      "Surface training t=16421, loss=0.027184507809579372\n",
      "Surface training t=16422, loss=0.029850424267351627\n",
      "Surface training t=16423, loss=0.0277108671143651\n",
      "Surface training t=16424, loss=0.03382356744259596\n",
      "Surface training t=16425, loss=0.023730918299406767\n",
      "Surface training t=16426, loss=0.02806045487523079\n",
      "Surface training t=16427, loss=0.03226765617728233\n",
      "Surface training t=16428, loss=0.021525993011891842\n",
      "Surface training t=16429, loss=0.020934145897626877\n",
      "Surface training t=16430, loss=0.022592995315790176\n",
      "Surface training t=16431, loss=0.023054025135934353\n",
      "Surface training t=16432, loss=0.021900855004787445\n",
      "Surface training t=16433, loss=0.02234954573214054\n",
      "Surface training t=16434, loss=0.02668941020965576\n",
      "Surface training t=16435, loss=0.02031866181641817\n",
      "Surface training t=16436, loss=0.02324147243052721\n",
      "Surface training t=16437, loss=0.025440492667257786\n",
      "Surface training t=16438, loss=0.02287794928997755\n",
      "Surface training t=16439, loss=0.01964613702148199\n",
      "Surface training t=16440, loss=0.017295848112553358\n",
      "Surface training t=16441, loss=0.02060666773468256\n",
      "Surface training t=16442, loss=0.018120872788131237\n",
      "Surface training t=16443, loss=0.02368335612118244\n",
      "Surface training t=16444, loss=0.03250245377421379\n",
      "Surface training t=16445, loss=0.03165388014167547\n",
      "Surface training t=16446, loss=0.028542708605527878\n",
      "Surface training t=16447, loss=0.029336141422390938\n",
      "Surface training t=16448, loss=0.02406575996428728\n",
      "Surface training t=16449, loss=0.02804518584161997\n",
      "Surface training t=16450, loss=0.03191572614014149\n",
      "Surface training t=16451, loss=0.02552442066371441\n",
      "Surface training t=16452, loss=0.019807720091193914\n",
      "Surface training t=16453, loss=0.022682610899209976\n",
      "Surface training t=16454, loss=0.023481158539652824\n",
      "Surface training t=16455, loss=0.03151437547057867\n",
      "Surface training t=16456, loss=0.021042809821665287\n",
      "Surface training t=16457, loss=0.028220168314874172\n",
      "Surface training t=16458, loss=0.02094428613781929\n",
      "Surface training t=16459, loss=0.02247368823736906\n",
      "Surface training t=16460, loss=0.029968634247779846\n",
      "Surface training t=16461, loss=0.048866672441363335\n",
      "Surface training t=16462, loss=0.038103317841887474\n",
      "Surface training t=16463, loss=0.029998132959008217\n",
      "Surface training t=16464, loss=0.027993088588118553\n",
      "Surface training t=16465, loss=0.03518468514084816\n",
      "Surface training t=16466, loss=0.0353864599019289\n",
      "Surface training t=16467, loss=0.02788715809583664\n",
      "Surface training t=16468, loss=0.03435869421809912\n",
      "Surface training t=16469, loss=0.041823145002126694\n",
      "Surface training t=16470, loss=0.061918025836348534\n",
      "Surface training t=16471, loss=0.04572908952832222\n",
      "Surface training t=16472, loss=0.049787646159529686\n",
      "Surface training t=16473, loss=0.06223512068390846\n",
      "Surface training t=16474, loss=0.04613376222550869\n",
      "Surface training t=16475, loss=0.06912872567772865\n",
      "Surface training t=16476, loss=0.05595632828772068\n",
      "Surface training t=16477, loss=0.061487575992941856\n",
      "Surface training t=16478, loss=0.05306325852870941\n",
      "Surface training t=16479, loss=0.04809614457190037\n",
      "Surface training t=16480, loss=0.02677128091454506\n",
      "Surface training t=16481, loss=0.0364770982414484\n",
      "Surface training t=16482, loss=0.04451717250049114\n",
      "Surface training t=16483, loss=0.030751998536288738\n",
      "Surface training t=16484, loss=0.04623355157673359\n",
      "Surface training t=16485, loss=0.03093886561691761\n",
      "Surface training t=16486, loss=0.0563829243183136\n",
      "Surface training t=16487, loss=0.03248154558241367\n",
      "Surface training t=16488, loss=0.030358964577317238\n",
      "Surface training t=16489, loss=0.028988574631512165\n",
      "Surface training t=16490, loss=0.030781203880906105\n",
      "Surface training t=16491, loss=0.0360313355922699\n",
      "Surface training t=16492, loss=0.030509041622281075\n",
      "Surface training t=16493, loss=0.02445027232170105\n",
      "Surface training t=16494, loss=0.03302035387605429\n",
      "Surface training t=16495, loss=0.037453994154930115\n",
      "Surface training t=16496, loss=0.046545276418328285\n",
      "Surface training t=16497, loss=0.035446980968117714\n",
      "Surface training t=16498, loss=0.03336193412542343\n",
      "Surface training t=16499, loss=0.031077515333890915\n",
      "Surface training t=16500, loss=0.03229360096156597\n",
      "Surface training t=16501, loss=0.038236310705542564\n",
      "Surface training t=16502, loss=0.04931769520044327\n",
      "Surface training t=16503, loss=0.04025224409997463\n",
      "Surface training t=16504, loss=0.037925418466329575\n",
      "Surface training t=16505, loss=0.05289265140891075\n",
      "Surface training t=16506, loss=0.04078955017030239\n",
      "Surface training t=16507, loss=0.04305725917220116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=16508, loss=0.037032321095466614\n",
      "Surface training t=16509, loss=0.03805629163980484\n",
      "Surface training t=16510, loss=0.03373712953180075\n",
      "Surface training t=16511, loss=0.055404506623744965\n",
      "Surface training t=16512, loss=0.036322965286672115\n",
      "Surface training t=16513, loss=0.041629732586443424\n",
      "Surface training t=16514, loss=0.04360599257051945\n",
      "Surface training t=16515, loss=0.056493811309337616\n",
      "Surface training t=16516, loss=0.03889295645058155\n",
      "Surface training t=16517, loss=0.03167607355862856\n",
      "Surface training t=16518, loss=0.032444680109620094\n",
      "Surface training t=16519, loss=0.03494578693062067\n",
      "Surface training t=16520, loss=0.037886640056967735\n",
      "Surface training t=16521, loss=0.04128059931099415\n",
      "Surface training t=16522, loss=0.04085224308073521\n",
      "Surface training t=16523, loss=0.04518573172390461\n",
      "Surface training t=16524, loss=0.039413370192050934\n",
      "Surface training t=16525, loss=0.0361388148739934\n",
      "Surface training t=16526, loss=0.04334353283047676\n",
      "Surface training t=16527, loss=0.05251511000096798\n",
      "Surface training t=16528, loss=0.033781224861741066\n",
      "Surface training t=16529, loss=0.032948995009064674\n",
      "Surface training t=16530, loss=0.029304049909114838\n",
      "Surface training t=16531, loss=0.02695510722696781\n",
      "Surface training t=16532, loss=0.03228787146508694\n",
      "Surface training t=16533, loss=0.028111422434449196\n",
      "Surface training t=16534, loss=0.028928584419190884\n",
      "Surface training t=16535, loss=0.03911754861474037\n",
      "Surface training t=16536, loss=0.049441464245319366\n",
      "Surface training t=16537, loss=0.054277513176202774\n",
      "Surface training t=16538, loss=0.04448366537690163\n",
      "Surface training t=16539, loss=0.07158500328660011\n",
      "Surface training t=16540, loss=0.05062966048717499\n",
      "Surface training t=16541, loss=0.05088074877858162\n",
      "Surface training t=16542, loss=0.03395059984177351\n",
      "Surface training t=16543, loss=0.04140484519302845\n",
      "Surface training t=16544, loss=0.02981684636324644\n",
      "Surface training t=16545, loss=0.026672491803765297\n",
      "Surface training t=16546, loss=0.027241414412856102\n",
      "Surface training t=16547, loss=0.027548267506062984\n",
      "Surface training t=16548, loss=0.021140468306839466\n",
      "Surface training t=16549, loss=0.03369438275694847\n",
      "Surface training t=16550, loss=0.029743660241365433\n",
      "Surface training t=16551, loss=0.025133632123470306\n",
      "Surface training t=16552, loss=0.024097809568047523\n",
      "Surface training t=16553, loss=0.02586285676807165\n",
      "Surface training t=16554, loss=0.022358118556439877\n",
      "Surface training t=16555, loss=0.020858864299952984\n",
      "Surface training t=16556, loss=0.021710840053856373\n",
      "Surface training t=16557, loss=0.022948870435357094\n",
      "Surface training t=16558, loss=0.029229414649307728\n",
      "Surface training t=16559, loss=0.0242338040843606\n",
      "Surface training t=16560, loss=0.023400388658046722\n",
      "Surface training t=16561, loss=0.016742076724767685\n",
      "Surface training t=16562, loss=0.020880959928035736\n",
      "Surface training t=16563, loss=0.02106680255383253\n",
      "Surface training t=16564, loss=0.019803408533334732\n",
      "Surface training t=16565, loss=0.03331035375595093\n",
      "Surface training t=16566, loss=0.03375473991036415\n",
      "Surface training t=16567, loss=0.02486183773726225\n",
      "Surface training t=16568, loss=0.023959770798683167\n",
      "Surface training t=16569, loss=0.022297963500022888\n",
      "Surface training t=16570, loss=0.02561325766146183\n",
      "Surface training t=16571, loss=0.01820218190550804\n",
      "Surface training t=16572, loss=0.022442461922764778\n",
      "Surface training t=16573, loss=0.02303839474916458\n",
      "Surface training t=16574, loss=0.021598055958747864\n",
      "Surface training t=16575, loss=0.025767543353140354\n",
      "Surface training t=16576, loss=0.01840376202017069\n",
      "Surface training t=16577, loss=0.021949397400021553\n",
      "Surface training t=16578, loss=0.02248771581798792\n",
      "Surface training t=16579, loss=0.02753440197557211\n",
      "Surface training t=16580, loss=0.020403752103447914\n",
      "Surface training t=16581, loss=0.020535767544060946\n",
      "Surface training t=16582, loss=0.022048368118703365\n",
      "Surface training t=16583, loss=0.022744636982679367\n",
      "Surface training t=16584, loss=0.02217222796753049\n",
      "Surface training t=16585, loss=0.02835727296769619\n",
      "Surface training t=16586, loss=0.022499914281070232\n",
      "Surface training t=16587, loss=0.028327864594757557\n",
      "Surface training t=16588, loss=0.030178780667483807\n",
      "Surface training t=16589, loss=0.026355349458754063\n",
      "Surface training t=16590, loss=0.024827025830745697\n",
      "Surface training t=16591, loss=0.022672835737466812\n",
      "Surface training t=16592, loss=0.020071073435246944\n",
      "Surface training t=16593, loss=0.02945668902248144\n",
      "Surface training t=16594, loss=0.031219071708619595\n",
      "Surface training t=16595, loss=0.02567861881107092\n",
      "Surface training t=16596, loss=0.028372477740049362\n",
      "Surface training t=16597, loss=0.028391565196216106\n",
      "Surface training t=16598, loss=0.02867375873029232\n",
      "Surface training t=16599, loss=0.029631061479449272\n",
      "Surface training t=16600, loss=0.0427066795527935\n",
      "Surface training t=16601, loss=0.027454177848994732\n",
      "Surface training t=16602, loss=0.035921018570661545\n",
      "Surface training t=16603, loss=0.041703877970576286\n",
      "Surface training t=16604, loss=0.028644603677093983\n",
      "Surface training t=16605, loss=0.037413718178868294\n",
      "Surface training t=16606, loss=0.028133021667599678\n",
      "Surface training t=16607, loss=0.02676427084952593\n",
      "Surface training t=16608, loss=0.03639199957251549\n",
      "Surface training t=16609, loss=0.024807466194033623\n",
      "Surface training t=16610, loss=0.023810449987649918\n",
      "Surface training t=16611, loss=0.024295253679156303\n",
      "Surface training t=16612, loss=0.03174920752644539\n",
      "Surface training t=16613, loss=0.0330655612051487\n",
      "Surface training t=16614, loss=0.029533461667597294\n",
      "Surface training t=16615, loss=0.0207185884937644\n",
      "Surface training t=16616, loss=0.02491953782737255\n",
      "Surface training t=16617, loss=0.04278707690536976\n",
      "Surface training t=16618, loss=0.041539013385772705\n",
      "Surface training t=16619, loss=0.02700225729495287\n",
      "Surface training t=16620, loss=0.03217608667910099\n",
      "Surface training t=16621, loss=0.03305695950984955\n",
      "Surface training t=16622, loss=0.03192208334803581\n",
      "Surface training t=16623, loss=0.029609423130750656\n",
      "Surface training t=16624, loss=0.03240860719233751\n",
      "Surface training t=16625, loss=0.03937637433409691\n",
      "Surface training t=16626, loss=0.03243590146303177\n",
      "Surface training t=16627, loss=0.03766484372317791\n",
      "Surface training t=16628, loss=0.03163471817970276\n",
      "Surface training t=16629, loss=0.028641541488468647\n",
      "Surface training t=16630, loss=0.04043937101960182\n",
      "Surface training t=16631, loss=0.03024353925138712\n",
      "Surface training t=16632, loss=0.0256710397079587\n",
      "Surface training t=16633, loss=0.04342948831617832\n",
      "Surface training t=16634, loss=0.03773460537195206\n",
      "Surface training t=16635, loss=0.04488668218255043\n",
      "Surface training t=16636, loss=0.045237625017762184\n",
      "Surface training t=16637, loss=0.06214573234319687\n",
      "Surface training t=16638, loss=0.03891785629093647\n",
      "Surface training t=16639, loss=0.043078821152448654\n",
      "Surface training t=16640, loss=0.029560789465904236\n",
      "Surface training t=16641, loss=0.029187487438321114\n",
      "Surface training t=16642, loss=0.03440050967037678\n",
      "Surface training t=16643, loss=0.03416547458618879\n",
      "Surface training t=16644, loss=0.03719628695398569\n",
      "Surface training t=16645, loss=0.027257022447884083\n",
      "Surface training t=16646, loss=0.03267100639641285\n",
      "Surface training t=16647, loss=0.03050166927278042\n",
      "Surface training t=16648, loss=0.034295327961444855\n",
      "Surface training t=16649, loss=0.027381245978176594\n",
      "Surface training t=16650, loss=0.026736771687865257\n",
      "Surface training t=16651, loss=0.03162945993244648\n",
      "Surface training t=16652, loss=0.03241174574941397\n",
      "Surface training t=16653, loss=0.025878234766423702\n",
      "Surface training t=16654, loss=0.023521721363067627\n",
      "Surface training t=16655, loss=0.02514189761132002\n",
      "Surface training t=16656, loss=0.03557890560477972\n",
      "Surface training t=16657, loss=0.035758545622229576\n",
      "Surface training t=16658, loss=0.03264770843088627\n",
      "Surface training t=16659, loss=0.027767078951001167\n",
      "Surface training t=16660, loss=0.026767365634441376\n",
      "Surface training t=16661, loss=0.032741015776991844\n",
      "Surface training t=16662, loss=0.057100383564829826\n",
      "Surface training t=16663, loss=0.03761504590511322\n",
      "Surface training t=16664, loss=0.042303575202822685\n",
      "Surface training t=16665, loss=0.04176541790366173\n",
      "Surface training t=16666, loss=0.05824187025427818\n",
      "Surface training t=16667, loss=0.04320630803704262\n",
      "Surface training t=16668, loss=0.053082121536135674\n",
      "Surface training t=16669, loss=0.03969983942806721\n",
      "Surface training t=16670, loss=0.039280456490814686\n",
      "Surface training t=16671, loss=0.0382473636418581\n",
      "Surface training t=16672, loss=0.04305766709148884\n",
      "Surface training t=16673, loss=0.06253624148666859\n",
      "Surface training t=16674, loss=0.044677646830677986\n",
      "Surface training t=16675, loss=0.048534358851611614\n",
      "Surface training t=16676, loss=0.03875587694346905\n",
      "Surface training t=16677, loss=0.026761743240058422\n",
      "Surface training t=16678, loss=0.030175630003213882\n",
      "Surface training t=16679, loss=0.03185168467462063\n",
      "Surface training t=16680, loss=0.040620118379592896\n",
      "Surface training t=16681, loss=0.029322911985218525\n",
      "Surface training t=16682, loss=0.03638938441872597\n",
      "Surface training t=16683, loss=0.03124036081135273\n",
      "Surface training t=16684, loss=0.032201423309743404\n",
      "Surface training t=16685, loss=0.036827338859438896\n",
      "Surface training t=16686, loss=0.03884384036064148\n",
      "Surface training t=16687, loss=0.04929085448384285\n",
      "Surface training t=16688, loss=0.036384815350174904\n",
      "Surface training t=16689, loss=0.04012669809162617\n",
      "Surface training t=16690, loss=0.03462773934006691\n",
      "Surface training t=16691, loss=0.032177750021219254\n",
      "Surface training t=16692, loss=0.023076494224369526\n",
      "Surface training t=16693, loss=0.027329332195222378\n",
      "Surface training t=16694, loss=0.027256103232502937\n",
      "Surface training t=16695, loss=0.027924024499952793\n",
      "Surface training t=16696, loss=0.02737865224480629\n",
      "Surface training t=16697, loss=0.03069761861115694\n",
      "Surface training t=16698, loss=0.024988187476992607\n",
      "Surface training t=16699, loss=0.027063393965363503\n",
      "Surface training t=16700, loss=0.026278101839125156\n",
      "Surface training t=16701, loss=0.03063024766743183\n",
      "Surface training t=16702, loss=0.026537321507930756\n",
      "Surface training t=16703, loss=0.028870228677988052\n",
      "Surface training t=16704, loss=0.025138321332633495\n",
      "Surface training t=16705, loss=0.0311528192833066\n",
      "Surface training t=16706, loss=0.02968204114586115\n",
      "Surface training t=16707, loss=0.023769154213368893\n",
      "Surface training t=16708, loss=0.02315420377999544\n",
      "Surface training t=16709, loss=0.03048010915517807\n",
      "Surface training t=16710, loss=0.028702554292976856\n",
      "Surface training t=16711, loss=0.03333977051079273\n",
      "Surface training t=16712, loss=0.03612964414060116\n",
      "Surface training t=16713, loss=0.03689524345099926\n",
      "Surface training t=16714, loss=0.03791424445807934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=16715, loss=0.040357569232583046\n",
      "Surface training t=16716, loss=0.03823794424533844\n",
      "Surface training t=16717, loss=0.03182263486087322\n",
      "Surface training t=16718, loss=0.0359104173257947\n",
      "Surface training t=16719, loss=0.03574240207672119\n",
      "Surface training t=16720, loss=0.04092278331518173\n",
      "Surface training t=16721, loss=0.039333514869213104\n",
      "Surface training t=16722, loss=0.030870873481035233\n",
      "Surface training t=16723, loss=0.035003986209630966\n",
      "Surface training t=16724, loss=0.03679851442575455\n",
      "Surface training t=16725, loss=0.03251748904585838\n",
      "Surface training t=16726, loss=0.029102702625095844\n",
      "Surface training t=16727, loss=0.04497768171131611\n",
      "Surface training t=16728, loss=0.04394981078803539\n",
      "Surface training t=16729, loss=0.038782721385359764\n",
      "Surface training t=16730, loss=0.043640635907649994\n",
      "Surface training t=16731, loss=0.048251571133732796\n",
      "Surface training t=16732, loss=0.0379223357886076\n",
      "Surface training t=16733, loss=0.03325304202735424\n",
      "Surface training t=16734, loss=0.03453715518116951\n",
      "Surface training t=16735, loss=0.02702760510146618\n",
      "Surface training t=16736, loss=0.02160272840410471\n",
      "Surface training t=16737, loss=0.03627642057836056\n",
      "Surface training t=16738, loss=0.035283831879496574\n",
      "Surface training t=16739, loss=0.024826199747622013\n",
      "Surface training t=16740, loss=0.0312634501606226\n",
      "Surface training t=16741, loss=0.02630033530294895\n",
      "Surface training t=16742, loss=0.024906408041715622\n",
      "Surface training t=16743, loss=0.03785277344286442\n",
      "Surface training t=16744, loss=0.03531294036656618\n",
      "Surface training t=16745, loss=0.041205400601029396\n",
      "Surface training t=16746, loss=0.04126616008579731\n",
      "Surface training t=16747, loss=0.04851755127310753\n",
      "Surface training t=16748, loss=0.04314504191279411\n",
      "Surface training t=16749, loss=0.04352403245866299\n",
      "Surface training t=16750, loss=0.023472920060157776\n",
      "Surface training t=16751, loss=0.0316363163292408\n",
      "Surface training t=16752, loss=0.02933089155703783\n",
      "Surface training t=16753, loss=0.026246478781104088\n",
      "Surface training t=16754, loss=0.03223979473114014\n",
      "Surface training t=16755, loss=0.03328578267246485\n",
      "Surface training t=16756, loss=0.02121552173048258\n",
      "Surface training t=16757, loss=0.01996680162847042\n",
      "Surface training t=16758, loss=0.019918072037398815\n",
      "Surface training t=16759, loss=0.024510754272341728\n",
      "Surface training t=16760, loss=0.021229859441518784\n",
      "Surface training t=16761, loss=0.019364195875823498\n",
      "Surface training t=16762, loss=0.01920162420719862\n",
      "Surface training t=16763, loss=0.023452493362128735\n",
      "Surface training t=16764, loss=0.028391826897859573\n",
      "Surface training t=16765, loss=0.026570497080683708\n",
      "Surface training t=16766, loss=0.029278071597218513\n",
      "Surface training t=16767, loss=0.026736498810350895\n",
      "Surface training t=16768, loss=0.025273107923567295\n",
      "Surface training t=16769, loss=0.028804251924157143\n",
      "Surface training t=16770, loss=0.02195405215024948\n",
      "Surface training t=16771, loss=0.028176596388220787\n",
      "Surface training t=16772, loss=0.023668533191084862\n",
      "Surface training t=16773, loss=0.02412494830787182\n",
      "Surface training t=16774, loss=0.035480461083352566\n",
      "Surface training t=16775, loss=0.026274518109858036\n",
      "Surface training t=16776, loss=0.029104312881827354\n",
      "Surface training t=16777, loss=0.0382551085203886\n",
      "Surface training t=16778, loss=0.026708736084401608\n",
      "Surface training t=16779, loss=0.032131507992744446\n",
      "Surface training t=16780, loss=0.04764382541179657\n",
      "Surface training t=16781, loss=0.028634214773774147\n",
      "Surface training t=16782, loss=0.04147365130484104\n",
      "Surface training t=16783, loss=0.031087848357856274\n",
      "Surface training t=16784, loss=0.03618050739169121\n",
      "Surface training t=16785, loss=0.04140852019190788\n",
      "Surface training t=16786, loss=0.03287195134907961\n",
      "Surface training t=16787, loss=0.041175175458192825\n",
      "Surface training t=16788, loss=0.03690348658710718\n",
      "Surface training t=16789, loss=0.03968711756169796\n",
      "Surface training t=16790, loss=0.04596997611224651\n",
      "Surface training t=16791, loss=0.03240921162068844\n",
      "Surface training t=16792, loss=0.05089152790606022\n",
      "Surface training t=16793, loss=0.03552028816193342\n",
      "Surface training t=16794, loss=0.030814683996140957\n",
      "Surface training t=16795, loss=0.03150124754756689\n",
      "Surface training t=16796, loss=0.026647505350410938\n",
      "Surface training t=16797, loss=0.02723396196961403\n",
      "Surface training t=16798, loss=0.03277401439845562\n",
      "Surface training t=16799, loss=0.0242389515042305\n",
      "Surface training t=16800, loss=0.024382933974266052\n",
      "Surface training t=16801, loss=0.025620303116738796\n",
      "Surface training t=16802, loss=0.02659190073609352\n",
      "Surface training t=16803, loss=0.025327746756374836\n",
      "Surface training t=16804, loss=0.02846681885421276\n",
      "Surface training t=16805, loss=0.039652179926633835\n",
      "Surface training t=16806, loss=0.025133997201919556\n",
      "Surface training t=16807, loss=0.03278477303683758\n",
      "Surface training t=16808, loss=0.02695626299828291\n",
      "Surface training t=16809, loss=0.02473453152924776\n",
      "Surface training t=16810, loss=0.027298825792968273\n",
      "Surface training t=16811, loss=0.0248075183480978\n",
      "Surface training t=16812, loss=0.020786000415682793\n",
      "Surface training t=16813, loss=0.02733856812119484\n",
      "Surface training t=16814, loss=0.025059704668819904\n",
      "Surface training t=16815, loss=0.017826302908360958\n",
      "Surface training t=16816, loss=0.018734272569417953\n",
      "Surface training t=16817, loss=0.024889998137950897\n",
      "Surface training t=16818, loss=0.0204814444296062\n",
      "Surface training t=16819, loss=0.031125403009355068\n",
      "Surface training t=16820, loss=0.024745911359786987\n",
      "Surface training t=16821, loss=0.026746200397610664\n",
      "Surface training t=16822, loss=0.029236377216875553\n",
      "Surface training t=16823, loss=0.035873886197805405\n",
      "Surface training t=16824, loss=0.030401872005313635\n",
      "Surface training t=16825, loss=0.03262939676642418\n",
      "Surface training t=16826, loss=0.036960527300834656\n",
      "Surface training t=16827, loss=0.02314073219895363\n",
      "Surface training t=16828, loss=0.03297483175992966\n",
      "Surface training t=16829, loss=0.02416976075619459\n",
      "Surface training t=16830, loss=0.03013236355036497\n",
      "Surface training t=16831, loss=0.027167698368430138\n",
      "Surface training t=16832, loss=0.023026066832244396\n",
      "Surface training t=16833, loss=0.040956635028123856\n",
      "Surface training t=16834, loss=0.02695001382380724\n",
      "Surface training t=16835, loss=0.028173294849693775\n",
      "Surface training t=16836, loss=0.024134954437613487\n",
      "Surface training t=16837, loss=0.0219071377068758\n",
      "Surface training t=16838, loss=0.02071821689605713\n",
      "Surface training t=16839, loss=0.023844941519200802\n",
      "Surface training t=16840, loss=0.031038555316627026\n",
      "Surface training t=16841, loss=0.031003624200820923\n",
      "Surface training t=16842, loss=0.030196494422852993\n",
      "Surface training t=16843, loss=0.03235499747097492\n",
      "Surface training t=16844, loss=0.03155623376369476\n",
      "Surface training t=16845, loss=0.025444489903748035\n",
      "Surface training t=16846, loss=0.022402029484510422\n",
      "Surface training t=16847, loss=0.024307530373334885\n",
      "Surface training t=16848, loss=0.03367782384157181\n",
      "Surface training t=16849, loss=0.03172742668539286\n",
      "Surface training t=16850, loss=0.024705804884433746\n",
      "Surface training t=16851, loss=0.029965488240122795\n",
      "Surface training t=16852, loss=0.024197343736886978\n",
      "Surface training t=16853, loss=0.0326794758439064\n",
      "Surface training t=16854, loss=0.03335465397685766\n",
      "Surface training t=16855, loss=0.03530846443027258\n",
      "Surface training t=16856, loss=0.03593330644071102\n",
      "Surface training t=16857, loss=0.03930609114468098\n",
      "Surface training t=16858, loss=0.025262516923248768\n",
      "Surface training t=16859, loss=0.02131655625998974\n",
      "Surface training t=16860, loss=0.024540637619793415\n",
      "Surface training t=16861, loss=0.03537771292030811\n",
      "Surface training t=16862, loss=0.025287600234150887\n",
      "Surface training t=16863, loss=0.030342884361743927\n",
      "Surface training t=16864, loss=0.0366698345169425\n",
      "Surface training t=16865, loss=0.02850495558232069\n",
      "Surface training t=16866, loss=0.023219910450279713\n",
      "Surface training t=16867, loss=0.02803113590925932\n",
      "Surface training t=16868, loss=0.024713712744414806\n",
      "Surface training t=16869, loss=0.021551421843469143\n",
      "Surface training t=16870, loss=0.020646817050874233\n",
      "Surface training t=16871, loss=0.02118243370205164\n",
      "Surface training t=16872, loss=0.026750548742711544\n",
      "Surface training t=16873, loss=0.01979443058371544\n",
      "Surface training t=16874, loss=0.015954388305544853\n",
      "Surface training t=16875, loss=0.022792035713791847\n",
      "Surface training t=16876, loss=0.027387420646846294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=16877, loss=0.02765876892954111\n",
      "Surface training t=16878, loss=0.024323039688169956\n",
      "Surface training t=16879, loss=0.035161200910806656\n",
      "Surface training t=16880, loss=0.036679744720458984\n",
      "Surface training t=16881, loss=0.03330775536596775\n",
      "Surface training t=16882, loss=0.034854414872825146\n",
      "Surface training t=16883, loss=0.033413128927350044\n",
      "Surface training t=16884, loss=0.028407078236341476\n",
      "Surface training t=16885, loss=0.026461812667548656\n",
      "Surface training t=16886, loss=0.029387153685092926\n",
      "Surface training t=16887, loss=0.030324376188218594\n",
      "Surface training t=16888, loss=0.026035704649984837\n",
      "Surface training t=16889, loss=0.03825869970023632\n",
      "Surface training t=16890, loss=0.03977174684405327\n",
      "Surface training t=16891, loss=0.035825615748763084\n",
      "Surface training t=16892, loss=0.04015125706791878\n",
      "Surface training t=16893, loss=0.044749392196536064\n",
      "Surface training t=16894, loss=0.03786090202629566\n",
      "Surface training t=16895, loss=0.03539412468671799\n",
      "Surface training t=16896, loss=0.024622438475489616\n",
      "Surface training t=16897, loss=0.026571856811642647\n",
      "Surface training t=16898, loss=0.03409089706838131\n",
      "Surface training t=16899, loss=0.030315518379211426\n",
      "Surface training t=16900, loss=0.034107401967048645\n",
      "Surface training t=16901, loss=0.030801215209066868\n",
      "Surface training t=16902, loss=0.0275934636592865\n",
      "Surface training t=16903, loss=0.0332571379840374\n",
      "Surface training t=16904, loss=0.048487547785043716\n",
      "Surface training t=16905, loss=0.03363138623535633\n",
      "Surface training t=16906, loss=0.049381762742996216\n",
      "Surface training t=16907, loss=0.061726298183202744\n",
      "Surface training t=16908, loss=0.04590521939098835\n",
      "Surface training t=16909, loss=0.039742592722177505\n",
      "Surface training t=16910, loss=0.03553416207432747\n",
      "Surface training t=16911, loss=0.04279514402151108\n",
      "Surface training t=16912, loss=0.042421722784638405\n",
      "Surface training t=16913, loss=0.028827964328229427\n",
      "Surface training t=16914, loss=0.03292322903871536\n",
      "Surface training t=16915, loss=0.02182009071111679\n",
      "Surface training t=16916, loss=0.0203335490077734\n",
      "Surface training t=16917, loss=0.018471458926796913\n",
      "Surface training t=16918, loss=0.02445415034890175\n",
      "Surface training t=16919, loss=0.025524886325001717\n",
      "Surface training t=16920, loss=0.028684048913419247\n",
      "Surface training t=16921, loss=0.04056201130151749\n",
      "Surface training t=16922, loss=0.03837187960743904\n",
      "Surface training t=16923, loss=0.028559932485222816\n",
      "Surface training t=16924, loss=0.030627604573965073\n",
      "Surface training t=16925, loss=0.03283332567662001\n",
      "Surface training t=16926, loss=0.03126310836523771\n",
      "Surface training t=16927, loss=0.029697089456021786\n",
      "Surface training t=16928, loss=0.03066536132246256\n",
      "Surface training t=16929, loss=0.03421420231461525\n",
      "Surface training t=16930, loss=0.02732130791991949\n",
      "Surface training t=16931, loss=0.03153032809495926\n",
      "Surface training t=16932, loss=0.03325449861586094\n",
      "Surface training t=16933, loss=0.028117732144892216\n",
      "Surface training t=16934, loss=0.045031238347291946\n",
      "Surface training t=16935, loss=0.03965388610959053\n",
      "Surface training t=16936, loss=0.04436380788683891\n",
      "Surface training t=16937, loss=0.042777493596076965\n",
      "Surface training t=16938, loss=0.06971592828631401\n",
      "Surface training t=16939, loss=0.044327542185783386\n",
      "Surface training t=16940, loss=0.07367116585373878\n",
      "Surface training t=16941, loss=0.050083838403224945\n",
      "Surface training t=16942, loss=0.043900664895772934\n",
      "Surface training t=16943, loss=0.03215455636382103\n",
      "Surface training t=16944, loss=0.029967631213366985\n",
      "Surface training t=16945, loss=0.02525426633656025\n",
      "Surface training t=16946, loss=0.029536670073866844\n",
      "Surface training t=16947, loss=0.039256131276488304\n",
      "Surface training t=16948, loss=0.033307408913969994\n",
      "Surface training t=16949, loss=0.03125878516584635\n",
      "Surface training t=16950, loss=0.027914763428270817\n",
      "Surface training t=16951, loss=0.02672833949327469\n",
      "Surface training t=16952, loss=0.03037348762154579\n",
      "Surface training t=16953, loss=0.027397863566875458\n",
      "Surface training t=16954, loss=0.021373098716139793\n",
      "Surface training t=16955, loss=0.03590207453817129\n",
      "Surface training t=16956, loss=0.04812307469546795\n",
      "Surface training t=16957, loss=0.04762020520865917\n",
      "Surface training t=16958, loss=0.042322827503085136\n",
      "Surface training t=16959, loss=0.03975861705839634\n",
      "Surface training t=16960, loss=0.06385424360632896\n",
      "Surface training t=16961, loss=0.04186368174850941\n",
      "Surface training t=16962, loss=0.035982752218842506\n",
      "Surface training t=16963, loss=0.02696843631565571\n",
      "Surface training t=16964, loss=0.03503398783504963\n",
      "Surface training t=16965, loss=0.024782990105450153\n",
      "Surface training t=16966, loss=0.03605117555707693\n",
      "Surface training t=16967, loss=0.02215930912643671\n",
      "Surface training t=16968, loss=0.020105593837797642\n",
      "Surface training t=16969, loss=0.024187379516661167\n",
      "Surface training t=16970, loss=0.027754557318985462\n",
      "Surface training t=16971, loss=0.02575585152953863\n",
      "Surface training t=16972, loss=0.025986001826822758\n",
      "Surface training t=16973, loss=0.02397919725626707\n",
      "Surface training t=16974, loss=0.026186369359493256\n",
      "Surface training t=16975, loss=0.0375699158757925\n",
      "Surface training t=16976, loss=0.02308105118572712\n",
      "Surface training t=16977, loss=0.03960248455405235\n",
      "Surface training t=16978, loss=0.03149121813476086\n",
      "Surface training t=16979, loss=0.029224454425275326\n",
      "Surface training t=16980, loss=0.04040909186005592\n",
      "Surface training t=16981, loss=0.03096429631114006\n",
      "Surface training t=16982, loss=0.0496795941144228\n",
      "Surface training t=16983, loss=0.038942726328969\n",
      "Surface training t=16984, loss=0.04870164208114147\n",
      "Surface training t=16985, loss=0.04304905980825424\n",
      "Surface training t=16986, loss=0.034797634929418564\n",
      "Surface training t=16987, loss=0.038700833916664124\n",
      "Surface training t=16988, loss=0.03174917213618755\n",
      "Surface training t=16989, loss=0.03940119594335556\n",
      "Surface training t=16990, loss=0.047383399680256844\n",
      "Surface training t=16991, loss=0.03541935048997402\n",
      "Surface training t=16992, loss=0.0430463831871748\n",
      "Surface training t=16993, loss=0.03538103774189949\n",
      "Surface training t=16994, loss=0.03717910312116146\n",
      "Surface training t=16995, loss=0.04689935967326164\n",
      "Surface training t=16996, loss=0.0371423214673996\n",
      "Surface training t=16997, loss=0.03731375187635422\n",
      "Surface training t=16998, loss=0.03294394351541996\n",
      "Surface training t=16999, loss=0.031672630459070206\n",
      "Surface training t=17000, loss=0.032572329975664616\n",
      "Surface training t=17001, loss=0.04298717528581619\n",
      "Surface training t=17002, loss=0.030425092205405235\n",
      "Surface training t=17003, loss=0.03342638537287712\n",
      "Surface training t=17004, loss=0.026509885676205158\n",
      "Surface training t=17005, loss=0.023437583819031715\n",
      "Surface training t=17006, loss=0.033952388912439346\n",
      "Surface training t=17007, loss=0.02312003169208765\n",
      "Surface training t=17008, loss=0.024500932544469833\n",
      "Surface training t=17009, loss=0.03369354456663132\n",
      "Surface training t=17010, loss=0.03316283505409956\n",
      "Surface training t=17011, loss=0.027041655033826828\n",
      "Surface training t=17012, loss=0.01988788601011038\n",
      "Surface training t=17013, loss=0.023537601344287395\n",
      "Surface training t=17014, loss=0.025325444526970387\n",
      "Surface training t=17015, loss=0.018389392644166946\n",
      "Surface training t=17016, loss=0.017671523615717888\n",
      "Surface training t=17017, loss=0.022358314134180546\n",
      "Surface training t=17018, loss=0.022459647618234158\n",
      "Surface training t=17019, loss=0.021058913320302963\n",
      "Surface training t=17020, loss=0.026171984151005745\n",
      "Surface training t=17021, loss=0.02212895080447197\n",
      "Surface training t=17022, loss=0.021665191277861595\n",
      "Surface training t=17023, loss=0.021975521929562092\n",
      "Surface training t=17024, loss=0.029833871871232986\n",
      "Surface training t=17025, loss=0.03326214849948883\n",
      "Surface training t=17026, loss=0.04117266647517681\n",
      "Surface training t=17027, loss=0.04356170631945133\n",
      "Surface training t=17028, loss=0.039002460427582264\n",
      "Surface training t=17029, loss=0.0303608113899827\n",
      "Surface training t=17030, loss=0.03704963997006416\n",
      "Surface training t=17031, loss=0.021804346702992916\n",
      "Surface training t=17032, loss=0.024419838562607765\n",
      "Surface training t=17033, loss=0.02368124481290579\n",
      "Surface training t=17034, loss=0.021743008866906166\n",
      "Surface training t=17035, loss=0.028373277746140957\n",
      "Surface training t=17036, loss=0.020413843914866447\n",
      "Surface training t=17037, loss=0.022391186095774174\n",
      "Surface training t=17038, loss=0.02573343925178051\n",
      "Surface training t=17039, loss=0.025026511400938034\n",
      "Surface training t=17040, loss=0.033957257866859436\n",
      "Surface training t=17041, loss=0.02502672653645277\n",
      "Surface training t=17042, loss=0.024151165038347244\n",
      "Surface training t=17043, loss=0.024159472435712814\n",
      "Surface training t=17044, loss=0.021432125009596348\n",
      "Surface training t=17045, loss=0.023598353378474712\n",
      "Surface training t=17046, loss=0.025566252879798412\n",
      "Surface training t=17047, loss=0.027020210400223732\n",
      "Surface training t=17048, loss=0.03580955043435097\n",
      "Surface training t=17049, loss=0.04715399071574211\n",
      "Surface training t=17050, loss=0.02805810421705246\n",
      "Surface training t=17051, loss=0.025381125509738922\n",
      "Surface training t=17052, loss=0.019750148057937622\n",
      "Surface training t=17053, loss=0.023647678084671497\n",
      "Surface training t=17054, loss=0.02511931210756302\n",
      "Surface training t=17055, loss=0.030976800248026848\n",
      "Surface training t=17056, loss=0.03411961626261473\n",
      "Surface training t=17057, loss=0.024789374321699142\n",
      "Surface training t=17058, loss=0.02348716091364622\n",
      "Surface training t=17059, loss=0.020197870209813118\n",
      "Surface training t=17060, loss=0.026338092982769012\n",
      "Surface training t=17061, loss=0.021897779777646065\n",
      "Surface training t=17062, loss=0.023446621373295784\n",
      "Surface training t=17063, loss=0.02722062822431326\n",
      "Surface training t=17064, loss=0.03114538360387087\n",
      "Surface training t=17065, loss=0.027765359729528427\n",
      "Surface training t=17066, loss=0.0297940531745553\n",
      "Surface training t=17067, loss=0.026583300903439522\n",
      "Surface training t=17068, loss=0.03018117882311344\n",
      "Surface training t=17069, loss=0.03410704340785742\n",
      "Surface training t=17070, loss=0.03070206195116043\n",
      "Surface training t=17071, loss=0.02624058537185192\n",
      "Surface training t=17072, loss=0.031929997727274895\n",
      "Surface training t=17073, loss=0.03609892912209034\n",
      "Surface training t=17074, loss=0.029154466465115547\n",
      "Surface training t=17075, loss=0.043863508850336075\n",
      "Surface training t=17076, loss=0.030607813969254494\n",
      "Surface training t=17077, loss=0.0359798688441515\n",
      "Surface training t=17078, loss=0.03388821892440319\n",
      "Surface training t=17079, loss=0.02923649549484253\n",
      "Surface training t=17080, loss=0.029191900976002216\n",
      "Surface training t=17081, loss=0.02977347932755947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=17082, loss=0.02247243281453848\n",
      "Surface training t=17083, loss=0.024423026479780674\n",
      "Surface training t=17084, loss=0.028342412784695625\n",
      "Surface training t=17085, loss=0.031279501505196095\n",
      "Surface training t=17086, loss=0.033174796029925346\n",
      "Surface training t=17087, loss=0.02648674789816141\n",
      "Surface training t=17088, loss=0.03135164640843868\n",
      "Surface training t=17089, loss=0.027635948732495308\n",
      "Surface training t=17090, loss=0.03149574063718319\n",
      "Surface training t=17091, loss=0.031083212234079838\n",
      "Surface training t=17092, loss=0.0316158551722765\n",
      "Surface training t=17093, loss=0.03308836929500103\n",
      "Surface training t=17094, loss=0.04526081308722496\n",
      "Surface training t=17095, loss=0.04576255567371845\n",
      "Surface training t=17096, loss=0.044194525107741356\n",
      "Surface training t=17097, loss=0.04940582439303398\n",
      "Surface training t=17098, loss=0.059252671897411346\n",
      "Surface training t=17099, loss=0.044714558869600296\n",
      "Surface training t=17100, loss=0.05558089353144169\n",
      "Surface training t=17101, loss=0.05504859983921051\n",
      "Surface training t=17102, loss=0.027336626313626766\n",
      "Surface training t=17103, loss=0.031087389215826988\n",
      "Surface training t=17104, loss=0.05146037042140961\n",
      "Surface training t=17105, loss=0.03506735526025295\n",
      "Surface training t=17106, loss=0.037096637301146984\n",
      "Surface training t=17107, loss=0.04183725453913212\n",
      "Surface training t=17108, loss=0.03616082854568958\n",
      "Surface training t=17109, loss=0.04912644624710083\n",
      "Surface training t=17110, loss=0.03217253368347883\n",
      "Surface training t=17111, loss=0.03480349853634834\n",
      "Surface training t=17112, loss=0.03471822664141655\n",
      "Surface training t=17113, loss=0.02762782759964466\n",
      "Surface training t=17114, loss=0.029876964166760445\n",
      "Surface training t=17115, loss=0.026551813818514347\n",
      "Surface training t=17116, loss=0.025202464312314987\n",
      "Surface training t=17117, loss=0.02905076090246439\n",
      "Surface training t=17118, loss=0.03010426089167595\n",
      "Surface training t=17119, loss=0.04257766902446747\n",
      "Surface training t=17120, loss=0.0341819915920496\n",
      "Surface training t=17121, loss=0.03288237750530243\n",
      "Surface training t=17122, loss=0.026063978672027588\n",
      "Surface training t=17123, loss=0.02491435967385769\n",
      "Surface training t=17124, loss=0.027993584983050823\n",
      "Surface training t=17125, loss=0.0276305191218853\n",
      "Surface training t=17126, loss=0.02135791163891554\n",
      "Surface training t=17127, loss=0.02146988734602928\n",
      "Surface training t=17128, loss=0.016454172786325216\n",
      "Surface training t=17129, loss=0.02156155277043581\n",
      "Surface training t=17130, loss=0.019641579128801823\n",
      "Surface training t=17131, loss=0.0195221034809947\n",
      "Surface training t=17132, loss=0.019159073941409588\n",
      "Surface training t=17133, loss=0.02045445516705513\n",
      "Surface training t=17134, loss=0.022050956264138222\n",
      "Surface training t=17135, loss=0.031273314729332924\n",
      "Surface training t=17136, loss=0.03433156758546829\n",
      "Surface training t=17137, loss=0.02994721382856369\n",
      "Surface training t=17138, loss=0.024876603856682777\n",
      "Surface training t=17139, loss=0.024342975579202175\n",
      "Surface training t=17140, loss=0.029107853770256042\n",
      "Surface training t=17141, loss=0.0221913680434227\n",
      "Surface training t=17142, loss=0.020764963701367378\n",
      "Surface training t=17143, loss=0.01994580216705799\n",
      "Surface training t=17144, loss=0.01774113718420267\n",
      "Surface training t=17145, loss=0.02217854093760252\n",
      "Surface training t=17146, loss=0.027989672496914864\n",
      "Surface training t=17147, loss=0.01745706796646118\n",
      "Surface training t=17148, loss=0.022597074508666992\n",
      "Surface training t=17149, loss=0.02374313585460186\n",
      "Surface training t=17150, loss=0.030383972451090813\n",
      "Surface training t=17151, loss=0.02193899266421795\n",
      "Surface training t=17152, loss=0.025751985609531403\n",
      "Surface training t=17153, loss=0.026492342352867126\n",
      "Surface training t=17154, loss=0.028702816925942898\n",
      "Surface training t=17155, loss=0.026717310771346092\n",
      "Surface training t=17156, loss=0.029462773352861404\n",
      "Surface training t=17157, loss=0.03014540020376444\n",
      "Surface training t=17158, loss=0.03131851926445961\n",
      "Surface training t=17159, loss=0.0252795135602355\n",
      "Surface training t=17160, loss=0.030463154427707195\n",
      "Surface training t=17161, loss=0.023786778561770916\n",
      "Surface training t=17162, loss=0.02288039866834879\n",
      "Surface training t=17163, loss=0.02191267814487219\n",
      "Surface training t=17164, loss=0.02242385968565941\n",
      "Surface training t=17165, loss=0.0268024867400527\n",
      "Surface training t=17166, loss=0.02132006548345089\n",
      "Surface training t=17167, loss=0.02040851302444935\n",
      "Surface training t=17168, loss=0.019907833077013493\n",
      "Surface training t=17169, loss=0.0181005597114563\n",
      "Surface training t=17170, loss=0.02250419743359089\n",
      "Surface training t=17171, loss=0.01927414070814848\n",
      "Surface training t=17172, loss=0.02178189903497696\n",
      "Surface training t=17173, loss=0.02295565977692604\n",
      "Surface training t=17174, loss=0.01702837459743023\n",
      "Surface training t=17175, loss=0.01173978322185576\n",
      "Surface training t=17176, loss=0.028077424503862858\n",
      "Surface training t=17177, loss=0.02799897827208042\n",
      "Surface training t=17178, loss=0.03452189639210701\n",
      "Surface training t=17179, loss=0.062161851674318314\n",
      "Surface training t=17180, loss=0.05244921613484621\n",
      "Surface training t=17181, loss=0.08312484249472618\n",
      "Surface training t=17182, loss=0.07111410610377789\n",
      "Surface training t=17183, loss=0.04895168822258711\n",
      "Surface training t=17184, loss=0.05938323773443699\n",
      "Surface training t=17185, loss=0.041164133697748184\n",
      "Surface training t=17186, loss=0.043485959991812706\n",
      "Surface training t=17187, loss=0.042509086430072784\n",
      "Surface training t=17188, loss=0.029546920210123062\n",
      "Surface training t=17189, loss=0.038723746314644814\n",
      "Surface training t=17190, loss=0.027436901815235615\n",
      "Surface training t=17191, loss=0.02437067497521639\n",
      "Surface training t=17192, loss=0.037638528272509575\n",
      "Surface training t=17193, loss=0.028708034195005894\n",
      "Surface training t=17194, loss=0.026659009978175163\n",
      "Surface training t=17195, loss=0.02405410446226597\n",
      "Surface training t=17196, loss=0.024131382815539837\n",
      "Surface training t=17197, loss=0.03219777252525091\n",
      "Surface training t=17198, loss=0.02828818093985319\n",
      "Surface training t=17199, loss=0.03191200643777847\n",
      "Surface training t=17200, loss=0.028627008199691772\n",
      "Surface training t=17201, loss=0.030087693594396114\n",
      "Surface training t=17202, loss=0.027251594699919224\n",
      "Surface training t=17203, loss=0.030064205639064312\n",
      "Surface training t=17204, loss=0.03109305165708065\n",
      "Surface training t=17205, loss=0.025932098738849163\n",
      "Surface training t=17206, loss=0.027135669253766537\n",
      "Surface training t=17207, loss=0.02321053296327591\n",
      "Surface training t=17208, loss=0.03013891726732254\n",
      "Surface training t=17209, loss=0.02466829214245081\n",
      "Surface training t=17210, loss=0.025438854470849037\n",
      "Surface training t=17211, loss=0.02061430085450411\n",
      "Surface training t=17212, loss=0.01615757355466485\n",
      "Surface training t=17213, loss=0.02261112444102764\n",
      "Surface training t=17214, loss=0.033055366948246956\n",
      "Surface training t=17215, loss=0.023857714608311653\n",
      "Surface training t=17216, loss=0.023780494928359985\n",
      "Surface training t=17217, loss=0.02122531086206436\n",
      "Surface training t=17218, loss=0.02328643761575222\n",
      "Surface training t=17219, loss=0.019817231222987175\n",
      "Surface training t=17220, loss=0.020547335036098957\n",
      "Surface training t=17221, loss=0.020918825641274452\n",
      "Surface training t=17222, loss=0.02172571513801813\n",
      "Surface training t=17223, loss=0.018000943586230278\n",
      "Surface training t=17224, loss=0.01709480583667755\n",
      "Surface training t=17225, loss=0.017729626968503\n",
      "Surface training t=17226, loss=0.02592580672353506\n",
      "Surface training t=17227, loss=0.020413712598383427\n",
      "Surface training t=17228, loss=0.0364536065608263\n",
      "Surface training t=17229, loss=0.03614092618227005\n",
      "Surface training t=17230, loss=0.02520456351339817\n",
      "Surface training t=17231, loss=0.035059068351984024\n",
      "Surface training t=17232, loss=0.024581169709563255\n",
      "Surface training t=17233, loss=0.027792994864284992\n",
      "Surface training t=17234, loss=0.021116943564265966\n",
      "Surface training t=17235, loss=0.028425575233995914\n",
      "Surface training t=17236, loss=0.024600493721663952\n",
      "Surface training t=17237, loss=0.026084945537149906\n",
      "Surface training t=17238, loss=0.02810743637382984\n",
      "Surface training t=17239, loss=0.022966565564274788\n",
      "Surface training t=17240, loss=0.028303146362304688\n",
      "Surface training t=17241, loss=0.023242074996232986\n",
      "Surface training t=17242, loss=0.026671605184674263\n",
      "Surface training t=17243, loss=0.02577152941375971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=17244, loss=0.02605059277266264\n",
      "Surface training t=17245, loss=0.027531618252396584\n",
      "Surface training t=17246, loss=0.02986212819814682\n",
      "Surface training t=17247, loss=0.022043670527637005\n",
      "Surface training t=17248, loss=0.029899895191192627\n",
      "Surface training t=17249, loss=0.036367734894156456\n",
      "Surface training t=17250, loss=0.03407189063727856\n",
      "Surface training t=17251, loss=0.028043786995112896\n",
      "Surface training t=17252, loss=0.03420039638876915\n",
      "Surface training t=17253, loss=0.035124488174915314\n",
      "Surface training t=17254, loss=0.034211342222988605\n",
      "Surface training t=17255, loss=0.025532438419759274\n",
      "Surface training t=17256, loss=0.029758643358945847\n",
      "Surface training t=17257, loss=0.031846167519688606\n",
      "Surface training t=17258, loss=0.03441682271659374\n",
      "Surface training t=17259, loss=0.03714477829635143\n",
      "Surface training t=17260, loss=0.039335738867521286\n",
      "Surface training t=17261, loss=0.031280904076993465\n",
      "Surface training t=17262, loss=0.04949171468615532\n",
      "Surface training t=17263, loss=0.033817075192928314\n",
      "Surface training t=17264, loss=0.03066884446889162\n",
      "Surface training t=17265, loss=0.033940753899514675\n",
      "Surface training t=17266, loss=0.022294062189757824\n",
      "Surface training t=17267, loss=0.01916955504566431\n",
      "Surface training t=17268, loss=0.01997498981654644\n",
      "Surface training t=17269, loss=0.018000179901719093\n",
      "Surface training t=17270, loss=0.022327310405671597\n",
      "Surface training t=17271, loss=0.02191033400595188\n",
      "Surface training t=17272, loss=0.024729350581765175\n",
      "Surface training t=17273, loss=0.02741761226207018\n",
      "Surface training t=17274, loss=0.026507890783250332\n",
      "Surface training t=17275, loss=0.02615637518465519\n",
      "Surface training t=17276, loss=0.03423935920000076\n",
      "Surface training t=17277, loss=0.03170820791274309\n",
      "Surface training t=17278, loss=0.025905005633831024\n",
      "Surface training t=17279, loss=0.023466680198907852\n",
      "Surface training t=17280, loss=0.024066012352705002\n",
      "Surface training t=17281, loss=0.02781268861144781\n",
      "Surface training t=17282, loss=0.038089148700237274\n",
      "Surface training t=17283, loss=0.03316027671098709\n",
      "Surface training t=17284, loss=0.02158122882246971\n",
      "Surface training t=17285, loss=0.030408106744289398\n",
      "Surface training t=17286, loss=0.030793704092502594\n",
      "Surface training t=17287, loss=0.028063330333679914\n",
      "Surface training t=17288, loss=0.06014002487063408\n",
      "Surface training t=17289, loss=0.05016695708036423\n",
      "Surface training t=17290, loss=0.05808980576694012\n",
      "Surface training t=17291, loss=0.0512368306517601\n",
      "Surface training t=17292, loss=0.03970503155142069\n",
      "Surface training t=17293, loss=0.03933372162282467\n",
      "Surface training t=17294, loss=0.04556090570986271\n",
      "Surface training t=17295, loss=0.04879115708172321\n",
      "Surface training t=17296, loss=0.04041684605181217\n",
      "Surface training t=17297, loss=0.03921999875456095\n",
      "Surface training t=17298, loss=0.05158640258014202\n",
      "Surface training t=17299, loss=0.03104546293616295\n",
      "Surface training t=17300, loss=0.028770477510988712\n",
      "Surface training t=17301, loss=0.02076564822345972\n",
      "Surface training t=17302, loss=0.026892442256212234\n",
      "Surface training t=17303, loss=0.025255324319005013\n",
      "Surface training t=17304, loss=0.021508402191102505\n",
      "Surface training t=17305, loss=0.027071598917245865\n",
      "Surface training t=17306, loss=0.03528414200991392\n",
      "Surface training t=17307, loss=0.03400981053709984\n",
      "Surface training t=17308, loss=0.04491078108549118\n",
      "Surface training t=17309, loss=0.04156409204006195\n",
      "Surface training t=17310, loss=0.04183636233210564\n",
      "Surface training t=17311, loss=0.03364076837897301\n",
      "Surface training t=17312, loss=0.03442357573658228\n",
      "Surface training t=17313, loss=0.037673795595765114\n",
      "Surface training t=17314, loss=0.02974405325949192\n",
      "Surface training t=17315, loss=0.042719632387161255\n",
      "Surface training t=17316, loss=0.03586006909608841\n",
      "Surface training t=17317, loss=0.02997852023690939\n",
      "Surface training t=17318, loss=0.030973791144788265\n",
      "Surface training t=17319, loss=0.021326661109924316\n",
      "Surface training t=17320, loss=0.023540837690234184\n",
      "Surface training t=17321, loss=0.020803475752472878\n",
      "Surface training t=17322, loss=0.025364653207361698\n",
      "Surface training t=17323, loss=0.02068957034498453\n",
      "Surface training t=17324, loss=0.023408123292028904\n",
      "Surface training t=17325, loss=0.027803567238152027\n",
      "Surface training t=17326, loss=0.02300045546144247\n",
      "Surface training t=17327, loss=0.02185802161693573\n",
      "Surface training t=17328, loss=0.022068369202315807\n",
      "Surface training t=17329, loss=0.02255604090169072\n",
      "Surface training t=17330, loss=0.025095969438552856\n",
      "Surface training t=17331, loss=0.021873432211577892\n",
      "Surface training t=17332, loss=0.037093931809067726\n",
      "Surface training t=17333, loss=0.027438275516033173\n",
      "Surface training t=17334, loss=0.033948495984077454\n",
      "Surface training t=17335, loss=0.033896422013640404\n",
      "Surface training t=17336, loss=0.03255616594105959\n",
      "Surface training t=17337, loss=0.03428614791482687\n",
      "Surface training t=17338, loss=0.031376760452985764\n",
      "Surface training t=17339, loss=0.03308518044650555\n",
      "Surface training t=17340, loss=0.029670956544578075\n",
      "Surface training t=17341, loss=0.027185263112187386\n",
      "Surface training t=17342, loss=0.02695541176944971\n",
      "Surface training t=17343, loss=0.024713745340704918\n",
      "Surface training t=17344, loss=0.019546481780707836\n",
      "Surface training t=17345, loss=0.034957529976964\n",
      "Surface training t=17346, loss=0.03084386233240366\n",
      "Surface training t=17347, loss=0.030164132826030254\n",
      "Surface training t=17348, loss=0.049015119671821594\n",
      "Surface training t=17349, loss=0.02792464569211006\n",
      "Surface training t=17350, loss=0.04035970754921436\n",
      "Surface training t=17351, loss=0.043814195320010185\n",
      "Surface training t=17352, loss=0.02797622885555029\n",
      "Surface training t=17353, loss=0.02833909634500742\n",
      "Surface training t=17354, loss=0.03632688522338867\n",
      "Surface training t=17355, loss=0.031053196638822556\n",
      "Surface training t=17356, loss=0.03287823311984539\n",
      "Surface training t=17357, loss=0.03038759157061577\n",
      "Surface training t=17358, loss=0.026082918047904968\n",
      "Surface training t=17359, loss=0.030177125707268715\n",
      "Surface training t=17360, loss=0.023521323688328266\n",
      "Surface training t=17361, loss=0.04834175854921341\n",
      "Surface training t=17362, loss=0.03699565306305885\n",
      "Surface training t=17363, loss=0.03713699243962765\n",
      "Surface training t=17364, loss=0.049972113221883774\n",
      "Surface training t=17365, loss=0.045376451686024666\n",
      "Surface training t=17366, loss=0.04121929407119751\n",
      "Surface training t=17367, loss=0.04389025643467903\n",
      "Surface training t=17368, loss=0.03064274787902832\n",
      "Surface training t=17369, loss=0.028814426623284817\n",
      "Surface training t=17370, loss=0.036362248472869396\n",
      "Surface training t=17371, loss=0.028727836906909943\n",
      "Surface training t=17372, loss=0.03543868288397789\n",
      "Surface training t=17373, loss=0.035285837948322296\n",
      "Surface training t=17374, loss=0.026346324011683464\n",
      "Surface training t=17375, loss=0.03379246033728123\n",
      "Surface training t=17376, loss=0.029687292873859406\n",
      "Surface training t=17377, loss=0.03100376483052969\n",
      "Surface training t=17378, loss=0.03068633656948805\n",
      "Surface training t=17379, loss=0.03716029692441225\n",
      "Surface training t=17380, loss=0.02160164900124073\n",
      "Surface training t=17381, loss=0.02305548917502165\n",
      "Surface training t=17382, loss=0.022183622233569622\n",
      "Surface training t=17383, loss=0.024379306472837925\n",
      "Surface training t=17384, loss=0.01898077502846718\n",
      "Surface training t=17385, loss=0.02253334131091833\n",
      "Surface training t=17386, loss=0.017033033072948456\n",
      "Surface training t=17387, loss=0.018968453630805016\n",
      "Surface training t=17388, loss=0.02484534401446581\n",
      "Surface training t=17389, loss=0.02244679257273674\n",
      "Surface training t=17390, loss=0.024722264148294926\n",
      "Surface training t=17391, loss=0.02691001445055008\n",
      "Surface training t=17392, loss=0.027773701585829258\n",
      "Surface training t=17393, loss=0.0242469422519207\n",
      "Surface training t=17394, loss=0.024854331277310848\n",
      "Surface training t=17395, loss=0.025221210904419422\n",
      "Surface training t=17396, loss=0.02723296359181404\n",
      "Surface training t=17397, loss=0.02934971544891596\n",
      "Surface training t=17398, loss=0.019878611899912357\n",
      "Surface training t=17399, loss=0.02974790334701538\n",
      "Surface training t=17400, loss=0.021270095370709896\n",
      "Surface training t=17401, loss=0.023620611988008022\n",
      "Surface training t=17402, loss=0.030624414794147015\n",
      "Surface training t=17403, loss=0.03011615015566349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=17404, loss=0.029288794845342636\n",
      "Surface training t=17405, loss=0.026573571376502514\n",
      "Surface training t=17406, loss=0.02612667065113783\n",
      "Surface training t=17407, loss=0.024883697740733624\n",
      "Surface training t=17408, loss=0.021583212539553642\n",
      "Surface training t=17409, loss=0.022355543449521065\n",
      "Surface training t=17410, loss=0.02185557223856449\n",
      "Surface training t=17411, loss=0.021544821560382843\n",
      "Surface training t=17412, loss=0.02396862395107746\n",
      "Surface training t=17413, loss=0.026456640101969242\n",
      "Surface training t=17414, loss=0.02629345841705799\n",
      "Surface training t=17415, loss=0.022769669070839882\n",
      "Surface training t=17416, loss=0.02554182428866625\n",
      "Surface training t=17417, loss=0.028730152174830437\n",
      "Surface training t=17418, loss=0.029226994141936302\n",
      "Surface training t=17419, loss=0.034919168800115585\n",
      "Surface training t=17420, loss=0.040198273956775665\n",
      "Surface training t=17421, loss=0.0366192851215601\n",
      "Surface training t=17422, loss=0.03680134750902653\n",
      "Surface training t=17423, loss=0.029743067920207977\n",
      "Surface training t=17424, loss=0.030151653103530407\n",
      "Surface training t=17425, loss=0.030220860615372658\n",
      "Surface training t=17426, loss=0.025801338255405426\n",
      "Surface training t=17427, loss=0.02390120830386877\n",
      "Surface training t=17428, loss=0.02333883009850979\n",
      "Surface training t=17429, loss=0.022978007793426514\n",
      "Surface training t=17430, loss=0.023467829450964928\n",
      "Surface training t=17431, loss=0.021858325228095055\n",
      "Surface training t=17432, loss=0.01996212638914585\n",
      "Surface training t=17433, loss=0.019341139122843742\n",
      "Surface training t=17434, loss=0.024320470169186592\n",
      "Surface training t=17435, loss=0.018118552397936583\n",
      "Surface training t=17436, loss=0.013811786659061909\n",
      "Surface training t=17437, loss=0.019473441876471043\n",
      "Surface training t=17438, loss=0.023122533224523067\n",
      "Surface training t=17439, loss=0.022452996112406254\n",
      "Surface training t=17440, loss=0.023817971348762512\n",
      "Surface training t=17441, loss=0.02803091984242201\n",
      "Surface training t=17442, loss=0.032123442739248276\n",
      "Surface training t=17443, loss=0.037432342767715454\n",
      "Surface training t=17444, loss=0.02807279210537672\n",
      "Surface training t=17445, loss=0.03692655637860298\n",
      "Surface training t=17446, loss=0.028703399002552032\n",
      "Surface training t=17447, loss=0.037786396220326424\n",
      "Surface training t=17448, loss=0.03559288941323757\n",
      "Surface training t=17449, loss=0.03140243887901306\n",
      "Surface training t=17450, loss=0.03936811164021492\n",
      "Surface training t=17451, loss=0.02735370397567749\n",
      "Surface training t=17452, loss=0.03264207765460014\n",
      "Surface training t=17453, loss=0.048965588212013245\n",
      "Surface training t=17454, loss=0.036356471478939056\n",
      "Surface training t=17455, loss=0.04529428295791149\n",
      "Surface training t=17456, loss=0.03271981794387102\n",
      "Surface training t=17457, loss=0.031424734741449356\n",
      "Surface training t=17458, loss=0.03489735163748264\n",
      "Surface training t=17459, loss=0.02999974973499775\n",
      "Surface training t=17460, loss=0.022771849762648344\n",
      "Surface training t=17461, loss=0.03623300790786743\n",
      "Surface training t=17462, loss=0.028121622279286385\n",
      "Surface training t=17463, loss=0.034115809947252274\n",
      "Surface training t=17464, loss=0.029534831643104553\n",
      "Surface training t=17465, loss=0.030095694586634636\n",
      "Surface training t=17466, loss=0.03187209367752075\n",
      "Surface training t=17467, loss=0.0303405849263072\n",
      "Surface training t=17468, loss=0.025636183097958565\n",
      "Surface training t=17469, loss=0.04186211712658405\n",
      "Surface training t=17470, loss=0.030989577062427998\n",
      "Surface training t=17471, loss=0.03913174569606781\n",
      "Surface training t=17472, loss=0.02910808566957712\n",
      "Surface training t=17473, loss=0.023875162936747074\n",
      "Surface training t=17474, loss=0.03362675942480564\n",
      "Surface training t=17475, loss=0.03319607861340046\n",
      "Surface training t=17476, loss=0.03181492164731026\n",
      "Surface training t=17477, loss=0.036324264481663704\n",
      "Surface training t=17478, loss=0.03348710108548403\n",
      "Surface training t=17479, loss=0.025055231526494026\n",
      "Surface training t=17480, loss=0.021988241001963615\n",
      "Surface training t=17481, loss=0.019034285563975573\n",
      "Surface training t=17482, loss=0.04485262557864189\n",
      "Surface training t=17483, loss=0.03435799293220043\n",
      "Surface training t=17484, loss=0.049358218908309937\n",
      "Surface training t=17485, loss=0.0370942335575819\n",
      "Surface training t=17486, loss=0.05476042442023754\n",
      "Surface training t=17487, loss=0.035403928719460964\n",
      "Surface training t=17488, loss=0.03762722574174404\n",
      "Surface training t=17489, loss=0.04673788137733936\n",
      "Surface training t=17490, loss=0.043769754469394684\n",
      "Surface training t=17491, loss=0.05654396302998066\n",
      "Surface training t=17492, loss=0.03378692455589771\n",
      "Surface training t=17493, loss=0.04600514471530914\n",
      "Surface training t=17494, loss=0.02220500260591507\n",
      "Surface training t=17495, loss=0.03243453986942768\n",
      "Surface training t=17496, loss=0.034285543486475945\n",
      "Surface training t=17497, loss=0.027494842186570168\n",
      "Surface training t=17498, loss=0.023015502840280533\n",
      "Surface training t=17499, loss=0.023740166798233986\n",
      "Surface training t=17500, loss=0.01828326191753149\n",
      "Surface training t=17501, loss=0.029649595730006695\n",
      "Surface training t=17502, loss=0.027170089073479176\n",
      "Surface training t=17503, loss=0.022758291102945805\n",
      "Surface training t=17504, loss=0.02283606957644224\n",
      "Surface training t=17505, loss=0.02173034008592367\n",
      "Surface training t=17506, loss=0.02023005113005638\n",
      "Surface training t=17507, loss=0.020137115381658077\n",
      "Surface training t=17508, loss=0.020177742466330528\n",
      "Surface training t=17509, loss=0.016783404164016247\n",
      "Surface training t=17510, loss=0.025791875086724758\n",
      "Surface training t=17511, loss=0.023154704831540585\n",
      "Surface training t=17512, loss=0.01984686590731144\n",
      "Surface training t=17513, loss=0.023560971952974796\n",
      "Surface training t=17514, loss=0.02570926584303379\n",
      "Surface training t=17515, loss=0.027222711592912674\n",
      "Surface training t=17516, loss=0.02328217215836048\n",
      "Surface training t=17517, loss=0.033819315023720264\n",
      "Surface training t=17518, loss=0.0338884349912405\n",
      "Surface training t=17519, loss=0.03846179321408272\n",
      "Surface training t=17520, loss=0.03057977557182312\n",
      "Surface training t=17521, loss=0.044560788199305534\n",
      "Surface training t=17522, loss=0.034255487844347954\n",
      "Surface training t=17523, loss=0.03216410428285599\n",
      "Surface training t=17524, loss=0.04245787858963013\n",
      "Surface training t=17525, loss=0.027184723876416683\n",
      "Surface training t=17526, loss=0.026112492196261883\n",
      "Surface training t=17527, loss=0.03577894903719425\n",
      "Surface training t=17528, loss=0.02933600451797247\n",
      "Surface training t=17529, loss=0.03460825886577368\n",
      "Surface training t=17530, loss=0.03259773273020983\n",
      "Surface training t=17531, loss=0.030092256143689156\n",
      "Surface training t=17532, loss=0.03914085403084755\n",
      "Surface training t=17533, loss=0.03388563450425863\n",
      "Surface training t=17534, loss=0.032845488749444485\n",
      "Surface training t=17535, loss=0.03531728591769934\n",
      "Surface training t=17536, loss=0.030037234537303448\n",
      "Surface training t=17537, loss=0.03253170847892761\n",
      "Surface training t=17538, loss=0.026293904520571232\n",
      "Surface training t=17539, loss=0.02852644957602024\n",
      "Surface training t=17540, loss=0.03542608395218849\n",
      "Surface training t=17541, loss=0.022754850797355175\n",
      "Surface training t=17542, loss=0.02507386263459921\n",
      "Surface training t=17543, loss=0.03705819230526686\n",
      "Surface training t=17544, loss=0.028524824418127537\n",
      "Surface training t=17545, loss=0.021732093766331673\n",
      "Surface training t=17546, loss=0.02330902684479952\n",
      "Surface training t=17547, loss=0.020023848861455917\n",
      "Surface training t=17548, loss=0.022290872409939766\n",
      "Surface training t=17549, loss=0.019484235905110836\n",
      "Surface training t=17550, loss=0.027145199477672577\n",
      "Surface training t=17551, loss=0.018386716954410076\n",
      "Surface training t=17552, loss=0.01469599548727274\n",
      "Surface training t=17553, loss=0.023090672679245472\n",
      "Surface training t=17554, loss=0.0215763496235013\n",
      "Surface training t=17555, loss=0.01796597708016634\n",
      "Surface training t=17556, loss=0.02365140337496996\n",
      "Surface training t=17557, loss=0.0204488025046885\n",
      "Surface training t=17558, loss=0.017794804647564888\n",
      "Surface training t=17559, loss=0.0193985803052783\n",
      "Surface training t=17560, loss=0.02296259067952633\n",
      "Surface training t=17561, loss=0.02533707208931446\n",
      "Surface training t=17562, loss=0.022917755879461765\n",
      "Surface training t=17563, loss=0.016173340380191803\n",
      "Surface training t=17564, loss=0.020822900347411633\n",
      "Surface training t=17565, loss=0.023143955506384373\n",
      "Surface training t=17566, loss=0.017038484569638968\n",
      "Surface training t=17567, loss=0.02563614957034588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=17568, loss=0.0170155824162066\n",
      "Surface training t=17569, loss=0.02449630480259657\n",
      "Surface training t=17570, loss=0.01946327742189169\n",
      "Surface training t=17571, loss=0.018318680115044117\n",
      "Surface training t=17572, loss=0.017633951269090176\n",
      "Surface training t=17573, loss=0.01927352510392666\n",
      "Surface training t=17574, loss=0.0184842050075531\n",
      "Surface training t=17575, loss=0.019810297526419163\n",
      "Surface training t=17576, loss=0.024507397785782814\n",
      "Surface training t=17577, loss=0.021259482018649578\n",
      "Surface training t=17578, loss=0.020144442096352577\n",
      "Surface training t=17579, loss=0.01955909002572298\n",
      "Surface training t=17580, loss=0.019617974758148193\n",
      "Surface training t=17581, loss=0.01917885895818472\n",
      "Surface training t=17582, loss=0.021656356751918793\n",
      "Surface training t=17583, loss=0.02280275709927082\n",
      "Surface training t=17584, loss=0.03364754468202591\n",
      "Surface training t=17585, loss=0.04221176728606224\n",
      "Surface training t=17586, loss=0.03309463895857334\n",
      "Surface training t=17587, loss=0.02663112711161375\n",
      "Surface training t=17588, loss=0.023622057400643826\n",
      "Surface training t=17589, loss=0.024714178405702114\n",
      "Surface training t=17590, loss=0.01645886106416583\n",
      "Surface training t=17591, loss=0.02569871675223112\n",
      "Surface training t=17592, loss=0.02233356051146984\n",
      "Surface training t=17593, loss=0.02388924639672041\n",
      "Surface training t=17594, loss=0.02640093769878149\n",
      "Surface training t=17595, loss=0.027448758482933044\n",
      "Surface training t=17596, loss=0.019103343598544598\n",
      "Surface training t=17597, loss=0.02120701316744089\n",
      "Surface training t=17598, loss=0.0253507848829031\n",
      "Surface training t=17599, loss=0.02816780097782612\n",
      "Surface training t=17600, loss=0.02445882186293602\n",
      "Surface training t=17601, loss=0.02421383000910282\n",
      "Surface training t=17602, loss=0.03472355008125305\n",
      "Surface training t=17603, loss=0.021693465299904346\n",
      "Surface training t=17604, loss=0.031860326416790485\n",
      "Surface training t=17605, loss=0.0237431637942791\n",
      "Surface training t=17606, loss=0.02899664919823408\n",
      "Surface training t=17607, loss=0.030477521009743214\n",
      "Surface training t=17608, loss=0.022762347012758255\n",
      "Surface training t=17609, loss=0.02701407205313444\n",
      "Surface training t=17610, loss=0.033366478979587555\n",
      "Surface training t=17611, loss=0.028277648612856865\n",
      "Surface training t=17612, loss=0.030387308448553085\n",
      "Surface training t=17613, loss=0.02522411197423935\n",
      "Surface training t=17614, loss=0.03202556073665619\n",
      "Surface training t=17615, loss=0.0333529831841588\n",
      "Surface training t=17616, loss=0.02785959281027317\n",
      "Surface training t=17617, loss=0.02717493660748005\n",
      "Surface training t=17618, loss=0.024716636165976524\n",
      "Surface training t=17619, loss=0.030892128124833107\n",
      "Surface training t=17620, loss=0.02634697314351797\n",
      "Surface training t=17621, loss=0.023659583181142807\n",
      "Surface training t=17622, loss=0.024977369233965874\n",
      "Surface training t=17623, loss=0.0380451213568449\n",
      "Surface training t=17624, loss=0.03429495356976986\n",
      "Surface training t=17625, loss=0.03485463932156563\n",
      "Surface training t=17626, loss=0.03058399073779583\n",
      "Surface training t=17627, loss=0.030028355307877064\n",
      "Surface training t=17628, loss=0.03475754614919424\n",
      "Surface training t=17629, loss=0.045924099162220955\n",
      "Surface training t=17630, loss=0.04497847147285938\n",
      "Surface training t=17631, loss=0.038035159930586815\n",
      "Surface training t=17632, loss=0.030179735273122787\n",
      "Surface training t=17633, loss=0.041191257536411285\n",
      "Surface training t=17634, loss=0.025457076728343964\n",
      "Surface training t=17635, loss=0.030999677255749702\n",
      "Surface training t=17636, loss=0.03963364660739899\n",
      "Surface training t=17637, loss=0.03148413635790348\n",
      "Surface training t=17638, loss=0.04543761722743511\n",
      "Surface training t=17639, loss=0.02875742968171835\n",
      "Surface training t=17640, loss=0.030702635645866394\n",
      "Surface training t=17641, loss=0.035210736095905304\n",
      "Surface training t=17642, loss=0.026896348223090172\n",
      "Surface training t=17643, loss=0.0355672026053071\n",
      "Surface training t=17644, loss=0.050154680386185646\n",
      "Surface training t=17645, loss=0.03685635328292847\n",
      "Surface training t=17646, loss=0.03122538421303034\n",
      "Surface training t=17647, loss=0.029534753412008286\n",
      "Surface training t=17648, loss=0.047543833032250404\n",
      "Surface training t=17649, loss=0.03773570992052555\n",
      "Surface training t=17650, loss=0.03515254147350788\n",
      "Surface training t=17651, loss=0.027783245779573917\n",
      "Surface training t=17652, loss=0.032386887818574905\n",
      "Surface training t=17653, loss=0.0325942886993289\n",
      "Surface training t=17654, loss=0.025035738945007324\n",
      "Surface training t=17655, loss=0.03590282425284386\n",
      "Surface training t=17656, loss=0.027324208058416843\n",
      "Surface training t=17657, loss=0.02529223170131445\n",
      "Surface training t=17658, loss=0.02399932872503996\n",
      "Surface training t=17659, loss=0.021037892438471317\n",
      "Surface training t=17660, loss=0.018217571545392275\n",
      "Surface training t=17661, loss=0.02349824644625187\n",
      "Surface training t=17662, loss=0.018926715943962336\n",
      "Surface training t=17663, loss=0.022877931594848633\n",
      "Surface training t=17664, loss=0.020449617877602577\n",
      "Surface training t=17665, loss=0.024448972195386887\n",
      "Surface training t=17666, loss=0.020868822932243347\n",
      "Surface training t=17667, loss=0.02310256566852331\n",
      "Surface training t=17668, loss=0.026317383162677288\n",
      "Surface training t=17669, loss=0.024005049839615822\n",
      "Surface training t=17670, loss=0.022026493214070797\n",
      "Surface training t=17671, loss=0.01904072519391775\n",
      "Surface training t=17672, loss=0.023670569993555546\n",
      "Surface training t=17673, loss=0.03796030953526497\n",
      "Surface training t=17674, loss=0.03302597068250179\n",
      "Surface training t=17675, loss=0.03975392132997513\n",
      "Surface training t=17676, loss=0.04236591421067715\n",
      "Surface training t=17677, loss=0.03999614156782627\n",
      "Surface training t=17678, loss=0.025914967991411686\n",
      "Surface training t=17679, loss=0.03516710549592972\n",
      "Surface training t=17680, loss=0.03035672288388014\n",
      "Surface training t=17681, loss=0.028693057131022215\n",
      "Surface training t=17682, loss=0.03436463791877031\n",
      "Surface training t=17683, loss=0.02440840657800436\n",
      "Surface training t=17684, loss=0.025732239708304405\n",
      "Surface training t=17685, loss=0.018170285038650036\n",
      "Surface training t=17686, loss=0.023150810040533543\n",
      "Surface training t=17687, loss=0.02342415601015091\n",
      "Surface training t=17688, loss=0.037733010947704315\n",
      "Surface training t=17689, loss=0.03090058360248804\n",
      "Surface training t=17690, loss=0.03305715508759022\n",
      "Surface training t=17691, loss=0.03309973794966936\n",
      "Surface training t=17692, loss=0.0305550554767251\n",
      "Surface training t=17693, loss=0.025747214443981647\n",
      "Surface training t=17694, loss=0.030545050278306007\n",
      "Surface training t=17695, loss=0.023376737721264362\n",
      "Surface training t=17696, loss=0.03251137025654316\n",
      "Surface training t=17697, loss=0.028675026260316372\n",
      "Surface training t=17698, loss=0.03383418545126915\n",
      "Surface training t=17699, loss=0.03529137372970581\n",
      "Surface training t=17700, loss=0.035067118704319\n",
      "Surface training t=17701, loss=0.038909804075956345\n",
      "Surface training t=17702, loss=0.03161101136356592\n",
      "Surface training t=17703, loss=0.031214511953294277\n",
      "Surface training t=17704, loss=0.03007975034415722\n",
      "Surface training t=17705, loss=0.027529625222086906\n",
      "Surface training t=17706, loss=0.022912712767720222\n",
      "Surface training t=17707, loss=0.021745421923696995\n",
      "Surface training t=17708, loss=0.01882587093859911\n",
      "Surface training t=17709, loss=0.021903540939092636\n",
      "Surface training t=17710, loss=0.01650030631572008\n",
      "Surface training t=17711, loss=0.014988831710070372\n",
      "Surface training t=17712, loss=0.022638323716819286\n",
      "Surface training t=17713, loss=0.031124775297939777\n",
      "Surface training t=17714, loss=0.031372408382594585\n",
      "Surface training t=17715, loss=0.03236761316657066\n",
      "Surface training t=17716, loss=0.02835831791162491\n",
      "Surface training t=17717, loss=0.03731299750506878\n",
      "Surface training t=17718, loss=0.05453605204820633\n",
      "Surface training t=17719, loss=0.03495601378381252\n",
      "Surface training t=17720, loss=0.03968465328216553\n",
      "Surface training t=17721, loss=0.03706164471805096\n",
      "Surface training t=17722, loss=0.04377802088856697\n",
      "Surface training t=17723, loss=0.05560874752700329\n",
      "Surface training t=17724, loss=0.03220159187912941\n",
      "Surface training t=17725, loss=0.04003124125301838\n",
      "Surface training t=17726, loss=0.04401226155459881\n",
      "Surface training t=17727, loss=0.04407153092324734\n",
      "Surface training t=17728, loss=0.03041328862309456\n",
      "Surface training t=17729, loss=0.029502195306122303\n",
      "Surface training t=17730, loss=0.03543359600007534\n",
      "Surface training t=17731, loss=0.029280662536621094\n",
      "Surface training t=17732, loss=0.028593353927135468\n",
      "Surface training t=17733, loss=0.024006841704249382\n",
      "Surface training t=17734, loss=0.026371633633971214\n",
      "Surface training t=17735, loss=0.020354412496089935\n",
      "Surface training t=17736, loss=0.02292540017515421\n",
      "Surface training t=17737, loss=0.033205388113856316\n",
      "Surface training t=17738, loss=0.03991683945059776\n",
      "Surface training t=17739, loss=0.05543776787817478\n",
      "Surface training t=17740, loss=0.04458486568182707\n",
      "Surface training t=17741, loss=0.05127907544374466\n",
      "Surface training t=17742, loss=0.06551926583051682\n",
      "Surface training t=17743, loss=0.04454567842185497\n",
      "Surface training t=17744, loss=0.0409510750323534\n",
      "Surface training t=17745, loss=0.029974041506648064\n",
      "Surface training t=17746, loss=0.06429845467209816\n",
      "Surface training t=17747, loss=0.04788583144545555\n",
      "Surface training t=17748, loss=0.06159549206495285\n",
      "Surface training t=17749, loss=0.0576750785112381\n",
      "Surface training t=17750, loss=0.042414652183651924\n",
      "Surface training t=17751, loss=0.05094937141984701\n",
      "Surface training t=17752, loss=0.0631623063236475\n",
      "Surface training t=17753, loss=0.0513322539627552\n",
      "Surface training t=17754, loss=0.0457290718331933\n",
      "Surface training t=17755, loss=0.03910844400525093\n",
      "Surface training t=17756, loss=0.03757173381745815\n",
      "Surface training t=17757, loss=0.03211945202201605\n",
      "Surface training t=17758, loss=0.023888584226369858\n",
      "Surface training t=17759, loss=0.027682323940098286\n",
      "Surface training t=17760, loss=0.02095501683652401\n",
      "Surface training t=17761, loss=0.022261444479227066\n",
      "Surface training t=17762, loss=0.031366665847599506\n",
      "Surface training t=17763, loss=0.03117667604237795\n",
      "Surface training t=17764, loss=0.026165545918047428\n",
      "Surface training t=17765, loss=0.0299757095053792\n",
      "Surface training t=17766, loss=0.02789277397096157\n",
      "Surface training t=17767, loss=0.026251248084008694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=17768, loss=0.028248817659914494\n",
      "Surface training t=17769, loss=0.021743816323578358\n",
      "Surface training t=17770, loss=0.027908464893698692\n",
      "Surface training t=17771, loss=0.019529948011040688\n",
      "Surface training t=17772, loss=0.027213802561163902\n",
      "Surface training t=17773, loss=0.02108877431601286\n",
      "Surface training t=17774, loss=0.02040100283920765\n",
      "Surface training t=17775, loss=0.02895673830062151\n",
      "Surface training t=17776, loss=0.03615056350827217\n",
      "Surface training t=17777, loss=0.0299901831895113\n",
      "Surface training t=17778, loss=0.021968121640384197\n",
      "Surface training t=17779, loss=0.022568381391465664\n",
      "Surface training t=17780, loss=0.019900239072740078\n",
      "Surface training t=17781, loss=0.021449970081448555\n",
      "Surface training t=17782, loss=0.028934646397829056\n",
      "Surface training t=17783, loss=0.025254049338400364\n",
      "Surface training t=17784, loss=0.02491874247789383\n",
      "Surface training t=17785, loss=0.03180280700325966\n",
      "Surface training t=17786, loss=0.03231982234865427\n",
      "Surface training t=17787, loss=0.030084618367254734\n",
      "Surface training t=17788, loss=0.025596588850021362\n",
      "Surface training t=17789, loss=0.02725768554955721\n",
      "Surface training t=17790, loss=0.02421305514872074\n",
      "Surface training t=17791, loss=0.024892060086131096\n",
      "Surface training t=17792, loss=0.025528867729008198\n",
      "Surface training t=17793, loss=0.024992002174258232\n",
      "Surface training t=17794, loss=0.026928987354040146\n",
      "Surface training t=17795, loss=0.022275525145232677\n",
      "Surface training t=17796, loss=0.023716717027127743\n",
      "Surface training t=17797, loss=0.02475676778703928\n",
      "Surface training t=17798, loss=0.019272944889962673\n",
      "Surface training t=17799, loss=0.027737009339034557\n",
      "Surface training t=17800, loss=0.030777888372540474\n",
      "Surface training t=17801, loss=0.05813452787697315\n",
      "Surface training t=17802, loss=0.04203322436660528\n",
      "Surface training t=17803, loss=0.04748054500669241\n",
      "Surface training t=17804, loss=0.05431228317320347\n",
      "Surface training t=17805, loss=0.04421943612396717\n",
      "Surface training t=17806, loss=0.040792882442474365\n",
      "Surface training t=17807, loss=0.058487094938755035\n",
      "Surface training t=17808, loss=0.04850354231894016\n",
      "Surface training t=17809, loss=0.03955302946269512\n",
      "Surface training t=17810, loss=0.03656096197664738\n",
      "Surface training t=17811, loss=0.052983902394771576\n",
      "Surface training t=17812, loss=0.029888607561588287\n",
      "Surface training t=17813, loss=0.02850282471626997\n",
      "Surface training t=17814, loss=0.040367985144257545\n",
      "Surface training t=17815, loss=0.049097223207354546\n",
      "Surface training t=17816, loss=0.06283377669751644\n",
      "Surface training t=17817, loss=0.043246316723525524\n",
      "Surface training t=17818, loss=0.0619044154882431\n",
      "Surface training t=17819, loss=0.06902073137462139\n",
      "Surface training t=17820, loss=0.04639125056564808\n",
      "Surface training t=17821, loss=0.05726386979222298\n",
      "Surface training t=17822, loss=0.04029383137822151\n",
      "Surface training t=17823, loss=0.032494429498910904\n",
      "Surface training t=17824, loss=0.042814772576093674\n",
      "Surface training t=17825, loss=0.036403427831828594\n",
      "Surface training t=17826, loss=0.035680899396538734\n",
      "Surface training t=17827, loss=0.03962968848645687\n",
      "Surface training t=17828, loss=0.033831361681222916\n",
      "Surface training t=17829, loss=0.03488772641867399\n",
      "Surface training t=17830, loss=0.03554348088800907\n",
      "Surface training t=17831, loss=0.0372008141130209\n",
      "Surface training t=17832, loss=0.03948610369116068\n",
      "Surface training t=17833, loss=0.02964515145868063\n",
      "Surface training t=17834, loss=0.03165290132164955\n",
      "Surface training t=17835, loss=0.02866766508668661\n",
      "Surface training t=17836, loss=0.028297943994402885\n",
      "Surface training t=17837, loss=0.04536001197993755\n",
      "Surface training t=17838, loss=0.023334010504186153\n",
      "Surface training t=17839, loss=0.027265489101409912\n",
      "Surface training t=17840, loss=0.03141337167471647\n",
      "Surface training t=17841, loss=0.02732884045690298\n",
      "Surface training t=17842, loss=0.019451663829386234\n",
      "Surface training t=17843, loss=0.02875613421201706\n",
      "Surface training t=17844, loss=0.023008487187325954\n",
      "Surface training t=17845, loss=0.03434720262885094\n",
      "Surface training t=17846, loss=0.02722785808146\n",
      "Surface training t=17847, loss=0.024411375634372234\n",
      "Surface training t=17848, loss=0.03204190079122782\n",
      "Surface training t=17849, loss=0.03235703334212303\n",
      "Surface training t=17850, loss=0.02824560645967722\n",
      "Surface training t=17851, loss=0.022696349769830704\n",
      "Surface training t=17852, loss=0.0388849712908268\n",
      "Surface training t=17853, loss=0.028597813099622726\n",
      "Surface training t=17854, loss=0.023103726096451283\n",
      "Surface training t=17855, loss=0.03624747321009636\n",
      "Surface training t=17856, loss=0.028940636664628983\n",
      "Surface training t=17857, loss=0.0306516382843256\n",
      "Surface training t=17858, loss=0.04388798028230667\n",
      "Surface training t=17859, loss=0.0323566785082221\n",
      "Surface training t=17860, loss=0.03330915793776512\n",
      "Surface training t=17861, loss=0.03656549192965031\n",
      "Surface training t=17862, loss=0.03232339024543762\n",
      "Surface training t=17863, loss=0.06332739070057869\n",
      "Surface training t=17864, loss=0.048918768763542175\n",
      "Surface training t=17865, loss=0.053193556144833565\n",
      "Surface training t=17866, loss=0.0639621801674366\n",
      "Surface training t=17867, loss=0.04738760832697153\n",
      "Surface training t=17868, loss=0.06623800098896027\n",
      "Surface training t=17869, loss=0.05561392568051815\n",
      "Surface training t=17870, loss=0.03786285035312176\n",
      "Surface training t=17871, loss=0.03493858501315117\n",
      "Surface training t=17872, loss=0.05440016835927963\n",
      "Surface training t=17873, loss=0.03892962448298931\n",
      "Surface training t=17874, loss=0.03918742947280407\n",
      "Surface training t=17875, loss=0.036437079310417175\n",
      "Surface training t=17876, loss=0.03454060200601816\n",
      "Surface training t=17877, loss=0.02438165806233883\n",
      "Surface training t=17878, loss=0.03897893987596035\n",
      "Surface training t=17879, loss=0.026369398459792137\n",
      "Surface training t=17880, loss=0.029417000710964203\n",
      "Surface training t=17881, loss=0.033766936510801315\n",
      "Surface training t=17882, loss=0.03034079074859619\n",
      "Surface training t=17883, loss=0.03721642307937145\n",
      "Surface training t=17884, loss=0.023467705585062504\n",
      "Surface training t=17885, loss=0.03022089321166277\n",
      "Surface training t=17886, loss=0.03226070664823055\n",
      "Surface training t=17887, loss=0.029836060479283333\n",
      "Surface training t=17888, loss=0.033236680552363396\n",
      "Surface training t=17889, loss=0.030564159154891968\n",
      "Surface training t=17890, loss=0.02556326612830162\n",
      "Surface training t=17891, loss=0.026120699010789394\n",
      "Surface training t=17892, loss=0.026853195391595364\n",
      "Surface training t=17893, loss=0.027485807426273823\n",
      "Surface training t=17894, loss=0.02174867782741785\n",
      "Surface training t=17895, loss=0.02315988764166832\n",
      "Surface training t=17896, loss=0.02650373335927725\n",
      "Surface training t=17897, loss=0.024947986006736755\n",
      "Surface training t=17898, loss=0.023701999336481094\n",
      "Surface training t=17899, loss=0.02809617668390274\n",
      "Surface training t=17900, loss=0.02845568861812353\n",
      "Surface training t=17901, loss=0.029718289151787758\n",
      "Surface training t=17902, loss=0.025448927655816078\n",
      "Surface training t=17903, loss=0.02602567244321108\n",
      "Surface training t=17904, loss=0.039237501099705696\n",
      "Surface training t=17905, loss=0.03399512730538845\n",
      "Surface training t=17906, loss=0.026670705527067184\n",
      "Surface training t=17907, loss=0.025237534195184708\n",
      "Surface training t=17908, loss=0.023416370153427124\n",
      "Surface training t=17909, loss=0.022011134773492813\n",
      "Surface training t=17910, loss=0.023835083469748497\n",
      "Surface training t=17911, loss=0.02503287047147751\n",
      "Surface training t=17912, loss=0.01819029077887535\n",
      "Surface training t=17913, loss=0.01587840262800455\n",
      "Surface training t=17914, loss=0.01469701761379838\n",
      "Surface training t=17915, loss=0.021831488236784935\n",
      "Surface training t=17916, loss=0.027196519076824188\n",
      "Surface training t=17917, loss=0.024103544652462006\n",
      "Surface training t=17918, loss=0.025577218271791935\n",
      "Surface training t=17919, loss=0.022875898517668247\n",
      "Surface training t=17920, loss=0.020158737897872925\n",
      "Surface training t=17921, loss=0.02042584214359522\n",
      "Surface training t=17922, loss=0.022109901532530785\n",
      "Surface training t=17923, loss=0.018247220665216446\n",
      "Surface training t=17924, loss=0.02625998854637146\n",
      "Surface training t=17925, loss=0.02578243613243103\n",
      "Surface training t=17926, loss=0.03104033600538969\n",
      "Surface training t=17927, loss=0.029652445577085018\n",
      "Surface training t=17928, loss=0.02563496772199869\n",
      "Surface training t=17929, loss=0.026352793909609318\n",
      "Surface training t=17930, loss=0.02643794845789671\n",
      "Surface training t=17931, loss=0.027184821665287018\n",
      "Surface training t=17932, loss=0.0264376662671566\n",
      "Surface training t=17933, loss=0.03044825792312622\n",
      "Surface training t=17934, loss=0.02801379654556513\n",
      "Surface training t=17935, loss=0.02748573198914528\n",
      "Surface training t=17936, loss=0.03476467914879322\n",
      "Surface training t=17937, loss=0.027200723066926003\n",
      "Surface training t=17938, loss=0.03055497631430626\n",
      "Surface training t=17939, loss=0.050029342994093895\n",
      "Surface training t=17940, loss=0.038702432066202164\n",
      "Surface training t=17941, loss=0.027218366973102093\n",
      "Surface training t=17942, loss=0.027480307035148144\n",
      "Surface training t=17943, loss=0.023476825561374426\n",
      "Surface training t=17944, loss=0.02906135655939579\n",
      "Surface training t=17945, loss=0.026317426934838295\n",
      "Surface training t=17946, loss=0.02850835770368576\n",
      "Surface training t=17947, loss=0.02266205009073019\n",
      "Surface training t=17948, loss=0.028324109502136707\n",
      "Surface training t=17949, loss=0.044641006737947464\n",
      "Surface training t=17950, loss=0.03698128089308739\n",
      "Surface training t=17951, loss=0.03726501949131489\n",
      "Surface training t=17952, loss=0.03560264594852924\n",
      "Surface training t=17953, loss=0.026044785045087337\n",
      "Surface training t=17954, loss=0.026331686414778233\n",
      "Surface training t=17955, loss=0.03677871264517307\n",
      "Surface training t=17956, loss=0.0301291448995471\n",
      "Surface training t=17957, loss=0.030676023103296757\n",
      "Surface training t=17958, loss=0.0506410151720047\n",
      "Surface training t=17959, loss=0.028936798684298992\n",
      "Surface training t=17960, loss=0.04662579298019409\n",
      "Surface training t=17961, loss=0.03393208608031273\n",
      "Surface training t=17962, loss=0.044426921755075455\n",
      "Surface training t=17963, loss=0.029530711472034454\n",
      "Surface training t=17964, loss=0.02775432448834181\n",
      "Surface training t=17965, loss=0.031946687027812004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=17966, loss=0.037903361953794956\n",
      "Surface training t=17967, loss=0.02964609768241644\n",
      "Surface training t=17968, loss=0.029368171468377113\n",
      "Surface training t=17969, loss=0.03515288420021534\n",
      "Surface training t=17970, loss=0.025575612671673298\n",
      "Surface training t=17971, loss=0.03275744989514351\n",
      "Surface training t=17972, loss=0.030722894705832005\n",
      "Surface training t=17973, loss=0.02333466336131096\n",
      "Surface training t=17974, loss=0.03740040957927704\n",
      "Surface training t=17975, loss=0.037162731401622295\n",
      "Surface training t=17976, loss=0.036664512008428574\n",
      "Surface training t=17977, loss=0.030927698127925396\n",
      "Surface training t=17978, loss=0.0277615524828434\n",
      "Surface training t=17979, loss=0.036489889957010746\n",
      "Surface training t=17980, loss=0.05584465153515339\n",
      "Surface training t=17981, loss=0.03223780356347561\n",
      "Surface training t=17982, loss=0.03786332160234451\n",
      "Surface training t=17983, loss=0.06433053314685822\n",
      "Surface training t=17984, loss=0.04068547394126654\n",
      "Surface training t=17985, loss=0.04745004419237375\n",
      "Surface training t=17986, loss=0.07073527202010155\n",
      "Surface training t=17987, loss=0.050762370228767395\n",
      "Surface training t=17988, loss=0.04066086560487747\n",
      "Surface training t=17989, loss=0.05611717887222767\n",
      "Surface training t=17990, loss=0.03784526325762272\n",
      "Surface training t=17991, loss=0.03563736379146576\n",
      "Surface training t=17992, loss=0.024563302285969257\n",
      "Surface training t=17993, loss=0.02338550705462694\n",
      "Surface training t=17994, loss=0.022443898022174835\n",
      "Surface training t=17995, loss=0.02523454651236534\n",
      "Surface training t=17996, loss=0.017277492210268974\n",
      "Surface training t=17997, loss=0.029210451059043407\n",
      "Surface training t=17998, loss=0.037940409034490585\n",
      "Surface training t=17999, loss=0.027523049153387547\n",
      "Surface training t=18000, loss=0.028114658780395985\n",
      "Surface training t=18001, loss=0.026966584846377373\n",
      "Surface training t=18002, loss=0.02430914994329214\n",
      "Surface training t=18003, loss=0.020764497108757496\n",
      "Surface training t=18004, loss=0.02265132963657379\n",
      "Surface training t=18005, loss=0.030728894285857677\n",
      "Surface training t=18006, loss=0.022790223360061646\n",
      "Surface training t=18007, loss=0.017420602031052113\n",
      "Surface training t=18008, loss=0.019421330653131008\n",
      "Surface training t=18009, loss=0.024856338277459145\n",
      "Surface training t=18010, loss=0.02618524618446827\n",
      "Surface training t=18011, loss=0.020855328999459743\n",
      "Surface training t=18012, loss=0.02655002661049366\n",
      "Surface training t=18013, loss=0.0289041455835104\n",
      "Surface training t=18014, loss=0.0501827597618103\n",
      "Surface training t=18015, loss=0.03519007097929716\n",
      "Surface training t=18016, loss=0.0382359866052866\n",
      "Surface training t=18017, loss=0.029769178479909897\n",
      "Surface training t=18018, loss=0.022110670804977417\n",
      "Surface training t=18019, loss=0.024619010742753744\n",
      "Surface training t=18020, loss=0.04268037900328636\n",
      "Surface training t=18021, loss=0.03708009421825409\n",
      "Surface training t=18022, loss=0.024282910861074924\n",
      "Surface training t=18023, loss=0.023949154652655125\n",
      "Surface training t=18024, loss=0.025203119032084942\n",
      "Surface training t=18025, loss=0.026100369170308113\n",
      "Surface training t=18026, loss=0.016279933974146843\n",
      "Surface training t=18027, loss=0.02697630226612091\n",
      "Surface training t=18028, loss=0.02350408025085926\n",
      "Surface training t=18029, loss=0.020522860810160637\n",
      "Surface training t=18030, loss=0.028006592765450478\n",
      "Surface training t=18031, loss=0.021608326584100723\n",
      "Surface training t=18032, loss=0.03051812667399645\n",
      "Surface training t=18033, loss=0.03800046443939209\n",
      "Surface training t=18034, loss=0.031153101474046707\n",
      "Surface training t=18035, loss=0.03953980840742588\n",
      "Surface training t=18036, loss=0.03322505485266447\n",
      "Surface training t=18037, loss=0.04402460716664791\n",
      "Surface training t=18038, loss=0.04266052879393101\n",
      "Surface training t=18039, loss=0.050993598997592926\n",
      "Surface training t=18040, loss=0.04240964353084564\n",
      "Surface training t=18041, loss=0.06308578699827194\n",
      "Surface training t=18042, loss=0.03776656370609999\n",
      "Surface training t=18043, loss=0.05978359840810299\n",
      "Surface training t=18044, loss=0.04124217480421066\n",
      "Surface training t=18045, loss=0.06313358806073666\n",
      "Surface training t=18046, loss=0.0351015068590641\n",
      "Surface training t=18047, loss=0.05254237726330757\n",
      "Surface training t=18048, loss=0.03546858951449394\n",
      "Surface training t=18049, loss=0.032643530517816544\n",
      "Surface training t=18050, loss=0.0362392570823431\n",
      "Surface training t=18051, loss=0.02695672120898962\n",
      "Surface training t=18052, loss=0.03012349270284176\n",
      "Surface training t=18053, loss=0.029030563309788704\n",
      "Surface training t=18054, loss=0.025538154877722263\n",
      "Surface training t=18055, loss=0.02430107071995735\n",
      "Surface training t=18056, loss=0.02742779813706875\n",
      "Surface training t=18057, loss=0.0293081346899271\n",
      "Surface training t=18058, loss=0.03135046362876892\n",
      "Surface training t=18059, loss=0.027042748406529427\n",
      "Surface training t=18060, loss=0.030244121327996254\n",
      "Surface training t=18061, loss=0.021171975880861282\n",
      "Surface training t=18062, loss=0.03516763914376497\n",
      "Surface training t=18063, loss=0.022069995291531086\n",
      "Surface training t=18064, loss=0.019984466023743153\n",
      "Surface training t=18065, loss=0.0309737641364336\n",
      "Surface training t=18066, loss=0.02742875274270773\n",
      "Surface training t=18067, loss=0.03186504915356636\n",
      "Surface training t=18068, loss=0.0322917178273201\n",
      "Surface training t=18069, loss=0.03151393588632345\n",
      "Surface training t=18070, loss=0.029535308480262756\n",
      "Surface training t=18071, loss=0.03437256067991257\n",
      "Surface training t=18072, loss=0.028100788593292236\n",
      "Surface training t=18073, loss=0.038303861394524574\n",
      "Surface training t=18074, loss=0.04121713526546955\n",
      "Surface training t=18075, loss=0.039477039128541946\n",
      "Surface training t=18076, loss=0.034951248206198215\n",
      "Surface training t=18077, loss=0.04275836609303951\n",
      "Surface training t=18078, loss=0.04585766792297363\n",
      "Surface training t=18079, loss=0.03898410499095917\n",
      "Surface training t=18080, loss=0.031953239813447\n",
      "Surface training t=18081, loss=0.02706417627632618\n",
      "Surface training t=18082, loss=0.030592131428420544\n",
      "Surface training t=18083, loss=0.031486536376178265\n",
      "Surface training t=18084, loss=0.03284759446978569\n",
      "Surface training t=18085, loss=0.035426429472863674\n",
      "Surface training t=18086, loss=0.037831418216228485\n",
      "Surface training t=18087, loss=0.03880752809345722\n",
      "Surface training t=18088, loss=0.027921481989324093\n",
      "Surface training t=18089, loss=0.021276511251926422\n",
      "Surface training t=18090, loss=0.03163769096136093\n",
      "Surface training t=18091, loss=0.01869419915601611\n",
      "Surface training t=18092, loss=0.025506709702312946\n",
      "Surface training t=18093, loss=0.029045388102531433\n",
      "Surface training t=18094, loss=0.028430012986063957\n",
      "Surface training t=18095, loss=0.03209620341658592\n",
      "Surface training t=18096, loss=0.027241968549787998\n",
      "Surface training t=18097, loss=0.02568646753206849\n",
      "Surface training t=18098, loss=0.036317262798547745\n",
      "Surface training t=18099, loss=0.02371145784854889\n",
      "Surface training t=18100, loss=0.024983879178762436\n",
      "Surface training t=18101, loss=0.028831559233367443\n",
      "Surface training t=18102, loss=0.022707059979438782\n",
      "Surface training t=18103, loss=0.027168930508196354\n",
      "Surface training t=18104, loss=0.02808204386383295\n",
      "Surface training t=18105, loss=0.0232708603143692\n",
      "Surface training t=18106, loss=0.02397774439305067\n",
      "Surface training t=18107, loss=0.02679489180445671\n",
      "Surface training t=18108, loss=0.023902573622763157\n",
      "Surface training t=18109, loss=0.02449156530201435\n",
      "Surface training t=18110, loss=0.02138742245733738\n",
      "Surface training t=18111, loss=0.022423764690756798\n",
      "Surface training t=18112, loss=0.027371468022465706\n",
      "Surface training t=18113, loss=0.048440221697092056\n",
      "Surface training t=18114, loss=0.04110567271709442\n",
      "Surface training t=18115, loss=0.04076877236366272\n",
      "Surface training t=18116, loss=0.042261785827577114\n",
      "Surface training t=18117, loss=0.05359257012605667\n",
      "Surface training t=18118, loss=0.03701692633330822\n",
      "Surface training t=18119, loss=0.030177478678524494\n",
      "Surface training t=18120, loss=0.03221895918250084\n",
      "Surface training t=18121, loss=0.027434222400188446\n",
      "Surface training t=18122, loss=0.02500599715858698\n",
      "Surface training t=18123, loss=0.02442264650017023\n",
      "Surface training t=18124, loss=0.020164810121059418\n",
      "Surface training t=18125, loss=0.024249390698969364\n",
      "Surface training t=18126, loss=0.02702342066913843\n",
      "Surface training t=18127, loss=0.02379583567380905\n",
      "Surface training t=18128, loss=0.03272007219493389\n",
      "Surface training t=18129, loss=0.027071957476437092\n",
      "Surface training t=18130, loss=0.022160744294524193\n",
      "Surface training t=18131, loss=0.021936064586043358\n",
      "Surface training t=18132, loss=0.023906310088932514\n",
      "Surface training t=18133, loss=0.024449662305414677\n",
      "Surface training t=18134, loss=0.02073570527136326\n",
      "Surface training t=18135, loss=0.03396477736532688\n",
      "Surface training t=18136, loss=0.0314508443698287\n",
      "Surface training t=18137, loss=0.02684288751333952\n",
      "Surface training t=18138, loss=0.023275715298950672\n",
      "Surface training t=18139, loss=0.02456589788198471\n",
      "Surface training t=18140, loss=0.024739598855376244\n",
      "Surface training t=18141, loss=0.028992008417844772\n",
      "Surface training t=18142, loss=0.024001735262572765\n",
      "Surface training t=18143, loss=0.019855891354382038\n",
      "Surface training t=18144, loss=0.024664527736604214\n",
      "Surface training t=18145, loss=0.021919923834502697\n",
      "Surface training t=18146, loss=0.02970554679632187\n",
      "Surface training t=18147, loss=0.020591710694134235\n",
      "Surface training t=18148, loss=0.022646416909992695\n",
      "Surface training t=18149, loss=0.025329903699457645\n",
      "Surface training t=18150, loss=0.024315398186445236\n",
      "Surface training t=18151, loss=0.02482045814394951\n",
      "Surface training t=18152, loss=0.024820080026984215\n",
      "Surface training t=18153, loss=0.019724013283848763\n",
      "Surface training t=18154, loss=0.02918710559606552\n",
      "Surface training t=18155, loss=0.029781104065477848\n",
      "Surface training t=18156, loss=0.022322550415992737\n",
      "Surface training t=18157, loss=0.02373044192790985\n",
      "Surface training t=18158, loss=0.029990151524543762\n",
      "Surface training t=18159, loss=0.024073628708720207\n",
      "Surface training t=18160, loss=0.027485277503728867\n",
      "Surface training t=18161, loss=0.032859452068805695\n",
      "Surface training t=18162, loss=0.018940279260277748\n",
      "Surface training t=18163, loss=0.02348163165152073\n",
      "Surface training t=18164, loss=0.029491914436221123\n",
      "Surface training t=18165, loss=0.02950236015021801\n",
      "Surface training t=18166, loss=0.031514834612607956\n",
      "Surface training t=18167, loss=0.03145487979054451\n",
      "Surface training t=18168, loss=0.034922205843031406\n",
      "Surface training t=18169, loss=0.030261196196079254\n",
      "Surface training t=18170, loss=0.039368148893117905\n",
      "Surface training t=18171, loss=0.05386165715754032\n",
      "Surface training t=18172, loss=0.043772172182798386\n",
      "Surface training t=18173, loss=0.04409675486385822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=18174, loss=0.062160808593034744\n",
      "Surface training t=18175, loss=0.04066605865955353\n",
      "Surface training t=18176, loss=0.04708436504006386\n",
      "Surface training t=18177, loss=0.07333500683307648\n",
      "Surface training t=18178, loss=0.04807300865650177\n",
      "Surface training t=18179, loss=0.06718670204281807\n",
      "Surface training t=18180, loss=0.057420914992690086\n",
      "Surface training t=18181, loss=0.042869674041867256\n",
      "Surface training t=18182, loss=0.05236964114010334\n",
      "Surface training t=18183, loss=0.04377138987183571\n",
      "Surface training t=18184, loss=0.033941312693059444\n",
      "Surface training t=18185, loss=0.03709413390606642\n",
      "Surface training t=18186, loss=0.030761657282710075\n",
      "Surface training t=18187, loss=0.04120021127164364\n",
      "Surface training t=18188, loss=0.03851923253387213\n",
      "Surface training t=18189, loss=0.02996982168406248\n",
      "Surface training t=18190, loss=0.05568835698068142\n",
      "Surface training t=18191, loss=0.0611927080899477\n",
      "Surface training t=18192, loss=0.04119708016514778\n",
      "Surface training t=18193, loss=0.04919114708900452\n",
      "Surface training t=18194, loss=0.03419191762804985\n",
      "Surface training t=18195, loss=0.033337485045194626\n",
      "Surface training t=18196, loss=0.02946324273943901\n",
      "Surface training t=18197, loss=0.022061594761908054\n",
      "Surface training t=18198, loss=0.024163429625332355\n",
      "Surface training t=18199, loss=0.024423004128038883\n",
      "Surface training t=18200, loss=0.020508707500994205\n",
      "Surface training t=18201, loss=0.01722982805222273\n",
      "Surface training t=18202, loss=0.015759858768433332\n",
      "Surface training t=18203, loss=0.017889101058244705\n",
      "Surface training t=18204, loss=0.0243051890283823\n",
      "Surface training t=18205, loss=0.028118858113884926\n",
      "Surface training t=18206, loss=0.025264764204621315\n",
      "Surface training t=18207, loss=0.02664338145405054\n",
      "Surface training t=18208, loss=0.044540053233504295\n",
      "Surface training t=18209, loss=0.030007985420525074\n",
      "Surface training t=18210, loss=0.0449665654450655\n",
      "Surface training t=18211, loss=0.03885788004845381\n",
      "Surface training t=18212, loss=0.05542089603841305\n",
      "Surface training t=18213, loss=0.034995125606656075\n",
      "Surface training t=18214, loss=0.045516710728406906\n",
      "Surface training t=18215, loss=0.04831496439874172\n",
      "Surface training t=18216, loss=0.042774369940161705\n",
      "Surface training t=18217, loss=0.03473977278918028\n",
      "Surface training t=18218, loss=0.033152601681649685\n",
      "Surface training t=18219, loss=0.024315258488059044\n",
      "Surface training t=18220, loss=0.0293319346383214\n",
      "Surface training t=18221, loss=0.02871120534837246\n",
      "Surface training t=18222, loss=0.025994270108640194\n",
      "Surface training t=18223, loss=0.017391699831932783\n",
      "Surface training t=18224, loss=0.02424163930118084\n",
      "Surface training t=18225, loss=0.02339857630431652\n",
      "Surface training t=18226, loss=0.022161215543746948\n",
      "Surface training t=18227, loss=0.02260524034500122\n",
      "Surface training t=18228, loss=0.01873728260397911\n",
      "Surface training t=18229, loss=0.02806886751204729\n",
      "Surface training t=18230, loss=0.018529538996517658\n",
      "Surface training t=18231, loss=0.023399430327117443\n",
      "Surface training t=18232, loss=0.019476591609418392\n",
      "Surface training t=18233, loss=0.024820171296596527\n",
      "Surface training t=18234, loss=0.03274345677345991\n",
      "Surface training t=18235, loss=0.026013514958322048\n",
      "Surface training t=18236, loss=0.02175194025039673\n",
      "Surface training t=18237, loss=0.024721545167267323\n",
      "Surface training t=18238, loss=0.029974515549838543\n",
      "Surface training t=18239, loss=0.025089633651077747\n",
      "Surface training t=18240, loss=0.02036797534674406\n",
      "Surface training t=18241, loss=0.02462714072316885\n",
      "Surface training t=18242, loss=0.02151736244559288\n",
      "Surface training t=18243, loss=0.03453364968299866\n",
      "Surface training t=18244, loss=0.028486053459346294\n",
      "Surface training t=18245, loss=0.026252547279000282\n",
      "Surface training t=18246, loss=0.03675534389913082\n",
      "Surface training t=18247, loss=0.029119259677827358\n",
      "Surface training t=18248, loss=0.056822769343853\n",
      "Surface training t=18249, loss=0.036504896357655525\n",
      "Surface training t=18250, loss=0.04993618279695511\n",
      "Surface training t=18251, loss=0.02651187777519226\n",
      "Surface training t=18252, loss=0.037272870540618896\n",
      "Surface training t=18253, loss=0.023008735850453377\n",
      "Surface training t=18254, loss=0.028286460787057877\n",
      "Surface training t=18255, loss=0.021945565938949585\n",
      "Surface training t=18256, loss=0.024929724633693695\n",
      "Surface training t=18257, loss=0.02638804353773594\n",
      "Surface training t=18258, loss=0.029981344006955624\n",
      "Surface training t=18259, loss=0.02972597163170576\n",
      "Surface training t=18260, loss=0.029245968908071518\n",
      "Surface training t=18261, loss=0.023549162782728672\n",
      "Surface training t=18262, loss=0.02075570821762085\n",
      "Surface training t=18263, loss=0.022442622110247612\n",
      "Surface training t=18264, loss=0.02408040501177311\n",
      "Surface training t=18265, loss=0.02725983876734972\n",
      "Surface training t=18266, loss=0.02679560799151659\n",
      "Surface training t=18267, loss=0.0243526604026556\n",
      "Surface training t=18268, loss=0.0269414521753788\n",
      "Surface training t=18269, loss=0.03206673730164766\n",
      "Surface training t=18270, loss=0.026094593107700348\n",
      "Surface training t=18271, loss=0.020633134059607983\n",
      "Surface training t=18272, loss=0.01641716994345188\n",
      "Surface training t=18273, loss=0.02061273669824004\n",
      "Surface training t=18274, loss=0.02378447912633419\n",
      "Surface training t=18275, loss=0.022665218450129032\n",
      "Surface training t=18276, loss=0.02898708824068308\n",
      "Surface training t=18277, loss=0.030555238015949726\n",
      "Surface training t=18278, loss=0.029431059956550598\n",
      "Surface training t=18279, loss=0.03574497438967228\n",
      "Surface training t=18280, loss=0.04119456186890602\n",
      "Surface training t=18281, loss=0.031845541670918465\n",
      "Surface training t=18282, loss=0.03449724428355694\n",
      "Surface training t=18283, loss=0.02601052261888981\n",
      "Surface training t=18284, loss=0.025927905924618244\n",
      "Surface training t=18285, loss=0.021202232223004103\n",
      "Surface training t=18286, loss=0.03441699966788292\n",
      "Surface training t=18287, loss=0.027939417399466038\n",
      "Surface training t=18288, loss=0.02528529055416584\n",
      "Surface training t=18289, loss=0.02223123051226139\n",
      "Surface training t=18290, loss=0.02585942205041647\n",
      "Surface training t=18291, loss=0.025106502696871758\n",
      "Surface training t=18292, loss=0.021117773838341236\n",
      "Surface training t=18293, loss=0.021846404299139977\n",
      "Surface training t=18294, loss=0.023655250668525696\n",
      "Surface training t=18295, loss=0.019908953458070755\n",
      "Surface training t=18296, loss=0.02266254834830761\n",
      "Surface training t=18297, loss=0.018895622342824936\n",
      "Surface training t=18298, loss=0.019157310016453266\n",
      "Surface training t=18299, loss=0.02418000902980566\n",
      "Surface training t=18300, loss=0.02783010248094797\n",
      "Surface training t=18301, loss=0.027544904500246048\n",
      "Surface training t=18302, loss=0.02545651327818632\n",
      "Surface training t=18303, loss=0.02868082281202078\n",
      "Surface training t=18304, loss=0.028846907429397106\n",
      "Surface training t=18305, loss=0.024726164527237415\n",
      "Surface training t=18306, loss=0.025336842983961105\n",
      "Surface training t=18307, loss=0.020265724509954453\n",
      "Surface training t=18308, loss=0.017343408428132534\n",
      "Surface training t=18309, loss=0.021138882264494896\n",
      "Surface training t=18310, loss=0.022956675849854946\n",
      "Surface training t=18311, loss=0.019530667923390865\n",
      "Surface training t=18312, loss=0.04063816927373409\n",
      "Surface training t=18313, loss=0.03003042284399271\n",
      "Surface training t=18314, loss=0.02992120571434498\n",
      "Surface training t=18315, loss=0.01731855794787407\n",
      "Surface training t=18316, loss=0.023824007250368595\n",
      "Surface training t=18317, loss=0.02050016913563013\n",
      "Surface training t=18318, loss=0.029667217284440994\n",
      "Surface training t=18319, loss=0.020182565785944462\n",
      "Surface training t=18320, loss=0.027305408380925655\n",
      "Surface training t=18321, loss=0.027041321620345116\n",
      "Surface training t=18322, loss=0.028231922537088394\n",
      "Surface training t=18323, loss=0.02951520960777998\n",
      "Surface training t=18324, loss=0.022759780287742615\n",
      "Surface training t=18325, loss=0.028607694432139397\n",
      "Surface training t=18326, loss=0.023136483039706945\n",
      "Surface training t=18327, loss=0.02059780340641737\n",
      "Surface training t=18328, loss=0.02986151911318302\n",
      "Surface training t=18329, loss=0.022891209460794926\n",
      "Surface training t=18330, loss=0.019905694760382175\n",
      "Surface training t=18331, loss=0.017779388464987278\n",
      "Surface training t=18332, loss=0.019928774796426296\n",
      "Surface training t=18333, loss=0.01763929147273302\n",
      "Surface training t=18334, loss=0.01833837479352951\n",
      "Surface training t=18335, loss=0.025946238078176975\n",
      "Surface training t=18336, loss=0.030638396739959717\n",
      "Surface training t=18337, loss=0.037979092448949814\n",
      "Surface training t=18338, loss=0.02772899717092514\n",
      "Surface training t=18339, loss=0.034315651282668114\n",
      "Surface training t=18340, loss=0.0328924935311079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=18341, loss=0.024600500240921974\n",
      "Surface training t=18342, loss=0.025121472775936127\n",
      "Surface training t=18343, loss=0.030995416454970837\n",
      "Surface training t=18344, loss=0.03255679830908775\n",
      "Surface training t=18345, loss=0.03082861937582493\n",
      "Surface training t=18346, loss=0.024720555171370506\n",
      "Surface training t=18347, loss=0.026947983540594578\n",
      "Surface training t=18348, loss=0.020093580707907677\n",
      "Surface training t=18349, loss=0.025640763342380524\n",
      "Surface training t=18350, loss=0.02505845297127962\n",
      "Surface training t=18351, loss=0.028290786780416965\n",
      "Surface training t=18352, loss=0.027735530398786068\n",
      "Surface training t=18353, loss=0.029311434365808964\n",
      "Surface training t=18354, loss=0.028070508502423763\n",
      "Surface training t=18355, loss=0.01956935692578554\n",
      "Surface training t=18356, loss=0.019121830817312002\n",
      "Surface training t=18357, loss=0.023871883749961853\n",
      "Surface training t=18358, loss=0.04226556420326233\n",
      "Surface training t=18359, loss=0.04345606081187725\n",
      "Surface training t=18360, loss=0.025216604582965374\n",
      "Surface training t=18361, loss=0.021093467250466347\n",
      "Surface training t=18362, loss=0.03816403076052666\n",
      "Surface training t=18363, loss=0.04297618940472603\n",
      "Surface training t=18364, loss=0.032757566310465336\n",
      "Surface training t=18365, loss=0.042384300380945206\n",
      "Surface training t=18366, loss=0.03634089231491089\n",
      "Surface training t=18367, loss=0.03809553198516369\n",
      "Surface training t=18368, loss=0.04890288971364498\n",
      "Surface training t=18369, loss=0.03857133351266384\n",
      "Surface training t=18370, loss=0.04815148003399372\n",
      "Surface training t=18371, loss=0.04109025001525879\n",
      "Surface training t=18372, loss=0.048971595242619514\n",
      "Surface training t=18373, loss=0.04486672207713127\n",
      "Surface training t=18374, loss=0.045394591987133026\n",
      "Surface training t=18375, loss=0.033068316988646984\n",
      "Surface training t=18376, loss=0.035339782014489174\n",
      "Surface training t=18377, loss=0.024017926305532455\n",
      "Surface training t=18378, loss=0.030710852704942226\n",
      "Surface training t=18379, loss=0.030751433223485947\n",
      "Surface training t=18380, loss=0.025278053246438503\n",
      "Surface training t=18381, loss=0.028423073701560497\n",
      "Surface training t=18382, loss=0.029204637743532658\n",
      "Surface training t=18383, loss=0.028566574677824974\n",
      "Surface training t=18384, loss=0.03974452055990696\n",
      "Surface training t=18385, loss=0.03992028348147869\n",
      "Surface training t=18386, loss=0.030072699300944805\n",
      "Surface training t=18387, loss=0.032301148399710655\n",
      "Surface training t=18388, loss=0.035325199365615845\n",
      "Surface training t=18389, loss=0.04899907298386097\n",
      "Surface training t=18390, loss=0.037285326048731804\n",
      "Surface training t=18391, loss=0.026776570826768875\n",
      "Surface training t=18392, loss=0.0291263647377491\n",
      "Surface training t=18393, loss=0.031140493229031563\n",
      "Surface training t=18394, loss=0.024764331057667732\n",
      "Surface training t=18395, loss=0.027282300405204296\n",
      "Surface training t=18396, loss=0.02803061716258526\n",
      "Surface training t=18397, loss=0.02436337899416685\n",
      "Surface training t=18398, loss=0.023536776192486286\n",
      "Surface training t=18399, loss=0.020783510990440845\n",
      "Surface training t=18400, loss=0.013768672943115234\n",
      "Surface training t=18401, loss=0.022480282001197338\n",
      "Surface training t=18402, loss=0.02173678856343031\n",
      "Surface training t=18403, loss=0.017990015912801027\n",
      "Surface training t=18404, loss=0.02120573353022337\n",
      "Surface training t=18405, loss=0.022063057869672775\n",
      "Surface training t=18406, loss=0.02437820378690958\n",
      "Surface training t=18407, loss=0.023956427350640297\n",
      "Surface training t=18408, loss=0.02472088113427162\n",
      "Surface training t=18409, loss=0.029708381742239\n",
      "Surface training t=18410, loss=0.02785038761794567\n",
      "Surface training t=18411, loss=0.03313133213669062\n",
      "Surface training t=18412, loss=0.034745652228593826\n",
      "Surface training t=18413, loss=0.026890048757195473\n",
      "Surface training t=18414, loss=0.03057989478111267\n",
      "Surface training t=18415, loss=0.029295346699655056\n",
      "Surface training t=18416, loss=0.04401092976331711\n",
      "Surface training t=18417, loss=0.04756644181907177\n",
      "Surface training t=18418, loss=0.0380328893661499\n",
      "Surface training t=18419, loss=0.05976167321205139\n",
      "Surface training t=18420, loss=0.0505708996206522\n",
      "Surface training t=18421, loss=0.04091780260205269\n",
      "Surface training t=18422, loss=0.028335344046354294\n",
      "Surface training t=18423, loss=0.0389114823192358\n",
      "Surface training t=18424, loss=0.028693833388388157\n",
      "Surface training t=18425, loss=0.04053913801908493\n",
      "Surface training t=18426, loss=0.03487199358642101\n",
      "Surface training t=18427, loss=0.04560473375022411\n",
      "Surface training t=18428, loss=0.03641730919480324\n",
      "Surface training t=18429, loss=0.04698866792023182\n",
      "Surface training t=18430, loss=0.037276034243404865\n",
      "Surface training t=18431, loss=0.058399030938744545\n",
      "Surface training t=18432, loss=0.03655400685966015\n",
      "Surface training t=18433, loss=0.031462715938687325\n",
      "Surface training t=18434, loss=0.02624756284058094\n",
      "Surface training t=18435, loss=0.03651177976280451\n",
      "Surface training t=18436, loss=0.028148232959210873\n",
      "Surface training t=18437, loss=0.025857017375528812\n",
      "Surface training t=18438, loss=0.024639561772346497\n",
      "Surface training t=18439, loss=0.023820560425519943\n",
      "Surface training t=18440, loss=0.03353814594447613\n",
      "Surface training t=18441, loss=0.03979232534766197\n",
      "Surface training t=18442, loss=0.04093479923903942\n",
      "Surface training t=18443, loss=0.03352319821715355\n",
      "Surface training t=18444, loss=0.034058403223752975\n",
      "Surface training t=18445, loss=0.04935048148036003\n",
      "Surface training t=18446, loss=0.037556158378720284\n",
      "Surface training t=18447, loss=0.03809955529868603\n",
      "Surface training t=18448, loss=0.04861313849687576\n",
      "Surface training t=18449, loss=0.05986964330077171\n",
      "Surface training t=18450, loss=0.04273863509297371\n",
      "Surface training t=18451, loss=0.04314294643700123\n",
      "Surface training t=18452, loss=0.08294949866831303\n",
      "Surface training t=18453, loss=0.047547440975904465\n",
      "Surface training t=18454, loss=0.037908753380179405\n",
      "Surface training t=18455, loss=0.037552230060100555\n",
      "Surface training t=18456, loss=0.04738234356045723\n",
      "Surface training t=18457, loss=0.039432477205991745\n",
      "Surface training t=18458, loss=0.052370110526680946\n",
      "Surface training t=18459, loss=0.04278913885354996\n",
      "Surface training t=18460, loss=0.03478042408823967\n",
      "Surface training t=18461, loss=0.03543051518499851\n",
      "Surface training t=18462, loss=0.02617967827245593\n",
      "Surface training t=18463, loss=0.0377980787307024\n",
      "Surface training t=18464, loss=0.029010397382080555\n",
      "Surface training t=18465, loss=0.03736218996345997\n",
      "Surface training t=18466, loss=0.02957933582365513\n",
      "Surface training t=18467, loss=0.027168055064976215\n",
      "Surface training t=18468, loss=0.03159619867801666\n",
      "Surface training t=18469, loss=0.031138278543949127\n",
      "Surface training t=18470, loss=0.031919363886117935\n",
      "Surface training t=18471, loss=0.037851533852517605\n",
      "Surface training t=18472, loss=0.051274511963129044\n",
      "Surface training t=18473, loss=0.0371808297932148\n",
      "Surface training t=18474, loss=0.03745945729315281\n",
      "Surface training t=18475, loss=0.028224140405654907\n",
      "Surface training t=18476, loss=0.030688087455928326\n",
      "Surface training t=18477, loss=0.0435821209102869\n",
      "Surface training t=18478, loss=0.0316021665930748\n",
      "Surface training t=18479, loss=0.019687194377183914\n",
      "Surface training t=18480, loss=0.02748957183212042\n",
      "Surface training t=18481, loss=0.032393477857112885\n",
      "Surface training t=18482, loss=0.034747449681162834\n",
      "Surface training t=18483, loss=0.025946239940822124\n",
      "Surface training t=18484, loss=0.025525451637804508\n",
      "Surface training t=18485, loss=0.027421499602496624\n",
      "Surface training t=18486, loss=0.026810327544808388\n",
      "Surface training t=18487, loss=0.02754884585738182\n",
      "Surface training t=18488, loss=0.02918890491127968\n",
      "Surface training t=18489, loss=0.026532883755862713\n",
      "Surface training t=18490, loss=0.016931507736444473\n",
      "Surface training t=18491, loss=0.025519177317619324\n",
      "Surface training t=18492, loss=0.03244328033179045\n",
      "Surface training t=18493, loss=0.023582860827445984\n",
      "Surface training t=18494, loss=0.03246194776147604\n",
      "Surface training t=18495, loss=0.0314780306071043\n",
      "Surface training t=18496, loss=0.02993699163198471\n",
      "Surface training t=18497, loss=0.03952104412019253\n",
      "Surface training t=18498, loss=0.034083892591297626\n",
      "Surface training t=18499, loss=0.030421167612075806\n",
      "Surface training t=18500, loss=0.02841702103614807\n",
      "Surface training t=18501, loss=0.02685073111206293\n",
      "Surface training t=18502, loss=0.023380734957754612\n",
      "Surface training t=18503, loss=0.022284272126853466\n",
      "Surface training t=18504, loss=0.025873213075101376\n",
      "Surface training t=18505, loss=0.03109641559422016\n",
      "Surface training t=18506, loss=0.023979168385267258\n",
      "Surface training t=18507, loss=0.02173769287765026\n",
      "Surface training t=18508, loss=0.021354339085519314\n",
      "Surface training t=18509, loss=0.02102862298488617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=18510, loss=0.015847364906221628\n",
      "Surface training t=18511, loss=0.017850205302238464\n",
      "Surface training t=18512, loss=0.01714788656681776\n",
      "Surface training t=18513, loss=0.018144166097044945\n",
      "Surface training t=18514, loss=0.019437896087765694\n",
      "Surface training t=18515, loss=0.03365481365472078\n",
      "Surface training t=18516, loss=0.025523042306303978\n",
      "Surface training t=18517, loss=0.02125887479633093\n",
      "Surface training t=18518, loss=0.024969463236629963\n",
      "Surface training t=18519, loss=0.029237879440188408\n",
      "Surface training t=18520, loss=0.036710746586322784\n",
      "Surface training t=18521, loss=0.031062012538313866\n",
      "Surface training t=18522, loss=0.03281337022781372\n",
      "Surface training t=18523, loss=0.028150811791419983\n",
      "Surface training t=18524, loss=0.032516008242964745\n",
      "Surface training t=18525, loss=0.027305521070957184\n",
      "Surface training t=18526, loss=0.026687242090702057\n",
      "Surface training t=18527, loss=0.02526533044874668\n",
      "Surface training t=18528, loss=0.02999833971261978\n",
      "Surface training t=18529, loss=0.03659799974411726\n",
      "Surface training t=18530, loss=0.02995496615767479\n",
      "Surface training t=18531, loss=0.04961103014647961\n",
      "Surface training t=18532, loss=0.03805346693843603\n",
      "Surface training t=18533, loss=0.06355192698538303\n",
      "Surface training t=18534, loss=0.035579707473516464\n",
      "Surface training t=18535, loss=0.03038726281374693\n",
      "Surface training t=18536, loss=0.02895805798470974\n",
      "Surface training t=18537, loss=0.028910853900015354\n",
      "Surface training t=18538, loss=0.03151147998869419\n",
      "Surface training t=18539, loss=0.028743465431034565\n",
      "Surface training t=18540, loss=0.035273099318146706\n",
      "Surface training t=18541, loss=0.03673746809363365\n",
      "Surface training t=18542, loss=0.054621100425720215\n",
      "Surface training t=18543, loss=0.05401116982102394\n",
      "Surface training t=18544, loss=0.042640868574380875\n",
      "Surface training t=18545, loss=0.04152565822005272\n",
      "Surface training t=18546, loss=0.043508950620889664\n",
      "Surface training t=18547, loss=0.029646960087120533\n",
      "Surface training t=18548, loss=0.027537398040294647\n",
      "Surface training t=18549, loss=0.033965473994612694\n",
      "Surface training t=18550, loss=0.025351736694574356\n",
      "Surface training t=18551, loss=0.023482020013034344\n",
      "Surface training t=18552, loss=0.0217939130961895\n",
      "Surface training t=18553, loss=0.04495716653764248\n",
      "Surface training t=18554, loss=0.030701416544616222\n",
      "Surface training t=18555, loss=0.027192319743335247\n",
      "Surface training t=18556, loss=0.023107540793716908\n",
      "Surface training t=18557, loss=0.026374979875981808\n",
      "Surface training t=18558, loss=0.025630848482251167\n",
      "Surface training t=18559, loss=0.02185603603720665\n",
      "Surface training t=18560, loss=0.01973490323871374\n",
      "Surface training t=18561, loss=0.02568892017006874\n",
      "Surface training t=18562, loss=0.02217678166925907\n",
      "Surface training t=18563, loss=0.02010231837630272\n",
      "Surface training t=18564, loss=0.02130728494375944\n",
      "Surface training t=18565, loss=0.025918020866811275\n",
      "Surface training t=18566, loss=0.029267863370478153\n",
      "Surface training t=18567, loss=0.017581273801624775\n",
      "Surface training t=18568, loss=0.0237486744299531\n",
      "Surface training t=18569, loss=0.025077766738831997\n",
      "Surface training t=18570, loss=0.025924263522028923\n",
      "Surface training t=18571, loss=0.027636516839265823\n",
      "Surface training t=18572, loss=0.024400427006185055\n",
      "Surface training t=18573, loss=0.022725502029061317\n",
      "Surface training t=18574, loss=0.025552667677402496\n",
      "Surface training t=18575, loss=0.033635860309004784\n",
      "Surface training t=18576, loss=0.02309450227767229\n",
      "Surface training t=18577, loss=0.02271289099007845\n",
      "Surface training t=18578, loss=0.021747115068137646\n",
      "Surface training t=18579, loss=0.022293899208307266\n",
      "Surface training t=18580, loss=0.02411291655153036\n",
      "Surface training t=18581, loss=0.026871069334447384\n",
      "Surface training t=18582, loss=0.027426132932305336\n",
      "Surface training t=18583, loss=0.02063809521496296\n",
      "Surface training t=18584, loss=0.017419646959751844\n",
      "Surface training t=18585, loss=0.021268350072205067\n",
      "Surface training t=18586, loss=0.025565428659319878\n",
      "Surface training t=18587, loss=0.0204881327226758\n",
      "Surface training t=18588, loss=0.020791797898709774\n",
      "Surface training t=18589, loss=0.02607191726565361\n",
      "Surface training t=18590, loss=0.02347339689731598\n",
      "Surface training t=18591, loss=0.015353268012404442\n",
      "Surface training t=18592, loss=0.01775519922375679\n",
      "Surface training t=18593, loss=0.01914398930966854\n",
      "Surface training t=18594, loss=0.016553625464439392\n",
      "Surface training t=18595, loss=0.024094752967357635\n",
      "Surface training t=18596, loss=0.022169554606080055\n",
      "Surface training t=18597, loss=0.01906159520149231\n",
      "Surface training t=18598, loss=0.029201542027294636\n",
      "Surface training t=18599, loss=0.026032278314232826\n",
      "Surface training t=18600, loss=0.03488859534263611\n",
      "Surface training t=18601, loss=0.029549487866461277\n",
      "Surface training t=18602, loss=0.02616945467889309\n",
      "Surface training t=18603, loss=0.03138568624854088\n",
      "Surface training t=18604, loss=0.034086731262505054\n",
      "Surface training t=18605, loss=0.026634950190782547\n",
      "Surface training t=18606, loss=0.04181336611509323\n",
      "Surface training t=18607, loss=0.026860413141548634\n",
      "Surface training t=18608, loss=0.02896197885274887\n",
      "Surface training t=18609, loss=0.03626945521682501\n",
      "Surface training t=18610, loss=0.02430787868797779\n",
      "Surface training t=18611, loss=0.03013077098876238\n",
      "Surface training t=18612, loss=0.021692092530429363\n",
      "Surface training t=18613, loss=0.029750248417258263\n",
      "Surface training t=18614, loss=0.02929709479212761\n",
      "Surface training t=18615, loss=0.02619398944079876\n",
      "Surface training t=18616, loss=0.026649726554751396\n",
      "Surface training t=18617, loss=0.028850805014371872\n",
      "Surface training t=18618, loss=0.02828103955835104\n",
      "Surface training t=18619, loss=0.028276885859668255\n",
      "Surface training t=18620, loss=0.023547050543129444\n",
      "Surface training t=18621, loss=0.029152074828743935\n",
      "Surface training t=18622, loss=0.028472357429564\n",
      "Surface training t=18623, loss=0.04454333521425724\n",
      "Surface training t=18624, loss=0.03483757749199867\n",
      "Surface training t=18625, loss=0.03837522678077221\n",
      "Surface training t=18626, loss=0.04272212367504835\n",
      "Surface training t=18627, loss=0.0787598043680191\n",
      "Surface training t=18628, loss=0.0443638376891613\n",
      "Surface training t=18629, loss=0.04185775574296713\n",
      "Surface training t=18630, loss=0.06506333500146866\n",
      "Surface training t=18631, loss=0.045039646327495575\n",
      "Surface training t=18632, loss=0.06660114228725433\n",
      "Surface training t=18633, loss=0.04925644397735596\n",
      "Surface training t=18634, loss=0.0439608059823513\n",
      "Surface training t=18635, loss=0.0647890493273735\n",
      "Surface training t=18636, loss=0.03567009698599577\n",
      "Surface training t=18637, loss=0.037252217531204224\n",
      "Surface training t=18638, loss=0.04045472014695406\n",
      "Surface training t=18639, loss=0.06625533476471901\n",
      "Surface training t=18640, loss=0.04151199944317341\n",
      "Surface training t=18641, loss=0.04578777216374874\n",
      "Surface training t=18642, loss=0.04950896464288235\n",
      "Surface training t=18643, loss=0.04193062521517277\n",
      "Surface training t=18644, loss=0.06258884817361832\n",
      "Surface training t=18645, loss=0.05134893022477627\n",
      "Surface training t=18646, loss=0.036865007132291794\n",
      "Surface training t=18647, loss=0.04819844290614128\n",
      "Surface training t=18648, loss=0.03537999652326107\n",
      "Surface training t=18649, loss=0.03675224632024765\n",
      "Surface training t=18650, loss=0.03832534700632095\n",
      "Surface training t=18651, loss=0.026155244559049606\n",
      "Surface training t=18652, loss=0.04487971216440201\n",
      "Surface training t=18653, loss=0.04187550209462643\n",
      "Surface training t=18654, loss=0.047136737033724785\n",
      "Surface training t=18655, loss=0.043899646028876305\n",
      "Surface training t=18656, loss=0.047745002433657646\n",
      "Surface training t=18657, loss=0.034292712807655334\n",
      "Surface training t=18658, loss=0.02715173549950123\n",
      "Surface training t=18659, loss=0.038953643292188644\n",
      "Surface training t=18660, loss=0.02494427841156721\n",
      "Surface training t=18661, loss=0.032467687502503395\n",
      "Surface training t=18662, loss=0.026328331790864468\n",
      "Surface training t=18663, loss=0.030854140408337116\n",
      "Surface training t=18664, loss=0.022205153480172157\n",
      "Surface training t=18665, loss=0.025530772283673286\n",
      "Surface training t=18666, loss=0.028284178115427494\n",
      "Surface training t=18667, loss=0.030320914462208748\n",
      "Surface training t=18668, loss=0.027068505063652992\n",
      "Surface training t=18669, loss=0.022084505297243595\n",
      "Surface training t=18670, loss=0.022850333712995052\n",
      "Surface training t=18671, loss=0.019302785396575928\n",
      "Surface training t=18672, loss=0.02120122406631708\n",
      "Surface training t=18673, loss=0.030235828831791878\n",
      "Surface training t=18674, loss=0.020824491046369076\n",
      "Surface training t=18675, loss=0.019288803450763226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=18676, loss=0.02115009445697069\n",
      "Surface training t=18677, loss=0.018843035213649273\n",
      "Surface training t=18678, loss=0.02752424031496048\n",
      "Surface training t=18679, loss=0.02271966263651848\n",
      "Surface training t=18680, loss=0.020018701441586018\n",
      "Surface training t=18681, loss=0.020950933918356895\n",
      "Surface training t=18682, loss=0.02124528679996729\n",
      "Surface training t=18683, loss=0.027882032096385956\n",
      "Surface training t=18684, loss=0.020635759457945824\n",
      "Surface training t=18685, loss=0.026532446034252644\n",
      "Surface training t=18686, loss=0.01938468124717474\n",
      "Surface training t=18687, loss=0.02834183257073164\n",
      "Surface training t=18688, loss=0.018460676074028015\n",
      "Surface training t=18689, loss=0.02785992342978716\n",
      "Surface training t=18690, loss=0.01999308168888092\n",
      "Surface training t=18691, loss=0.028115435503423214\n",
      "Surface training t=18692, loss=0.018935689702630043\n",
      "Surface training t=18693, loss=0.01685618096962571\n",
      "Surface training t=18694, loss=0.018733719363808632\n",
      "Surface training t=18695, loss=0.018669508397579193\n",
      "Surface training t=18696, loss=0.02277335338294506\n",
      "Surface training t=18697, loss=0.026319502852857113\n",
      "Surface training t=18698, loss=0.032651206478476524\n",
      "Surface training t=18699, loss=0.024798449128866196\n",
      "Surface training t=18700, loss=0.04374583996832371\n",
      "Surface training t=18701, loss=0.0349369328469038\n",
      "Surface training t=18702, loss=0.026337455958127975\n",
      "Surface training t=18703, loss=0.02362640853971243\n",
      "Surface training t=18704, loss=0.015300127677619457\n",
      "Surface training t=18705, loss=0.019371533766388893\n",
      "Surface training t=18706, loss=0.01830014493316412\n",
      "Surface training t=18707, loss=0.01947220414876938\n",
      "Surface training t=18708, loss=0.023420230485498905\n",
      "Surface training t=18709, loss=0.023797782137989998\n",
      "Surface training t=18710, loss=0.024843464139848948\n",
      "Surface training t=18711, loss=0.025271172635257244\n",
      "Surface training t=18712, loss=0.029001510702073574\n",
      "Surface training t=18713, loss=0.025752251967787743\n",
      "Surface training t=18714, loss=0.02509206160902977\n",
      "Surface training t=18715, loss=0.027396051213145256\n",
      "Surface training t=18716, loss=0.03837398253381252\n",
      "Surface training t=18717, loss=0.0277751162648201\n",
      "Surface training t=18718, loss=0.028240512125194073\n",
      "Surface training t=18719, loss=0.04099300503730774\n",
      "Surface training t=18720, loss=0.030685195699334145\n",
      "Surface training t=18721, loss=0.03986651822924614\n",
      "Surface training t=18722, loss=0.03423864208161831\n",
      "Surface training t=18723, loss=0.03356047533452511\n",
      "Surface training t=18724, loss=0.03955840319395065\n",
      "Surface training t=18725, loss=0.033946478739380836\n",
      "Surface training t=18726, loss=0.03949507884681225\n",
      "Surface training t=18727, loss=0.03498780820518732\n",
      "Surface training t=18728, loss=0.029735164251178503\n",
      "Surface training t=18729, loss=0.06765133887529373\n",
      "Surface training t=18730, loss=0.04497847333550453\n",
      "Surface training t=18731, loss=0.06142117455601692\n",
      "Surface training t=18732, loss=0.036660932935774326\n",
      "Surface training t=18733, loss=0.04735780507326126\n",
      "Surface training t=18734, loss=0.030804005451500416\n",
      "Surface training t=18735, loss=0.03842867538332939\n",
      "Surface training t=18736, loss=0.021809504367411137\n",
      "Surface training t=18737, loss=0.024078725837171078\n",
      "Surface training t=18738, loss=0.030078189447522163\n",
      "Surface training t=18739, loss=0.02887069433927536\n",
      "Surface training t=18740, loss=0.03171989321708679\n",
      "Surface training t=18741, loss=0.025746146216988564\n",
      "Surface training t=18742, loss=0.02148358430713415\n",
      "Surface training t=18743, loss=0.03011000994592905\n",
      "Surface training t=18744, loss=0.04600195214152336\n",
      "Surface training t=18745, loss=0.03792464919388294\n",
      "Surface training t=18746, loss=0.03901875205338001\n",
      "Surface training t=18747, loss=0.05808126740157604\n",
      "Surface training t=18748, loss=0.049986908212304115\n",
      "Surface training t=18749, loss=0.038150353357195854\n",
      "Surface training t=18750, loss=0.06960175558924675\n",
      "Surface training t=18751, loss=0.049874911084771156\n",
      "Surface training t=18752, loss=0.044361162930727005\n",
      "Surface training t=18753, loss=0.08221209794282913\n",
      "Surface training t=18754, loss=0.04191631264984608\n",
      "Surface training t=18755, loss=0.05198961682617664\n",
      "Surface training t=18756, loss=0.031561958603560925\n",
      "Surface training t=18757, loss=0.04087106790393591\n",
      "Surface training t=18758, loss=0.03945896588265896\n",
      "Surface training t=18759, loss=0.03148580342531204\n",
      "Surface training t=18760, loss=0.03730293456465006\n",
      "Surface training t=18761, loss=0.03186099324375391\n",
      "Surface training t=18762, loss=0.025312711484730244\n",
      "Surface training t=18763, loss=0.03346697520464659\n",
      "Surface training t=18764, loss=0.03419573325663805\n",
      "Surface training t=18765, loss=0.02814518753439188\n",
      "Surface training t=18766, loss=0.037886811420321465\n",
      "Surface training t=18767, loss=0.033369033597409725\n",
      "Surface training t=18768, loss=0.02485367190092802\n",
      "Surface training t=18769, loss=0.028745213523507118\n",
      "Surface training t=18770, loss=0.03261330258101225\n",
      "Surface training t=18771, loss=0.03074295725673437\n",
      "Surface training t=18772, loss=0.05141402594745159\n",
      "Surface training t=18773, loss=0.03234943933784962\n",
      "Surface training t=18774, loss=0.038301607593894005\n",
      "Surface training t=18775, loss=0.028593608178198338\n",
      "Surface training t=18776, loss=0.02382540423423052\n",
      "Surface training t=18777, loss=0.0313392523676157\n",
      "Surface training t=18778, loss=0.03386366553604603\n",
      "Surface training t=18779, loss=0.03397518489509821\n",
      "Surface training t=18780, loss=0.01978888362646103\n",
      "Surface training t=18781, loss=0.03445352241396904\n",
      "Surface training t=18782, loss=0.028940286487340927\n",
      "Surface training t=18783, loss=0.023430073633790016\n",
      "Surface training t=18784, loss=0.03164389077574015\n",
      "Surface training t=18785, loss=0.023162376135587692\n",
      "Surface training t=18786, loss=0.023447985760867596\n",
      "Surface training t=18787, loss=0.024286204017698765\n",
      "Surface training t=18788, loss=0.027869317680597305\n",
      "Surface training t=18789, loss=0.026135160587728024\n",
      "Surface training t=18790, loss=0.02094560954719782\n",
      "Surface training t=18791, loss=0.022930964827537537\n",
      "Surface training t=18792, loss=0.020749136805534363\n",
      "Surface training t=18793, loss=0.034146866761147976\n",
      "Surface training t=18794, loss=0.024546343833208084\n",
      "Surface training t=18795, loss=0.018783917650580406\n",
      "Surface training t=18796, loss=0.02622848190367222\n",
      "Surface training t=18797, loss=0.0227441368624568\n",
      "Surface training t=18798, loss=0.020590429194271564\n",
      "Surface training t=18799, loss=0.02337688487023115\n",
      "Surface training t=18800, loss=0.02293708547949791\n",
      "Surface training t=18801, loss=0.021785754710435867\n",
      "Surface training t=18802, loss=0.019401359371840954\n",
      "Surface training t=18803, loss=0.02227720059454441\n",
      "Surface training t=18804, loss=0.023386049084365368\n",
      "Surface training t=18805, loss=0.02371781598776579\n",
      "Surface training t=18806, loss=0.02056167833507061\n",
      "Surface training t=18807, loss=0.025775520130991936\n",
      "Surface training t=18808, loss=0.03224503993988037\n",
      "Surface training t=18809, loss=0.035646671429276466\n",
      "Surface training t=18810, loss=0.03166641015559435\n",
      "Surface training t=18811, loss=0.03975506126880646\n",
      "Surface training t=18812, loss=0.044764185324311256\n",
      "Surface training t=18813, loss=0.0399506650865078\n",
      "Surface training t=18814, loss=0.04165147617459297\n",
      "Surface training t=18815, loss=0.02887207828462124\n",
      "Surface training t=18816, loss=0.026319258380681276\n",
      "Surface training t=18817, loss=0.037272462621331215\n",
      "Surface training t=18818, loss=0.022339219227433205\n",
      "Surface training t=18819, loss=0.024952784180641174\n",
      "Surface training t=18820, loss=0.04340929165482521\n",
      "Surface training t=18821, loss=0.03311527706682682\n",
      "Surface training t=18822, loss=0.04832680709660053\n",
      "Surface training t=18823, loss=0.03499872516840696\n",
      "Surface training t=18824, loss=0.027716964948922396\n",
      "Surface training t=18825, loss=0.03023180365562439\n",
      "Surface training t=18826, loss=0.025820380076766014\n",
      "Surface training t=18827, loss=0.02458053082227707\n",
      "Surface training t=18828, loss=0.029101979918777943\n",
      "Surface training t=18829, loss=0.028917621821165085\n",
      "Surface training t=18830, loss=0.029857251793146133\n",
      "Surface training t=18831, loss=0.029256150126457214\n",
      "Surface training t=18832, loss=0.021969717927277088\n",
      "Surface training t=18833, loss=0.030414171516895294\n",
      "Surface training t=18834, loss=0.03563578426837921\n",
      "Surface training t=18835, loss=0.04469364695250988\n",
      "Surface training t=18836, loss=0.0336257740855217\n",
      "Surface training t=18837, loss=0.036103349179029465\n",
      "Surface training t=18838, loss=0.03832100797444582\n",
      "Surface training t=18839, loss=0.0511623527854681\n",
      "Surface training t=18840, loss=0.02903411816805601\n",
      "Surface training t=18841, loss=0.027191045694053173\n",
      "Surface training t=18842, loss=0.022555979900062084\n",
      "Surface training t=18843, loss=0.024393292143940926\n",
      "Surface training t=18844, loss=0.015176340937614441\n",
      "Surface training t=18845, loss=0.02599063701927662\n",
      "Surface training t=18846, loss=0.0344246756285429\n",
      "Surface training t=18847, loss=0.02260720729827881\n",
      "Surface training t=18848, loss=0.0223252447322011\n",
      "Surface training t=18849, loss=0.02134440839290619\n",
      "Surface training t=18850, loss=0.023541906848549843\n",
      "Surface training t=18851, loss=0.020399161614477634\n",
      "Surface training t=18852, loss=0.021923599764704704\n",
      "Surface training t=18853, loss=0.02788413967937231\n",
      "Surface training t=18854, loss=0.02594965137541294\n",
      "Surface training t=18855, loss=0.02521556429564953\n",
      "Surface training t=18856, loss=0.025332010351121426\n",
      "Surface training t=18857, loss=0.02357518393546343\n",
      "Surface training t=18858, loss=0.0237817894667387\n",
      "Surface training t=18859, loss=0.02481988538056612\n",
      "Surface training t=18860, loss=0.022992650978267193\n",
      "Surface training t=18861, loss=0.027764925733208656\n",
      "Surface training t=18862, loss=0.024480539374053478\n",
      "Surface training t=18863, loss=0.019419833086431026\n",
      "Surface training t=18864, loss=0.021134653128683567\n",
      "Surface training t=18865, loss=0.02175941225141287\n",
      "Surface training t=18866, loss=0.027035115286707878\n",
      "Surface training t=18867, loss=0.030989437364041805\n",
      "Surface training t=18868, loss=0.0259628901258111\n",
      "Surface training t=18869, loss=0.043772364035248756\n",
      "Surface training t=18870, loss=0.0292910598218441\n",
      "Surface training t=18871, loss=0.046014873310923576\n",
      "Surface training t=18872, loss=0.024500688537955284\n",
      "Surface training t=18873, loss=0.02897549606859684\n",
      "Surface training t=18874, loss=0.04064350388944149\n",
      "Surface training t=18875, loss=0.023641126230359077\n",
      "Surface training t=18876, loss=0.02839219756424427\n",
      "Surface training t=18877, loss=0.02630164846777916\n",
      "Surface training t=18878, loss=0.02129045221954584\n",
      "Surface training t=18879, loss=0.017505998723208904\n",
      "Surface training t=18880, loss=0.016750623006373644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=18881, loss=0.017429271712899208\n",
      "Surface training t=18882, loss=0.022142193280160427\n",
      "Surface training t=18883, loss=0.032253109849989414\n",
      "Surface training t=18884, loss=0.028555527329444885\n",
      "Surface training t=18885, loss=0.03711826354265213\n",
      "Surface training t=18886, loss=0.027229532599449158\n",
      "Surface training t=18887, loss=0.027973131276667118\n",
      "Surface training t=18888, loss=0.0380180012434721\n",
      "Surface training t=18889, loss=0.028255286626517773\n",
      "Surface training t=18890, loss=0.02844485081732273\n",
      "Surface training t=18891, loss=0.02289173472672701\n",
      "Surface training t=18892, loss=0.022212697193026543\n",
      "Surface training t=18893, loss=0.023199154995381832\n",
      "Surface training t=18894, loss=0.024914350360631943\n",
      "Surface training t=18895, loss=0.022639752365648746\n",
      "Surface training t=18896, loss=0.024117582477629185\n",
      "Surface training t=18897, loss=0.021953603252768517\n",
      "Surface training t=18898, loss=0.02325343620032072\n",
      "Surface training t=18899, loss=0.0200877133756876\n",
      "Surface training t=18900, loss=0.022645659744739532\n",
      "Surface training t=18901, loss=0.02278866060078144\n",
      "Surface training t=18902, loss=0.022661618888378143\n",
      "Surface training t=18903, loss=0.027754975482821465\n",
      "Surface training t=18904, loss=0.025807475671172142\n",
      "Surface training t=18905, loss=0.02495165914297104\n",
      "Surface training t=18906, loss=0.030142350122332573\n",
      "Surface training t=18907, loss=0.029443666338920593\n",
      "Surface training t=18908, loss=0.02575782686471939\n",
      "Surface training t=18909, loss=0.030280835926532745\n",
      "Surface training t=18910, loss=0.02747551165521145\n",
      "Surface training t=18911, loss=0.04446673393249512\n",
      "Surface training t=18912, loss=0.0486226174980402\n",
      "Surface training t=18913, loss=0.0355975367128849\n",
      "Surface training t=18914, loss=0.04024174343794584\n",
      "Surface training t=18915, loss=0.03931163623929024\n",
      "Surface training t=18916, loss=0.05585422366857529\n",
      "Surface training t=18917, loss=0.03529421426355839\n",
      "Surface training t=18918, loss=0.039031242951750755\n",
      "Surface training t=18919, loss=0.041717540472745895\n",
      "Surface training t=18920, loss=0.07481301575899124\n",
      "Surface training t=18921, loss=0.04764499142765999\n",
      "Surface training t=18922, loss=0.056064389646053314\n",
      "Surface training t=18923, loss=0.05562265403568745\n",
      "Surface training t=18924, loss=0.040097981691360474\n",
      "Surface training t=18925, loss=0.047261301428079605\n",
      "Surface training t=18926, loss=0.03741220198571682\n",
      "Surface training t=18927, loss=0.035765497013926506\n",
      "Surface training t=18928, loss=0.03236268553882837\n",
      "Surface training t=18929, loss=0.02770307846367359\n",
      "Surface training t=18930, loss=0.029136299155652523\n",
      "Surface training t=18931, loss=0.029435131698846817\n",
      "Surface training t=18932, loss=0.03789007477462292\n",
      "Surface training t=18933, loss=0.05435854755342007\n",
      "Surface training t=18934, loss=0.04402298480272293\n",
      "Surface training t=18935, loss=0.05732870101928711\n",
      "Surface training t=18936, loss=0.060965776443481445\n",
      "Surface training t=18937, loss=0.045448074117302895\n",
      "Surface training t=18938, loss=0.06609497591853142\n",
      "Surface training t=18939, loss=0.04888562671840191\n",
      "Surface training t=18940, loss=0.045384667813777924\n",
      "Surface training t=18941, loss=0.05420570820569992\n",
      "Surface training t=18942, loss=0.05204097740352154\n",
      "Surface training t=18943, loss=0.03871411457657814\n",
      "Surface training t=18944, loss=0.033522460609674454\n",
      "Surface training t=18945, loss=0.04077109880745411\n",
      "Surface training t=18946, loss=0.037702725268900394\n",
      "Surface training t=18947, loss=0.027097743935883045\n",
      "Surface training t=18948, loss=0.025615090504288673\n",
      "Surface training t=18949, loss=0.034057583659887314\n",
      "Surface training t=18950, loss=0.027170151472091675\n",
      "Surface training t=18951, loss=0.028939847834408283\n",
      "Surface training t=18952, loss=0.021833787206560373\n",
      "Surface training t=18953, loss=0.02039904799312353\n",
      "Surface training t=18954, loss=0.03054851107299328\n",
      "Surface training t=18955, loss=0.027583337388932705\n",
      "Surface training t=18956, loss=0.032002244144678116\n",
      "Surface training t=18957, loss=0.04097872041165829\n",
      "Surface training t=18958, loss=0.03285280056297779\n",
      "Surface training t=18959, loss=0.03436691965907812\n",
      "Surface training t=18960, loss=0.034409102983772755\n",
      "Surface training t=18961, loss=0.030863418243825436\n",
      "Surface training t=18962, loss=0.03558726795017719\n",
      "Surface training t=18963, loss=0.03652810491621494\n",
      "Surface training t=18964, loss=0.046750202775001526\n",
      "Surface training t=18965, loss=0.03029068000614643\n",
      "Surface training t=18966, loss=0.03370555862784386\n",
      "Surface training t=18967, loss=0.03276167716830969\n",
      "Surface training t=18968, loss=0.026946444995701313\n",
      "Surface training t=18969, loss=0.026128536090254784\n",
      "Surface training t=18970, loss=0.038851646706461906\n",
      "Surface training t=18971, loss=0.043342528864741325\n",
      "Surface training t=18972, loss=0.03181721828877926\n",
      "Surface training t=18973, loss=0.0330179650336504\n",
      "Surface training t=18974, loss=0.028910587541759014\n",
      "Surface training t=18975, loss=0.035690028220415115\n",
      "Surface training t=18976, loss=0.025335107930004597\n",
      "Surface training t=18977, loss=0.025532055646181107\n",
      "Surface training t=18978, loss=0.01705573871731758\n",
      "Surface training t=18979, loss=0.017565938644111156\n",
      "Surface training t=18980, loss=0.021235072053968906\n",
      "Surface training t=18981, loss=0.01812368631362915\n",
      "Surface training t=18982, loss=0.01653917506337166\n",
      "Surface training t=18983, loss=0.026706106960773468\n",
      "Surface training t=18984, loss=0.02098600286990404\n",
      "Surface training t=18985, loss=0.016805690713226795\n",
      "Surface training t=18986, loss=0.0215713232755661\n",
      "Surface training t=18987, loss=0.01811239216476679\n",
      "Surface training t=18988, loss=0.02789194416254759\n",
      "Surface training t=18989, loss=0.02292050141841173\n",
      "Surface training t=18990, loss=0.02329422067850828\n",
      "Surface training t=18991, loss=0.0172975012101233\n",
      "Surface training t=18992, loss=0.04594063013792038\n",
      "Surface training t=18993, loss=0.04090814106166363\n",
      "Surface training t=18994, loss=0.03299967758357525\n",
      "Surface training t=18995, loss=0.03850477747619152\n",
      "Surface training t=18996, loss=0.04544902592897415\n",
      "Surface training t=18997, loss=0.02567806839942932\n",
      "Surface training t=18998, loss=0.027447249740362167\n",
      "Surface training t=18999, loss=0.030159685760736465\n",
      "Surface training t=19000, loss=0.024505932815372944\n",
      "Surface training t=19001, loss=0.026733938604593277\n",
      "Surface training t=19002, loss=0.028243886306881905\n",
      "Surface training t=19003, loss=0.029591457918286324\n",
      "Surface training t=19004, loss=0.024710020050406456\n",
      "Surface training t=19005, loss=0.0252009816467762\n",
      "Surface training t=19006, loss=0.027314890176057816\n",
      "Surface training t=19007, loss=0.02054622210562229\n",
      "Surface training t=19008, loss=0.019934196956455708\n",
      "Surface training t=19009, loss=0.020376003347337246\n",
      "Surface training t=19010, loss=0.0238143359310925\n",
      "Surface training t=19011, loss=0.02397319208830595\n",
      "Surface training t=19012, loss=0.022830097004771233\n",
      "Surface training t=19013, loss=0.019352066330611706\n",
      "Surface training t=19014, loss=0.021434461697936058\n",
      "Surface training t=19015, loss=0.02137142326682806\n",
      "Surface training t=19016, loss=0.017395148053765297\n",
      "Surface training t=19017, loss=0.01896187663078308\n",
      "Surface training t=19018, loss=0.02598575409501791\n",
      "Surface training t=19019, loss=0.026220115832984447\n",
      "Surface training t=19020, loss=0.022493084892630577\n",
      "Surface training t=19021, loss=0.029542313888669014\n",
      "Surface training t=19022, loss=0.02148790005594492\n",
      "Surface training t=19023, loss=0.028432182036340237\n",
      "Surface training t=19024, loss=0.029207834042608738\n",
      "Surface training t=19025, loss=0.02852700836956501\n",
      "Surface training t=19026, loss=0.030867887660861015\n",
      "Surface training t=19027, loss=0.053568992763757706\n",
      "Surface training t=19028, loss=0.034973192028701305\n",
      "Surface training t=19029, loss=0.03541976399719715\n",
      "Surface training t=19030, loss=0.04263397678732872\n",
      "Surface training t=19031, loss=0.03761918283998966\n",
      "Surface training t=19032, loss=0.03213353734463453\n",
      "Surface training t=19033, loss=0.02547448966652155\n",
      "Surface training t=19034, loss=0.020208336412906647\n",
      "Surface training t=19035, loss=0.019191103987395763\n",
      "Surface training t=19036, loss=0.02145093772560358\n",
      "Surface training t=19037, loss=0.03977985866367817\n",
      "Surface training t=19038, loss=0.030263572931289673\n",
      "Surface training t=19039, loss=0.02931718435138464\n",
      "Surface training t=19040, loss=0.02963598072528839\n",
      "Surface training t=19041, loss=0.035046108067035675\n",
      "Surface training t=19042, loss=0.03797571919858456\n",
      "Surface training t=19043, loss=0.02751684281975031\n",
      "Surface training t=19044, loss=0.026170900091528893\n",
      "Surface training t=19045, loss=0.028021381236612797\n",
      "Surface training t=19046, loss=0.02968095149844885\n",
      "Surface training t=19047, loss=0.028403368778526783\n",
      "Surface training t=19048, loss=0.033318581990897655\n",
      "Surface training t=19049, loss=0.03302058484405279\n",
      "Surface training t=19050, loss=0.03357721120119095\n",
      "Surface training t=19051, loss=0.042969752103090286\n",
      "Surface training t=19052, loss=0.041171303018927574\n",
      "Surface training t=19053, loss=0.032633643597364426\n",
      "Surface training t=19054, loss=0.027277770452201366\n",
      "Surface training t=19055, loss=0.02983943372964859\n",
      "Surface training t=19056, loss=0.032796407118439674\n",
      "Surface training t=19057, loss=0.025472331792116165\n",
      "Surface training t=19058, loss=0.03508869558572769\n",
      "Surface training t=19059, loss=0.033767344430089\n",
      "Surface training t=19060, loss=0.0242653526365757\n",
      "Surface training t=19061, loss=0.024310613051056862\n",
      "Surface training t=19062, loss=0.020598609931766987\n",
      "Surface training t=19063, loss=0.016629491467028856\n",
      "Surface training t=19064, loss=0.028572346083819866\n",
      "Surface training t=19065, loss=0.019403060898184776\n",
      "Surface training t=19066, loss=0.021537100430577993\n",
      "Surface training t=19067, loss=0.014654605649411678\n",
      "Surface training t=19068, loss=0.021582706831395626\n",
      "Surface training t=19069, loss=0.02388667780905962\n",
      "Surface training t=19070, loss=0.03000995609909296\n",
      "Surface training t=19071, loss=0.033065360970795155\n",
      "Surface training t=19072, loss=0.030495325103402138\n",
      "Surface training t=19073, loss=0.03851735778152943\n",
      "Surface training t=19074, loss=0.03968777507543564\n",
      "Surface training t=19075, loss=0.031158058904111385\n",
      "Surface training t=19076, loss=0.030926182866096497\n",
      "Surface training t=19077, loss=0.031129665672779083\n",
      "Surface training t=19078, loss=0.028784306719899178\n",
      "Surface training t=19079, loss=0.03035545628517866\n",
      "Surface training t=19080, loss=0.024246003478765488\n",
      "Surface training t=19081, loss=0.029911172576248646\n",
      "Surface training t=19082, loss=0.021003120578825474\n",
      "Surface training t=19083, loss=0.02831923682242632\n",
      "Surface training t=19084, loss=0.023224356584250927\n",
      "Surface training t=19085, loss=0.025723563507199287\n",
      "Surface training t=19086, loss=0.0260881083086133\n",
      "Surface training t=19087, loss=0.018076074309647083\n",
      "Surface training t=19088, loss=0.02346659917384386\n",
      "Surface training t=19089, loss=0.029275108128786087\n",
      "Surface training t=19090, loss=0.021462520584464073\n",
      "Surface training t=19091, loss=0.031269386410713196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=19092, loss=0.03615706413984299\n",
      "Surface training t=19093, loss=0.023903701454401016\n",
      "Surface training t=19094, loss=0.020472198724746704\n",
      "Surface training t=19095, loss=0.02122425101697445\n",
      "Surface training t=19096, loss=0.022273295558989048\n",
      "Surface training t=19097, loss=0.017736688256263733\n",
      "Surface training t=19098, loss=0.017408220563083887\n",
      "Surface training t=19099, loss=0.018550795502960682\n",
      "Surface training t=19100, loss=0.015434825327247381\n",
      "Surface training t=19101, loss=0.018655038438737392\n",
      "Surface training t=19102, loss=0.01646647183224559\n",
      "Surface training t=19103, loss=0.020238052122294903\n",
      "Surface training t=19104, loss=0.018946785479784012\n",
      "Surface training t=19105, loss=0.02833507675677538\n",
      "Surface training t=19106, loss=0.024136608466506004\n",
      "Surface training t=19107, loss=0.044293906539678574\n",
      "Surface training t=19108, loss=0.029435252770781517\n",
      "Surface training t=19109, loss=0.03055785410106182\n",
      "Surface training t=19110, loss=0.03223545663058758\n",
      "Surface training t=19111, loss=0.031048309057950974\n",
      "Surface training t=19112, loss=0.02756099496036768\n",
      "Surface training t=19113, loss=0.029111037030816078\n",
      "Surface training t=19114, loss=0.03543538972735405\n",
      "Surface training t=19115, loss=0.03471940942108631\n",
      "Surface training t=19116, loss=0.04563688300549984\n",
      "Surface training t=19117, loss=0.02831685170531273\n",
      "Surface training t=19118, loss=0.03406645264476538\n",
      "Surface training t=19119, loss=0.027395923621952534\n",
      "Surface training t=19120, loss=0.02776081208139658\n",
      "Surface training t=19121, loss=0.02402978576719761\n",
      "Surface training t=19122, loss=0.034776993095874786\n",
      "Surface training t=19123, loss=0.04196866415441036\n",
      "Surface training t=19124, loss=0.03649330697953701\n",
      "Surface training t=19125, loss=0.03184235282242298\n",
      "Surface training t=19126, loss=0.04911557212471962\n",
      "Surface training t=19127, loss=0.04847564361989498\n",
      "Surface training t=19128, loss=0.03976357541978359\n",
      "Surface training t=19129, loss=0.03604034520685673\n",
      "Surface training t=19130, loss=0.044068533927202225\n",
      "Surface training t=19131, loss=0.039944084361195564\n",
      "Surface training t=19132, loss=0.03600706346333027\n",
      "Surface training t=19133, loss=0.04589940793812275\n",
      "Surface training t=19134, loss=0.03749215230345726\n",
      "Surface training t=19135, loss=0.049797214567661285\n",
      "Surface training t=19136, loss=0.03460821695625782\n",
      "Surface training t=19137, loss=0.030932284891605377\n",
      "Surface training t=19138, loss=0.024078717455267906\n",
      "Surface training t=19139, loss=0.023693881928920746\n",
      "Surface training t=19140, loss=0.02513246238231659\n",
      "Surface training t=19141, loss=0.022038709372282028\n",
      "Surface training t=19142, loss=0.0247573284432292\n",
      "Surface training t=19143, loss=0.020874561741948128\n",
      "Surface training t=19144, loss=0.02510414458811283\n",
      "Surface training t=19145, loss=0.020452280528843403\n",
      "Surface training t=19146, loss=0.029370601288974285\n",
      "Surface training t=19147, loss=0.026082689873874187\n",
      "Surface training t=19148, loss=0.020683715119957924\n",
      "Surface training t=19149, loss=0.02185728494077921\n",
      "Surface training t=19150, loss=0.020829422399401665\n",
      "Surface training t=19151, loss=0.026475748978555202\n",
      "Surface training t=19152, loss=0.028519033454358578\n",
      "Surface training t=19153, loss=0.022654651664197445\n",
      "Surface training t=19154, loss=0.022825533524155617\n",
      "Surface training t=19155, loss=0.04463471285998821\n",
      "Surface training t=19156, loss=0.03180287592113018\n",
      "Surface training t=19157, loss=0.037955776788294315\n",
      "Surface training t=19158, loss=0.03857298195362091\n",
      "Surface training t=19159, loss=0.048513902351260185\n",
      "Surface training t=19160, loss=0.037938106805086136\n",
      "Surface training t=19161, loss=0.043704356998205185\n",
      "Surface training t=19162, loss=0.05773773789405823\n",
      "Surface training t=19163, loss=0.030609571374952793\n",
      "Surface training t=19164, loss=0.03406004048883915\n",
      "Surface training t=19165, loss=0.033122096210718155\n",
      "Surface training t=19166, loss=0.03160632401704788\n",
      "Surface training t=19167, loss=0.03308836556971073\n",
      "Surface training t=19168, loss=0.024262364022433758\n",
      "Surface training t=19169, loss=0.026386890560388565\n",
      "Surface training t=19170, loss=0.03752793371677399\n",
      "Surface training t=19171, loss=0.026675083674490452\n",
      "Surface training t=19172, loss=0.026026331819593906\n",
      "Surface training t=19173, loss=0.019411684945225716\n",
      "Surface training t=19174, loss=0.03172595053911209\n",
      "Surface training t=19175, loss=0.03081009816378355\n",
      "Surface training t=19176, loss=0.030044586397707462\n",
      "Surface training t=19177, loss=0.02238901238888502\n",
      "Surface training t=19178, loss=0.02147562988102436\n",
      "Surface training t=19179, loss=0.029254302382469177\n",
      "Surface training t=19180, loss=0.025563081726431847\n",
      "Surface training t=19181, loss=0.02931112702935934\n",
      "Surface training t=19182, loss=0.029101126827299595\n",
      "Surface training t=19183, loss=0.027683992870151997\n",
      "Surface training t=19184, loss=0.023417015559971333\n",
      "Surface training t=19185, loss=0.019166339188814163\n",
      "Surface training t=19186, loss=0.018393070437014103\n",
      "Surface training t=19187, loss=0.02566620334982872\n",
      "Surface training t=19188, loss=0.0290741128847003\n",
      "Surface training t=19189, loss=0.0240536630153656\n",
      "Surface training t=19190, loss=0.027449660934507847\n",
      "Surface training t=19191, loss=0.03352315351366997\n",
      "Surface training t=19192, loss=0.026516113430261612\n",
      "Surface training t=19193, loss=0.029328633099794388\n",
      "Surface training t=19194, loss=0.03056559432297945\n",
      "Surface training t=19195, loss=0.024257829412817955\n",
      "Surface training t=19196, loss=0.0206381157040596\n",
      "Surface training t=19197, loss=0.030937512405216694\n",
      "Surface training t=19198, loss=0.02397564984858036\n",
      "Surface training t=19199, loss=0.027800027281045914\n",
      "Surface training t=19200, loss=0.031235181726515293\n",
      "Surface training t=19201, loss=0.03257554117590189\n",
      "Surface training t=19202, loss=0.04369224235415459\n",
      "Surface training t=19203, loss=0.028446029871702194\n",
      "Surface training t=19204, loss=0.024179335683584213\n",
      "Surface training t=19205, loss=0.030148879624903202\n",
      "Surface training t=19206, loss=0.02660804335027933\n",
      "Surface training t=19207, loss=0.028223682194948196\n",
      "Surface training t=19208, loss=0.025977613404393196\n",
      "Surface training t=19209, loss=0.027489778585731983\n",
      "Surface training t=19210, loss=0.025674641132354736\n",
      "Surface training t=19211, loss=0.026796423364430666\n",
      "Surface training t=19212, loss=0.028354616835713387\n",
      "Surface training t=19213, loss=0.020008115097880363\n",
      "Surface training t=19214, loss=0.01853172294795513\n",
      "Surface training t=19215, loss=0.02290862612426281\n",
      "Surface training t=19216, loss=0.01757931150496006\n",
      "Surface training t=19217, loss=0.017027053982019424\n",
      "Surface training t=19218, loss=0.01784997433423996\n",
      "Surface training t=19219, loss=0.01688218768686056\n",
      "Surface training t=19220, loss=0.018333817832171917\n",
      "Surface training t=19221, loss=0.01767820492386818\n",
      "Surface training t=19222, loss=0.02189108356833458\n",
      "Surface training t=19223, loss=0.02142186276614666\n",
      "Surface training t=19224, loss=0.022831613197922707\n",
      "Surface training t=19225, loss=0.02449890226125717\n",
      "Surface training t=19226, loss=0.026714827865362167\n",
      "Surface training t=19227, loss=0.04083384573459625\n",
      "Surface training t=19228, loss=0.029601246118545532\n",
      "Surface training t=19229, loss=0.03571154549717903\n",
      "Surface training t=19230, loss=0.020862650126218796\n",
      "Surface training t=19231, loss=0.02115015033632517\n",
      "Surface training t=19232, loss=0.026160159148275852\n",
      "Surface training t=19233, loss=0.02020571567118168\n",
      "Surface training t=19234, loss=0.028247028589248657\n",
      "Surface training t=19235, loss=0.01932372711598873\n",
      "Surface training t=19236, loss=0.026316866278648376\n",
      "Surface training t=19237, loss=0.023290676064789295\n",
      "Surface training t=19238, loss=0.028041056357324123\n",
      "Surface training t=19239, loss=0.030392072163522243\n",
      "Surface training t=19240, loss=0.027677814476191998\n",
      "Surface training t=19241, loss=0.02289648074656725\n",
      "Surface training t=19242, loss=0.02484931517392397\n",
      "Surface training t=19243, loss=0.02955257147550583\n",
      "Surface training t=19244, loss=0.0285806804895401\n",
      "Surface training t=19245, loss=0.03050590679049492\n",
      "Surface training t=19246, loss=0.037342462688684464\n",
      "Surface training t=19247, loss=0.026953776367008686\n",
      "Surface training t=19248, loss=0.02214324939996004\n",
      "Surface training t=19249, loss=0.02450560498982668\n",
      "Surface training t=19250, loss=0.03043030109256506\n",
      "Surface training t=19251, loss=0.03340025618672371\n",
      "Surface training t=19252, loss=0.026595214381814003\n",
      "Surface training t=19253, loss=0.0234891539439559\n",
      "Surface training t=19254, loss=0.03526457957923412\n",
      "Surface training t=19255, loss=0.029568816535174847\n",
      "Surface training t=19256, loss=0.031076451763510704\n",
      "Surface training t=19257, loss=0.02662674244493246\n",
      "Surface training t=19258, loss=0.027867138385772705\n",
      "Surface training t=19259, loss=0.020509891211986542\n",
      "Surface training t=19260, loss=0.021179776173084974\n",
      "Surface training t=19261, loss=0.020655019208788872\n",
      "Surface training t=19262, loss=0.02035499829798937\n",
      "Surface training t=19263, loss=0.01623894553631544\n",
      "Surface training t=19264, loss=0.01714291237294674\n",
      "Surface training t=19265, loss=0.02190880011767149\n",
      "Surface training t=19266, loss=0.025836714543402195\n",
      "Surface training t=19267, loss=0.025963726453483105\n",
      "Surface training t=19268, loss=0.020615827292203903\n",
      "Surface training t=19269, loss=0.02698405645787716\n",
      "Surface training t=19270, loss=0.019047516398131847\n",
      "Surface training t=19271, loss=0.020186602603644133\n",
      "Surface training t=19272, loss=0.031631721183657646\n",
      "Surface training t=19273, loss=0.02563111949712038\n",
      "Surface training t=19274, loss=0.023626320995390415\n",
      "Surface training t=19275, loss=0.026431974954903126\n",
      "Surface training t=19276, loss=0.033789169043302536\n",
      "Surface training t=19277, loss=0.027607234194874763\n",
      "Surface training t=19278, loss=0.03461205214262009\n",
      "Surface training t=19279, loss=0.04737929254770279\n",
      "Surface training t=19280, loss=0.028718690387904644\n",
      "Surface training t=19281, loss=0.04045366868376732\n",
      "Surface training t=19282, loss=0.030837549827992916\n",
      "Surface training t=19283, loss=0.03313680551946163\n",
      "Surface training t=19284, loss=0.039962952956557274\n",
      "Surface training t=19285, loss=0.028910471126437187\n",
      "Surface training t=19286, loss=0.032651268877089024\n",
      "Surface training t=19287, loss=0.03325558826327324\n",
      "Surface training t=19288, loss=0.03371916804462671\n",
      "Surface training t=19289, loss=0.05401771888136864\n",
      "Surface training t=19290, loss=0.03617572411894798\n",
      "Surface training t=19291, loss=0.033295552246272564\n",
      "Surface training t=19292, loss=0.02998028416186571\n",
      "Surface training t=19293, loss=0.020198252983391285\n",
      "Surface training t=19294, loss=0.022224433720111847\n",
      "Surface training t=19295, loss=0.022347104735672474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=19296, loss=0.02447976917028427\n",
      "Surface training t=19297, loss=0.029814566485583782\n",
      "Surface training t=19298, loss=0.022829878143966198\n",
      "Surface training t=19299, loss=0.04576869308948517\n",
      "Surface training t=19300, loss=0.03373274486511946\n",
      "Surface training t=19301, loss=0.030826931819319725\n",
      "Surface training t=19302, loss=0.025882665067911148\n",
      "Surface training t=19303, loss=0.027226727455854416\n",
      "Surface training t=19304, loss=0.028325943276286125\n",
      "Surface training t=19305, loss=0.028030688874423504\n",
      "Surface training t=19306, loss=0.02899343241006136\n",
      "Surface training t=19307, loss=0.029543538577854633\n",
      "Surface training t=19308, loss=0.02783307060599327\n",
      "Surface training t=19309, loss=0.02601524256169796\n",
      "Surface training t=19310, loss=0.023929317481815815\n",
      "Surface training t=19311, loss=0.023244080133736134\n",
      "Surface training t=19312, loss=0.02839100919663906\n",
      "Surface training t=19313, loss=0.02098469901829958\n",
      "Surface training t=19314, loss=0.02213306911289692\n",
      "Surface training t=19315, loss=0.01885297428816557\n",
      "Surface training t=19316, loss=0.02316190116107464\n",
      "Surface training t=19317, loss=0.0205710856243968\n",
      "Surface training t=19318, loss=0.02530717384070158\n",
      "Surface training t=19319, loss=0.018518184311687946\n",
      "Surface training t=19320, loss=0.021182998083531857\n",
      "Surface training t=19321, loss=0.024036847054958344\n",
      "Surface training t=19322, loss=0.02577818837016821\n",
      "Surface training t=19323, loss=0.02845878340303898\n",
      "Surface training t=19324, loss=0.03657008521258831\n",
      "Surface training t=19325, loss=0.03661796636879444\n",
      "Surface training t=19326, loss=0.04245118796825409\n",
      "Surface training t=19327, loss=0.04328353889286518\n",
      "Surface training t=19328, loss=0.036425478756427765\n",
      "Surface training t=19329, loss=0.0294626597315073\n",
      "Surface training t=19330, loss=0.02626338880509138\n",
      "Surface training t=19331, loss=0.028149543330073357\n",
      "Surface training t=19332, loss=0.023397342301905155\n",
      "Surface training t=19333, loss=0.022198372520506382\n",
      "Surface training t=19334, loss=0.022817404009401798\n",
      "Surface training t=19335, loss=0.018077381886541843\n",
      "Surface training t=19336, loss=0.016365982592105865\n",
      "Surface training t=19337, loss=0.021529544610530138\n",
      "Surface training t=19338, loss=0.029676794074475765\n",
      "Surface training t=19339, loss=0.028146888129413128\n",
      "Surface training t=19340, loss=0.021589046344161034\n",
      "Surface training t=19341, loss=0.025886324234306812\n",
      "Surface training t=19342, loss=0.02884708810597658\n",
      "Surface training t=19343, loss=0.020028342492878437\n",
      "Surface training t=19344, loss=0.02219773642718792\n",
      "Surface training t=19345, loss=0.02023110631853342\n",
      "Surface training t=19346, loss=0.024101569317281246\n",
      "Surface training t=19347, loss=0.0304489154368639\n",
      "Surface training t=19348, loss=0.038420746102929115\n",
      "Surface training t=19349, loss=0.028760971501469612\n",
      "Surface training t=19350, loss=0.02952750027179718\n",
      "Surface training t=19351, loss=0.03150566574186087\n",
      "Surface training t=19352, loss=0.022263416089117527\n",
      "Surface training t=19353, loss=0.03050842694938183\n",
      "Surface training t=19354, loss=0.022971199825406075\n",
      "Surface training t=19355, loss=0.023024702444672585\n",
      "Surface training t=19356, loss=0.02835483942180872\n",
      "Surface training t=19357, loss=0.033629460260272026\n",
      "Surface training t=19358, loss=0.029037853702902794\n",
      "Surface training t=19359, loss=0.032216114923357964\n",
      "Surface training t=19360, loss=0.02451338618993759\n",
      "Surface training t=19361, loss=0.019606771413236856\n",
      "Surface training t=19362, loss=0.02702682465314865\n",
      "Surface training t=19363, loss=0.025476640090346336\n",
      "Surface training t=19364, loss=0.026128814555704594\n",
      "Surface training t=19365, loss=0.024757666513323784\n",
      "Surface training t=19366, loss=0.01987969595938921\n",
      "Surface training t=19367, loss=0.02877496089786291\n",
      "Surface training t=19368, loss=0.027093540877103806\n",
      "Surface training t=19369, loss=0.025671612471342087\n",
      "Surface training t=19370, loss=0.028261461295187473\n",
      "Surface training t=19371, loss=0.04267270676791668\n",
      "Surface training t=19372, loss=0.025515795685350895\n",
      "Surface training t=19373, loss=0.023439793847501278\n",
      "Surface training t=19374, loss=0.03676370903849602\n",
      "Surface training t=19375, loss=0.0334645789116621\n",
      "Surface training t=19376, loss=0.046198904514312744\n",
      "Surface training t=19377, loss=0.0312954131513834\n",
      "Surface training t=19378, loss=0.026194040663540363\n",
      "Surface training t=19379, loss=0.029205639846622944\n",
      "Surface training t=19380, loss=0.0289579126983881\n",
      "Surface training t=19381, loss=0.0251217819750309\n",
      "Surface training t=19382, loss=0.03371494077146053\n",
      "Surface training t=19383, loss=0.027349389158189297\n",
      "Surface training t=19384, loss=0.03069189004600048\n",
      "Surface training t=19385, loss=0.03391957562416792\n",
      "Surface training t=19386, loss=0.04859808459877968\n",
      "Surface training t=19387, loss=0.03388134576380253\n",
      "Surface training t=19388, loss=0.03438736870884895\n",
      "Surface training t=19389, loss=0.03253296576440334\n",
      "Surface training t=19390, loss=0.0447763167321682\n",
      "Surface training t=19391, loss=0.03985609859228134\n",
      "Surface training t=19392, loss=0.03173111937940121\n",
      "Surface training t=19393, loss=0.04596828669309616\n",
      "Surface training t=19394, loss=0.03623471409082413\n",
      "Surface training t=19395, loss=0.04021142236888409\n",
      "Surface training t=19396, loss=0.041860468685626984\n",
      "Surface training t=19397, loss=0.0442796815186739\n",
      "Surface training t=19398, loss=0.037842157296836376\n",
      "Surface training t=19399, loss=0.051211657002568245\n",
      "Surface training t=19400, loss=0.04016480594873428\n",
      "Surface training t=19401, loss=0.04028371535241604\n",
      "Surface training t=19402, loss=0.03332239203155041\n",
      "Surface training t=19403, loss=0.034619370475411415\n",
      "Surface training t=19404, loss=0.03406895138323307\n",
      "Surface training t=19405, loss=0.02888004668056965\n",
      "Surface training t=19406, loss=0.033275581896305084\n",
      "Surface training t=19407, loss=0.029052920639514923\n",
      "Surface training t=19408, loss=0.02530544623732567\n",
      "Surface training t=19409, loss=0.02732988353818655\n",
      "Surface training t=19410, loss=0.02382023259997368\n",
      "Surface training t=19411, loss=0.026460700668394566\n",
      "Surface training t=19412, loss=0.022820642217993736\n",
      "Surface training t=19413, loss=0.031747532077133656\n",
      "Surface training t=19414, loss=0.019290735013782978\n",
      "Surface training t=19415, loss=0.027305505238473415\n",
      "Surface training t=19416, loss=0.03248198144137859\n",
      "Surface training t=19417, loss=0.03345273435115814\n",
      "Surface training t=19418, loss=0.02773681003600359\n",
      "Surface training t=19419, loss=0.021839470602571964\n",
      "Surface training t=19420, loss=0.016915570944547653\n",
      "Surface training t=19421, loss=0.020700477063655853\n",
      "Surface training t=19422, loss=0.023242519237101078\n",
      "Surface training t=19423, loss=0.022175646387040615\n",
      "Surface training t=19424, loss=0.02331089973449707\n",
      "Surface training t=19425, loss=0.024734560400247574\n",
      "Surface training t=19426, loss=0.018624682910740376\n",
      "Surface training t=19427, loss=0.02284873090684414\n",
      "Surface training t=19428, loss=0.023898297920823097\n",
      "Surface training t=19429, loss=0.03459697961807251\n",
      "Surface training t=19430, loss=0.025426615960896015\n",
      "Surface training t=19431, loss=0.025044475682079792\n",
      "Surface training t=19432, loss=0.03547689691185951\n",
      "Surface training t=19433, loss=0.032982755452394485\n",
      "Surface training t=19434, loss=0.025450490415096283\n",
      "Surface training t=19435, loss=0.029838792979717255\n",
      "Surface training t=19436, loss=0.0337908286601305\n",
      "Surface training t=19437, loss=0.05501899681985378\n",
      "Surface training t=19438, loss=0.043433827348053455\n",
      "Surface training t=19439, loss=0.03514071926474571\n",
      "Surface training t=19440, loss=0.025876703672111034\n",
      "Surface training t=19441, loss=0.02167054172605276\n",
      "Surface training t=19442, loss=0.026432267390191555\n",
      "Surface training t=19443, loss=0.022547940723598003\n",
      "Surface training t=19444, loss=0.019999329932034016\n",
      "Surface training t=19445, loss=0.021983565762639046\n",
      "Surface training t=19446, loss=0.024414289742708206\n",
      "Surface training t=19447, loss=0.02298723254352808\n",
      "Surface training t=19448, loss=0.028274724259972572\n",
      "Surface training t=19449, loss=0.025289690122008324\n",
      "Surface training t=19450, loss=0.021683918312191963\n",
      "Surface training t=19451, loss=0.025138703174889088\n",
      "Surface training t=19452, loss=0.01796992216259241\n",
      "Surface training t=19453, loss=0.022608019411563873\n",
      "Surface training t=19454, loss=0.021338392049074173\n",
      "Surface training t=19455, loss=0.020556552335619926\n",
      "Surface training t=19456, loss=0.017112883739173412\n",
      "Surface training t=19457, loss=0.026120107620954514\n",
      "Surface training t=19458, loss=0.05430148355662823\n",
      "Surface training t=19459, loss=0.041624920442700386\n",
      "Surface training t=19460, loss=0.043177319690585136\n",
      "Surface training t=19461, loss=0.05180440470576286\n",
      "Surface training t=19462, loss=0.043242571875452995\n",
      "Surface training t=19463, loss=0.03984127193689346\n",
      "Surface training t=19464, loss=0.0556504912674427\n",
      "Surface training t=19465, loss=0.034627677872776985\n",
      "Surface training t=19466, loss=0.03464563749730587\n",
      "Surface training t=19467, loss=0.0383713785558939\n",
      "Surface training t=19468, loss=0.029910223558545113\n",
      "Surface training t=19469, loss=0.02062078472226858\n",
      "Surface training t=19470, loss=0.021939309779554605\n",
      "Surface training t=19471, loss=0.019615568220615387\n",
      "Surface training t=19472, loss=0.02159689925611019\n",
      "Surface training t=19473, loss=0.02369664516299963\n",
      "Surface training t=19474, loss=0.035506490617990494\n",
      "Surface training t=19475, loss=0.03860236518085003\n",
      "Surface training t=19476, loss=0.03430892713367939\n",
      "Surface training t=19477, loss=0.025002523325383663\n",
      "Surface training t=19478, loss=0.030371760949492455\n",
      "Surface training t=19479, loss=0.015447060577571392\n",
      "Surface training t=19480, loss=0.025279270485043526\n",
      "Surface training t=19481, loss=0.02307813335210085\n",
      "Surface training t=19482, loss=0.030262066051363945\n",
      "Surface training t=19483, loss=0.021297624334692955\n",
      "Surface training t=19484, loss=0.027406301349401474\n",
      "Surface training t=19485, loss=0.024243115447461605\n",
      "Surface training t=19486, loss=0.02309860661625862\n",
      "Surface training t=19487, loss=0.0192884411662817\n",
      "Surface training t=19488, loss=0.02297057770192623\n",
      "Surface training t=19489, loss=0.018986367620527744\n",
      "Surface training t=19490, loss=0.03017263486981392\n",
      "Surface training t=19491, loss=0.026713968254625797\n",
      "Surface training t=19492, loss=0.026383090764284134\n",
      "Surface training t=19493, loss=0.02639754954725504\n",
      "Surface training t=19494, loss=0.025862018577754498\n",
      "Surface training t=19495, loss=0.024143866263329983\n",
      "Surface training t=19496, loss=0.026979731395840645\n",
      "Surface training t=19497, loss=0.019693786278367043\n",
      "Surface training t=19498, loss=0.02710024919360876\n",
      "Surface training t=19499, loss=0.026461567729711533\n",
      "Surface training t=19500, loss=0.023237657733261585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=19501, loss=0.02547570690512657\n",
      "Surface training t=19502, loss=0.03973386250436306\n",
      "Surface training t=19503, loss=0.038252104073762894\n",
      "Surface training t=19504, loss=0.026176366955041885\n",
      "Surface training t=19505, loss=0.0277692973613739\n",
      "Surface training t=19506, loss=0.026869427412748337\n",
      "Surface training t=19507, loss=0.025520281866192818\n",
      "Surface training t=19508, loss=0.031609621830284595\n",
      "Surface training t=19509, loss=0.039483508095145226\n",
      "Surface training t=19510, loss=0.02842501923441887\n",
      "Surface training t=19511, loss=0.027374531142413616\n",
      "Surface training t=19512, loss=0.04866071976721287\n",
      "Surface training t=19513, loss=0.03815426863729954\n",
      "Surface training t=19514, loss=0.029949449002742767\n",
      "Surface training t=19515, loss=0.01974374521523714\n",
      "Surface training t=19516, loss=0.019890709780156612\n",
      "Surface training t=19517, loss=0.022236154414713383\n",
      "Surface training t=19518, loss=0.028264986351132393\n",
      "Surface training t=19519, loss=0.028531979769468307\n",
      "Surface training t=19520, loss=0.026388377882540226\n",
      "Surface training t=19521, loss=0.024993100203573704\n",
      "Surface training t=19522, loss=0.026495336554944515\n",
      "Surface training t=19523, loss=0.020444068126380444\n",
      "Surface training t=19524, loss=0.025158556178212166\n",
      "Surface training t=19525, loss=0.02256264165043831\n",
      "Surface training t=19526, loss=0.023679266683757305\n",
      "Surface training t=19527, loss=0.024801489897072315\n",
      "Surface training t=19528, loss=0.027632719837129116\n",
      "Surface training t=19529, loss=0.032899050042033195\n",
      "Surface training t=19530, loss=0.026995113119482994\n",
      "Surface training t=19531, loss=0.02980896458029747\n",
      "Surface training t=19532, loss=0.04118333198130131\n",
      "Surface training t=19533, loss=0.03665469214320183\n",
      "Surface training t=19534, loss=0.03488632570952177\n",
      "Surface training t=19535, loss=0.035017864778637886\n",
      "Surface training t=19536, loss=0.044708194211125374\n",
      "Surface training t=19537, loss=0.029411708936095238\n",
      "Surface training t=19538, loss=0.025516540743410587\n",
      "Surface training t=19539, loss=0.025521536357700825\n",
      "Surface training t=19540, loss=0.023894071578979492\n",
      "Surface training t=19541, loss=0.02260194532573223\n",
      "Surface training t=19542, loss=0.02383573353290558\n",
      "Surface training t=19543, loss=0.01724785892292857\n",
      "Surface training t=19544, loss=0.01839348115026951\n",
      "Surface training t=19545, loss=0.020735333673655987\n",
      "Surface training t=19546, loss=0.018429258838295937\n",
      "Surface training t=19547, loss=0.021532380022108555\n",
      "Surface training t=19548, loss=0.02431392576545477\n",
      "Surface training t=19549, loss=0.021190320141613483\n",
      "Surface training t=19550, loss=0.03057187795639038\n",
      "Surface training t=19551, loss=0.02886067144572735\n",
      "Surface training t=19552, loss=0.04239766299724579\n",
      "Surface training t=19553, loss=0.043503524735569954\n",
      "Surface training t=19554, loss=0.03638637810945511\n",
      "Surface training t=19555, loss=0.038552237674593925\n",
      "Surface training t=19556, loss=0.037705209106206894\n",
      "Surface training t=19557, loss=0.03596346452832222\n",
      "Surface training t=19558, loss=0.03551306203007698\n",
      "Surface training t=19559, loss=0.03741758596152067\n",
      "Surface training t=19560, loss=0.05913885682821274\n",
      "Surface training t=19561, loss=0.03717963956296444\n",
      "Surface training t=19562, loss=0.034054556861519814\n",
      "Surface training t=19563, loss=0.052296265959739685\n",
      "Surface training t=19564, loss=0.03333461098372936\n",
      "Surface training t=19565, loss=0.03592217434197664\n",
      "Surface training t=19566, loss=0.03484339267015457\n",
      "Surface training t=19567, loss=0.027928801253437996\n",
      "Surface training t=19568, loss=0.024153235368430614\n",
      "Surface training t=19569, loss=0.022618619725108147\n",
      "Surface training t=19570, loss=0.025050165131688118\n",
      "Surface training t=19571, loss=0.022024688310921192\n",
      "Surface training t=19572, loss=0.026687820442020893\n",
      "Surface training t=19573, loss=0.03210266958922148\n",
      "Surface training t=19574, loss=0.02979058399796486\n",
      "Surface training t=19575, loss=0.04091889411211014\n",
      "Surface training t=19576, loss=0.028226489201188087\n",
      "Surface training t=19577, loss=0.03201435413211584\n",
      "Surface training t=19578, loss=0.025726772844791412\n",
      "Surface training t=19579, loss=0.028903039172291756\n",
      "Surface training t=19580, loss=0.028404378332197666\n",
      "Surface training t=19581, loss=0.0268010376021266\n",
      "Surface training t=19582, loss=0.020877699367702007\n",
      "Surface training t=19583, loss=0.02798857819288969\n",
      "Surface training t=19584, loss=0.025239943526685238\n",
      "Surface training t=19585, loss=0.022002043202519417\n",
      "Surface training t=19586, loss=0.02285071089863777\n",
      "Surface training t=19587, loss=0.02524685114622116\n",
      "Surface training t=19588, loss=0.02133702114224434\n",
      "Surface training t=19589, loss=0.02632372733205557\n",
      "Surface training t=19590, loss=0.03036053478717804\n",
      "Surface training t=19591, loss=0.025878879241645336\n",
      "Surface training t=19592, loss=0.033571356907486916\n",
      "Surface training t=19593, loss=0.037881530821323395\n",
      "Surface training t=19594, loss=0.025909249670803547\n",
      "Surface training t=19595, loss=0.03266176115721464\n",
      "Surface training t=19596, loss=0.0485939085483551\n",
      "Surface training t=19597, loss=0.04569156467914581\n",
      "Surface training t=19598, loss=0.043865425512194633\n",
      "Surface training t=19599, loss=0.04680367931723595\n",
      "Surface training t=19600, loss=0.03898940980434418\n",
      "Surface training t=19601, loss=0.028811215423047543\n",
      "Surface training t=19602, loss=0.03371504135429859\n",
      "Surface training t=19603, loss=0.04378622584044933\n",
      "Surface training t=19604, loss=0.053857600316405296\n",
      "Surface training t=19605, loss=0.03366341441869736\n",
      "Surface training t=19606, loss=0.04692118056118488\n",
      "Surface training t=19607, loss=0.04566270671784878\n",
      "Surface training t=19608, loss=0.04587176255881786\n",
      "Surface training t=19609, loss=0.038807155564427376\n",
      "Surface training t=19610, loss=0.031542375683784485\n",
      "Surface training t=19611, loss=0.06554282642900944\n",
      "Surface training t=19612, loss=0.041639034636318684\n",
      "Surface training t=19613, loss=0.04523713234812021\n",
      "Surface training t=19614, loss=0.06879374757409096\n",
      "Surface training t=19615, loss=0.04584375023841858\n",
      "Surface training t=19616, loss=0.06348884291946888\n",
      "Surface training t=19617, loss=0.05423019826412201\n",
      "Surface training t=19618, loss=0.04569404013454914\n",
      "Surface training t=19619, loss=0.08542434871196747\n",
      "Surface training t=19620, loss=0.04896957986056805\n",
      "Surface training t=19621, loss=0.06946088746190071\n",
      "Surface training t=19622, loss=0.053547944873571396\n",
      "Surface training t=19623, loss=0.04226197022944689\n",
      "Surface training t=19624, loss=0.030489327386021614\n",
      "Surface training t=19625, loss=0.029040959663689137\n",
      "Surface training t=19626, loss=0.023618209175765514\n",
      "Surface training t=19627, loss=0.02168319933116436\n",
      "Surface training t=19628, loss=0.018970392644405365\n",
      "Surface training t=19629, loss=0.017494848929345608\n",
      "Surface training t=19630, loss=0.02200852893292904\n",
      "Surface training t=19631, loss=0.02517597284168005\n",
      "Surface training t=19632, loss=0.021976413205266\n",
      "Surface training t=19633, loss=0.018494999036192894\n",
      "Surface training t=19634, loss=0.017255883663892746\n",
      "Surface training t=19635, loss=0.01791500113904476\n",
      "Surface training t=19636, loss=0.01831412361934781\n",
      "Surface training t=19637, loss=0.019870320335030556\n",
      "Surface training t=19638, loss=0.02404002845287323\n",
      "Surface training t=19639, loss=0.01790825743228197\n",
      "Surface training t=19640, loss=0.017331678420305252\n",
      "Surface training t=19641, loss=0.021096698939800262\n",
      "Surface training t=19642, loss=0.020176433958113194\n",
      "Surface training t=19643, loss=0.021525898948311806\n",
      "Surface training t=19644, loss=0.022268161177635193\n",
      "Surface training t=19645, loss=0.028686881065368652\n",
      "Surface training t=19646, loss=0.021233491599559784\n",
      "Surface training t=19647, loss=0.02457241527736187\n",
      "Surface training t=19648, loss=0.02275815512984991\n",
      "Surface training t=19649, loss=0.027205375023186207\n",
      "Surface training t=19650, loss=0.024565573781728745\n",
      "Surface training t=19651, loss=0.027104566805064678\n",
      "Surface training t=19652, loss=0.026237049140036106\n",
      "Surface training t=19653, loss=0.030543417669832706\n",
      "Surface training t=19654, loss=0.020851739682257175\n",
      "Surface training t=19655, loss=0.019159969873726368\n",
      "Surface training t=19656, loss=0.02601812407374382\n",
      "Surface training t=19657, loss=0.030852576717734337\n",
      "Surface training t=19658, loss=0.03211180865764618\n",
      "Surface training t=19659, loss=0.029768583364784718\n",
      "Surface training t=19660, loss=0.03702030889689922\n",
      "Surface training t=19661, loss=0.031364805996418\n",
      "Surface training t=19662, loss=0.02833712939172983\n",
      "Surface training t=19663, loss=0.03918788395822048\n",
      "Surface training t=19664, loss=0.023858854547142982\n",
      "Surface training t=19665, loss=0.023354959674179554\n",
      "Surface training t=19666, loss=0.03806021064519882\n",
      "Surface training t=19667, loss=0.02096517849713564\n",
      "Surface training t=19668, loss=0.025253308936953545\n",
      "Surface training t=19669, loss=0.025212152861058712\n",
      "Surface training t=19670, loss=0.03407724015414715\n",
      "Surface training t=19671, loss=0.02796074002981186\n",
      "Surface training t=19672, loss=0.0344418128952384\n",
      "Surface training t=19673, loss=0.032227372750639915\n",
      "Surface training t=19674, loss=0.04016759991645813\n",
      "Surface training t=19675, loss=0.034564822912216187\n",
      "Surface training t=19676, loss=0.029072237201035023\n",
      "Surface training t=19677, loss=0.02579261176288128\n",
      "Surface training t=19678, loss=0.026324551552534103\n",
      "Surface training t=19679, loss=0.025725715793669224\n",
      "Surface training t=19680, loss=0.02215744834393263\n",
      "Surface training t=19681, loss=0.023262828588485718\n",
      "Surface training t=19682, loss=0.0290099261328578\n",
      "Surface training t=19683, loss=0.04219382256269455\n",
      "Surface training t=19684, loss=0.03449461981654167\n",
      "Surface training t=19685, loss=0.029957205057144165\n",
      "Surface training t=19686, loss=0.03204443212598562\n",
      "Surface training t=19687, loss=0.027501462027430534\n",
      "Surface training t=19688, loss=0.026149646379053593\n",
      "Surface training t=19689, loss=0.018088663928210735\n",
      "Surface training t=19690, loss=0.019529838114976883\n",
      "Surface training t=19691, loss=0.024151286110281944\n",
      "Surface training t=19692, loss=0.02129034884274006\n",
      "Surface training t=19693, loss=0.023205921053886414\n",
      "Surface training t=19694, loss=0.021647012792527676\n",
      "Surface training t=19695, loss=0.020861783996224403\n",
      "Surface training t=19696, loss=0.025825245305895805\n",
      "Surface training t=19697, loss=0.02674736361950636\n",
      "Surface training t=19698, loss=0.01774397911503911\n",
      "Surface training t=19699, loss=0.02431863360106945\n",
      "Surface training t=19700, loss=0.030031434260308743\n",
      "Surface training t=19701, loss=0.023238827474415302\n",
      "Surface training t=19702, loss=0.02863071858882904\n",
      "Surface training t=19703, loss=0.029459835030138493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=19704, loss=0.026273265480995178\n",
      "Surface training t=19705, loss=0.025782075710594654\n",
      "Surface training t=19706, loss=0.04873053357005119\n",
      "Surface training t=19707, loss=0.03233227878808975\n",
      "Surface training t=19708, loss=0.04664721339941025\n",
      "Surface training t=19709, loss=0.03716982156038284\n",
      "Surface training t=19710, loss=0.030978089198470116\n",
      "Surface training t=19711, loss=0.03650231659412384\n",
      "Surface training t=19712, loss=0.037811778485774994\n",
      "Surface training t=19713, loss=0.026196717284619808\n",
      "Surface training t=19714, loss=0.03628969192504883\n",
      "Surface training t=19715, loss=0.0249154232442379\n",
      "Surface training t=19716, loss=0.026327433064579964\n",
      "Surface training t=19717, loss=0.028633534908294678\n",
      "Surface training t=19718, loss=0.020853273570537567\n",
      "Surface training t=19719, loss=0.031143469735980034\n",
      "Surface training t=19720, loss=0.02386366855353117\n",
      "Surface training t=19721, loss=0.022000232711434364\n",
      "Surface training t=19722, loss=0.023435145616531372\n",
      "Surface training t=19723, loss=0.024534614756703377\n",
      "Surface training t=19724, loss=0.02617180533707142\n",
      "Surface training t=19725, loss=0.025195871479809284\n",
      "Surface training t=19726, loss=0.021479297429323196\n",
      "Surface training t=19727, loss=0.019250732846558094\n",
      "Surface training t=19728, loss=0.02165449783205986\n",
      "Surface training t=19729, loss=0.02125107403844595\n",
      "Surface training t=19730, loss=0.026020357385277748\n",
      "Surface training t=19731, loss=0.023079835809767246\n",
      "Surface training t=19732, loss=0.027998545207083225\n",
      "Surface training t=19733, loss=0.023626376874744892\n",
      "Surface training t=19734, loss=0.028254774399101734\n",
      "Surface training t=19735, loss=0.025285979732871056\n",
      "Surface training t=19736, loss=0.028096113353967667\n",
      "Surface training t=19737, loss=0.02712654136121273\n",
      "Surface training t=19738, loss=0.026221266016364098\n",
      "Surface training t=19739, loss=0.024728257209062576\n",
      "Surface training t=19740, loss=0.025031135417521\n",
      "Surface training t=19741, loss=0.0595815796405077\n",
      "Surface training t=19742, loss=0.03604660090059042\n",
      "Surface training t=19743, loss=0.041529832407832146\n",
      "Surface training t=19744, loss=0.0383134251460433\n",
      "Surface training t=19745, loss=0.03931148909032345\n",
      "Surface training t=19746, loss=0.02446055691689253\n",
      "Surface training t=19747, loss=0.024211146868765354\n",
      "Surface training t=19748, loss=0.02645346149802208\n",
      "Surface training t=19749, loss=0.0293277595192194\n",
      "Surface training t=19750, loss=0.02491725981235504\n",
      "Surface training t=19751, loss=0.029299181886017323\n",
      "Surface training t=19752, loss=0.018806911073625088\n",
      "Surface training t=19753, loss=0.017505604308098555\n",
      "Surface training t=19754, loss=0.017430076375603676\n",
      "Surface training t=19755, loss=0.015008797869086266\n",
      "Surface training t=19756, loss=0.02608901634812355\n",
      "Surface training t=19757, loss=0.03580537438392639\n",
      "Surface training t=19758, loss=0.030205126851797104\n",
      "Surface training t=19759, loss=0.03464826941490173\n",
      "Surface training t=19760, loss=0.025060313753783703\n",
      "Surface training t=19761, loss=0.02982878591865301\n",
      "Surface training t=19762, loss=0.035444462671875954\n",
      "Surface training t=19763, loss=0.029030509293079376\n",
      "Surface training t=19764, loss=0.04078814573585987\n",
      "Surface training t=19765, loss=0.038604941219091415\n",
      "Surface training t=19766, loss=0.034080617129802704\n",
      "Surface training t=19767, loss=0.029555155895650387\n",
      "Surface training t=19768, loss=0.020983416587114334\n",
      "Surface training t=19769, loss=0.018283925019204617\n",
      "Surface training t=19770, loss=0.01968110166490078\n",
      "Surface training t=19771, loss=0.01683575939387083\n",
      "Surface training t=19772, loss=0.020872389897704124\n",
      "Surface training t=19773, loss=0.018281958997249603\n",
      "Surface training t=19774, loss=0.02702986355870962\n",
      "Surface training t=19775, loss=0.028435250744223595\n",
      "Surface training t=19776, loss=0.024004449136555195\n",
      "Surface training t=19777, loss=0.022321108728647232\n",
      "Surface training t=19778, loss=0.02381033729761839\n",
      "Surface training t=19779, loss=0.02514536678791046\n",
      "Surface training t=19780, loss=0.025209720246493816\n",
      "Surface training t=19781, loss=0.02374451607465744\n",
      "Surface training t=19782, loss=0.025771464221179485\n",
      "Surface training t=19783, loss=0.02969326637685299\n",
      "Surface training t=19784, loss=0.023514853790402412\n",
      "Surface training t=19785, loss=0.027218112722039223\n",
      "Surface training t=19786, loss=0.02459246851503849\n",
      "Surface training t=19787, loss=0.024317733012139797\n",
      "Surface training t=19788, loss=0.027202134020626545\n",
      "Surface training t=19789, loss=0.02331505622714758\n",
      "Surface training t=19790, loss=0.02831483632326126\n",
      "Surface training t=19791, loss=0.03564136102795601\n",
      "Surface training t=19792, loss=0.036221943795681\n",
      "Surface training t=19793, loss=0.026875565759837627\n",
      "Surface training t=19794, loss=0.02469357941299677\n",
      "Surface training t=19795, loss=0.020670829340815544\n",
      "Surface training t=19796, loss=0.021747016347944736\n",
      "Surface training t=19797, loss=0.022632666863501072\n",
      "Surface training t=19798, loss=0.02121836692094803\n",
      "Surface training t=19799, loss=0.022506103850901127\n",
      "Surface training t=19800, loss=0.02331650350242853\n",
      "Surface training t=19801, loss=0.026681887917220592\n",
      "Surface training t=19802, loss=0.02470538718625903\n",
      "Surface training t=19803, loss=0.034444672986865044\n",
      "Surface training t=19804, loss=0.03954175300896168\n",
      "Surface training t=19805, loss=0.03822402749210596\n",
      "Surface training t=19806, loss=0.038044318556785583\n",
      "Surface training t=19807, loss=0.030666240490972996\n",
      "Surface training t=19808, loss=0.038296086713671684\n",
      "Surface training t=19809, loss=0.03695705533027649\n",
      "Surface training t=19810, loss=0.034045591950416565\n",
      "Surface training t=19811, loss=0.036077797412872314\n",
      "Surface training t=19812, loss=0.03788881190121174\n",
      "Surface training t=19813, loss=0.04300547577440739\n",
      "Surface training t=19814, loss=0.03813609853386879\n",
      "Surface training t=19815, loss=0.039772938936948776\n",
      "Surface training t=19816, loss=0.0298797944560647\n",
      "Surface training t=19817, loss=0.029055447317659855\n",
      "Surface training t=19818, loss=0.036771535873413086\n",
      "Surface training t=19819, loss=0.03084552939981222\n",
      "Surface training t=19820, loss=0.05143975466489792\n",
      "Surface training t=19821, loss=0.031352031044662\n",
      "Surface training t=19822, loss=0.048918696120381355\n",
      "Surface training t=19823, loss=0.02971815038472414\n",
      "Surface training t=19824, loss=0.03373763244599104\n",
      "Surface training t=19825, loss=0.04523997940123081\n",
      "Surface training t=19826, loss=0.02945621684193611\n",
      "Surface training t=19827, loss=0.03414612449705601\n",
      "Surface training t=19828, loss=0.0321211377158761\n",
      "Surface training t=19829, loss=0.034771865233778954\n",
      "Surface training t=19830, loss=0.03805803321301937\n",
      "Surface training t=19831, loss=0.03765817731618881\n",
      "Surface training t=19832, loss=0.032491923309862614\n",
      "Surface training t=19833, loss=0.038432011380791664\n",
      "Surface training t=19834, loss=0.0413817185908556\n",
      "Surface training t=19835, loss=0.04521712101995945\n",
      "Surface training t=19836, loss=0.0373597526922822\n",
      "Surface training t=19837, loss=0.03462142311036587\n",
      "Surface training t=19838, loss=0.03048372082412243\n",
      "Surface training t=19839, loss=0.04123618267476559\n",
      "Surface training t=19840, loss=0.03527805395424366\n",
      "Surface training t=19841, loss=0.03417506814002991\n",
      "Surface training t=19842, loss=0.03133593872189522\n",
      "Surface training t=19843, loss=0.031601312570273876\n",
      "Surface training t=19844, loss=0.028356259688735008\n",
      "Surface training t=19845, loss=0.037555987015366554\n",
      "Surface training t=19846, loss=0.031588380225002766\n",
      "Surface training t=19847, loss=0.02234977949410677\n",
      "Surface training t=19848, loss=0.01954750344157219\n",
      "Surface training t=19849, loss=0.028191322460770607\n",
      "Surface training t=19850, loss=0.021893804892897606\n",
      "Surface training t=19851, loss=0.01811556238681078\n",
      "Surface training t=19852, loss=0.02055931370705366\n",
      "Surface training t=19853, loss=0.025850224308669567\n",
      "Surface training t=19854, loss=0.02492122072726488\n",
      "Surface training t=19855, loss=0.023446492850780487\n",
      "Surface training t=19856, loss=0.024679601192474365\n",
      "Surface training t=19857, loss=0.021765530109405518\n",
      "Surface training t=19858, loss=0.01901791524142027\n",
      "Surface training t=19859, loss=0.017934351228177547\n",
      "Surface training t=19860, loss=0.02161113079637289\n",
      "Surface training t=19861, loss=0.024407136254012585\n",
      "Surface training t=19862, loss=0.025717997923493385\n",
      "Surface training t=19863, loss=0.02072080969810486\n",
      "Surface training t=19864, loss=0.020590134896337986\n",
      "Surface training t=19865, loss=0.025572399608790874\n",
      "Surface training t=19866, loss=0.02307821996510029\n",
      "Surface training t=19867, loss=0.02008369006216526\n",
      "Surface training t=19868, loss=0.016688343603163958\n",
      "Surface training t=19869, loss=0.02722254302352667\n",
      "Surface training t=19870, loss=0.030045119114220142\n",
      "Surface training t=19871, loss=0.02984856255352497\n",
      "Surface training t=19872, loss=0.033417679369449615\n",
      "Surface training t=19873, loss=0.02608050685375929\n",
      "Surface training t=19874, loss=0.023945797234773636\n",
      "Surface training t=19875, loss=0.029103949666023254\n",
      "Surface training t=19876, loss=0.026390031911432743\n",
      "Surface training t=19877, loss=0.02526350226253271\n",
      "Surface training t=19878, loss=0.02231062389910221\n",
      "Surface training t=19879, loss=0.023337936960160732\n",
      "Surface training t=19880, loss=0.028263083659112453\n",
      "Surface training t=19881, loss=0.01926576904952526\n",
      "Surface training t=19882, loss=0.021781710907816887\n",
      "Surface training t=19883, loss=0.01923274528235197\n",
      "Surface training t=19884, loss=0.02559772226959467\n",
      "Surface training t=19885, loss=0.02038942091166973\n",
      "Surface training t=19886, loss=0.019600551575422287\n",
      "Surface training t=19887, loss=0.025264158844947815\n",
      "Surface training t=19888, loss=0.022113682702183723\n",
      "Surface training t=19889, loss=0.022033164277672768\n",
      "Surface training t=19890, loss=0.019643012434244156\n",
      "Surface training t=19891, loss=0.022095692344009876\n",
      "Surface training t=19892, loss=0.02223256789147854\n",
      "Surface training t=19893, loss=0.021027175709605217\n",
      "Surface training t=19894, loss=0.019419527612626553\n",
      "Surface training t=19895, loss=0.021282052621245384\n",
      "Surface training t=19896, loss=0.026704804971814156\n",
      "Surface training t=19897, loss=0.019499696791172028\n",
      "Surface training t=19898, loss=0.02349590603262186\n",
      "Surface training t=19899, loss=0.019888989627361298\n",
      "Surface training t=19900, loss=0.022141320630908012\n",
      "Surface training t=19901, loss=0.02079333458095789\n",
      "Surface training t=19902, loss=0.021933529525995255\n",
      "Surface training t=19903, loss=0.02116916235536337\n",
      "Surface training t=19904, loss=0.01785162091255188\n",
      "Surface training t=19905, loss=0.03094605077058077\n",
      "Surface training t=19906, loss=0.025389312766492367\n",
      "Surface training t=19907, loss=0.021682722494006157\n",
      "Surface training t=19908, loss=0.01673178654164076\n",
      "Surface training t=19909, loss=0.019736606627702713\n",
      "Surface training t=19910, loss=0.018141007982194424\n",
      "Surface training t=19911, loss=0.019349319860339165\n",
      "Surface training t=19912, loss=0.023355137556791306\n",
      "Surface training t=19913, loss=0.02296228799968958\n",
      "Surface training t=19914, loss=0.02063751593232155\n",
      "Surface training t=19915, loss=0.02450063731521368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=19916, loss=0.03659021481871605\n",
      "Surface training t=19917, loss=0.031647466123104095\n",
      "Surface training t=19918, loss=0.03639490343630314\n",
      "Surface training t=19919, loss=0.022159265354275703\n",
      "Surface training t=19920, loss=0.02242245990782976\n",
      "Surface training t=19921, loss=0.04344047233462334\n",
      "Surface training t=19922, loss=0.035775721073150635\n",
      "Surface training t=19923, loss=0.044285472482442856\n",
      "Surface training t=19924, loss=0.023743455298244953\n",
      "Surface training t=19925, loss=0.02708072680979967\n",
      "Surface training t=19926, loss=0.0324779162183404\n",
      "Surface training t=19927, loss=0.045582856982946396\n",
      "Surface training t=19928, loss=0.04598384350538254\n",
      "Surface training t=19929, loss=0.03670616168528795\n",
      "Surface training t=19930, loss=0.032670581713318825\n",
      "Surface training t=19931, loss=0.042905136942863464\n",
      "Surface training t=19932, loss=0.029250883497297764\n",
      "Surface training t=19933, loss=0.03374652564525604\n",
      "Surface training t=19934, loss=0.03155357018113136\n",
      "Surface training t=19935, loss=0.030170083977282047\n",
      "Surface training t=19936, loss=0.05843422748148441\n",
      "Surface training t=19937, loss=0.03910988196730614\n",
      "Surface training t=19938, loss=0.031219374388456345\n",
      "Surface training t=19939, loss=0.026401625014841557\n",
      "Surface training t=19940, loss=0.02891353890299797\n",
      "Surface training t=19941, loss=0.025164960883557796\n",
      "Surface training t=19942, loss=0.020179198123514652\n",
      "Surface training t=19943, loss=0.02022415306419134\n",
      "Surface training t=19944, loss=0.023006537929177284\n",
      "Surface training t=19945, loss=0.018944283947348595\n",
      "Surface training t=19946, loss=0.022498883306980133\n",
      "Surface training t=19947, loss=0.02745607402175665\n",
      "Surface training t=19948, loss=0.019018887542188168\n",
      "Surface training t=19949, loss=0.031663802452385426\n",
      "Surface training t=19950, loss=0.020102398470044136\n",
      "Surface training t=19951, loss=0.020532277412712574\n",
      "Surface training t=19952, loss=0.01628805883228779\n",
      "Surface training t=19953, loss=0.018744218163192272\n",
      "Surface training t=19954, loss=0.022283810190856457\n",
      "Surface training t=19955, loss=0.02043162379413843\n",
      "Surface training t=19956, loss=0.018334662541747093\n",
      "Surface training t=19957, loss=0.019129562191665173\n",
      "Surface training t=19958, loss=0.02046904806047678\n",
      "Surface training t=19959, loss=0.02230812329798937\n",
      "Surface training t=19960, loss=0.025470728054642677\n",
      "Surface training t=19961, loss=0.027074233628809452\n",
      "Surface training t=19962, loss=0.021214314736425877\n",
      "Surface training t=19963, loss=0.023387078195810318\n",
      "Surface training t=19964, loss=0.024557641707360744\n",
      "Surface training t=19965, loss=0.028060123324394226\n",
      "Surface training t=19966, loss=0.01661314070224762\n",
      "Surface training t=19967, loss=0.018225226551294327\n",
      "Surface training t=19968, loss=0.03983119875192642\n",
      "Surface training t=19969, loss=0.03735828399658203\n",
      "Surface training t=19970, loss=0.03432184923440218\n",
      "Surface training t=19971, loss=0.03821852430701256\n",
      "Surface training t=19972, loss=0.04551827721297741\n",
      "Surface training t=19973, loss=0.035549335181713104\n",
      "Surface training t=19974, loss=0.03300528600811958\n",
      "Surface training t=19975, loss=0.029824632219970226\n",
      "Surface training t=19976, loss=0.02251691371202469\n",
      "Surface training t=19977, loss=0.02852354757487774\n",
      "Surface training t=19978, loss=0.027623982168734074\n",
      "Surface training t=19979, loss=0.029263311997056007\n",
      "Surface training t=19980, loss=0.03138424828648567\n",
      "Surface training t=19981, loss=0.026231421157717705\n",
      "Surface training t=19982, loss=0.02476012520492077\n",
      "Surface training t=19983, loss=0.024424515664577484\n",
      "Surface training t=19984, loss=0.027892548590898514\n",
      "Surface training t=19985, loss=0.03332259226590395\n",
      "Surface training t=19986, loss=0.03795342333614826\n",
      "Surface training t=19987, loss=0.04778651334345341\n",
      "Surface training t=19988, loss=0.0344923036172986\n",
      "Surface training t=19989, loss=0.02308732643723488\n",
      "Surface training t=19990, loss=0.02068356331437826\n",
      "Surface training t=19991, loss=0.020106342621147633\n",
      "Surface training t=19992, loss=0.020212368108332157\n",
      "Surface training t=19993, loss=0.020805472508072853\n",
      "Surface training t=19994, loss=0.018375628627836704\n",
      "Surface training t=19995, loss=0.014293062966316938\n",
      "Surface training t=19996, loss=0.0202871635556221\n",
      "Surface training t=19997, loss=0.023852885700762272\n",
      "Surface training t=19998, loss=0.027839007787406445\n",
      "Surface training t=19999, loss=0.03551662340760231\n",
      "Surface training t=20000, loss=0.029339592903852463\n",
      "Surface training t=20001, loss=0.030880363658070564\n",
      "Surface training t=20002, loss=0.044310782104730606\n",
      "Surface training t=20003, loss=0.03333027195185423\n",
      "Surface training t=20004, loss=0.04247448034584522\n",
      "Surface training t=20005, loss=0.03751538321375847\n",
      "Surface training t=20006, loss=0.03550533205270767\n",
      "Surface training t=20007, loss=0.026259416714310646\n",
      "Surface training t=20008, loss=0.02284063957631588\n",
      "Surface training t=20009, loss=0.03236461617052555\n",
      "Surface training t=20010, loss=0.030648007057607174\n",
      "Surface training t=20011, loss=0.02520463801920414\n",
      "Surface training t=20012, loss=0.029340925626456738\n",
      "Surface training t=20013, loss=0.022115902975201607\n",
      "Surface training t=20014, loss=0.04070243239402771\n",
      "Surface training t=20015, loss=0.0315028065815568\n",
      "Surface training t=20016, loss=0.030358956195414066\n",
      "Surface training t=20017, loss=0.02568539883941412\n",
      "Surface training t=20018, loss=0.023835274390876293\n",
      "Surface training t=20019, loss=0.022770056035369635\n",
      "Surface training t=20020, loss=0.0300398338586092\n",
      "Surface training t=20021, loss=0.021922226063907146\n",
      "Surface training t=20022, loss=0.0225975438952446\n",
      "Surface training t=20023, loss=0.023781973868608475\n",
      "Surface training t=20024, loss=0.024779516272246838\n",
      "Surface training t=20025, loss=0.024955032393336296\n",
      "Surface training t=20026, loss=0.02405533753335476\n",
      "Surface training t=20027, loss=0.025003327056765556\n",
      "Surface training t=20028, loss=0.025003250688314438\n",
      "Surface training t=20029, loss=0.021635047625750303\n",
      "Surface training t=20030, loss=0.024226923007518053\n",
      "Surface training t=20031, loss=0.031738998368382454\n",
      "Surface training t=20032, loss=0.029782839119434357\n",
      "Surface training t=20033, loss=0.03099009580910206\n",
      "Surface training t=20034, loss=0.0317172110080719\n",
      "Surface training t=20035, loss=0.03462965041399002\n",
      "Surface training t=20036, loss=0.029010158963501453\n",
      "Surface training t=20037, loss=0.02528361137956381\n",
      "Surface training t=20038, loss=0.027573492377996445\n",
      "Surface training t=20039, loss=0.026452815160155296\n",
      "Surface training t=20040, loss=0.022349344566464424\n",
      "Surface training t=20041, loss=0.02265013102442026\n",
      "Surface training t=20042, loss=0.03181634657084942\n",
      "Surface training t=20043, loss=0.027691313065588474\n",
      "Surface training t=20044, loss=0.020038985647261143\n",
      "Surface training t=20045, loss=0.021483459509909153\n",
      "Surface training t=20046, loss=0.02381883468478918\n",
      "Surface training t=20047, loss=0.0241236574947834\n",
      "Surface training t=20048, loss=0.018162505701184273\n",
      "Surface training t=20049, loss=0.018962860107421875\n",
      "Surface training t=20050, loss=0.021938307210803032\n",
      "Surface training t=20051, loss=0.01742655038833618\n",
      "Surface training t=20052, loss=0.021159637719392776\n",
      "Surface training t=20053, loss=0.020322157070040703\n",
      "Surface training t=20054, loss=0.022766857407987118\n",
      "Surface training t=20055, loss=0.023023778572678566\n",
      "Surface training t=20056, loss=0.018173612654209137\n",
      "Surface training t=20057, loss=0.017583385575562716\n",
      "Surface training t=20058, loss=0.0264816889539361\n",
      "Surface training t=20059, loss=0.02760987915098667\n",
      "Surface training t=20060, loss=0.029146154411137104\n",
      "Surface training t=20061, loss=0.032486699521541595\n",
      "Surface training t=20062, loss=0.027164896950125694\n",
      "Surface training t=20063, loss=0.029340889304876328\n",
      "Surface training t=20064, loss=0.027240300551056862\n",
      "Surface training t=20065, loss=0.03241738211363554\n",
      "Surface training t=20066, loss=0.02500811405479908\n",
      "Surface training t=20067, loss=0.0351168867200613\n",
      "Surface training t=20068, loss=0.029782342724502087\n",
      "Surface training t=20069, loss=0.03393415827304125\n",
      "Surface training t=20070, loss=0.03621396981179714\n",
      "Surface training t=20071, loss=0.03614359349012375\n",
      "Surface training t=20072, loss=0.048618581146001816\n",
      "Surface training t=20073, loss=0.029543728567659855\n",
      "Surface training t=20074, loss=0.03035399317741394\n",
      "Surface training t=20075, loss=0.026242658495903015\n",
      "Surface training t=20076, loss=0.0290963901206851\n",
      "Surface training t=20077, loss=0.024174486752599478\n",
      "Surface training t=20078, loss=0.02201382163912058\n",
      "Surface training t=20079, loss=0.028193993493914604\n",
      "Surface training t=20080, loss=0.024370728991925716\n",
      "Surface training t=20081, loss=0.02108531165868044\n",
      "Surface training t=20082, loss=0.029376008547842503\n",
      "Surface training t=20083, loss=0.03048190474510193\n",
      "Surface training t=20084, loss=0.01762908650562167\n",
      "Surface training t=20085, loss=0.018092534504830837\n",
      "Surface training t=20086, loss=0.017801177222281694\n",
      "Surface training t=20087, loss=0.02138759195804596\n",
      "Surface training t=20088, loss=0.025707795284688473\n",
      "Surface training t=20089, loss=0.0283293928951025\n",
      "Surface training t=20090, loss=0.03306322917342186\n",
      "Surface training t=20091, loss=0.035669757053256035\n",
      "Surface training t=20092, loss=0.04616696946322918\n",
      "Surface training t=20093, loss=0.0398526880890131\n",
      "Surface training t=20094, loss=0.03319173771888018\n",
      "Surface training t=20095, loss=0.03082333132624626\n",
      "Surface training t=20096, loss=0.02525688149034977\n",
      "Surface training t=20097, loss=0.03529965691268444\n",
      "Surface training t=20098, loss=0.027940521016716957\n",
      "Surface training t=20099, loss=0.03850376605987549\n",
      "Surface training t=20100, loss=0.026942459866404533\n",
      "Surface training t=20101, loss=0.030740205198526382\n",
      "Surface training t=20102, loss=0.05291145667433739\n",
      "Surface training t=20103, loss=0.038571758195757866\n",
      "Surface training t=20104, loss=0.046182382851839066\n",
      "Surface training t=20105, loss=0.03795763198286295\n",
      "Surface training t=20106, loss=0.04277733154594898\n",
      "Surface training t=20107, loss=0.031033179722726345\n",
      "Surface training t=20108, loss=0.03369335550814867\n",
      "Surface training t=20109, loss=0.05221303179860115\n",
      "Surface training t=20110, loss=0.03620783053338528\n",
      "Surface training t=20111, loss=0.04774472303688526\n",
      "Surface training t=20112, loss=0.030386648140847683\n",
      "Surface training t=20113, loss=0.060796692967414856\n",
      "Surface training t=20114, loss=0.035533067770302296\n",
      "Surface training t=20115, loss=0.03695022128522396\n",
      "Surface training t=20116, loss=0.03319763392210007\n",
      "Surface training t=20117, loss=0.02440656628459692\n",
      "Surface training t=20118, loss=0.029776986688375473\n",
      "Surface training t=20119, loss=0.033763784915208817\n",
      "Surface training t=20120, loss=0.028579755686223507\n",
      "Surface training t=20121, loss=0.023058651480823755\n",
      "Surface training t=20122, loss=0.03401780501008034\n",
      "Surface training t=20123, loss=0.022261606063693762\n",
      "Surface training t=20124, loss=0.02479590754956007\n",
      "Surface training t=20125, loss=0.030933852307498455\n",
      "Surface training t=20126, loss=0.02616073004901409\n",
      "Surface training t=20127, loss=0.017054004594683647\n",
      "Surface training t=20128, loss=0.021756443195044994\n",
      "Surface training t=20129, loss=0.021558314561843872\n",
      "Surface training t=20130, loss=0.024140384048223495\n",
      "Surface training t=20131, loss=0.026796058751642704\n",
      "Surface training t=20132, loss=0.019224495626986027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=20133, loss=0.019076397642493248\n",
      "Surface training t=20134, loss=0.028185730800032616\n",
      "Surface training t=20135, loss=0.028092237189412117\n",
      "Surface training t=20136, loss=0.03023108933120966\n",
      "Surface training t=20137, loss=0.026698870584368706\n",
      "Surface training t=20138, loss=0.027963707223534584\n",
      "Surface training t=20139, loss=0.023592823185026646\n",
      "Surface training t=20140, loss=0.02354530245065689\n",
      "Surface training t=20141, loss=0.020706295035779476\n",
      "Surface training t=20142, loss=0.019566360861063004\n",
      "Surface training t=20143, loss=0.021430905908346176\n",
      "Surface training t=20144, loss=0.015877285040915012\n",
      "Surface training t=20145, loss=0.021939842961728573\n",
      "Surface training t=20146, loss=0.024923008866608143\n",
      "Surface training t=20147, loss=0.01992921344935894\n",
      "Surface training t=20148, loss=0.022227061912417412\n",
      "Surface training t=20149, loss=0.024896152317523956\n",
      "Surface training t=20150, loss=0.02561044692993164\n",
      "Surface training t=20151, loss=0.029857716523110867\n",
      "Surface training t=20152, loss=0.025795653462409973\n",
      "Surface training t=20153, loss=0.027361206710338593\n",
      "Surface training t=20154, loss=0.02842830866575241\n",
      "Surface training t=20155, loss=0.030270801857113838\n",
      "Surface training t=20156, loss=0.03158912807703018\n",
      "Surface training t=20157, loss=0.027572307735681534\n",
      "Surface training t=20158, loss=0.028927405830472708\n",
      "Surface training t=20159, loss=0.041815342381596565\n",
      "Surface training t=20160, loss=0.041406163945794106\n",
      "Surface training t=20161, loss=0.03471854142844677\n",
      "Surface training t=20162, loss=0.03178325481712818\n",
      "Surface training t=20163, loss=0.05114220455288887\n",
      "Surface training t=20164, loss=0.03265538811683655\n",
      "Surface training t=20165, loss=0.03408942185342312\n",
      "Surface training t=20166, loss=0.035087740048766136\n",
      "Surface training t=20167, loss=0.03126250859349966\n",
      "Surface training t=20168, loss=0.0355348140001297\n",
      "Surface training t=20169, loss=0.02871963381767273\n",
      "Surface training t=20170, loss=0.03610003925859928\n",
      "Surface training t=20171, loss=0.04097955487668514\n",
      "Surface training t=20172, loss=0.04389617033302784\n",
      "Surface training t=20173, loss=0.03963095508515835\n",
      "Surface training t=20174, loss=0.048901449888944626\n",
      "Surface training t=20175, loss=0.03802751563489437\n",
      "Surface training t=20176, loss=0.03310505021363497\n",
      "Surface training t=20177, loss=0.03801857028156519\n",
      "Surface training t=20178, loss=0.04277381673455238\n",
      "Surface training t=20179, loss=0.04667660780251026\n",
      "Surface training t=20180, loss=0.03566327877342701\n",
      "Surface training t=20181, loss=0.03869590722024441\n",
      "Surface training t=20182, loss=0.03711986541748047\n",
      "Surface training t=20183, loss=0.025406821630895138\n",
      "Surface training t=20184, loss=0.03418097831308842\n",
      "Surface training t=20185, loss=0.025703643448650837\n",
      "Surface training t=20186, loss=0.028605900704860687\n",
      "Surface training t=20187, loss=0.02791641652584076\n",
      "Surface training t=20188, loss=0.023338355123996735\n",
      "Surface training t=20189, loss=0.030602119863033295\n",
      "Surface training t=20190, loss=0.024317385628819466\n",
      "Surface training t=20191, loss=0.028992559760808945\n",
      "Surface training t=20192, loss=0.024346109479665756\n",
      "Surface training t=20193, loss=0.028336748480796814\n",
      "Surface training t=20194, loss=0.02938426099717617\n",
      "Surface training t=20195, loss=0.029658127576112747\n",
      "Surface training t=20196, loss=0.03561241552233696\n",
      "Surface training t=20197, loss=0.02479364164173603\n",
      "Surface training t=20198, loss=0.027144762687385082\n",
      "Surface training t=20199, loss=0.025891108438372612\n",
      "Surface training t=20200, loss=0.027374339289963245\n",
      "Surface training t=20201, loss=0.02995685674250126\n",
      "Surface training t=20202, loss=0.02636506874114275\n",
      "Surface training t=20203, loss=0.025019729509949684\n",
      "Surface training t=20204, loss=0.022459021769464016\n",
      "Surface training t=20205, loss=0.030111970379948616\n",
      "Surface training t=20206, loss=0.03154766280204058\n",
      "Surface training t=20207, loss=0.031090570613741875\n",
      "Surface training t=20208, loss=0.021454112604260445\n",
      "Surface training t=20209, loss=0.027566482312977314\n",
      "Surface training t=20210, loss=0.028855076991021633\n",
      "Surface training t=20211, loss=0.03727484680712223\n",
      "Surface training t=20212, loss=0.028705136850476265\n",
      "Surface training t=20213, loss=0.03593736235052347\n",
      "Surface training t=20214, loss=0.038885992020368576\n",
      "Surface training t=20215, loss=0.031499093398451805\n",
      "Surface training t=20216, loss=0.03200560435652733\n",
      "Surface training t=20217, loss=0.03596420772373676\n",
      "Surface training t=20218, loss=0.0288350535556674\n",
      "Surface training t=20219, loss=0.03571888245642185\n",
      "Surface training t=20220, loss=0.023303011432290077\n",
      "Surface training t=20221, loss=0.027457017451524734\n",
      "Surface training t=20222, loss=0.028283029794692993\n",
      "Surface training t=20223, loss=0.026945320889353752\n",
      "Surface training t=20224, loss=0.023071018047630787\n",
      "Surface training t=20225, loss=0.023533891886472702\n",
      "Surface training t=20226, loss=0.026789122261106968\n",
      "Surface training t=20227, loss=0.024200462736189365\n",
      "Surface training t=20228, loss=0.025054403580725193\n",
      "Surface training t=20229, loss=0.015549805015325546\n",
      "Surface training t=20230, loss=0.01801608642563224\n",
      "Surface training t=20231, loss=0.021362573839724064\n",
      "Surface training t=20232, loss=0.024569197557866573\n",
      "Surface training t=20233, loss=0.024079669266939163\n",
      "Surface training t=20234, loss=0.026334283873438835\n",
      "Surface training t=20235, loss=0.030703308060765266\n",
      "Surface training t=20236, loss=0.027569085359573364\n",
      "Surface training t=20237, loss=0.030299248173832893\n",
      "Surface training t=20238, loss=0.034203458577394485\n",
      "Surface training t=20239, loss=0.02344674989581108\n",
      "Surface training t=20240, loss=0.028957966715097427\n",
      "Surface training t=20241, loss=0.03670883923768997\n",
      "Surface training t=20242, loss=0.028676213696599007\n",
      "Surface training t=20243, loss=0.06232621520757675\n",
      "Surface training t=20244, loss=0.03665483556687832\n",
      "Surface training t=20245, loss=0.04058147221803665\n",
      "Surface training t=20246, loss=0.03023324627429247\n",
      "Surface training t=20247, loss=0.03001983929425478\n",
      "Surface training t=20248, loss=0.04379806108772755\n",
      "Surface training t=20249, loss=0.047743276692926884\n",
      "Surface training t=20250, loss=0.03574923612177372\n",
      "Surface training t=20251, loss=0.039736793376505375\n",
      "Surface training t=20252, loss=0.045208705589175224\n",
      "Surface training t=20253, loss=0.03829490207135677\n",
      "Surface training t=20254, loss=0.028050675988197327\n",
      "Surface training t=20255, loss=0.02746966853737831\n",
      "Surface training t=20256, loss=0.032542912289500237\n",
      "Surface training t=20257, loss=0.023331341333687305\n",
      "Surface training t=20258, loss=0.022537211887538433\n",
      "Surface training t=20259, loss=0.026603251695632935\n",
      "Surface training t=20260, loss=0.026912829838693142\n",
      "Surface training t=20261, loss=0.024288932792842388\n",
      "Surface training t=20262, loss=0.028817668557167053\n",
      "Surface training t=20263, loss=0.023065170273184776\n",
      "Surface training t=20264, loss=0.033618295565247536\n",
      "Surface training t=20265, loss=0.028023780323565006\n",
      "Surface training t=20266, loss=0.04167390614748001\n",
      "Surface training t=20267, loss=0.029005737975239754\n",
      "Surface training t=20268, loss=0.04372112452983856\n",
      "Surface training t=20269, loss=0.02888430282473564\n",
      "Surface training t=20270, loss=0.02311350591480732\n",
      "Surface training t=20271, loss=0.036236075684428215\n",
      "Surface training t=20272, loss=0.02613083738833666\n",
      "Surface training t=20273, loss=0.028754408471286297\n",
      "Surface training t=20274, loss=0.04605506733059883\n",
      "Surface training t=20275, loss=0.03701785393059254\n",
      "Surface training t=20276, loss=0.03252433426678181\n",
      "Surface training t=20277, loss=0.047160739079117775\n",
      "Surface training t=20278, loss=0.051862286403775215\n",
      "Surface training t=20279, loss=0.03919637016952038\n",
      "Surface training t=20280, loss=0.042567165568470955\n",
      "Surface training t=20281, loss=0.04461614415049553\n",
      "Surface training t=20282, loss=0.034814637154340744\n",
      "Surface training t=20283, loss=0.0347845871001482\n",
      "Surface training t=20284, loss=0.023212074767798185\n",
      "Surface training t=20285, loss=0.025999268516898155\n",
      "Surface training t=20286, loss=0.03642270155251026\n",
      "Surface training t=20287, loss=0.03201225399971008\n",
      "Surface training t=20288, loss=0.03770194388926029\n",
      "Surface training t=20289, loss=0.025049599818885326\n",
      "Surface training t=20290, loss=0.024156593717634678\n",
      "Surface training t=20291, loss=0.03178012743592262\n",
      "Surface training t=20292, loss=0.024758552201092243\n",
      "Surface training t=20293, loss=0.031365374103188515\n",
      "Surface training t=20294, loss=0.025985023006796837\n",
      "Surface training t=20295, loss=0.044398630037903786\n",
      "Surface training t=20296, loss=0.0372330229729414\n",
      "Surface training t=20297, loss=0.038249874487519264\n",
      "Surface training t=20298, loss=0.03621509298682213\n",
      "Surface training t=20299, loss=0.050207046791911125\n",
      "Surface training t=20300, loss=0.044827280566096306\n",
      "Surface training t=20301, loss=0.0359586626291275\n",
      "Surface training t=20302, loss=0.05334496684372425\n",
      "Surface training t=20303, loss=0.03867069445550442\n",
      "Surface training t=20304, loss=0.03723360598087311\n",
      "Surface training t=20305, loss=0.03612906113266945\n",
      "Surface training t=20306, loss=0.058687103912234306\n",
      "Surface training t=20307, loss=0.03877768386155367\n",
      "Surface training t=20308, loss=0.04630329832434654\n",
      "Surface training t=20309, loss=0.06501692906022072\n",
      "Surface training t=20310, loss=0.04587544500827789\n",
      "Surface training t=20311, loss=0.0567751731723547\n",
      "Surface training t=20312, loss=0.054311931133270264\n",
      "Surface training t=20313, loss=0.03993340767920017\n",
      "Surface training t=20314, loss=0.040089141577482224\n",
      "Surface training t=20315, loss=0.030789158307015896\n",
      "Surface training t=20316, loss=0.030351976864039898\n",
      "Surface training t=20317, loss=0.03396839369088411\n",
      "Surface training t=20318, loss=0.031050628051161766\n",
      "Surface training t=20319, loss=0.03251381777226925\n",
      "Surface training t=20320, loss=0.028538133949041367\n",
      "Surface training t=20321, loss=0.02899135183542967\n",
      "Surface training t=20322, loss=0.023789169266819954\n",
      "Surface training t=20323, loss=0.02851026877760887\n",
      "Surface training t=20324, loss=0.024349289014935493\n",
      "Surface training t=20325, loss=0.01963112410157919\n",
      "Surface training t=20326, loss=0.02052252274006605\n",
      "Surface training t=20327, loss=0.023750399239361286\n",
      "Surface training t=20328, loss=0.0314504224807024\n",
      "Surface training t=20329, loss=0.033943381160497665\n",
      "Surface training t=20330, loss=0.025152952410280704\n",
      "Surface training t=20331, loss=0.025089601054787636\n",
      "Surface training t=20332, loss=0.02772643882781267\n",
      "Surface training t=20333, loss=0.026590035296976566\n",
      "Surface training t=20334, loss=0.028343725949525833\n",
      "Surface training t=20335, loss=0.03752748481929302\n",
      "Surface training t=20336, loss=0.04562371410429478\n",
      "Surface training t=20337, loss=0.03169416822493076\n",
      "Surface training t=20338, loss=0.036279210820794106\n",
      "Surface training t=20339, loss=0.025391685776412487\n",
      "Surface training t=20340, loss=0.03981561399996281\n",
      "Surface training t=20341, loss=0.029887117445468903\n",
      "Surface training t=20342, loss=0.0223060492426157\n",
      "Surface training t=20343, loss=0.035136107355356216\n",
      "Surface training t=20344, loss=0.02372110355645418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=20345, loss=0.019982176832854748\n",
      "Surface training t=20346, loss=0.022860780358314514\n",
      "Surface training t=20347, loss=0.019883224740624428\n",
      "Surface training t=20348, loss=0.021752397529780865\n",
      "Surface training t=20349, loss=0.030461642891168594\n",
      "Surface training t=20350, loss=0.02212789934128523\n",
      "Surface training t=20351, loss=0.022720126435160637\n",
      "Surface training t=20352, loss=0.019647885113954544\n",
      "Surface training t=20353, loss=0.02753149438649416\n",
      "Surface training t=20354, loss=0.024510550312697887\n",
      "Surface training t=20355, loss=0.026400701142847538\n",
      "Surface training t=20356, loss=0.032316249795258045\n",
      "Surface training t=20357, loss=0.0261352751404047\n",
      "Surface training t=20358, loss=0.03338050842285156\n",
      "Surface training t=20359, loss=0.041431570425629616\n",
      "Surface training t=20360, loss=0.029892430640757084\n",
      "Surface training t=20361, loss=0.042469197884202003\n",
      "Surface training t=20362, loss=0.03526262938976288\n",
      "Surface training t=20363, loss=0.03160012327134609\n",
      "Surface training t=20364, loss=0.029747013933956623\n",
      "Surface training t=20365, loss=0.027804700657725334\n",
      "Surface training t=20366, loss=0.031770781613886356\n",
      "Surface training t=20367, loss=0.03591315168887377\n",
      "Surface training t=20368, loss=0.03185892663896084\n",
      "Surface training t=20369, loss=0.03408539667725563\n",
      "Surface training t=20370, loss=0.02183835580945015\n",
      "Surface training t=20371, loss=0.039127686992287636\n",
      "Surface training t=20372, loss=0.03618542104959488\n",
      "Surface training t=20373, loss=0.027116520330309868\n",
      "Surface training t=20374, loss=0.027270854450762272\n",
      "Surface training t=20375, loss=0.027939146384596825\n",
      "Surface training t=20376, loss=0.029128076508641243\n",
      "Surface training t=20377, loss=0.030407174490392208\n",
      "Surface training t=20378, loss=0.0376689825206995\n",
      "Surface training t=20379, loss=0.0422707125544548\n",
      "Surface training t=20380, loss=0.03918983414769173\n",
      "Surface training t=20381, loss=0.052119866013526917\n",
      "Surface training t=20382, loss=0.07507980242371559\n",
      "Surface training t=20383, loss=0.04762963578104973\n",
      "Surface training t=20384, loss=0.05910881422460079\n",
      "Surface training t=20385, loss=0.04402434825897217\n",
      "Surface training t=20386, loss=0.03221306763589382\n",
      "Surface training t=20387, loss=0.03144186083227396\n",
      "Surface training t=20388, loss=0.02593237068504095\n",
      "Surface training t=20389, loss=0.04111766070127487\n",
      "Surface training t=20390, loss=0.02963760308921337\n",
      "Surface training t=20391, loss=0.02989712357521057\n",
      "Surface training t=20392, loss=0.03303599450737238\n",
      "Surface training t=20393, loss=0.04035675898194313\n",
      "Surface training t=20394, loss=0.03945271112024784\n",
      "Surface training t=20395, loss=0.029528611339628696\n",
      "Surface training t=20396, loss=0.026322852820158005\n",
      "Surface training t=20397, loss=0.028531155548989773\n",
      "Surface training t=20398, loss=0.033472124487161636\n",
      "Surface training t=20399, loss=0.040668344125151634\n",
      "Surface training t=20400, loss=0.04220324382185936\n",
      "Surface training t=20401, loss=0.028658488765358925\n",
      "Surface training t=20402, loss=0.02769116684794426\n",
      "Surface training t=20403, loss=0.033596547320485115\n",
      "Surface training t=20404, loss=0.035306576639413834\n",
      "Surface training t=20405, loss=0.02799694985151291\n",
      "Surface training t=20406, loss=0.02027328871190548\n",
      "Surface training t=20407, loss=0.019916316494345665\n",
      "Surface training t=20408, loss=0.01826518028974533\n",
      "Surface training t=20409, loss=0.02268066257238388\n",
      "Surface training t=20410, loss=0.023745164275169373\n",
      "Surface training t=20411, loss=0.027629390358924866\n",
      "Surface training t=20412, loss=0.019702534191310406\n",
      "Surface training t=20413, loss=0.018233737908303738\n",
      "Surface training t=20414, loss=0.02172186877578497\n",
      "Surface training t=20415, loss=0.022285958752036095\n",
      "Surface training t=20416, loss=0.019754801876842976\n",
      "Surface training t=20417, loss=0.023827049881219864\n",
      "Surface training t=20418, loss=0.02043826226145029\n",
      "Surface training t=20419, loss=0.01813726406544447\n",
      "Surface training t=20420, loss=0.014811324421316385\n",
      "Surface training t=20421, loss=0.016846087761223316\n",
      "Surface training t=20422, loss=0.016543484292924404\n",
      "Surface training t=20423, loss=0.020014988258481026\n",
      "Surface training t=20424, loss=0.01893212180584669\n",
      "Surface training t=20425, loss=0.016980770509690046\n",
      "Surface training t=20426, loss=0.021835933439433575\n",
      "Surface training t=20427, loss=0.021864820271730423\n",
      "Surface training t=20428, loss=0.026936430484056473\n",
      "Surface training t=20429, loss=0.0247690686956048\n",
      "Surface training t=20430, loss=0.024178802967071533\n",
      "Surface training t=20431, loss=0.03163476847112179\n",
      "Surface training t=20432, loss=0.027163577266037464\n",
      "Surface training t=20433, loss=0.022608362138271332\n",
      "Surface training t=20434, loss=0.03401972260326147\n",
      "Surface training t=20435, loss=0.03055666945874691\n",
      "Surface training t=20436, loss=0.023154899012297392\n",
      "Surface training t=20437, loss=0.04253249242901802\n",
      "Surface training t=20438, loss=0.030143569223582745\n",
      "Surface training t=20439, loss=0.029613839462399483\n",
      "Surface training t=20440, loss=0.01970855798572302\n",
      "Surface training t=20441, loss=0.02556877676397562\n",
      "Surface training t=20442, loss=0.026561470702290535\n",
      "Surface training t=20443, loss=0.02285151556134224\n",
      "Surface training t=20444, loss=0.018394489772617817\n",
      "Surface training t=20445, loss=0.014586267061531544\n",
      "Surface training t=20446, loss=0.021177465096116066\n",
      "Surface training t=20447, loss=0.022677425295114517\n",
      "Surface training t=20448, loss=0.02797921746969223\n",
      "Surface training t=20449, loss=0.02791001694276929\n",
      "Surface training t=20450, loss=0.03479485213756561\n",
      "Surface training t=20451, loss=0.02303607389330864\n",
      "Surface training t=20452, loss=0.023511130828410387\n",
      "Surface training t=20453, loss=0.030754616484045982\n",
      "Surface training t=20454, loss=0.024778754450380802\n",
      "Surface training t=20455, loss=0.02153928391635418\n",
      "Surface training t=20456, loss=0.021078813821077347\n",
      "Surface training t=20457, loss=0.02243607398122549\n",
      "Surface training t=20458, loss=0.028489038348197937\n",
      "Surface training t=20459, loss=0.019375475123524666\n",
      "Surface training t=20460, loss=0.01780979335308075\n",
      "Surface training t=20461, loss=0.020327435806393623\n",
      "Surface training t=20462, loss=0.021729699335992336\n",
      "Surface training t=20463, loss=0.025314398109912872\n",
      "Surface training t=20464, loss=0.02529071643948555\n",
      "Surface training t=20465, loss=0.02681918628513813\n",
      "Surface training t=20466, loss=0.02767102513462305\n",
      "Surface training t=20467, loss=0.02623437624424696\n",
      "Surface training t=20468, loss=0.03351369686424732\n",
      "Surface training t=20469, loss=0.02612004056572914\n",
      "Surface training t=20470, loss=0.029473261907696724\n",
      "Surface training t=20471, loss=0.02505799848586321\n",
      "Surface training t=20472, loss=0.022236096672713757\n",
      "Surface training t=20473, loss=0.017577726393938065\n",
      "Surface training t=20474, loss=0.020658467896282673\n",
      "Surface training t=20475, loss=0.01931087952107191\n",
      "Surface training t=20476, loss=0.025279001332819462\n",
      "Surface training t=20477, loss=0.027718684636056423\n",
      "Surface training t=20478, loss=0.025537905283272266\n",
      "Surface training t=20479, loss=0.023980174213647842\n",
      "Surface training t=20480, loss=0.02701470721513033\n",
      "Surface training t=20481, loss=0.020359485410153866\n",
      "Surface training t=20482, loss=0.0205033291131258\n",
      "Surface training t=20483, loss=0.015195590443909168\n",
      "Surface training t=20484, loss=0.019215965643525124\n",
      "Surface training t=20485, loss=0.029480070807039738\n",
      "Surface training t=20486, loss=0.03291884995996952\n",
      "Surface training t=20487, loss=0.034194111824035645\n",
      "Surface training t=20488, loss=0.03352321032434702\n",
      "Surface training t=20489, loss=0.026737903244793415\n",
      "Surface training t=20490, loss=0.03924754448235035\n",
      "Surface training t=20491, loss=0.03892207145690918\n",
      "Surface training t=20492, loss=0.0317848976701498\n",
      "Surface training t=20493, loss=0.04723488166928291\n",
      "Surface training t=20494, loss=0.03301762975752354\n",
      "Surface training t=20495, loss=0.03928246535360813\n",
      "Surface training t=20496, loss=0.027385849505662918\n",
      "Surface training t=20497, loss=0.024210148490965366\n",
      "Surface training t=20498, loss=0.03125778120011091\n",
      "Surface training t=20499, loss=0.027563240379095078\n",
      "Surface training t=20500, loss=0.032425662502646446\n",
      "Surface training t=20501, loss=0.021872430108487606\n",
      "Surface training t=20502, loss=0.027242702431976795\n",
      "Surface training t=20503, loss=0.021091210655868053\n",
      "Surface training t=20504, loss=0.016544760670512915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=20505, loss=0.025228796526789665\n",
      "Surface training t=20506, loss=0.020813601091504097\n",
      "Surface training t=20507, loss=0.015674125868827105\n",
      "Surface training t=20508, loss=0.018332085572183132\n",
      "Surface training t=20509, loss=0.016468352638185024\n",
      "Surface training t=20510, loss=0.017580727115273476\n",
      "Surface training t=20511, loss=0.0210881307721138\n",
      "Surface training t=20512, loss=0.02037572581321001\n",
      "Surface training t=20513, loss=0.019968404434621334\n",
      "Surface training t=20514, loss=0.02182141598314047\n",
      "Surface training t=20515, loss=0.02170114405453205\n",
      "Surface training t=20516, loss=0.018330604303628206\n",
      "Surface training t=20517, loss=0.018486608751118183\n",
      "Surface training t=20518, loss=0.017801275476813316\n",
      "Surface training t=20519, loss=0.016334581188857555\n",
      "Surface training t=20520, loss=0.01866749208420515\n",
      "Surface training t=20521, loss=0.015258261002600193\n",
      "Surface training t=20522, loss=0.0189519040286541\n",
      "Surface training t=20523, loss=0.01856479560956359\n",
      "Surface training t=20524, loss=0.0168069489300251\n",
      "Surface training t=20525, loss=0.018259460106492043\n",
      "Surface training t=20526, loss=0.016329155303537846\n",
      "Surface training t=20527, loss=0.016406390815973282\n",
      "Surface training t=20528, loss=0.01672972459346056\n",
      "Surface training t=20529, loss=0.018923972733318806\n",
      "Surface training t=20530, loss=0.02040155977010727\n",
      "Surface training t=20531, loss=0.018143839202821255\n",
      "Surface training t=20532, loss=0.014189634472131729\n",
      "Surface training t=20533, loss=0.01911008544266224\n",
      "Surface training t=20534, loss=0.014153762720525265\n",
      "Surface training t=20535, loss=0.02372333500534296\n",
      "Surface training t=20536, loss=0.02660489547997713\n",
      "Surface training t=20537, loss=0.027946291491389275\n",
      "Surface training t=20538, loss=0.03306981362402439\n",
      "Surface training t=20539, loss=0.041349856182932854\n",
      "Surface training t=20540, loss=0.036865755915641785\n",
      "Surface training t=20541, loss=0.023511948995292187\n",
      "Surface training t=20542, loss=0.02791784144937992\n",
      "Surface training t=20543, loss=0.026936324313282967\n",
      "Surface training t=20544, loss=0.031005287542939186\n",
      "Surface training t=20545, loss=0.025223448872566223\n",
      "Surface training t=20546, loss=0.03904411941766739\n",
      "Surface training t=20547, loss=0.03262963332235813\n",
      "Surface training t=20548, loss=0.035860439762473106\n",
      "Surface training t=20549, loss=0.027037962339818478\n",
      "Surface training t=20550, loss=0.02832077629864216\n",
      "Surface training t=20551, loss=0.025742092169821262\n",
      "Surface training t=20552, loss=0.028340015560388565\n",
      "Surface training t=20553, loss=0.03351443633437157\n",
      "Surface training t=20554, loss=0.029482770711183548\n",
      "Surface training t=20555, loss=0.02416868321597576\n",
      "Surface training t=20556, loss=0.02639959566295147\n",
      "Surface training t=20557, loss=0.025204656645655632\n",
      "Surface training t=20558, loss=0.021821776404976845\n",
      "Surface training t=20559, loss=0.022273494862020016\n",
      "Surface training t=20560, loss=0.027029333636164665\n",
      "Surface training t=20561, loss=0.02694527618587017\n",
      "Surface training t=20562, loss=0.025972407311201096\n",
      "Surface training t=20563, loss=0.03569401055574417\n",
      "Surface training t=20564, loss=0.0290315393358469\n",
      "Surface training t=20565, loss=0.027715016156435013\n",
      "Surface training t=20566, loss=0.017616388853639364\n",
      "Surface training t=20567, loss=0.025501050055027008\n",
      "Surface training t=20568, loss=0.02465300541371107\n",
      "Surface training t=20569, loss=0.019410892389714718\n",
      "Surface training t=20570, loss=0.03582334518432617\n",
      "Surface training t=20571, loss=0.035212866961956024\n",
      "Surface training t=20572, loss=0.03053381573408842\n",
      "Surface training t=20573, loss=0.032165151089429855\n",
      "Surface training t=20574, loss=0.028979208320379257\n",
      "Surface training t=20575, loss=0.03270299732685089\n",
      "Surface training t=20576, loss=0.03167400509119034\n",
      "Surface training t=20577, loss=0.030437068082392216\n",
      "Surface training t=20578, loss=0.03591681458055973\n",
      "Surface training t=20579, loss=0.033153923228383064\n",
      "Surface training t=20580, loss=0.04772073030471802\n",
      "Surface training t=20581, loss=0.027682531625032425\n",
      "Surface training t=20582, loss=0.027220288291573524\n",
      "Surface training t=20583, loss=0.03284614905714989\n",
      "Surface training t=20584, loss=0.038634803146123886\n",
      "Surface training t=20585, loss=0.054583990946412086\n",
      "Surface training t=20586, loss=0.03783484362065792\n",
      "Surface training t=20587, loss=0.036209711804986\n",
      "Surface training t=20588, loss=0.02664254419505596\n",
      "Surface training t=20589, loss=0.027789552696049213\n",
      "Surface training t=20590, loss=0.02301068976521492\n",
      "Surface training t=20591, loss=0.030357598327100277\n",
      "Surface training t=20592, loss=0.02722999732941389\n",
      "Surface training t=20593, loss=0.04586941562592983\n",
      "Surface training t=20594, loss=0.03335350379347801\n",
      "Surface training t=20595, loss=0.031508262269198895\n",
      "Surface training t=20596, loss=0.03847783990204334\n",
      "Surface training t=20597, loss=0.025634759105741978\n",
      "Surface training t=20598, loss=0.03978290036320686\n",
      "Surface training t=20599, loss=0.0275389077141881\n",
      "Surface training t=20600, loss=0.023160229437053204\n",
      "Surface training t=20601, loss=0.02365154679864645\n",
      "Surface training t=20602, loss=0.027614504098892212\n",
      "Surface training t=20603, loss=0.018664591945707798\n",
      "Surface training t=20604, loss=0.01978880725800991\n",
      "Surface training t=20605, loss=0.023612423799932003\n",
      "Surface training t=20606, loss=0.024767412804067135\n",
      "Surface training t=20607, loss=0.030438479967415333\n",
      "Surface training t=20608, loss=0.02575257420539856\n",
      "Surface training t=20609, loss=0.024966352619230747\n",
      "Surface training t=20610, loss=0.03944513760507107\n",
      "Surface training t=20611, loss=0.03278292901813984\n",
      "Surface training t=20612, loss=0.029850791208446026\n",
      "Surface training t=20613, loss=0.024840237572789192\n",
      "Surface training t=20614, loss=0.0279221273958683\n",
      "Surface training t=20615, loss=0.024667139165103436\n",
      "Surface training t=20616, loss=0.02531229518353939\n",
      "Surface training t=20617, loss=0.024636724032461643\n",
      "Surface training t=20618, loss=0.02179604023694992\n",
      "Surface training t=20619, loss=0.02521125040948391\n",
      "Surface training t=20620, loss=0.025196907110512257\n",
      "Surface training t=20621, loss=0.02077170042321086\n",
      "Surface training t=20622, loss=0.02172455843538046\n",
      "Surface training t=20623, loss=0.02661601174622774\n",
      "Surface training t=20624, loss=0.028514862060546875\n",
      "Surface training t=20625, loss=0.02008743677288294\n",
      "Surface training t=20626, loss=0.024658052250742912\n",
      "Surface training t=20627, loss=0.023684967309236526\n",
      "Surface training t=20628, loss=0.02712428942322731\n",
      "Surface training t=20629, loss=0.024202164262533188\n",
      "Surface training t=20630, loss=0.02037161961197853\n",
      "Surface training t=20631, loss=0.031326331198215485\n",
      "Surface training t=20632, loss=0.01911903079599142\n",
      "Surface training t=20633, loss=0.027418147772550583\n",
      "Surface training t=20634, loss=0.0404457151889801\n",
      "Surface training t=20635, loss=0.03125848062336445\n",
      "Surface training t=20636, loss=0.03293458092957735\n",
      "Surface training t=20637, loss=0.02859581634402275\n",
      "Surface training t=20638, loss=0.027836700901389122\n",
      "Surface training t=20639, loss=0.026137537322938442\n",
      "Surface training t=20640, loss=0.027375416830182076\n",
      "Surface training t=20641, loss=0.04269211180508137\n",
      "Surface training t=20642, loss=0.046657394617795944\n",
      "Surface training t=20643, loss=0.03646354749798775\n",
      "Surface training t=20644, loss=0.029064268339425325\n",
      "Surface training t=20645, loss=0.04639446176588535\n",
      "Surface training t=20646, loss=0.03809581324458122\n",
      "Surface training t=20647, loss=0.03972426988184452\n",
      "Surface training t=20648, loss=0.048824336379766464\n",
      "Surface training t=20649, loss=0.0523833055049181\n",
      "Surface training t=20650, loss=0.0477202944457531\n",
      "Surface training t=20651, loss=0.04796839505434036\n",
      "Surface training t=20652, loss=0.035582590848207474\n",
      "Surface training t=20653, loss=0.040594542399048805\n",
      "Surface training t=20654, loss=0.04638809338212013\n",
      "Surface training t=20655, loss=0.027979616075754166\n",
      "Surface training t=20656, loss=0.03533932939171791\n",
      "Surface training t=20657, loss=0.025533460080623627\n",
      "Surface training t=20658, loss=0.02706314716488123\n",
      "Surface training t=20659, loss=0.026731627993285656\n",
      "Surface training t=20660, loss=0.01916277315467596\n",
      "Surface training t=20661, loss=0.01927812397480011\n",
      "Surface training t=20662, loss=0.026904163882136345\n",
      "Surface training t=20663, loss=0.02379481215029955\n",
      "Surface training t=20664, loss=0.021613663993775845\n",
      "Surface training t=20665, loss=0.021422239020466805\n",
      "Surface training t=20666, loss=0.022036511451005936\n",
      "Surface training t=20667, loss=0.019079644232988358\n",
      "Surface training t=20668, loss=0.022556213662028313\n",
      "Surface training t=20669, loss=0.02134575042873621\n",
      "Surface training t=20670, loss=0.019657387398183346\n",
      "Surface training t=20671, loss=0.02775486931204796\n",
      "Surface training t=20672, loss=0.032092489302158356\n",
      "Surface training t=20673, loss=0.02815714105963707\n",
      "Surface training t=20674, loss=0.034236736595630646\n",
      "Surface training t=20675, loss=0.02873704582452774\n",
      "Surface training t=20676, loss=0.03324972093105316\n",
      "Surface training t=20677, loss=0.03821666445583105\n",
      "Surface training t=20678, loss=0.03740701451897621\n",
      "Surface training t=20679, loss=0.04113827086985111\n",
      "Surface training t=20680, loss=0.028520425781607628\n",
      "Surface training t=20681, loss=0.025940789841115475\n",
      "Surface training t=20682, loss=0.025445235893130302\n",
      "Surface training t=20683, loss=0.025660589337348938\n",
      "Surface training t=20684, loss=0.025953383184969425\n",
      "Surface training t=20685, loss=0.02663864754140377\n",
      "Surface training t=20686, loss=0.031252057291567326\n",
      "Surface training t=20687, loss=0.030456609092652798\n",
      "Surface training t=20688, loss=0.02832004800438881\n",
      "Surface training t=20689, loss=0.021527692675590515\n",
      "Surface training t=20690, loss=0.031810387037694454\n",
      "Surface training t=20691, loss=0.021085046231746674\n",
      "Surface training t=20692, loss=0.026041788049042225\n",
      "Surface training t=20693, loss=0.041055142879486084\n",
      "Surface training t=20694, loss=0.029037308879196644\n",
      "Surface training t=20695, loss=0.01862485520541668\n",
      "Surface training t=20696, loss=0.02555990032851696\n",
      "Surface training t=20697, loss=0.02019003964960575\n",
      "Surface training t=20698, loss=0.019615447148680687\n",
      "Surface training t=20699, loss=0.02552153542637825\n",
      "Surface training t=20700, loss=0.01771371439099312\n",
      "Surface training t=20701, loss=0.02642889227718115\n",
      "Surface training t=20702, loss=0.031181510537862778\n",
      "Surface training t=20703, loss=0.020067522302269936\n",
      "Surface training t=20704, loss=0.02737988531589508\n",
      "Surface training t=20705, loss=0.024887307547032833\n",
      "Surface training t=20706, loss=0.022780545987188816\n",
      "Surface training t=20707, loss=0.02237193752080202\n",
      "Surface training t=20708, loss=0.025710160844027996\n",
      "Surface training t=20709, loss=0.028540014289319515\n",
      "Surface training t=20710, loss=0.02634057216346264\n",
      "Surface training t=20711, loss=0.025478621944785118\n",
      "Surface training t=20712, loss=0.022877161391079426\n",
      "Surface training t=20713, loss=0.033913166262209415\n",
      "Surface training t=20714, loss=0.030797562561929226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=20715, loss=0.03216402232646942\n",
      "Surface training t=20716, loss=0.03305526450276375\n",
      "Surface training t=20717, loss=0.02928946167230606\n",
      "Surface training t=20718, loss=0.018947805278003216\n",
      "Surface training t=20719, loss=0.021223132498562336\n",
      "Surface training t=20720, loss=0.022281911224126816\n",
      "Surface training t=20721, loss=0.02253139764070511\n",
      "Surface training t=20722, loss=0.021804079413414\n",
      "Surface training t=20723, loss=0.01943944115191698\n",
      "Surface training t=20724, loss=0.025356893427670002\n",
      "Surface training t=20725, loss=0.02765277959406376\n",
      "Surface training t=20726, loss=0.03483089804649353\n",
      "Surface training t=20727, loss=0.032016571611166\n",
      "Surface training t=20728, loss=0.036178089678287506\n",
      "Surface training t=20729, loss=0.03938571177423\n",
      "Surface training t=20730, loss=0.03169005922973156\n",
      "Surface training t=20731, loss=0.031937780790030956\n",
      "Surface training t=20732, loss=0.02484313864260912\n",
      "Surface training t=20733, loss=0.027461311779916286\n",
      "Surface training t=20734, loss=0.030116306617856026\n",
      "Surface training t=20735, loss=0.02426969725638628\n",
      "Surface training t=20736, loss=0.029675652272999287\n",
      "Surface training t=20737, loss=0.031744321808218956\n",
      "Surface training t=20738, loss=0.02712931390851736\n",
      "Surface training t=20739, loss=0.025108080357313156\n",
      "Surface training t=20740, loss=0.031450262293219566\n",
      "Surface training t=20741, loss=0.03317440673708916\n",
      "Surface training t=20742, loss=0.03049117885529995\n",
      "Surface training t=20743, loss=0.03363278601318598\n",
      "Surface training t=20744, loss=0.03726934641599655\n",
      "Surface training t=20745, loss=0.042140161618590355\n",
      "Surface training t=20746, loss=0.024744276888668537\n",
      "Surface training t=20747, loss=0.017109776847064495\n",
      "Surface training t=20748, loss=0.021464689634740353\n",
      "Surface training t=20749, loss=0.021693005692213774\n",
      "Surface training t=20750, loss=0.024064429104328156\n",
      "Surface training t=20751, loss=0.026006446219980717\n",
      "Surface training t=20752, loss=0.026043754070997238\n",
      "Surface training t=20753, loss=0.020504246465861797\n",
      "Surface training t=20754, loss=0.024055308662354946\n",
      "Surface training t=20755, loss=0.023634405806660652\n",
      "Surface training t=20756, loss=0.02715793438255787\n",
      "Surface training t=20757, loss=0.021318349987268448\n",
      "Surface training t=20758, loss=0.01889317436143756\n",
      "Surface training t=20759, loss=0.020015298388898373\n",
      "Surface training t=20760, loss=0.02934932243078947\n",
      "Surface training t=20761, loss=0.03129446506500244\n",
      "Surface training t=20762, loss=0.04074283875524998\n",
      "Surface training t=20763, loss=0.020507211796939373\n",
      "Surface training t=20764, loss=0.021213544066995382\n",
      "Surface training t=20765, loss=0.022486278787255287\n",
      "Surface training t=20766, loss=0.020757772959768772\n",
      "Surface training t=20767, loss=0.02052378561347723\n",
      "Surface training t=20768, loss=0.02162877144291997\n",
      "Surface training t=20769, loss=0.026446864940226078\n",
      "Surface training t=20770, loss=0.025641479529440403\n",
      "Surface training t=20771, loss=0.027749430388212204\n",
      "Surface training t=20772, loss=0.020536575466394424\n",
      "Surface training t=20773, loss=0.02171298209577799\n",
      "Surface training t=20774, loss=0.01831859163939953\n",
      "Surface training t=20775, loss=0.021115477196872234\n",
      "Surface training t=20776, loss=0.029863807372748852\n",
      "Surface training t=20777, loss=0.025736669078469276\n",
      "Surface training t=20778, loss=0.023724062368273735\n",
      "Surface training t=20779, loss=0.018908752128481865\n",
      "Surface training t=20780, loss=0.01883106492459774\n",
      "Surface training t=20781, loss=0.02428403589874506\n",
      "Surface training t=20782, loss=0.02471337281167507\n",
      "Surface training t=20783, loss=0.030081331729888916\n",
      "Surface training t=20784, loss=0.02043346967548132\n",
      "Surface training t=20785, loss=0.020649641752243042\n",
      "Surface training t=20786, loss=0.01991023402661085\n",
      "Surface training t=20787, loss=0.021644411608576775\n",
      "Surface training t=20788, loss=0.04281949624419212\n",
      "Surface training t=20789, loss=0.02979943063110113\n",
      "Surface training t=20790, loss=0.027449453249573708\n",
      "Surface training t=20791, loss=0.02407877705991268\n",
      "Surface training t=20792, loss=0.02607161272317171\n",
      "Surface training t=20793, loss=0.029780450277030468\n",
      "Surface training t=20794, loss=0.02871676441282034\n",
      "Surface training t=20795, loss=0.024607861414551735\n",
      "Surface training t=20796, loss=0.021776482462882996\n",
      "Surface training t=20797, loss=0.024472219869494438\n",
      "Surface training t=20798, loss=0.024466448463499546\n",
      "Surface training t=20799, loss=0.02833949401974678\n",
      "Surface training t=20800, loss=0.026888689026236534\n",
      "Surface training t=20801, loss=0.024729772470891476\n",
      "Surface training t=20802, loss=0.028692644089460373\n",
      "Surface training t=20803, loss=0.035697681829333305\n",
      "Surface training t=20804, loss=0.040980011224746704\n",
      "Surface training t=20805, loss=0.03165839798748493\n",
      "Surface training t=20806, loss=0.03451740834861994\n",
      "Surface training t=20807, loss=0.028415916487574577\n",
      "Surface training t=20808, loss=0.03167187329381704\n",
      "Surface training t=20809, loss=0.028451007790863514\n",
      "Surface training t=20810, loss=0.026135762222111225\n",
      "Surface training t=20811, loss=0.045116448774933815\n",
      "Surface training t=20812, loss=0.027472447603940964\n",
      "Surface training t=20813, loss=0.027021522633731365\n",
      "Surface training t=20814, loss=0.03253590315580368\n",
      "Surface training t=20815, loss=0.03078855387866497\n",
      "Surface training t=20816, loss=0.02925784420222044\n",
      "Surface training t=20817, loss=0.026409713551402092\n",
      "Surface training t=20818, loss=0.03032063879072666\n",
      "Surface training t=20819, loss=0.025699716061353683\n",
      "Surface training t=20820, loss=0.03256985079497099\n",
      "Surface training t=20821, loss=0.02674282155930996\n",
      "Surface training t=20822, loss=0.02520196046680212\n",
      "Surface training t=20823, loss=0.03031967394053936\n",
      "Surface training t=20824, loss=0.03739828243851662\n",
      "Surface training t=20825, loss=0.03094286099076271\n",
      "Surface training t=20826, loss=0.04089629836380482\n",
      "Surface training t=20827, loss=0.03167560324072838\n",
      "Surface training t=20828, loss=0.024622402153909206\n",
      "Surface training t=20829, loss=0.03445508796721697\n",
      "Surface training t=20830, loss=0.026420247741043568\n",
      "Surface training t=20831, loss=0.0383475236594677\n",
      "Surface training t=20832, loss=0.03462680149823427\n",
      "Surface training t=20833, loss=0.036130091175436974\n",
      "Surface training t=20834, loss=0.028718091547489166\n",
      "Surface training t=20835, loss=0.028851831331849098\n",
      "Surface training t=20836, loss=0.030713072046637535\n",
      "Surface training t=20837, loss=0.03156670555472374\n",
      "Surface training t=20838, loss=0.03228737972676754\n",
      "Surface training t=20839, loss=0.04075850732624531\n",
      "Surface training t=20840, loss=0.033099555876106024\n",
      "Surface training t=20841, loss=0.03683118801563978\n",
      "Surface training t=20842, loss=0.05715177208185196\n",
      "Surface training t=20843, loss=0.0474043358117342\n",
      "Surface training t=20844, loss=0.05333239957690239\n",
      "Surface training t=20845, loss=0.056698767468333244\n",
      "Surface training t=20846, loss=0.042063500732183456\n",
      "Surface training t=20847, loss=0.050249191001057625\n",
      "Surface training t=20848, loss=0.04263758659362793\n",
      "Surface training t=20849, loss=0.032640259712934494\n",
      "Surface training t=20850, loss=0.03419610671699047\n",
      "Surface training t=20851, loss=0.03237161412835121\n",
      "Surface training t=20852, loss=0.026759304106235504\n",
      "Surface training t=20853, loss=0.028745386749505997\n",
      "Surface training t=20854, loss=0.028282051905989647\n",
      "Surface training t=20855, loss=0.021260748617351055\n",
      "Surface training t=20856, loss=0.0231617521494627\n",
      "Surface training t=20857, loss=0.01944195106625557\n",
      "Surface training t=20858, loss=0.01862121745944023\n",
      "Surface training t=20859, loss=0.026444262824952602\n",
      "Surface training t=20860, loss=0.02406415529549122\n",
      "Surface training t=20861, loss=0.026171665638685226\n",
      "Surface training t=20862, loss=0.03973759524524212\n",
      "Surface training t=20863, loss=0.029376329854130745\n",
      "Surface training t=20864, loss=0.03541799634695053\n",
      "Surface training t=20865, loss=0.03428763896226883\n",
      "Surface training t=20866, loss=0.025736900977790356\n",
      "Surface training t=20867, loss=0.0245898412540555\n",
      "Surface training t=20868, loss=0.02223710436373949\n",
      "Surface training t=20869, loss=0.024366140365600586\n",
      "Surface training t=20870, loss=0.024876183830201626\n",
      "Surface training t=20871, loss=0.03080084268003702\n",
      "Surface training t=20872, loss=0.02535046823322773\n",
      "Surface training t=20873, loss=0.030386989936232567\n",
      "Surface training t=20874, loss=0.02520565502345562\n",
      "Surface training t=20875, loss=0.021449347026646137\n",
      "Surface training t=20876, loss=0.02053904440253973\n",
      "Surface training t=20877, loss=0.02681146375834942\n",
      "Surface training t=20878, loss=0.020127519965171814\n",
      "Surface training t=20879, loss=0.02347178291529417\n",
      "Surface training t=20880, loss=0.02923002280294895\n",
      "Surface training t=20881, loss=0.03030510526150465\n",
      "Surface training t=20882, loss=0.025696461088955402\n",
      "Surface training t=20883, loss=0.03149927593767643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=20884, loss=0.026646414771676064\n",
      "Surface training t=20885, loss=0.031570544466376305\n",
      "Surface training t=20886, loss=0.03184364549815655\n",
      "Surface training t=20887, loss=0.027632676996290684\n",
      "Surface training t=20888, loss=0.022828884422779083\n",
      "Surface training t=20889, loss=0.02008148282766342\n",
      "Surface training t=20890, loss=0.01870668213814497\n",
      "Surface training t=20891, loss=0.02172080148011446\n",
      "Surface training t=20892, loss=0.01682224916294217\n",
      "Surface training t=20893, loss=0.019463942386209965\n",
      "Surface training t=20894, loss=0.019789794459939003\n",
      "Surface training t=20895, loss=0.0188770922832191\n",
      "Surface training t=20896, loss=0.02276946511119604\n",
      "Surface training t=20897, loss=0.02768571861088276\n",
      "Surface training t=20898, loss=0.025326523929834366\n",
      "Surface training t=20899, loss=0.02495846524834633\n",
      "Surface training t=20900, loss=0.02025972120463848\n",
      "Surface training t=20901, loss=0.01775606069713831\n",
      "Surface training t=20902, loss=0.020466658286750317\n",
      "Surface training t=20903, loss=0.031364623457193375\n",
      "Surface training t=20904, loss=0.023269430734217167\n",
      "Surface training t=20905, loss=0.032232675701379776\n",
      "Surface training t=20906, loss=0.039407266303896904\n",
      "Surface training t=20907, loss=0.03404393047094345\n",
      "Surface training t=20908, loss=0.03987481631338596\n",
      "Surface training t=20909, loss=0.031689366325736046\n",
      "Surface training t=20910, loss=0.046474775299429893\n",
      "Surface training t=20911, loss=0.030004380270838737\n",
      "Surface training t=20912, loss=0.033948528580367565\n",
      "Surface training t=20913, loss=0.03353084158152342\n",
      "Surface training t=20914, loss=0.028794651851058006\n",
      "Surface training t=20915, loss=0.03218280151486397\n",
      "Surface training t=20916, loss=0.03262628335505724\n",
      "Surface training t=20917, loss=0.03450444154441357\n",
      "Surface training t=20918, loss=0.025066151283681393\n",
      "Surface training t=20919, loss=0.04760670103132725\n",
      "Surface training t=20920, loss=0.03654247894883156\n",
      "Surface training t=20921, loss=0.03957895375788212\n",
      "Surface training t=20922, loss=0.028039241209626198\n",
      "Surface training t=20923, loss=0.027447648346424103\n",
      "Surface training t=20924, loss=0.033381205052137375\n",
      "Surface training t=20925, loss=0.031730709597468376\n",
      "Surface training t=20926, loss=0.03617124259471893\n",
      "Surface training t=20927, loss=0.034046513959765434\n",
      "Surface training t=20928, loss=0.03365215193480253\n",
      "Surface training t=20929, loss=0.029759394004940987\n",
      "Surface training t=20930, loss=0.022900931537151337\n",
      "Surface training t=20931, loss=0.03465473745018244\n",
      "Surface training t=20932, loss=0.030543294735252857\n",
      "Surface training t=20933, loss=0.023177954368293285\n",
      "Surface training t=20934, loss=0.02028606366366148\n",
      "Surface training t=20935, loss=0.024003885686397552\n",
      "Surface training t=20936, loss=0.0252751549705863\n",
      "Surface training t=20937, loss=0.026678942143917084\n",
      "Surface training t=20938, loss=0.025353585369884968\n",
      "Surface training t=20939, loss=0.028774306178092957\n",
      "Surface training t=20940, loss=0.018590310588479042\n",
      "Surface training t=20941, loss=0.018784047104418278\n",
      "Surface training t=20942, loss=0.01951674558222294\n",
      "Surface training t=20943, loss=0.022158904932439327\n",
      "Surface training t=20944, loss=0.022124996408820152\n",
      "Surface training t=20945, loss=0.02778130117803812\n",
      "Surface training t=20946, loss=0.027455279603600502\n",
      "Surface training t=20947, loss=0.02388747315853834\n",
      "Surface training t=20948, loss=0.016877504996955395\n",
      "Surface training t=20949, loss=0.025474986992776394\n",
      "Surface training t=20950, loss=0.023916004225611687\n",
      "Surface training t=20951, loss=0.023117284290492535\n",
      "Surface training t=20952, loss=0.029085250571370125\n",
      "Surface training t=20953, loss=0.026762887835502625\n",
      "Surface training t=20954, loss=0.023023759946227074\n",
      "Surface training t=20955, loss=0.03828777000308037\n",
      "Surface training t=20956, loss=0.03099399246275425\n",
      "Surface training t=20957, loss=0.028864210471510887\n",
      "Surface training t=20958, loss=0.029615866020321846\n",
      "Surface training t=20959, loss=0.026488805189728737\n",
      "Surface training t=20960, loss=0.0256817564368248\n",
      "Surface training t=20961, loss=0.023544834926724434\n",
      "Surface training t=20962, loss=0.027903683483600616\n",
      "Surface training t=20963, loss=0.027906115166842937\n",
      "Surface training t=20964, loss=0.024660021997988224\n",
      "Surface training t=20965, loss=0.028900972567498684\n",
      "Surface training t=20966, loss=0.0306490371003747\n",
      "Surface training t=20967, loss=0.027441115118563175\n",
      "Surface training t=20968, loss=0.02464826311916113\n",
      "Surface training t=20969, loss=0.02147500915452838\n",
      "Surface training t=20970, loss=0.0233149491250515\n",
      "Surface training t=20971, loss=0.022696238942444324\n",
      "Surface training t=20972, loss=0.01967035699635744\n",
      "Surface training t=20973, loss=0.02836617548018694\n",
      "Surface training t=20974, loss=0.031265187077224255\n",
      "Surface training t=20975, loss=0.0246169688180089\n",
      "Surface training t=20976, loss=0.029334666207432747\n",
      "Surface training t=20977, loss=0.021046594716608524\n",
      "Surface training t=20978, loss=0.0396423302590847\n",
      "Surface training t=20979, loss=0.03361790254712105\n",
      "Surface training t=20980, loss=0.043896911665797234\n",
      "Surface training t=20981, loss=0.03894796967506409\n",
      "Surface training t=20982, loss=0.032064288854599\n",
      "Surface training t=20983, loss=0.03684722259640694\n",
      "Surface training t=20984, loss=0.030268486589193344\n",
      "Surface training t=20985, loss=0.03314049728214741\n",
      "Surface training t=20986, loss=0.036349330097436905\n",
      "Surface training t=20987, loss=0.0391242690384388\n",
      "Surface training t=20988, loss=0.035471558570861816\n",
      "Surface training t=20989, loss=0.038196017034351826\n",
      "Surface training t=20990, loss=0.060002025216817856\n",
      "Surface training t=20991, loss=0.0451786033809185\n",
      "Surface training t=20992, loss=0.05687698908150196\n",
      "Surface training t=20993, loss=0.051311070099473\n",
      "Surface training t=20994, loss=0.040492646396160126\n",
      "Surface training t=20995, loss=0.036154464818537235\n",
      "Surface training t=20996, loss=0.048866888508200645\n",
      "Surface training t=20997, loss=0.03665111027657986\n",
      "Surface training t=20998, loss=0.036302377469837666\n",
      "Surface training t=20999, loss=0.04900567606091499\n",
      "Surface training t=21000, loss=0.03731474652886391\n",
      "Surface training t=21001, loss=0.03544512763619423\n",
      "Surface training t=21002, loss=0.05083620920777321\n",
      "Surface training t=21003, loss=0.04518536292016506\n",
      "Surface training t=21004, loss=0.03289706166833639\n",
      "Surface training t=21005, loss=0.036224303767085075\n",
      "Surface training t=21006, loss=0.03964037820696831\n",
      "Surface training t=21007, loss=0.03548740595579147\n",
      "Surface training t=21008, loss=0.02672354131937027\n",
      "Surface training t=21009, loss=0.027028420940041542\n",
      "Surface training t=21010, loss=0.03062563482671976\n",
      "Surface training t=21011, loss=0.030257517471909523\n",
      "Surface training t=21012, loss=0.03104264196008444\n",
      "Surface training t=21013, loss=0.04765316843986511\n",
      "Surface training t=21014, loss=0.03952419012784958\n",
      "Surface training t=21015, loss=0.038392962887883186\n",
      "Surface training t=21016, loss=0.043871862813830376\n",
      "Surface training t=21017, loss=0.04566829279065132\n",
      "Surface training t=21018, loss=0.03911098465323448\n",
      "Surface training t=21019, loss=0.03824386186897755\n",
      "Surface training t=21020, loss=0.03836130537092686\n",
      "Surface training t=21021, loss=0.04201751947402954\n",
      "Surface training t=21022, loss=0.03716679569333792\n",
      "Surface training t=21023, loss=0.036530278623104095\n",
      "Surface training t=21024, loss=0.03783584386110306\n",
      "Surface training t=21025, loss=0.04485098458826542\n",
      "Surface training t=21026, loss=0.03837111406028271\n",
      "Surface training t=21027, loss=0.06323586590588093\n",
      "Surface training t=21028, loss=0.04317517206072807\n",
      "Surface training t=21029, loss=0.03971880488097668\n",
      "Surface training t=21030, loss=0.038666306994855404\n",
      "Surface training t=21031, loss=0.04463542439043522\n",
      "Surface training t=21032, loss=0.03383772447705269\n",
      "Surface training t=21033, loss=0.040186066180467606\n",
      "Surface training t=21034, loss=0.035911801271140575\n",
      "Surface training t=21035, loss=0.04918799176812172\n",
      "Surface training t=21036, loss=0.03579124994575977\n",
      "Surface training t=21037, loss=0.04238861799240112\n",
      "Surface training t=21038, loss=0.03512124344706535\n",
      "Surface training t=21039, loss=0.035043977200984955\n",
      "Surface training t=21040, loss=0.03316319826990366\n",
      "Surface training t=21041, loss=0.028566349763423204\n",
      "Surface training t=21042, loss=0.037989942356944084\n",
      "Surface training t=21043, loss=0.03662245534360409\n",
      "Surface training t=21044, loss=0.024055088870227337\n",
      "Surface training t=21045, loss=0.028539453633129597\n",
      "Surface training t=21046, loss=0.03205419331789017\n",
      "Surface training t=21047, loss=0.02663896232843399\n",
      "Surface training t=21048, loss=0.030729865655303\n",
      "Surface training t=21049, loss=0.029278995469212532\n",
      "Surface training t=21050, loss=0.02666336204856634\n",
      "Surface training t=21051, loss=0.029864849522709846\n",
      "Surface training t=21052, loss=0.02339557744562626\n",
      "Surface training t=21053, loss=0.02309716586023569\n",
      "Surface training t=21054, loss=0.031906998716294765\n",
      "Surface training t=21055, loss=0.02732210699468851\n",
      "Surface training t=21056, loss=0.024386974051594734\n",
      "Surface training t=21057, loss=0.029974620789289474\n",
      "Surface training t=21058, loss=0.020201578736305237\n",
      "Surface training t=21059, loss=0.0245660487562418\n",
      "Surface training t=21060, loss=0.0235163401812315\n",
      "Surface training t=21061, loss=0.020708266645669937\n",
      "Surface training t=21062, loss=0.02969791367650032\n",
      "Surface training t=21063, loss=0.03297042101621628\n",
      "Surface training t=21064, loss=0.038439213298261166\n",
      "Surface training t=21065, loss=0.03608609549701214\n",
      "Surface training t=21066, loss=0.03774411045014858\n",
      "Surface training t=21067, loss=0.046468159183859825\n",
      "Surface training t=21068, loss=0.030194892548024654\n",
      "Surface training t=21069, loss=0.03581838123500347\n",
      "Surface training t=21070, loss=0.03137221932411194\n",
      "Surface training t=21071, loss=0.03364155441522598\n",
      "Surface training t=21072, loss=0.051899541169404984\n",
      "Surface training t=21073, loss=0.03818534221500158\n",
      "Surface training t=21074, loss=0.04560122452676296\n",
      "Surface training t=21075, loss=0.04326883517205715\n",
      "Surface training t=21076, loss=0.027875246480107307\n",
      "Surface training t=21077, loss=0.021216796711087227\n",
      "Surface training t=21078, loss=0.02223571389913559\n",
      "Surface training t=21079, loss=0.022174875251948833\n",
      "Surface training t=21080, loss=0.023017769679427147\n",
      "Surface training t=21081, loss=0.028493568301200867\n",
      "Surface training t=21082, loss=0.02934955805540085\n",
      "Surface training t=21083, loss=0.03294860105961561\n",
      "Surface training t=21084, loss=0.029722084291279316\n",
      "Surface training t=21085, loss=0.03151035029441118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=21086, loss=0.02851267158985138\n",
      "Surface training t=21087, loss=0.02347736805677414\n",
      "Surface training t=21088, loss=0.021423927508294582\n",
      "Surface training t=21089, loss=0.011900193989276886\n",
      "Surface training t=21090, loss=0.021494422107934952\n",
      "Surface training t=21091, loss=0.027599562890827656\n",
      "Surface training t=21092, loss=0.02418349590152502\n",
      "Surface training t=21093, loss=0.04279728792607784\n",
      "Surface training t=21094, loss=0.033435091376304626\n",
      "Surface training t=21095, loss=0.04125528782606125\n",
      "Surface training t=21096, loss=0.03517073392868042\n",
      "Surface training t=21097, loss=0.029130362905561924\n",
      "Surface training t=21098, loss=0.0355890691280365\n",
      "Surface training t=21099, loss=0.026581769809126854\n",
      "Surface training t=21100, loss=0.03924156352877617\n",
      "Surface training t=21101, loss=0.028420340269804\n",
      "Surface training t=21102, loss=0.0250373687595129\n",
      "Surface training t=21103, loss=0.034556543454527855\n",
      "Surface training t=21104, loss=0.027215597219765186\n",
      "Surface training t=21105, loss=0.026454242877662182\n",
      "Surface training t=21106, loss=0.031049540266394615\n",
      "Surface training t=21107, loss=0.0246507590636611\n",
      "Surface training t=21108, loss=0.022660814225673676\n",
      "Surface training t=21109, loss=0.0288744755089283\n",
      "Surface training t=21110, loss=0.0210021510720253\n",
      "Surface training t=21111, loss=0.022075394168496132\n",
      "Surface training t=21112, loss=0.02307989075779915\n",
      "Surface training t=21113, loss=0.018201232887804508\n",
      "Surface training t=21114, loss=0.016272657550871372\n",
      "Surface training t=21115, loss=0.021182444877922535\n",
      "Surface training t=21116, loss=0.02503401692956686\n",
      "Surface training t=21117, loss=0.020811681635677814\n",
      "Surface training t=21118, loss=0.02109692431986332\n",
      "Surface training t=21119, loss=0.017929512076079845\n",
      "Surface training t=21120, loss=0.022632308304309845\n",
      "Surface training t=21121, loss=0.025131159462034702\n",
      "Surface training t=21122, loss=0.02138747274875641\n",
      "Surface training t=21123, loss=0.02791146282106638\n",
      "Surface training t=21124, loss=0.023746307007968426\n",
      "Surface training t=21125, loss=0.03462986461818218\n",
      "Surface training t=21126, loss=0.024840330705046654\n",
      "Surface training t=21127, loss=0.031377341598272324\n",
      "Surface training t=21128, loss=0.031085627153515816\n",
      "Surface training t=21129, loss=0.027398458682000637\n",
      "Surface training t=21130, loss=0.028075644746422768\n",
      "Surface training t=21131, loss=0.02868152502924204\n",
      "Surface training t=21132, loss=0.021960768848657608\n",
      "Surface training t=21133, loss=0.025331432931125164\n",
      "Surface training t=21134, loss=0.026185990311205387\n",
      "Surface training t=21135, loss=0.03234447352588177\n",
      "Surface training t=21136, loss=0.026186853647232056\n",
      "Surface training t=21137, loss=0.03203666117042303\n",
      "Surface training t=21138, loss=0.03679118864238262\n",
      "Surface training t=21139, loss=0.02461963426321745\n",
      "Surface training t=21140, loss=0.028015590272843838\n",
      "Surface training t=21141, loss=0.03166778851300478\n",
      "Surface training t=21142, loss=0.02467846218496561\n",
      "Surface training t=21143, loss=0.028315644711256027\n",
      "Surface training t=21144, loss=0.034047143533825874\n",
      "Surface training t=21145, loss=0.02429136447608471\n",
      "Surface training t=21146, loss=0.020772553980350494\n",
      "Surface training t=21147, loss=0.023548833094537258\n",
      "Surface training t=21148, loss=0.026283694431185722\n",
      "Surface training t=21149, loss=0.022877386771142483\n",
      "Surface training t=21150, loss=0.023280435241758823\n",
      "Surface training t=21151, loss=0.021722872741520405\n",
      "Surface training t=21152, loss=0.020947732962667942\n",
      "Surface training t=21153, loss=0.02219986915588379\n",
      "Surface training t=21154, loss=0.020572765730321407\n",
      "Surface training t=21155, loss=0.024941200390458107\n",
      "Surface training t=21156, loss=0.024411235004663467\n",
      "Surface training t=21157, loss=0.022958562709391117\n",
      "Surface training t=21158, loss=0.027601192705333233\n",
      "Surface training t=21159, loss=0.032631890848279\n",
      "Surface training t=21160, loss=0.030382728204131126\n",
      "Surface training t=21161, loss=0.029448647052049637\n",
      "Surface training t=21162, loss=0.02036145981401205\n",
      "Surface training t=21163, loss=0.01888793334364891\n",
      "Surface training t=21164, loss=0.019394180737435818\n",
      "Surface training t=21165, loss=0.02843465469777584\n",
      "Surface training t=21166, loss=0.030280272476375103\n",
      "Surface training t=21167, loss=0.025644640438258648\n",
      "Surface training t=21168, loss=0.02726569212973118\n",
      "Surface training t=21169, loss=0.024463624693453312\n",
      "Surface training t=21170, loss=0.02189505845308304\n",
      "Surface training t=21171, loss=0.02236600499600172\n",
      "Surface training t=21172, loss=0.025405202992260456\n",
      "Surface training t=21173, loss=0.018660059198737144\n",
      "Surface training t=21174, loss=0.02751045674085617\n",
      "Surface training t=21175, loss=0.021249043755233288\n",
      "Surface training t=21176, loss=0.020956589840352535\n",
      "Surface training t=21177, loss=0.02087627863511443\n",
      "Surface training t=21178, loss=0.023592377081513405\n",
      "Surface training t=21179, loss=0.02043743897229433\n",
      "Surface training t=21180, loss=0.021476924419403076\n",
      "Surface training t=21181, loss=0.0242149131372571\n",
      "Surface training t=21182, loss=0.024322797544300556\n",
      "Surface training t=21183, loss=0.023335623554885387\n",
      "Surface training t=21184, loss=0.02491065114736557\n",
      "Surface training t=21185, loss=0.02201333176344633\n",
      "Surface training t=21186, loss=0.017759538255631924\n",
      "Surface training t=21187, loss=0.019341816194355488\n",
      "Surface training t=21188, loss=0.017840202897787094\n",
      "Surface training t=21189, loss=0.019530734978616238\n",
      "Surface training t=21190, loss=0.024378289468586445\n",
      "Surface training t=21191, loss=0.05191126465797424\n",
      "Surface training t=21192, loss=0.04127870965749025\n",
      "Surface training t=21193, loss=0.035875190049409866\n",
      "Surface training t=21194, loss=0.04126736521720886\n",
      "Surface training t=21195, loss=0.03292288910597563\n",
      "Surface training t=21196, loss=0.025812702253460884\n",
      "Surface training t=21197, loss=0.030982056632637978\n",
      "Surface training t=21198, loss=0.029350082390010357\n",
      "Surface training t=21199, loss=0.03127561882138252\n",
      "Surface training t=21200, loss=0.022800342179834843\n",
      "Surface training t=21201, loss=0.02716340310871601\n",
      "Surface training t=21202, loss=0.026337914168834686\n",
      "Surface training t=21203, loss=0.025601886212825775\n",
      "Surface training t=21204, loss=0.02815163880586624\n",
      "Surface training t=21205, loss=0.028496908023953438\n",
      "Surface training t=21206, loss=0.02878706343472004\n",
      "Surface training t=21207, loss=0.03549780510365963\n",
      "Surface training t=21208, loss=0.03620602376759052\n",
      "Surface training t=21209, loss=0.026443741284310818\n",
      "Surface training t=21210, loss=0.02344083972275257\n",
      "Surface training t=21211, loss=0.021132142283022404\n",
      "Surface training t=21212, loss=0.0215193759649992\n",
      "Surface training t=21213, loss=0.021035542711615562\n",
      "Surface training t=21214, loss=0.032533567398786545\n",
      "Surface training t=21215, loss=0.029006773605942726\n",
      "Surface training t=21216, loss=0.028557716868817806\n",
      "Surface training t=21217, loss=0.020174052100628614\n",
      "Surface training t=21218, loss=0.016668779775500298\n",
      "Surface training t=21219, loss=0.015099091455340385\n",
      "Surface training t=21220, loss=0.02096129208803177\n",
      "Surface training t=21221, loss=0.021119918674230576\n",
      "Surface training t=21222, loss=0.0265206815674901\n",
      "Surface training t=21223, loss=0.022282710298895836\n",
      "Surface training t=21224, loss=0.01927876938134432\n",
      "Surface training t=21225, loss=0.022303403355181217\n",
      "Surface training t=21226, loss=0.024375864304602146\n",
      "Surface training t=21227, loss=0.026481923647224903\n",
      "Surface training t=21228, loss=0.02360923122614622\n",
      "Surface training t=21229, loss=0.018962460570037365\n",
      "Surface training t=21230, loss=0.020936154760420322\n",
      "Surface training t=21231, loss=0.028795539401471615\n",
      "Surface training t=21232, loss=0.02413418795913458\n",
      "Surface training t=21233, loss=0.022336901165544987\n",
      "Surface training t=21234, loss=0.0249814810231328\n",
      "Surface training t=21235, loss=0.018225345760583878\n",
      "Surface training t=21236, loss=0.017566371709108353\n",
      "Surface training t=21237, loss=0.014132903423160315\n",
      "Surface training t=21238, loss=0.022814552299678326\n",
      "Surface training t=21239, loss=0.023646327666938305\n",
      "Surface training t=21240, loss=0.025318571366369724\n",
      "Surface training t=21241, loss=0.025036984123289585\n",
      "Surface training t=21242, loss=0.026366745121777058\n",
      "Surface training t=21243, loss=0.026617166586220264\n",
      "Surface training t=21244, loss=0.024222341366112232\n",
      "Surface training t=21245, loss=0.027074899524450302\n",
      "Surface training t=21246, loss=0.02559935813769698\n",
      "Surface training t=21247, loss=0.025389382615685463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=21248, loss=0.025202177464962006\n",
      "Surface training t=21249, loss=0.041814396157860756\n",
      "Surface training t=21250, loss=0.036826951429247856\n",
      "Surface training t=21251, loss=0.030024860985577106\n",
      "Surface training t=21252, loss=0.0309679564088583\n",
      "Surface training t=21253, loss=0.033722284249961376\n",
      "Surface training t=21254, loss=0.03606336936354637\n",
      "Surface training t=21255, loss=0.02547960914671421\n",
      "Surface training t=21256, loss=0.03505334071815014\n",
      "Surface training t=21257, loss=0.028345118276774883\n",
      "Surface training t=21258, loss=0.032305581495165825\n",
      "Surface training t=21259, loss=0.031811388209462166\n",
      "Surface training t=21260, loss=0.020210236310958862\n",
      "Surface training t=21261, loss=0.03268925100564957\n",
      "Surface training t=21262, loss=0.02865532413125038\n",
      "Surface training t=21263, loss=0.031233718618750572\n",
      "Surface training t=21264, loss=0.027567433193325996\n",
      "Surface training t=21265, loss=0.02909621875733137\n",
      "Surface training t=21266, loss=0.037277158349752426\n",
      "Surface training t=21267, loss=0.03847863711416721\n",
      "Surface training t=21268, loss=0.029391005635261536\n",
      "Surface training t=21269, loss=0.03218562062829733\n",
      "Surface training t=21270, loss=0.024621364660561085\n",
      "Surface training t=21271, loss=0.031475807540118694\n",
      "Surface training t=21272, loss=0.03392752446234226\n",
      "Surface training t=21273, loss=0.029535888694226742\n",
      "Surface training t=21274, loss=0.02277699112892151\n",
      "Surface training t=21275, loss=0.022246235981583595\n",
      "Surface training t=21276, loss=0.02849073614925146\n",
      "Surface training t=21277, loss=0.024241674691438675\n",
      "Surface training t=21278, loss=0.01986080314964056\n",
      "Surface training t=21279, loss=0.020815186202526093\n",
      "Surface training t=21280, loss=0.016189563553780317\n",
      "Surface training t=21281, loss=0.014676582533866167\n",
      "Surface training t=21282, loss=0.014719734899699688\n",
      "Surface training t=21283, loss=0.020191731862723827\n",
      "Surface training t=21284, loss=0.019423277117311954\n",
      "Surface training t=21285, loss=0.017721005715429783\n",
      "Surface training t=21286, loss=0.019087331369519234\n",
      "Surface training t=21287, loss=0.01838372927159071\n",
      "Surface training t=21288, loss=0.020701966248452663\n",
      "Surface training t=21289, loss=0.02742917463183403\n",
      "Surface training t=21290, loss=0.0231482470408082\n",
      "Surface training t=21291, loss=0.021373441442847252\n",
      "Surface training t=21292, loss=0.026933856308460236\n",
      "Surface training t=21293, loss=0.020614569075405598\n",
      "Surface training t=21294, loss=0.02398943342268467\n",
      "Surface training t=21295, loss=0.03337348997592926\n",
      "Surface training t=21296, loss=0.02482431475073099\n",
      "Surface training t=21297, loss=0.02731137163937092\n",
      "Surface training t=21298, loss=0.02627119980752468\n",
      "Surface training t=21299, loss=0.02088580373674631\n",
      "Surface training t=21300, loss=0.026229550130665302\n",
      "Surface training t=21301, loss=0.022276089526712894\n",
      "Surface training t=21302, loss=0.026806795969605446\n",
      "Surface training t=21303, loss=0.02143879234790802\n",
      "Surface training t=21304, loss=0.02034169342368841\n",
      "Surface training t=21305, loss=0.02037130482494831\n",
      "Surface training t=21306, loss=0.021290499716997147\n",
      "Surface training t=21307, loss=0.02980750985443592\n",
      "Surface training t=21308, loss=0.034371353685855865\n",
      "Surface training t=21309, loss=0.03968701884150505\n",
      "Surface training t=21310, loss=0.030240400694310665\n",
      "Surface training t=21311, loss=0.039231352508068085\n",
      "Surface training t=21312, loss=0.035490863025188446\n",
      "Surface training t=21313, loss=0.020427390933036804\n",
      "Surface training t=21314, loss=0.029488214291632175\n",
      "Surface training t=21315, loss=0.022375376895070076\n",
      "Surface training t=21316, loss=0.028984902426600456\n",
      "Surface training t=21317, loss=0.024693419225513935\n",
      "Surface training t=21318, loss=0.028373087756335735\n",
      "Surface training t=21319, loss=0.027461374178528786\n",
      "Surface training t=21320, loss=0.025923624634742737\n",
      "Surface training t=21321, loss=0.03211577143520117\n",
      "Surface training t=21322, loss=0.02312405500560999\n",
      "Surface training t=21323, loss=0.023352299816906452\n",
      "Surface training t=21324, loss=0.024105552583932877\n",
      "Surface training t=21325, loss=0.029715324752032757\n",
      "Surface training t=21326, loss=0.029014755971729755\n",
      "Surface training t=21327, loss=0.02266595419496298\n",
      "Surface training t=21328, loss=0.022788112983107567\n",
      "Surface training t=21329, loss=0.02743288315832615\n",
      "Surface training t=21330, loss=0.02839820086956024\n",
      "Surface training t=21331, loss=0.01854623667895794\n",
      "Surface training t=21332, loss=0.02546517737209797\n",
      "Surface training t=21333, loss=0.01804745476692915\n",
      "Surface training t=21334, loss=0.019197551533579826\n",
      "Surface training t=21335, loss=0.02045085607096553\n",
      "Surface training t=21336, loss=0.02649354748427868\n",
      "Surface training t=21337, loss=0.024643990211188793\n",
      "Surface training t=21338, loss=0.029895068146288395\n",
      "Surface training t=21339, loss=0.02630645502358675\n",
      "Surface training t=21340, loss=0.021169797517359257\n",
      "Surface training t=21341, loss=0.022874029353260994\n",
      "Surface training t=21342, loss=0.023925275541841984\n",
      "Surface training t=21343, loss=0.027243499644100666\n",
      "Surface training t=21344, loss=0.03233738150447607\n",
      "Surface training t=21345, loss=0.02957226987928152\n",
      "Surface training t=21346, loss=0.04964941926300526\n",
      "Surface training t=21347, loss=0.03934285044670105\n",
      "Surface training t=21348, loss=0.04024473391473293\n",
      "Surface training t=21349, loss=0.03150595538318157\n",
      "Surface training t=21350, loss=0.046736326068639755\n",
      "Surface training t=21351, loss=0.030973633751273155\n",
      "Surface training t=21352, loss=0.04091336950659752\n",
      "Surface training t=21353, loss=0.04165882430970669\n",
      "Surface training t=21354, loss=0.04456061124801636\n",
      "Surface training t=21355, loss=0.03834149427711964\n",
      "Surface training t=21356, loss=0.047417838126420975\n",
      "Surface training t=21357, loss=0.03654848039150238\n",
      "Surface training t=21358, loss=0.035212479531764984\n",
      "Surface training t=21359, loss=0.0327763007953763\n",
      "Surface training t=21360, loss=0.02562774531543255\n",
      "Surface training t=21361, loss=0.060343584045767784\n",
      "Surface training t=21362, loss=0.03692791797220707\n",
      "Surface training t=21363, loss=0.038302091881632805\n",
      "Surface training t=21364, loss=0.03902176208794117\n",
      "Surface training t=21365, loss=0.039389763958752155\n",
      "Surface training t=21366, loss=0.03062061034142971\n",
      "Surface training t=21367, loss=0.033946624025702477\n",
      "Surface training t=21368, loss=0.04478758946061134\n",
      "Surface training t=21369, loss=0.03767531644552946\n",
      "Surface training t=21370, loss=0.05417132005095482\n",
      "Surface training t=21371, loss=0.043188994750380516\n",
      "Surface training t=21372, loss=0.03958316333591938\n",
      "Surface training t=21373, loss=0.0358536671847105\n",
      "Surface training t=21374, loss=0.030373058281838894\n",
      "Surface training t=21375, loss=0.03696069493889809\n",
      "Surface training t=21376, loss=0.031447289511561394\n",
      "Surface training t=21377, loss=0.03111441805958748\n",
      "Surface training t=21378, loss=0.0342782698571682\n",
      "Surface training t=21379, loss=0.02876055333763361\n",
      "Surface training t=21380, loss=0.03692036308348179\n",
      "Surface training t=21381, loss=0.03083059284836054\n",
      "Surface training t=21382, loss=0.02847286406904459\n",
      "Surface training t=21383, loss=0.03194059059023857\n",
      "Surface training t=21384, loss=0.02794259786605835\n",
      "Surface training t=21385, loss=0.02422668505460024\n",
      "Surface training t=21386, loss=0.025546863675117493\n",
      "Surface training t=21387, loss=0.01983871217817068\n",
      "Surface training t=21388, loss=0.02599494345486164\n",
      "Surface training t=21389, loss=0.021948304027318954\n",
      "Surface training t=21390, loss=0.023352291900664568\n",
      "Surface training t=21391, loss=0.022050444036722183\n",
      "Surface training t=21392, loss=0.02808996755629778\n",
      "Surface training t=21393, loss=0.03141951747238636\n",
      "Surface training t=21394, loss=0.036280933767557144\n",
      "Surface training t=21395, loss=0.028087889775633812\n",
      "Surface training t=21396, loss=0.02977265603840351\n",
      "Surface training t=21397, loss=0.03415117785334587\n",
      "Surface training t=21398, loss=0.030699370428919792\n",
      "Surface training t=21399, loss=0.02010000916197896\n",
      "Surface training t=21400, loss=0.021285468712449074\n",
      "Surface training t=21401, loss=0.024421253241598606\n",
      "Surface training t=21402, loss=0.022488773800432682\n",
      "Surface training t=21403, loss=0.021915840916335583\n",
      "Surface training t=21404, loss=0.025458932854235172\n",
      "Surface training t=21405, loss=0.020635697059333324\n",
      "Surface training t=21406, loss=0.030579768121242523\n",
      "Surface training t=21407, loss=0.03066375758498907\n",
      "Surface training t=21408, loss=0.024252346716821194\n",
      "Surface training t=21409, loss=0.038097208365797997\n",
      "Surface training t=21410, loss=0.026724104769527912\n",
      "Surface training t=21411, loss=0.026437530294060707\n",
      "Surface training t=21412, loss=0.026101605966687202\n",
      "Surface training t=21413, loss=0.022462490014731884\n",
      "Surface training t=21414, loss=0.02331394050270319\n",
      "Surface training t=21415, loss=0.02310659224167466\n",
      "Surface training t=21416, loss=0.024119009263813496\n",
      "Surface training t=21417, loss=0.02837938815355301\n",
      "Surface training t=21418, loss=0.02510091755539179\n",
      "Surface training t=21419, loss=0.027727175503969193\n",
      "Surface training t=21420, loss=0.02556633297353983\n",
      "Surface training t=21421, loss=0.02497836761176586\n",
      "Surface training t=21422, loss=0.02449776604771614\n",
      "Surface training t=21423, loss=0.026810786686837673\n",
      "Surface training t=21424, loss=0.025559869594871998\n",
      "Surface training t=21425, loss=0.027850709855556488\n",
      "Surface training t=21426, loss=0.03061551135033369\n",
      "Surface training t=21427, loss=0.036535607650876045\n",
      "Surface training t=21428, loss=0.03763011749833822\n",
      "Surface training t=21429, loss=0.034697397612035275\n",
      "Surface training t=21430, loss=0.03495690878480673\n",
      "Surface training t=21431, loss=0.041788375005126\n",
      "Surface training t=21432, loss=0.04738427326083183\n",
      "Surface training t=21433, loss=0.037603070959448814\n",
      "Surface training t=21434, loss=0.04913277551531792\n",
      "Surface training t=21435, loss=0.04211024194955826\n",
      "Surface training t=21436, loss=0.03765042871236801\n",
      "Surface training t=21437, loss=0.03926895745098591\n",
      "Surface training t=21438, loss=0.04467210918664932\n",
      "Surface training t=21439, loss=0.037966202944517136\n",
      "Surface training t=21440, loss=0.028217323124408722\n",
      "Surface training t=21441, loss=0.03619222156703472\n",
      "Surface training t=21442, loss=0.029270361177623272\n",
      "Surface training t=21443, loss=0.0322959553450346\n",
      "Surface training t=21444, loss=0.026713619008660316\n",
      "Surface training t=21445, loss=0.025817795656621456\n",
      "Surface training t=21446, loss=0.027270939201116562\n",
      "Surface training t=21447, loss=0.019918246194720268\n",
      "Surface training t=21448, loss=0.032972484827041626\n",
      "Surface training t=21449, loss=0.027255243621766567\n",
      "Surface training t=21450, loss=0.030392988584935665\n",
      "Surface training t=21451, loss=0.030727896839380264\n",
      "Surface training t=21452, loss=0.031732989475131035\n",
      "Surface training t=21453, loss=0.03198009263724089\n",
      "Surface training t=21454, loss=0.05401547811925411\n",
      "Surface training t=21455, loss=0.03761093970388174\n",
      "Surface training t=21456, loss=0.03229042328894138\n",
      "Surface training t=21457, loss=0.03255950286984444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=21458, loss=0.022234815172851086\n",
      "Surface training t=21459, loss=0.03244722541421652\n",
      "Surface training t=21460, loss=0.026587704196572304\n",
      "Surface training t=21461, loss=0.023888892494142056\n",
      "Surface training t=21462, loss=0.03072529938071966\n",
      "Surface training t=21463, loss=0.025668189860880375\n",
      "Surface training t=21464, loss=0.029351123608648777\n",
      "Surface training t=21465, loss=0.023952621966600418\n",
      "Surface training t=21466, loss=0.036188364028930664\n",
      "Surface training t=21467, loss=0.02754777856171131\n",
      "Surface training t=21468, loss=0.038378747180104256\n",
      "Surface training t=21469, loss=0.04065706953406334\n",
      "Surface training t=21470, loss=0.031091120094060898\n",
      "Surface training t=21471, loss=0.047223666682839394\n",
      "Surface training t=21472, loss=0.037185841239988804\n",
      "Surface training t=21473, loss=0.04616134986281395\n",
      "Surface training t=21474, loss=0.029106718488037586\n",
      "Surface training t=21475, loss=0.03789085987955332\n",
      "Surface training t=21476, loss=0.02867411356419325\n",
      "Surface training t=21477, loss=0.02800044696778059\n",
      "Surface training t=21478, loss=0.027306340634822845\n",
      "Surface training t=21479, loss=0.020256327465176582\n",
      "Surface training t=21480, loss=0.019668453373014927\n",
      "Surface training t=21481, loss=0.02071920409798622\n",
      "Surface training t=21482, loss=0.023933159187436104\n",
      "Surface training t=21483, loss=0.020482034888118505\n",
      "Surface training t=21484, loss=0.02475624717772007\n",
      "Surface training t=21485, loss=0.023641787469387054\n",
      "Surface training t=21486, loss=0.02259428333491087\n",
      "Surface training t=21487, loss=0.029444877989590168\n",
      "Surface training t=21488, loss=0.02488008141517639\n",
      "Surface training t=21489, loss=0.02568050380796194\n",
      "Surface training t=21490, loss=0.027366015128791332\n",
      "Surface training t=21491, loss=0.020395209081470966\n",
      "Surface training t=21492, loss=0.02627928275614977\n",
      "Surface training t=21493, loss=0.03623081836849451\n",
      "Surface training t=21494, loss=0.024990873411297798\n",
      "Surface training t=21495, loss=0.022429184056818485\n",
      "Surface training t=21496, loss=0.025672731921076775\n",
      "Surface training t=21497, loss=0.023341662250459194\n",
      "Surface training t=21498, loss=0.021542293950915337\n",
      "Surface training t=21499, loss=0.023128492757678032\n",
      "Surface training t=21500, loss=0.018560227937996387\n",
      "Surface training t=21501, loss=0.018843521364033222\n",
      "Surface training t=21502, loss=0.019854003563523293\n",
      "Surface training t=21503, loss=0.016406106762588024\n",
      "Surface training t=21504, loss=0.020199786871671677\n",
      "Surface training t=21505, loss=0.016765916720032692\n",
      "Surface training t=21506, loss=0.023068583104759455\n",
      "Surface training t=21507, loss=0.019205302000045776\n",
      "Surface training t=21508, loss=0.021001002751290798\n",
      "Surface training t=21509, loss=0.02196784596890211\n",
      "Surface training t=21510, loss=0.031185586005449295\n",
      "Surface training t=21511, loss=0.02267395006492734\n",
      "Surface training t=21512, loss=0.03680977784097195\n",
      "Surface training t=21513, loss=0.03093615360558033\n",
      "Surface training t=21514, loss=0.03040378913283348\n",
      "Surface training t=21515, loss=0.021319828927516937\n",
      "Surface training t=21516, loss=0.016289942665025592\n",
      "Surface training t=21517, loss=0.023810037411749363\n",
      "Surface training t=21518, loss=0.01574592851102352\n",
      "Surface training t=21519, loss=0.021295717917382717\n",
      "Surface training t=21520, loss=0.015505009330809116\n",
      "Surface training t=21521, loss=0.024483362212777138\n",
      "Surface training t=21522, loss=0.020901932381093502\n",
      "Surface training t=21523, loss=0.03202198352664709\n",
      "Surface training t=21524, loss=0.041463326662778854\n",
      "Surface training t=21525, loss=0.03484135679900646\n",
      "Surface training t=21526, loss=0.028684397228062153\n",
      "Surface training t=21527, loss=0.023414142429828644\n",
      "Surface training t=21528, loss=0.030868190340697765\n",
      "Surface training t=21529, loss=0.04533121548593044\n",
      "Surface training t=21530, loss=0.02764403261244297\n",
      "Surface training t=21531, loss=0.02697410061955452\n",
      "Surface training t=21532, loss=0.02604234218597412\n",
      "Surface training t=21533, loss=0.026712985709309578\n",
      "Surface training t=21534, loss=0.03446668013930321\n",
      "Surface training t=21535, loss=0.042076315730810165\n",
      "Surface training t=21536, loss=0.03465436026453972\n",
      "Surface training t=21537, loss=0.027936089783906937\n",
      "Surface training t=21538, loss=0.03696741163730621\n",
      "Surface training t=21539, loss=0.03600815124809742\n",
      "Surface training t=21540, loss=0.038376384414732456\n",
      "Surface training t=21541, loss=0.03445295337587595\n",
      "Surface training t=21542, loss=0.038609977811574936\n",
      "Surface training t=21543, loss=0.03921383246779442\n",
      "Surface training t=21544, loss=0.05120376870036125\n",
      "Surface training t=21545, loss=0.0355652179569006\n",
      "Surface training t=21546, loss=0.032977937720716\n",
      "Surface training t=21547, loss=0.04046604223549366\n",
      "Surface training t=21548, loss=0.037579067051410675\n",
      "Surface training t=21549, loss=0.03375392220914364\n",
      "Surface training t=21550, loss=0.02887570485472679\n",
      "Surface training t=21551, loss=0.03512807469815016\n",
      "Surface training t=21552, loss=0.0288936635479331\n",
      "Surface training t=21553, loss=0.02433035336434841\n",
      "Surface training t=21554, loss=0.019860696978867054\n",
      "Surface training t=21555, loss=0.01958797825500369\n",
      "Surface training t=21556, loss=0.01703047752380371\n",
      "Surface training t=21557, loss=0.019036862533539534\n",
      "Surface training t=21558, loss=0.021088520996272564\n",
      "Surface training t=21559, loss=0.018672706559300423\n",
      "Surface training t=21560, loss=0.02017765585333109\n",
      "Surface training t=21561, loss=0.01886660046875477\n",
      "Surface training t=21562, loss=0.02223925106227398\n",
      "Surface training t=21563, loss=0.020099577959626913\n",
      "Surface training t=21564, loss=0.03133266605436802\n",
      "Surface training t=21565, loss=0.025428620167076588\n",
      "Surface training t=21566, loss=0.024342111311852932\n",
      "Surface training t=21567, loss=0.031577604822814465\n",
      "Surface training t=21568, loss=0.020503384992480278\n",
      "Surface training t=21569, loss=0.024695314466953278\n",
      "Surface training t=21570, loss=0.028326820582151413\n",
      "Surface training t=21571, loss=0.02740997914224863\n",
      "Surface training t=21572, loss=0.024696199223399162\n",
      "Surface training t=21573, loss=0.04085026867687702\n",
      "Surface training t=21574, loss=0.024314086884260178\n",
      "Surface training t=21575, loss=0.024330277927219868\n",
      "Surface training t=21576, loss=0.029706377536058426\n",
      "Surface training t=21577, loss=0.025604162365198135\n",
      "Surface training t=21578, loss=0.04220405034720898\n",
      "Surface training t=21579, loss=0.038801681250333786\n",
      "Surface training t=21580, loss=0.038459863513708115\n",
      "Surface training t=21581, loss=0.04044543392956257\n",
      "Surface training t=21582, loss=0.032697263173758984\n",
      "Surface training t=21583, loss=0.038514770567417145\n",
      "Surface training t=21584, loss=0.029834279790520668\n",
      "Surface training t=21585, loss=0.023099849931895733\n",
      "Surface training t=21586, loss=0.027649189345538616\n",
      "Surface training t=21587, loss=0.025420275516808033\n",
      "Surface training t=21588, loss=0.022776301950216293\n",
      "Surface training t=21589, loss=0.015405082143843174\n",
      "Surface training t=21590, loss=0.02522910013794899\n",
      "Surface training t=21591, loss=0.017158829141408205\n",
      "Surface training t=21592, loss=0.01992075890302658\n",
      "Surface training t=21593, loss=0.014780095778405666\n",
      "Surface training t=21594, loss=0.021537919528782368\n",
      "Surface training t=21595, loss=0.0235019875690341\n",
      "Surface training t=21596, loss=0.026183122768998146\n",
      "Surface training t=21597, loss=0.02355912607163191\n",
      "Surface training t=21598, loss=0.025284098461270332\n",
      "Surface training t=21599, loss=0.024537439458072186\n",
      "Surface training t=21600, loss=0.02407114114612341\n",
      "Surface training t=21601, loss=0.037270525470376015\n",
      "Surface training t=21602, loss=0.03176810033619404\n",
      "Surface training t=21603, loss=0.027080493979156017\n",
      "Surface training t=21604, loss=0.03142978064715862\n",
      "Surface training t=21605, loss=0.02360380534082651\n",
      "Surface training t=21606, loss=0.032290331088006496\n",
      "Surface training t=21607, loss=0.026012607850134373\n",
      "Surface training t=21608, loss=0.026945711113512516\n",
      "Surface training t=21609, loss=0.024480437859892845\n",
      "Surface training t=21610, loss=0.025033137761056423\n",
      "Surface training t=21611, loss=0.02800885308533907\n",
      "Surface training t=21612, loss=0.03438507951796055\n",
      "Surface training t=21613, loss=0.027759747579693794\n",
      "Surface training t=21614, loss=0.05070088803768158\n",
      "Surface training t=21615, loss=0.03761475533246994\n",
      "Surface training t=21616, loss=0.05721103772521019\n",
      "Surface training t=21617, loss=0.04446774395182729\n",
      "Surface training t=21618, loss=0.038719410076737404\n",
      "Surface training t=21619, loss=0.04919988475739956\n",
      "Surface training t=21620, loss=0.04013039916753769\n",
      "Surface training t=21621, loss=0.030300578102469444\n",
      "Surface training t=21622, loss=0.026980871334671974\n",
      "Surface training t=21623, loss=0.02713505830615759\n",
      "Surface training t=21624, loss=0.024330069310963154\n",
      "Surface training t=21625, loss=0.02329519484192133\n",
      "Surface training t=21626, loss=0.02209513308480382\n",
      "Surface training t=21627, loss=0.031121304258704185\n",
      "Surface training t=21628, loss=0.03430100344121456\n",
      "Surface training t=21629, loss=0.03478276450186968\n",
      "Surface training t=21630, loss=0.03354920353740454\n",
      "Surface training t=21631, loss=0.03191813360899687\n",
      "Surface training t=21632, loss=0.02811240777373314\n",
      "Surface training t=21633, loss=0.03391058184206486\n",
      "Surface training t=21634, loss=0.02535158023238182\n",
      "Surface training t=21635, loss=0.021848345175385475\n",
      "Surface training t=21636, loss=0.02055727643892169\n",
      "Surface training t=21637, loss=0.027780921198427677\n",
      "Surface training t=21638, loss=0.047736600041389465\n",
      "Surface training t=21639, loss=0.041602025739848614\n",
      "Surface training t=21640, loss=0.042208677157759666\n",
      "Surface training t=21641, loss=0.0386333093047142\n",
      "Surface training t=21642, loss=0.036039071157574654\n",
      "Surface training t=21643, loss=0.022984202951192856\n",
      "Surface training t=21644, loss=0.03129534795880318\n",
      "Surface training t=21645, loss=0.03293480910360813\n",
      "Surface training t=21646, loss=0.025973578914999962\n",
      "Surface training t=21647, loss=0.038580892607569695\n",
      "Surface training t=21648, loss=0.03189646266400814\n",
      "Surface training t=21649, loss=0.04635729640722275\n",
      "Surface training t=21650, loss=0.03528723493218422\n",
      "Surface training t=21651, loss=0.04332461394369602\n",
      "Surface training t=21652, loss=0.026380016468465328\n",
      "Surface training t=21653, loss=0.031201466917991638\n",
      "Surface training t=21654, loss=0.03488171100616455\n",
      "Surface training t=21655, loss=0.024996746331453323\n",
      "Surface training t=21656, loss=0.02733377180993557\n",
      "Surface training t=21657, loss=0.03556813485920429\n",
      "Surface training t=21658, loss=0.03359830752015114\n",
      "Surface training t=21659, loss=0.026219391264021397\n",
      "Surface training t=21660, loss=0.027353743091225624\n",
      "Surface training t=21661, loss=0.030481360852718353\n",
      "Surface training t=21662, loss=0.02042154874652624\n",
      "Surface training t=21663, loss=0.016047317069023848\n",
      "Surface training t=21664, loss=0.021299325861036777\n",
      "Surface training t=21665, loss=0.027691065333783627\n",
      "Surface training t=21666, loss=0.021850167773663998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=21667, loss=0.023913699202239513\n",
      "Surface training t=21668, loss=0.02731403987854719\n",
      "Surface training t=21669, loss=0.023276268504559994\n",
      "Surface training t=21670, loss=0.01911099348217249\n",
      "Surface training t=21671, loss=0.020967775024473667\n",
      "Surface training t=21672, loss=0.02675826009362936\n",
      "Surface training t=21673, loss=0.023419198114424944\n",
      "Surface training t=21674, loss=0.032192484475672245\n",
      "Surface training t=21675, loss=0.027596752159297466\n",
      "Surface training t=21676, loss=0.03448530659079552\n",
      "Surface training t=21677, loss=0.02662041410803795\n",
      "Surface training t=21678, loss=0.03235559072345495\n",
      "Surface training t=21679, loss=0.03336536232382059\n",
      "Surface training t=21680, loss=0.030717574059963226\n",
      "Surface training t=21681, loss=0.04870051145553589\n",
      "Surface training t=21682, loss=0.03854638431221247\n",
      "Surface training t=21683, loss=0.04036450386047363\n",
      "Surface training t=21684, loss=0.033301373943686485\n",
      "Surface training t=21685, loss=0.05383830703794956\n",
      "Surface training t=21686, loss=0.03512281831353903\n",
      "Surface training t=21687, loss=0.03562774695456028\n",
      "Surface training t=21688, loss=0.04986953362822533\n",
      "Surface training t=21689, loss=0.03847232833504677\n",
      "Surface training t=21690, loss=0.035033978056162596\n",
      "Surface training t=21691, loss=0.050399694591760635\n",
      "Surface training t=21692, loss=0.03333266917616129\n",
      "Surface training t=21693, loss=0.030343121849000454\n",
      "Surface training t=21694, loss=0.026752651669085026\n",
      "Surface training t=21695, loss=0.028189698234200478\n",
      "Surface training t=21696, loss=0.030089296400547028\n",
      "Surface training t=21697, loss=0.023768139071762562\n",
      "Surface training t=21698, loss=0.026160607114434242\n",
      "Surface training t=21699, loss=0.025224427692592144\n",
      "Surface training t=21700, loss=0.020536973141133785\n",
      "Surface training t=21701, loss=0.022042522206902504\n",
      "Surface training t=21702, loss=0.015731782652437687\n",
      "Surface training t=21703, loss=0.017768600955605507\n",
      "Surface training t=21704, loss=0.02904553711414337\n",
      "Surface training t=21705, loss=0.02819317113608122\n",
      "Surface training t=21706, loss=0.032567258924245834\n",
      "Surface training t=21707, loss=0.03188618365675211\n",
      "Surface training t=21708, loss=0.02585391141474247\n",
      "Surface training t=21709, loss=0.029332755133509636\n",
      "Surface training t=21710, loss=0.03096399176865816\n",
      "Surface training t=21711, loss=0.03194124437868595\n",
      "Surface training t=21712, loss=0.04954160377383232\n",
      "Surface training t=21713, loss=0.03540880233049393\n",
      "Surface training t=21714, loss=0.05250548757612705\n",
      "Surface training t=21715, loss=0.04028295911848545\n",
      "Surface training t=21716, loss=0.03674272261559963\n",
      "Surface training t=21717, loss=0.03690615575760603\n",
      "Surface training t=21718, loss=0.049660421907901764\n",
      "Surface training t=21719, loss=0.043383764103055\n",
      "Surface training t=21720, loss=0.03810702171176672\n",
      "Surface training t=21721, loss=0.047721141949296\n",
      "Surface training t=21722, loss=0.03359268233180046\n",
      "Surface training t=21723, loss=0.03388562612235546\n",
      "Surface training t=21724, loss=0.04845987819135189\n",
      "Surface training t=21725, loss=0.0308984462171793\n",
      "Surface training t=21726, loss=0.03086042031645775\n",
      "Surface training t=21727, loss=0.03228084649890661\n",
      "Surface training t=21728, loss=0.03359062597155571\n",
      "Surface training t=21729, loss=0.036198921501636505\n",
      "Surface training t=21730, loss=0.024671271443367004\n",
      "Surface training t=21731, loss=0.02685539424419403\n",
      "Surface training t=21732, loss=0.035146502777934074\n",
      "Surface training t=21733, loss=0.022773086093366146\n",
      "Surface training t=21734, loss=0.020463897846639156\n",
      "Surface training t=21735, loss=0.02215358428657055\n",
      "Surface training t=21736, loss=0.025305128656327724\n",
      "Surface training t=21737, loss=0.015757578425109386\n",
      "Surface training t=21738, loss=0.029367215931415558\n",
      "Surface training t=21739, loss=0.04615177772939205\n",
      "Surface training t=21740, loss=0.03322731051594019\n",
      "Surface training t=21741, loss=0.05297279916703701\n",
      "Surface training t=21742, loss=0.03657032921910286\n",
      "Surface training t=21743, loss=0.02282409742474556\n",
      "Surface training t=21744, loss=0.02546819392591715\n",
      "Surface training t=21745, loss=0.02820486668497324\n",
      "Surface training t=21746, loss=0.03399219550192356\n",
      "Surface training t=21747, loss=0.024672201368957758\n",
      "Surface training t=21748, loss=0.029171042144298553\n",
      "Surface training t=21749, loss=0.028662708587944508\n",
      "Surface training t=21750, loss=0.03260771371424198\n",
      "Surface training t=21751, loss=0.0316489152610302\n",
      "Surface training t=21752, loss=0.040560683235526085\n",
      "Surface training t=21753, loss=0.030201501213014126\n",
      "Surface training t=21754, loss=0.030984295532107353\n",
      "Surface training t=21755, loss=0.029468581080436707\n",
      "Surface training t=21756, loss=0.026891231536865234\n",
      "Surface training t=21757, loss=0.02578129991889\n",
      "Surface training t=21758, loss=0.024559013545513153\n",
      "Surface training t=21759, loss=0.02301288302987814\n",
      "Surface training t=21760, loss=0.021833025850355625\n",
      "Surface training t=21761, loss=0.021022078581154346\n",
      "Surface training t=21762, loss=0.02345573715865612\n",
      "Surface training t=21763, loss=0.023843485862016678\n",
      "Surface training t=21764, loss=0.022100436501204967\n",
      "Surface training t=21765, loss=0.02678530104458332\n",
      "Surface training t=21766, loss=0.03724412992596626\n",
      "Surface training t=21767, loss=0.02343988325446844\n",
      "Surface training t=21768, loss=0.01964804157614708\n",
      "Surface training t=21769, loss=0.03453070670366287\n",
      "Surface training t=21770, loss=0.03161912504583597\n",
      "Surface training t=21771, loss=0.028008099645376205\n",
      "Surface training t=21772, loss=0.04631374403834343\n",
      "Surface training t=21773, loss=0.034547604620456696\n",
      "Surface training t=21774, loss=0.03416453115642071\n",
      "Surface training t=21775, loss=0.043374694883823395\n",
      "Surface training t=21776, loss=0.027023455128073692\n",
      "Surface training t=21777, loss=0.030982178635895252\n",
      "Surface training t=21778, loss=0.03200922906398773\n",
      "Surface training t=21779, loss=0.03266952931880951\n",
      "Surface training t=21780, loss=0.057505372911691666\n",
      "Surface training t=21781, loss=0.03972264379262924\n",
      "Surface training t=21782, loss=0.03964346460998058\n",
      "Surface training t=21783, loss=0.031468553468585014\n",
      "Surface training t=21784, loss=0.0169390756636858\n",
      "Surface training t=21785, loss=0.021722939796745777\n",
      "Surface training t=21786, loss=0.014982030726969242\n",
      "Surface training t=21787, loss=0.019528023898601532\n",
      "Surface training t=21788, loss=0.01724552223458886\n",
      "Surface training t=21789, loss=0.02001897059381008\n",
      "Surface training t=21790, loss=0.021845081821084023\n",
      "Surface training t=21791, loss=0.02554892562329769\n",
      "Surface training t=21792, loss=0.0223913062363863\n",
      "Surface training t=21793, loss=0.029867137782275677\n",
      "Surface training t=21794, loss=0.026113787665963173\n",
      "Surface training t=21795, loss=0.037322597578167915\n",
      "Surface training t=21796, loss=0.03559928014874458\n",
      "Surface training t=21797, loss=0.029262810945510864\n",
      "Surface training t=21798, loss=0.028211964294314384\n",
      "Surface training t=21799, loss=0.02759556006640196\n",
      "Surface training t=21800, loss=0.026779781095683575\n",
      "Surface training t=21801, loss=0.03126519639045\n",
      "Surface training t=21802, loss=0.024610360153019428\n",
      "Surface training t=21803, loss=0.025325112976133823\n",
      "Surface training t=21804, loss=0.043606771156191826\n",
      "Surface training t=21805, loss=0.03109322302043438\n",
      "Surface training t=21806, loss=0.03836149163544178\n",
      "Surface training t=21807, loss=0.02714721579104662\n",
      "Surface training t=21808, loss=0.032503400929272175\n",
      "Surface training t=21809, loss=0.030068015679717064\n",
      "Surface training t=21810, loss=0.024281645193696022\n",
      "Surface training t=21811, loss=0.03398643061518669\n",
      "Surface training t=21812, loss=0.04623265005648136\n",
      "Surface training t=21813, loss=0.031037429347634315\n",
      "Surface training t=21814, loss=0.041015300899744034\n",
      "Surface training t=21815, loss=0.05816659517586231\n",
      "Surface training t=21816, loss=0.034718869253993034\n",
      "Surface training t=21817, loss=0.03409002721309662\n",
      "Surface training t=21818, loss=0.03792899288237095\n",
      "Surface training t=21819, loss=0.02607994433492422\n",
      "Surface training t=21820, loss=0.02336259838193655\n",
      "Surface training t=21821, loss=0.029125122353434563\n",
      "Surface training t=21822, loss=0.023746035993099213\n",
      "Surface training t=21823, loss=0.02733614295721054\n",
      "Surface training t=21824, loss=0.034204806201159954\n",
      "Surface training t=21825, loss=0.024952873587608337\n",
      "Surface training t=21826, loss=0.02833533100783825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=21827, loss=0.01958723831921816\n",
      "Surface training t=21828, loss=0.02973203267902136\n",
      "Surface training t=21829, loss=0.027159756049513817\n",
      "Surface training t=21830, loss=0.027486205101013184\n",
      "Surface training t=21831, loss=0.03011213429272175\n",
      "Surface training t=21832, loss=0.029724691063165665\n",
      "Surface training t=21833, loss=0.03835131786763668\n",
      "Surface training t=21834, loss=0.03415237832814455\n",
      "Surface training t=21835, loss=0.03468907531350851\n",
      "Surface training t=21836, loss=0.04445369727909565\n",
      "Surface training t=21837, loss=0.05301660671830177\n",
      "Surface training t=21838, loss=0.03979112859815359\n",
      "Surface training t=21839, loss=0.04738844931125641\n",
      "Surface training t=21840, loss=0.06511749885976315\n",
      "Surface training t=21841, loss=0.04151454195380211\n",
      "Surface training t=21842, loss=0.06140783429145813\n",
      "Surface training t=21843, loss=0.03864374291151762\n",
      "Surface training t=21844, loss=0.04352196119725704\n",
      "Surface training t=21845, loss=0.046903908252716064\n",
      "Surface training t=21846, loss=0.03790607023984194\n",
      "Surface training t=21847, loss=0.03277277201414108\n",
      "Surface training t=21848, loss=0.02688418235629797\n",
      "Surface training t=21849, loss=0.03116686176508665\n",
      "Surface training t=21850, loss=0.029471924528479576\n",
      "Surface training t=21851, loss=0.01943836361169815\n",
      "Surface training t=21852, loss=0.028660383075475693\n",
      "Surface training t=21853, loss=0.019006221555173397\n",
      "Surface training t=21854, loss=0.021529200486838818\n",
      "Surface training t=21855, loss=0.02150859124958515\n",
      "Surface training t=21856, loss=0.02090397570282221\n",
      "Surface training t=21857, loss=0.022288069128990173\n",
      "Surface training t=21858, loss=0.026789240539073944\n",
      "Surface training t=21859, loss=0.02445532474666834\n",
      "Surface training t=21860, loss=0.01522510638460517\n",
      "Surface training t=21861, loss=0.021235172636806965\n",
      "Surface training t=21862, loss=0.02327130176126957\n",
      "Surface training t=21863, loss=0.015618134289979935\n",
      "Surface training t=21864, loss=0.02030748315155506\n",
      "Surface training t=21865, loss=0.020341251976788044\n",
      "Surface training t=21866, loss=0.016174377873539925\n",
      "Surface training t=21867, loss=0.016333014238625765\n",
      "Surface training t=21868, loss=0.014934925362467766\n",
      "Surface training t=21869, loss=0.021160189993679523\n",
      "Surface training t=21870, loss=0.021559210494160652\n",
      "Surface training t=21871, loss=0.022078149020671844\n",
      "Surface training t=21872, loss=0.020376751199364662\n",
      "Surface training t=21873, loss=0.027612227015197277\n",
      "Surface training t=21874, loss=0.019479037262499332\n",
      "Surface training t=21875, loss=0.020140690729022026\n",
      "Surface training t=21876, loss=0.019203534349799156\n",
      "Surface training t=21877, loss=0.026932156644761562\n",
      "Surface training t=21878, loss=0.024646727368235588\n",
      "Surface training t=21879, loss=0.030411496758461\n",
      "Surface training t=21880, loss=0.029523639008402824\n",
      "Surface training t=21881, loss=0.029986579902470112\n",
      "Surface training t=21882, loss=0.034255195409059525\n",
      "Surface training t=21883, loss=0.02383920829743147\n",
      "Surface training t=21884, loss=0.033062005415558815\n",
      "Surface training t=21885, loss=0.025177057832479477\n",
      "Surface training t=21886, loss=0.023219166323542595\n",
      "Surface training t=21887, loss=0.018564917147159576\n",
      "Surface training t=21888, loss=0.023983248509466648\n",
      "Surface training t=21889, loss=0.025863737799227238\n",
      "Surface training t=21890, loss=0.02299104444682598\n",
      "Surface training t=21891, loss=0.02303921338170767\n",
      "Surface training t=21892, loss=0.023495899513363838\n",
      "Surface training t=21893, loss=0.02153288759291172\n",
      "Surface training t=21894, loss=0.026256700046360493\n",
      "Surface training t=21895, loss=0.024012946523725986\n",
      "Surface training t=21896, loss=0.039604317396879196\n",
      "Surface training t=21897, loss=0.031906234100461006\n",
      "Surface training t=21898, loss=0.022643295116722584\n",
      "Surface training t=21899, loss=0.025224853307008743\n",
      "Surface training t=21900, loss=0.025370162911713123\n",
      "Surface training t=21901, loss=0.029147121123969555\n",
      "Surface training t=21902, loss=0.023508411832153797\n",
      "Surface training t=21903, loss=0.029756025411188602\n",
      "Surface training t=21904, loss=0.023164893500506878\n",
      "Surface training t=21905, loss=0.035267503932118416\n",
      "Surface training t=21906, loss=0.03093410935252905\n",
      "Surface training t=21907, loss=0.02812265232205391\n",
      "Surface training t=21908, loss=0.02938767522573471\n",
      "Surface training t=21909, loss=0.030295075848698616\n",
      "Surface training t=21910, loss=0.03194954991340637\n",
      "Surface training t=21911, loss=0.03936807066202164\n",
      "Surface training t=21912, loss=0.032171727158129215\n",
      "Surface training t=21913, loss=0.031315071508288383\n",
      "Surface training t=21914, loss=0.029621806927025318\n",
      "Surface training t=21915, loss=0.036180973052978516\n",
      "Surface training t=21916, loss=0.027438117191195488\n",
      "Surface training t=21917, loss=0.020259263925254345\n",
      "Surface training t=21918, loss=0.01884859846904874\n",
      "Surface training t=21919, loss=0.025490853935480118\n",
      "Surface training t=21920, loss=0.027456965297460556\n",
      "Surface training t=21921, loss=0.020473815500736237\n",
      "Surface training t=21922, loss=0.02246087323874235\n",
      "Surface training t=21923, loss=0.021299540996551514\n",
      "Surface training t=21924, loss=0.02117115817964077\n",
      "Surface training t=21925, loss=0.02872623596340418\n",
      "Surface training t=21926, loss=0.026447154581546783\n",
      "Surface training t=21927, loss=0.034858908504247665\n",
      "Surface training t=21928, loss=0.0313609978184104\n",
      "Surface training t=21929, loss=0.02895522303879261\n",
      "Surface training t=21930, loss=0.022700142115354538\n",
      "Surface training t=21931, loss=0.02645322121679783\n",
      "Surface training t=21932, loss=0.019453072920441628\n",
      "Surface training t=21933, loss=0.01886102184653282\n",
      "Surface training t=21934, loss=0.021324929781258106\n",
      "Surface training t=21935, loss=0.019323689863085747\n",
      "Surface training t=21936, loss=0.017066495027393103\n",
      "Surface training t=21937, loss=0.016700667329132557\n",
      "Surface training t=21938, loss=0.0274136527441442\n",
      "Surface training t=21939, loss=0.028755689971148968\n",
      "Surface training t=21940, loss=0.0273045776411891\n",
      "Surface training t=21941, loss=0.03230142407119274\n",
      "Surface training t=21942, loss=0.03308899328112602\n",
      "Surface training t=21943, loss=0.038244593888521194\n",
      "Surface training t=21944, loss=0.03167398925870657\n",
      "Surface training t=21945, loss=0.04082885570824146\n",
      "Surface training t=21946, loss=0.027063782326877117\n",
      "Surface training t=21947, loss=0.029519672505557537\n",
      "Surface training t=21948, loss=0.031431566923856735\n",
      "Surface training t=21949, loss=0.0255977064371109\n",
      "Surface training t=21950, loss=0.023784220218658447\n",
      "Surface training t=21951, loss=0.032254984602332115\n",
      "Surface training t=21952, loss=0.025649666786193848\n",
      "Surface training t=21953, loss=0.020804069936275482\n",
      "Surface training t=21954, loss=0.0181803060695529\n",
      "Surface training t=21955, loss=0.016671735793352127\n",
      "Surface training t=21956, loss=0.018540259450674057\n",
      "Surface training t=21957, loss=0.022368046455085278\n",
      "Surface training t=21958, loss=0.018260996788740158\n",
      "Surface training t=21959, loss=0.02001813519746065\n",
      "Surface training t=21960, loss=0.016458925791084766\n",
      "Surface training t=21961, loss=0.017367747612297535\n",
      "Surface training t=21962, loss=0.016163108870387077\n",
      "Surface training t=21963, loss=0.016094432212412357\n",
      "Surface training t=21964, loss=0.014816096983850002\n",
      "Surface training t=21965, loss=0.017527151852846146\n",
      "Surface training t=21966, loss=0.015276785008609295\n",
      "Surface training t=21967, loss=0.019249681383371353\n",
      "Surface training t=21968, loss=0.01911934744566679\n",
      "Surface training t=21969, loss=0.023074024356901646\n",
      "Surface training t=21970, loss=0.02295816410332918\n",
      "Surface training t=21971, loss=0.020876367576420307\n",
      "Surface training t=21972, loss=0.02143208682537079\n",
      "Surface training t=21973, loss=0.023689360357820988\n",
      "Surface training t=21974, loss=0.03368273191154003\n",
      "Surface training t=21975, loss=0.020981808193027973\n",
      "Surface training t=21976, loss=0.02303288411349058\n",
      "Surface training t=21977, loss=0.019984391517937183\n",
      "Surface training t=21978, loss=0.021265593357384205\n",
      "Surface training t=21979, loss=0.018386327661573887\n",
      "Surface training t=21980, loss=0.01452052965760231\n",
      "Surface training t=21981, loss=0.015825101174414158\n",
      "Surface training t=21982, loss=0.02242917288094759\n",
      "Surface training t=21983, loss=0.017047908157110214\n",
      "Surface training t=21984, loss=0.017735950648784637\n",
      "Surface training t=21985, loss=0.017212113365530968\n",
      "Surface training t=21986, loss=0.02082775440067053\n",
      "Surface training t=21987, loss=0.020311848260462284\n",
      "Surface training t=21988, loss=0.01711027417331934\n",
      "Surface training t=21989, loss=0.02376052923500538\n",
      "Surface training t=21990, loss=0.019434668123722076\n",
      "Surface training t=21991, loss=0.022000327706336975\n",
      "Surface training t=21992, loss=0.0267439940944314\n",
      "Surface training t=21993, loss=0.027873612940311432\n",
      "Surface training t=21994, loss=0.031369538977742195\n",
      "Surface training t=21995, loss=0.025330240838229656\n",
      "Surface training t=21996, loss=0.02822575718164444\n",
      "Surface training t=21997, loss=0.026100759394466877\n",
      "Surface training t=21998, loss=0.017117645125836134\n",
      "Surface training t=21999, loss=0.020074275322258472\n",
      "Surface training t=22000, loss=0.01805905904620886\n",
      "Surface training t=22001, loss=0.019593817181885242\n",
      "Surface training t=22002, loss=0.018254661932587624\n",
      "Surface training t=22003, loss=0.018052690662443638\n",
      "Surface training t=22004, loss=0.02008549775928259\n",
      "Surface training t=22005, loss=0.014559752773493528\n",
      "Surface training t=22006, loss=0.014936991035938263\n",
      "Surface training t=22007, loss=0.01438913308084011\n",
      "Surface training t=22008, loss=0.019683662801980972\n",
      "Surface training t=22009, loss=0.02031740080565214\n",
      "Surface training t=22010, loss=0.014579713344573975\n",
      "Surface training t=22011, loss=0.021446830593049526\n",
      "Surface training t=22012, loss=0.020975119434297085\n",
      "Surface training t=22013, loss=0.018532930873334408\n",
      "Surface training t=22014, loss=0.024740011431276798\n",
      "Surface training t=22015, loss=0.024621211923658848\n",
      "Surface training t=22016, loss=0.024679090827703476\n",
      "Surface training t=22017, loss=0.03429005853831768\n",
      "Surface training t=22018, loss=0.03533833287656307\n",
      "Surface training t=22019, loss=0.03424096293747425\n",
      "Surface training t=22020, loss=0.03313743881881237\n",
      "Surface training t=22021, loss=0.03730759769678116\n",
      "Surface training t=22022, loss=0.042337816208601\n",
      "Surface training t=22023, loss=0.03532858472317457\n",
      "Surface training t=22024, loss=0.03877643123269081\n",
      "Surface training t=22025, loss=0.06144435331225395\n",
      "Surface training t=22026, loss=0.03770169056952\n",
      "Surface training t=22027, loss=0.035323429852724075\n",
      "Surface training t=22028, loss=0.02723597176373005\n",
      "Surface training t=22029, loss=0.029076875187456608\n",
      "Surface training t=22030, loss=0.021669792011380196\n",
      "Surface training t=22031, loss=0.026148436591029167\n",
      "Surface training t=22032, loss=0.02405739575624466\n",
      "Surface training t=22033, loss=0.02349831350147724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=22034, loss=0.020944871939718723\n",
      "Surface training t=22035, loss=0.020173529628664255\n",
      "Surface training t=22036, loss=0.022378754802048206\n",
      "Surface training t=22037, loss=0.02766764536499977\n",
      "Surface training t=22038, loss=0.019730858504772186\n",
      "Surface training t=22039, loss=0.02195363398641348\n",
      "Surface training t=22040, loss=0.025115694850683212\n",
      "Surface training t=22041, loss=0.024355227127671242\n",
      "Surface training t=22042, loss=0.03167299926280975\n",
      "Surface training t=22043, loss=0.02493910025805235\n",
      "Surface training t=22044, loss=0.0299476170912385\n",
      "Surface training t=22045, loss=0.028612300753593445\n",
      "Surface training t=22046, loss=0.033866655081510544\n",
      "Surface training t=22047, loss=0.030710672959685326\n",
      "Surface training t=22048, loss=0.028366507031023502\n",
      "Surface training t=22049, loss=0.02780803106725216\n",
      "Surface training t=22050, loss=0.024282537400722504\n",
      "Surface training t=22051, loss=0.021304904483258724\n",
      "Surface training t=22052, loss=0.019043284468352795\n",
      "Surface training t=22053, loss=0.019506114535033703\n",
      "Surface training t=22054, loss=0.019829455763101578\n",
      "Surface training t=22055, loss=0.015371652320027351\n",
      "Surface training t=22056, loss=0.019289330579340458\n",
      "Surface training t=22057, loss=0.016033754218369722\n",
      "Surface training t=22058, loss=0.026238571852445602\n",
      "Surface training t=22059, loss=0.027731155045330524\n",
      "Surface training t=22060, loss=0.023201516829431057\n",
      "Surface training t=22061, loss=0.023094844073057175\n",
      "Surface training t=22062, loss=0.025434906594455242\n",
      "Surface training t=22063, loss=0.019774585030972958\n",
      "Surface training t=22064, loss=0.01674804976209998\n",
      "Surface training t=22065, loss=0.020491620525717735\n",
      "Surface training t=22066, loss=0.017678342293947935\n",
      "Surface training t=22067, loss=0.02238804567605257\n",
      "Surface training t=22068, loss=0.01932976022362709\n",
      "Surface training t=22069, loss=0.02300236001610756\n",
      "Surface training t=22070, loss=0.027213560417294502\n",
      "Surface training t=22071, loss=0.024753552861511707\n",
      "Surface training t=22072, loss=0.026827488094568253\n",
      "Surface training t=22073, loss=0.04142565652728081\n",
      "Surface training t=22074, loss=0.030854661017656326\n",
      "Surface training t=22075, loss=0.02638243045657873\n",
      "Surface training t=22076, loss=0.030371437780559063\n",
      "Surface training t=22077, loss=0.03329404070973396\n",
      "Surface training t=22078, loss=0.02230676729232073\n",
      "Surface training t=22079, loss=0.030799967236816883\n",
      "Surface training t=22080, loss=0.03708258457481861\n",
      "Surface training t=22081, loss=0.03143364563584328\n",
      "Surface training t=22082, loss=0.027375422418117523\n",
      "Surface training t=22083, loss=0.02305808011442423\n",
      "Surface training t=22084, loss=0.029941482469439507\n",
      "Surface training t=22085, loss=0.03313327208161354\n",
      "Surface training t=22086, loss=0.02943903859704733\n",
      "Surface training t=22087, loss=0.045471396297216415\n",
      "Surface training t=22088, loss=0.03672424703836441\n",
      "Surface training t=22089, loss=0.046415312215685844\n",
      "Surface training t=22090, loss=0.033335404470562935\n",
      "Surface training t=22091, loss=0.04219293221831322\n",
      "Surface training t=22092, loss=0.0341014014557004\n",
      "Surface training t=22093, loss=0.04039068799465895\n",
      "Surface training t=22094, loss=0.03230410907417536\n",
      "Surface training t=22095, loss=0.02546554058790207\n",
      "Surface training t=22096, loss=0.02310125157237053\n",
      "Surface training t=22097, loss=0.030323386192321777\n",
      "Surface training t=22098, loss=0.024986449629068375\n",
      "Surface training t=22099, loss=0.025408298708498478\n",
      "Surface training t=22100, loss=0.02156020514667034\n",
      "Surface training t=22101, loss=0.03696836903691292\n",
      "Surface training t=22102, loss=0.02639601193368435\n",
      "Surface training t=22103, loss=0.018873692024499178\n",
      "Surface training t=22104, loss=0.03162059001624584\n",
      "Surface training t=22105, loss=0.02914297580718994\n",
      "Surface training t=22106, loss=0.01994136441498995\n",
      "Surface training t=22107, loss=0.01805378682911396\n",
      "Surface training t=22108, loss=0.01619199151173234\n",
      "Surface training t=22109, loss=0.017208095639944077\n",
      "Surface training t=22110, loss=0.018331391736865044\n",
      "Surface training t=22111, loss=0.015499481931328773\n",
      "Surface training t=22112, loss=0.017643450759351254\n",
      "Surface training t=22113, loss=0.015401795040816069\n",
      "Surface training t=22114, loss=0.01768196327611804\n",
      "Surface training t=22115, loss=0.021138238720595837\n",
      "Surface training t=22116, loss=0.020853672176599503\n",
      "Surface training t=22117, loss=0.011932802386581898\n",
      "Surface training t=22118, loss=0.020850143395364285\n",
      "Surface training t=22119, loss=0.021626679692417383\n",
      "Surface training t=22120, loss=0.02498659584671259\n",
      "Surface training t=22121, loss=0.0265636146068573\n",
      "Surface training t=22122, loss=0.03751029446721077\n",
      "Surface training t=22123, loss=0.02427726238965988\n",
      "Surface training t=22124, loss=0.03359448350965977\n",
      "Surface training t=22125, loss=0.02741170395165682\n",
      "Surface training t=22126, loss=0.022335371002554893\n",
      "Surface training t=22127, loss=0.021823765709996223\n",
      "Surface training t=22128, loss=0.023171219043433666\n",
      "Surface training t=22129, loss=0.02258879877626896\n",
      "Surface training t=22130, loss=0.025764700956642628\n",
      "Surface training t=22131, loss=0.026804691180586815\n",
      "Surface training t=22132, loss=0.0338806351646781\n",
      "Surface training t=22133, loss=0.02304884511977434\n",
      "Surface training t=22134, loss=0.025408009998500347\n",
      "Surface training t=22135, loss=0.022382422350347042\n",
      "Surface training t=22136, loss=0.030883818864822388\n",
      "Surface training t=22137, loss=0.02730818372219801\n",
      "Surface training t=22138, loss=0.027209005318582058\n",
      "Surface training t=22139, loss=0.021846835501492023\n",
      "Surface training t=22140, loss=0.02110040094703436\n",
      "Surface training t=22141, loss=0.019222023896872997\n",
      "Surface training t=22142, loss=0.018853568471968174\n",
      "Surface training t=22143, loss=0.019844548776745796\n",
      "Surface training t=22144, loss=0.021893958561122417\n",
      "Surface training t=22145, loss=0.02038543112576008\n",
      "Surface training t=22146, loss=0.033813174813985825\n",
      "Surface training t=22147, loss=0.019955262541770935\n",
      "Surface training t=22148, loss=0.023740987293422222\n",
      "Surface training t=22149, loss=0.02049935609102249\n",
      "Surface training t=22150, loss=0.023009211756289005\n",
      "Surface training t=22151, loss=0.02101675048470497\n",
      "Surface training t=22152, loss=0.023094735108315945\n",
      "Surface training t=22153, loss=0.02009246777743101\n",
      "Surface training t=22154, loss=0.02343863807618618\n",
      "Surface training t=22155, loss=0.01684616692364216\n",
      "Surface training t=22156, loss=0.01996735204011202\n",
      "Surface training t=22157, loss=0.022059504874050617\n",
      "Surface training t=22158, loss=0.023024968802928925\n",
      "Surface training t=22159, loss=0.02387849520891905\n",
      "Surface training t=22160, loss=0.024930210784077644\n",
      "Surface training t=22161, loss=0.022398442961275578\n",
      "Surface training t=22162, loss=0.017278220504522324\n",
      "Surface training t=22163, loss=0.018124084919691086\n",
      "Surface training t=22164, loss=0.02283111959695816\n",
      "Surface training t=22165, loss=0.022378121502697468\n",
      "Surface training t=22166, loss=0.022245616652071476\n",
      "Surface training t=22167, loss=0.02303978055715561\n",
      "Surface training t=22168, loss=0.02211393415927887\n",
      "Surface training t=22169, loss=0.02765951119363308\n",
      "Surface training t=22170, loss=0.03140908665955067\n",
      "Surface training t=22171, loss=0.0339322155341506\n",
      "Surface training t=22172, loss=0.03370491601526737\n",
      "Surface training t=22173, loss=0.02417199220508337\n",
      "Surface training t=22174, loss=0.024796297773718834\n",
      "Surface training t=22175, loss=0.026678369380533695\n",
      "Surface training t=22176, loss=0.02003865409642458\n",
      "Surface training t=22177, loss=0.027308964170515537\n",
      "Surface training t=22178, loss=0.020093907602131367\n",
      "Surface training t=22179, loss=0.021379142999649048\n",
      "Surface training t=22180, loss=0.017577914521098137\n",
      "Surface training t=22181, loss=0.023236659355461597\n",
      "Surface training t=22182, loss=0.0325038880109787\n",
      "Surface training t=22183, loss=0.022088536992669106\n",
      "Surface training t=22184, loss=0.021251355297863483\n",
      "Surface training t=22185, loss=0.029987200163304806\n",
      "Surface training t=22186, loss=0.020947290584445\n",
      "Surface training t=22187, loss=0.02269593719393015\n",
      "Surface training t=22188, loss=0.01627424033358693\n",
      "Surface training t=22189, loss=0.023996470496058464\n",
      "Surface training t=22190, loss=0.019044709391891956\n",
      "Surface training t=22191, loss=0.022026083432137966\n",
      "Surface training t=22192, loss=0.019190829247236252\n",
      "Surface training t=22193, loss=0.022114960476756096\n",
      "Surface training t=22194, loss=0.021957959048449993\n",
      "Surface training t=22195, loss=0.02168547362089157\n",
      "Surface training t=22196, loss=0.025663860142230988\n",
      "Surface training t=22197, loss=0.019417785108089447\n",
      "Surface training t=22198, loss=0.022152460180222988\n",
      "Surface training t=22199, loss=0.024253268726170063\n",
      "Surface training t=22200, loss=0.028946511447429657\n",
      "Surface training t=22201, loss=0.023215548135340214\n",
      "Surface training t=22202, loss=0.020150686614215374\n",
      "Surface training t=22203, loss=0.023154964670538902\n",
      "Surface training t=22204, loss=0.02159926388412714\n",
      "Surface training t=22205, loss=0.024614552967250347\n",
      "Surface training t=22206, loss=0.021034798584878445\n",
      "Surface training t=22207, loss=0.0204908000305295\n",
      "Surface training t=22208, loss=0.019336744211614132\n",
      "Surface training t=22209, loss=0.024744918569922447\n",
      "Surface training t=22210, loss=0.023544996045529842\n",
      "Surface training t=22211, loss=0.021539966575801373\n",
      "Surface training t=22212, loss=0.02730951551347971\n",
      "Surface training t=22213, loss=0.036867326125502586\n",
      "Surface training t=22214, loss=0.042712049558758736\n",
      "Surface training t=22215, loss=0.03268701396882534\n",
      "Surface training t=22216, loss=0.02591981738805771\n",
      "Surface training t=22217, loss=0.031215570867061615\n",
      "Surface training t=22218, loss=0.03587471880018711\n",
      "Surface training t=22219, loss=0.026174183934926987\n",
      "Surface training t=22220, loss=0.03374961577355862\n",
      "Surface training t=22221, loss=0.02847965620458126\n",
      "Surface training t=22222, loss=0.02589882630854845\n",
      "Surface training t=22223, loss=0.04100225865840912\n",
      "Surface training t=22224, loss=0.036050545051693916\n",
      "Surface training t=22225, loss=0.041593991219997406\n",
      "Surface training t=22226, loss=0.03040639776736498\n",
      "Surface training t=22227, loss=0.02084320317953825\n",
      "Surface training t=22228, loss=0.025520446710288525\n",
      "Surface training t=22229, loss=0.018978755921125412\n",
      "Surface training t=22230, loss=0.022469403222203255\n",
      "Surface training t=22231, loss=0.019528496079146862\n",
      "Surface training t=22232, loss=0.021427473053336143\n",
      "Surface training t=22233, loss=0.02073366940021515\n",
      "Surface training t=22234, loss=0.02945620473474264\n",
      "Surface training t=22235, loss=0.02858730312436819\n",
      "Surface training t=22236, loss=0.026251166127622128\n",
      "Surface training t=22237, loss=0.03134562447667122\n",
      "Surface training t=22238, loss=0.03375545423477888\n",
      "Surface training t=22239, loss=0.0306667173281312\n",
      "Surface training t=22240, loss=0.030639919452369213\n",
      "Surface training t=22241, loss=0.03693337365984917\n",
      "Surface training t=22242, loss=0.022749319672584534\n",
      "Surface training t=22243, loss=0.020102476701140404\n",
      "Surface training t=22244, loss=0.0241486057639122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=22245, loss=0.026278779841959476\n",
      "Surface training t=22246, loss=0.021260621026158333\n",
      "Surface training t=22247, loss=0.01869295071810484\n",
      "Surface training t=22248, loss=0.02079432550817728\n",
      "Surface training t=22249, loss=0.032512047328054905\n",
      "Surface training t=22250, loss=0.0278725391253829\n",
      "Surface training t=22251, loss=0.03020638506859541\n",
      "Surface training t=22252, loss=0.045377178117632866\n",
      "Surface training t=22253, loss=0.02690794412046671\n",
      "Surface training t=22254, loss=0.03273206762969494\n",
      "Surface training t=22255, loss=0.028425457887351513\n",
      "Surface training t=22256, loss=0.02570098452270031\n",
      "Surface training t=22257, loss=0.03610428795218468\n",
      "Surface training t=22258, loss=0.036138296127319336\n",
      "Surface training t=22259, loss=0.03892001882195473\n",
      "Surface training t=22260, loss=0.03497541509568691\n",
      "Surface training t=22261, loss=0.034638142213225365\n",
      "Surface training t=22262, loss=0.025197656825184822\n",
      "Surface training t=22263, loss=0.030630631372332573\n",
      "Surface training t=22264, loss=0.02601648773998022\n",
      "Surface training t=22265, loss=0.02288190834224224\n",
      "Surface training t=22266, loss=0.021338921040296555\n",
      "Surface training t=22267, loss=0.01881693582981825\n",
      "Surface training t=22268, loss=0.023011820390820503\n",
      "Surface training t=22269, loss=0.015250120777636766\n",
      "Surface training t=22270, loss=0.016835561953485012\n",
      "Surface training t=22271, loss=0.01897760108113289\n",
      "Surface training t=22272, loss=0.017164526507258415\n",
      "Surface training t=22273, loss=0.02074800617992878\n",
      "Surface training t=22274, loss=0.020999412052333355\n",
      "Surface training t=22275, loss=0.023724243976175785\n",
      "Surface training t=22276, loss=0.018390589393675327\n",
      "Surface training t=22277, loss=0.019276902079582214\n",
      "Surface training t=22278, loss=0.017328675836324692\n",
      "Surface training t=22279, loss=0.018512113019824028\n",
      "Surface training t=22280, loss=0.019214823842048645\n",
      "Surface training t=22281, loss=0.020094089210033417\n",
      "Surface training t=22282, loss=0.018871144391596317\n",
      "Surface training t=22283, loss=0.019732595421373844\n",
      "Surface training t=22284, loss=0.018809632398188114\n",
      "Surface training t=22285, loss=0.031354802660644054\n",
      "Surface training t=22286, loss=0.03316064924001694\n",
      "Surface training t=22287, loss=0.02742354478687048\n",
      "Surface training t=22288, loss=0.02808411791920662\n",
      "Surface training t=22289, loss=0.02685677446424961\n",
      "Surface training t=22290, loss=0.021733082830905914\n",
      "Surface training t=22291, loss=0.018567136954516172\n",
      "Surface training t=22292, loss=0.016592197120189667\n",
      "Surface training t=22293, loss=0.016131756827235222\n",
      "Surface training t=22294, loss=0.01800608914345503\n",
      "Surface training t=22295, loss=0.024792302399873734\n",
      "Surface training t=22296, loss=0.028383287601172924\n",
      "Surface training t=22297, loss=0.04605773836374283\n",
      "Surface training t=22298, loss=0.03658100590109825\n",
      "Surface training t=22299, loss=0.034529659897089005\n",
      "Surface training t=22300, loss=0.0255509028211236\n",
      "Surface training t=22301, loss=0.022126412019133568\n",
      "Surface training t=22302, loss=0.02400073315948248\n",
      "Surface training t=22303, loss=0.023563257418572903\n",
      "Surface training t=22304, loss=0.023676727898418903\n",
      "Surface training t=22305, loss=0.028400693088769913\n",
      "Surface training t=22306, loss=0.0326172411441803\n",
      "Surface training t=22307, loss=0.034476340748369694\n",
      "Surface training t=22308, loss=0.03180182818323374\n",
      "Surface training t=22309, loss=0.03561925143003464\n",
      "Surface training t=22310, loss=0.03407362103462219\n",
      "Surface training t=22311, loss=0.03409909829497337\n",
      "Surface training t=22312, loss=0.028757828287780285\n",
      "Surface training t=22313, loss=0.041042692959308624\n",
      "Surface training t=22314, loss=0.034267423674464226\n",
      "Surface training t=22315, loss=0.031907690688967705\n",
      "Surface training t=22316, loss=0.0298880310729146\n",
      "Surface training t=22317, loss=0.038090264424681664\n",
      "Surface training t=22318, loss=0.028200636617839336\n",
      "Surface training t=22319, loss=0.032078216783702374\n",
      "Surface training t=22320, loss=0.030085205100476742\n",
      "Surface training t=22321, loss=0.025295785628259182\n",
      "Surface training t=22322, loss=0.02848039288073778\n",
      "Surface training t=22323, loss=0.027363763190805912\n",
      "Surface training t=22324, loss=0.022056668996810913\n",
      "Surface training t=22325, loss=0.021558872424066067\n",
      "Surface training t=22326, loss=0.028770262375473976\n",
      "Surface training t=22327, loss=0.02459173183888197\n",
      "Surface training t=22328, loss=0.03159311693161726\n",
      "Surface training t=22329, loss=0.03147165942937136\n",
      "Surface training t=22330, loss=0.026995855383574963\n",
      "Surface training t=22331, loss=0.028410110622644424\n",
      "Surface training t=22332, loss=0.03256215061992407\n",
      "Surface training t=22333, loss=0.023856007494032383\n",
      "Surface training t=22334, loss=0.024925402365624905\n",
      "Surface training t=22335, loss=0.035433707758784294\n",
      "Surface training t=22336, loss=0.03337001521140337\n",
      "Surface training t=22337, loss=0.03233751840889454\n",
      "Surface training t=22338, loss=0.0284184068441391\n",
      "Surface training t=22339, loss=0.026835890486836433\n",
      "Surface training t=22340, loss=0.04230893403291702\n",
      "Surface training t=22341, loss=0.03264816477894783\n",
      "Surface training t=22342, loss=0.03680145274847746\n",
      "Surface training t=22343, loss=0.028649368323385715\n",
      "Surface training t=22344, loss=0.0313374875113368\n",
      "Surface training t=22345, loss=0.04392137564718723\n",
      "Surface training t=22346, loss=0.028400946408510208\n",
      "Surface training t=22347, loss=0.026291591115295887\n",
      "Surface training t=22348, loss=0.023524442687630653\n",
      "Surface training t=22349, loss=0.023875592276453972\n",
      "Surface training t=22350, loss=0.03479215409606695\n",
      "Surface training t=22351, loss=0.03074601199477911\n",
      "Surface training t=22352, loss=0.03047350514680147\n",
      "Surface training t=22353, loss=0.031909787096083164\n",
      "Surface training t=22354, loss=0.026208672672510147\n",
      "Surface training t=22355, loss=0.024388394318521023\n",
      "Surface training t=22356, loss=0.019878643564879894\n",
      "Surface training t=22357, loss=0.01938471570611\n",
      "Surface training t=22358, loss=0.01684977300465107\n",
      "Surface training t=22359, loss=0.02374393492937088\n",
      "Surface training t=22360, loss=0.026234904304146767\n",
      "Surface training t=22361, loss=0.027574289590120316\n",
      "Surface training t=22362, loss=0.025199316442012787\n",
      "Surface training t=22363, loss=0.02131143305450678\n",
      "Surface training t=22364, loss=0.020318150520324707\n",
      "Surface training t=22365, loss=0.02744324877858162\n",
      "Surface training t=22366, loss=0.017327317036688328\n",
      "Surface training t=22367, loss=0.02404334582388401\n",
      "Surface training t=22368, loss=0.027294624596834183\n",
      "Surface training t=22369, loss=0.03810802847146988\n",
      "Surface training t=22370, loss=0.037460289895534515\n",
      "Surface training t=22371, loss=0.029695109464228153\n",
      "Surface training t=22372, loss=0.028750259429216385\n",
      "Surface training t=22373, loss=0.026685421355068684\n",
      "Surface training t=22374, loss=0.02233980130404234\n",
      "Surface training t=22375, loss=0.025850525125861168\n",
      "Surface training t=22376, loss=0.02579017635434866\n",
      "Surface training t=22377, loss=0.026741462759673595\n",
      "Surface training t=22378, loss=0.02880065143108368\n",
      "Surface training t=22379, loss=0.039933785796165466\n",
      "Surface training t=22380, loss=0.030540059320628643\n",
      "Surface training t=22381, loss=0.03586895391345024\n",
      "Surface training t=22382, loss=0.03244900889694691\n",
      "Surface training t=22383, loss=0.03353030048310757\n",
      "Surface training t=22384, loss=0.03710535168647766\n",
      "Surface training t=22385, loss=0.030046092346310616\n",
      "Surface training t=22386, loss=0.037510182708501816\n",
      "Surface training t=22387, loss=0.02085623424500227\n",
      "Surface training t=22388, loss=0.022931253537535667\n",
      "Surface training t=22389, loss=0.030009374022483826\n",
      "Surface training t=22390, loss=0.020036606118083\n",
      "Surface training t=22391, loss=0.024912971071898937\n",
      "Surface training t=22392, loss=0.02422327548265457\n",
      "Surface training t=22393, loss=0.01770987268537283\n",
      "Surface training t=22394, loss=0.023765080608427525\n",
      "Surface training t=22395, loss=0.019720707088708878\n",
      "Surface training t=22396, loss=0.02430999930948019\n",
      "Surface training t=22397, loss=0.02241578698158264\n",
      "Surface training t=22398, loss=0.02152479998767376\n",
      "Surface training t=22399, loss=0.021147051826119423\n",
      "Surface training t=22400, loss=0.028180494904518127\n",
      "Surface training t=22401, loss=0.0242965966463089\n",
      "Surface training t=22402, loss=0.024056500755250454\n",
      "Surface training t=22403, loss=0.02547011710703373\n",
      "Surface training t=22404, loss=0.022685031406581402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=22405, loss=0.02963394671678543\n",
      "Surface training t=22406, loss=0.02367174904793501\n",
      "Surface training t=22407, loss=0.02717871218919754\n",
      "Surface training t=22408, loss=0.025893285870552063\n",
      "Surface training t=22409, loss=0.024837389588356018\n",
      "Surface training t=22410, loss=0.025295970030128956\n",
      "Surface training t=22411, loss=0.026368318125605583\n",
      "Surface training t=22412, loss=0.029950976371765137\n",
      "Surface training t=22413, loss=0.02586204931139946\n",
      "Surface training t=22414, loss=0.03473497927188873\n",
      "Surface training t=22415, loss=0.04260912165045738\n",
      "Surface training t=22416, loss=0.03634548094123602\n",
      "Surface training t=22417, loss=0.03435340337455273\n",
      "Surface training t=22418, loss=0.03650118876248598\n",
      "Surface training t=22419, loss=0.04053688794374466\n",
      "Surface training t=22420, loss=0.03888477757573128\n",
      "Surface training t=22421, loss=0.03104351833462715\n",
      "Surface training t=22422, loss=0.02507508173584938\n",
      "Surface training t=22423, loss=0.026498226448893547\n",
      "Surface training t=22424, loss=0.026632492430508137\n",
      "Surface training t=22425, loss=0.028180853463709354\n",
      "Surface training t=22426, loss=0.03136356174945831\n",
      "Surface training t=22427, loss=0.024496243335306644\n",
      "Surface training t=22428, loss=0.02478130953386426\n",
      "Surface training t=22429, loss=0.029169931076467037\n",
      "Surface training t=22430, loss=0.02551737893372774\n",
      "Surface training t=22431, loss=0.028697348199784756\n",
      "Surface training t=22432, loss=0.027673236094415188\n",
      "Surface training t=22433, loss=0.01686945930123329\n",
      "Surface training t=22434, loss=0.01822714600712061\n",
      "Surface training t=22435, loss=0.022807314060628414\n",
      "Surface training t=22436, loss=0.03850308433175087\n",
      "Surface training t=22437, loss=0.027933740057051182\n",
      "Surface training t=22438, loss=0.03286487236618996\n",
      "Surface training t=22439, loss=0.026392746716737747\n",
      "Surface training t=22440, loss=0.029054854065179825\n",
      "Surface training t=22441, loss=0.02636373322457075\n",
      "Surface training t=22442, loss=0.02748740464448929\n",
      "Surface training t=22443, loss=0.02688795141875744\n",
      "Surface training t=22444, loss=0.02156748529523611\n",
      "Surface training t=22445, loss=0.02390687633305788\n",
      "Surface training t=22446, loss=0.0242245988920331\n",
      "Surface training t=22447, loss=0.029829245060682297\n",
      "Surface training t=22448, loss=0.030339475721120834\n",
      "Surface training t=22449, loss=0.021229715086519718\n",
      "Surface training t=22450, loss=0.027606758289039135\n",
      "Surface training t=22451, loss=0.02143212128430605\n",
      "Surface training t=22452, loss=0.0372482780367136\n",
      "Surface training t=22453, loss=0.02156259771436453\n",
      "Surface training t=22454, loss=0.024728644639253616\n",
      "Surface training t=22455, loss=0.021527145989239216\n",
      "Surface training t=22456, loss=0.02042229939252138\n",
      "Surface training t=22457, loss=0.02426253817975521\n",
      "Surface training t=22458, loss=0.023559301160275936\n",
      "Surface training t=22459, loss=0.019799519795924425\n",
      "Surface training t=22460, loss=0.021428795531392097\n",
      "Surface training t=22461, loss=0.014930030331015587\n",
      "Surface training t=22462, loss=0.01578678237274289\n",
      "Surface training t=22463, loss=0.019185824319720268\n",
      "Surface training t=22464, loss=0.023310906253755093\n",
      "Surface training t=22465, loss=0.025945018976926804\n",
      "Surface training t=22466, loss=0.020961718633770943\n",
      "Surface training t=22467, loss=0.02259136736392975\n",
      "Surface training t=22468, loss=0.022563009522855282\n",
      "Surface training t=22469, loss=0.01680830167606473\n",
      "Surface training t=22470, loss=0.0195852043107152\n",
      "Surface training t=22471, loss=0.023390863090753555\n",
      "Surface training t=22472, loss=0.023253142833709717\n",
      "Surface training t=22473, loss=0.02328452654182911\n",
      "Surface training t=22474, loss=0.023072444833815098\n",
      "Surface training t=22475, loss=0.01710372418165207\n",
      "Surface training t=22476, loss=0.020488837733864784\n",
      "Surface training t=22477, loss=0.025842148810625076\n",
      "Surface training t=22478, loss=0.016186955850571394\n",
      "Surface training t=22479, loss=0.02019700314849615\n",
      "Surface training t=22480, loss=0.017098655458539724\n",
      "Surface training t=22481, loss=0.0193854421377182\n",
      "Surface training t=22482, loss=0.023200816474854946\n",
      "Surface training t=22483, loss=0.022410244680941105\n",
      "Surface training t=22484, loss=0.014926822856068611\n",
      "Surface training t=22485, loss=0.021687575615942478\n",
      "Surface training t=22486, loss=0.02695622108876705\n",
      "Surface training t=22487, loss=0.030409245751798153\n",
      "Surface training t=22488, loss=0.02975583355873823\n",
      "Surface training t=22489, loss=0.026109528727829456\n",
      "Surface training t=22490, loss=0.02079428918659687\n",
      "Surface training t=22491, loss=0.021335287019610405\n",
      "Surface training t=22492, loss=0.030683537013828754\n",
      "Surface training t=22493, loss=0.035587869584560394\n",
      "Surface training t=22494, loss=0.05536830425262451\n",
      "Surface training t=22495, loss=0.04124355874955654\n",
      "Surface training t=22496, loss=0.03499554097652435\n",
      "Surface training t=22497, loss=0.029681839048862457\n",
      "Surface training t=22498, loss=0.030182842165231705\n",
      "Surface training t=22499, loss=0.02875596471130848\n",
      "Surface training t=22500, loss=0.027032921090722084\n",
      "Surface training t=22501, loss=0.03098737634718418\n",
      "Surface training t=22502, loss=0.027059320360422134\n",
      "Surface training t=22503, loss=0.031351691111922264\n",
      "Surface training t=22504, loss=0.03782500699162483\n",
      "Surface training t=22505, loss=0.026743371970951557\n",
      "Surface training t=22506, loss=0.030323498882353306\n",
      "Surface training t=22507, loss=0.0309979896992445\n",
      "Surface training t=22508, loss=0.028120623901486397\n",
      "Surface training t=22509, loss=0.04921649768948555\n",
      "Surface training t=22510, loss=0.03743505850434303\n",
      "Surface training t=22511, loss=0.03649340011179447\n",
      "Surface training t=22512, loss=0.03608825337141752\n",
      "Surface training t=22513, loss=0.039776913821697235\n",
      "Surface training t=22514, loss=0.027316215448081493\n",
      "Surface training t=22515, loss=0.02070888876914978\n",
      "Surface training t=22516, loss=0.016083240043371916\n",
      "Surface training t=22517, loss=0.02186404913663864\n",
      "Surface training t=22518, loss=0.015328405890613794\n",
      "Surface training t=22519, loss=0.011747962795197964\n",
      "Surface training t=22520, loss=0.022454104386270046\n",
      "Surface training t=22521, loss=0.018579737283289433\n",
      "Surface training t=22522, loss=0.02212690655142069\n",
      "Surface training t=22523, loss=0.021591821685433388\n",
      "Surface training t=22524, loss=0.03334908187389374\n",
      "Surface training t=22525, loss=0.028987967874854803\n",
      "Surface training t=22526, loss=0.03260847460478544\n",
      "Surface training t=22527, loss=0.05367981269955635\n",
      "Surface training t=22528, loss=0.038245758041739464\n",
      "Surface training t=22529, loss=0.0366282369941473\n",
      "Surface training t=22530, loss=0.031210916116833687\n",
      "Surface training t=22531, loss=0.05280222184956074\n",
      "Surface training t=22532, loss=0.03219612315297127\n",
      "Surface training t=22533, loss=0.032162873074412346\n",
      "Surface training t=22534, loss=0.025090080685913563\n",
      "Surface training t=22535, loss=0.028018981218338013\n",
      "Surface training t=22536, loss=0.031800538301467896\n",
      "Surface training t=22537, loss=0.020368196070194244\n",
      "Surface training t=22538, loss=0.020695547573268414\n",
      "Surface training t=22539, loss=0.020646777004003525\n",
      "Surface training t=22540, loss=0.01940792566165328\n",
      "Surface training t=22541, loss=0.019631068222224712\n",
      "Surface training t=22542, loss=0.01556614926084876\n",
      "Surface training t=22543, loss=0.017262442037463188\n",
      "Surface training t=22544, loss=0.019510905258357525\n",
      "Surface training t=22545, loss=0.018805847503244877\n",
      "Surface training t=22546, loss=0.018046190030872822\n",
      "Surface training t=22547, loss=0.02282961830496788\n",
      "Surface training t=22548, loss=0.03449215553700924\n",
      "Surface training t=22549, loss=0.03525449335575104\n",
      "Surface training t=22550, loss=0.026605715043842793\n",
      "Surface training t=22551, loss=0.03178435005247593\n",
      "Surface training t=22552, loss=0.030425832606852055\n",
      "Surface training t=22553, loss=0.032142700627446175\n",
      "Surface training t=22554, loss=0.04742267727851868\n",
      "Surface training t=22555, loss=0.030612755566835403\n",
      "Surface training t=22556, loss=0.029110086150467396\n",
      "Surface training t=22557, loss=0.03386676497757435\n",
      "Surface training t=22558, loss=0.05132400617003441\n",
      "Surface training t=22559, loss=0.03663117624819279\n",
      "Surface training t=22560, loss=0.029171663336455822\n",
      "Surface training t=22561, loss=0.025369973853230476\n",
      "Surface training t=22562, loss=0.024290475994348526\n",
      "Surface training t=22563, loss=0.03368986025452614\n",
      "Surface training t=22564, loss=0.023788686841726303\n",
      "Surface training t=22565, loss=0.025069057941436768\n",
      "Surface training t=22566, loss=0.030302799306809902\n",
      "Surface training t=22567, loss=0.026182997971773148\n",
      "Surface training t=22568, loss=0.02197780553251505\n",
      "Surface training t=22569, loss=0.027866963297128677\n",
      "Surface training t=22570, loss=0.03427165001630783\n",
      "Surface training t=22571, loss=0.027193758636713028\n",
      "Surface training t=22572, loss=0.021670174784958363\n",
      "Surface training t=22573, loss=0.03249570354819298\n",
      "Surface training t=22574, loss=0.02565052919089794\n",
      "Surface training t=22575, loss=0.023767651990056038\n",
      "Surface training t=22576, loss=0.026515514589846134\n",
      "Surface training t=22577, loss=0.02232396323233843\n",
      "Surface training t=22578, loss=0.02621052134782076\n",
      "Surface training t=22579, loss=0.025471731089055538\n",
      "Surface training t=22580, loss=0.04123345576226711\n",
      "Surface training t=22581, loss=0.027751334942877293\n",
      "Surface training t=22582, loss=0.0273220157250762\n",
      "Surface training t=22583, loss=0.03349333722144365\n",
      "Surface training t=22584, loss=0.03731625899672508\n",
      "Surface training t=22585, loss=0.04222276248037815\n",
      "Surface training t=22586, loss=0.03148658014833927\n",
      "Surface training t=22587, loss=0.03067880868911743\n",
      "Surface training t=22588, loss=0.046522125601768494\n",
      "Surface training t=22589, loss=0.042879618704319\n",
      "Surface training t=22590, loss=0.04045167565345764\n",
      "Surface training t=22591, loss=0.033219615928828716\n",
      "Surface training t=22592, loss=0.04223962686955929\n",
      "Surface training t=22593, loss=0.03544230107218027\n",
      "Surface training t=22594, loss=0.04119467921555042\n",
      "Surface training t=22595, loss=0.05192556045949459\n",
      "Surface training t=22596, loss=0.02986164763569832\n",
      "Surface training t=22597, loss=0.03187308367341757\n",
      "Surface training t=22598, loss=0.03662859182804823\n",
      "Surface training t=22599, loss=0.04307898320257664\n",
      "Surface training t=22600, loss=0.032034192234277725\n",
      "Surface training t=22601, loss=0.024943213909864426\n",
      "Surface training t=22602, loss=0.02540867030620575\n",
      "Surface training t=22603, loss=0.02805788442492485\n",
      "Surface training t=22604, loss=0.02885811123996973\n",
      "Surface training t=22605, loss=0.030113838613033295\n",
      "Surface training t=22606, loss=0.03230107203125954\n",
      "Surface training t=22607, loss=0.022217484191060066\n",
      "Surface training t=22608, loss=0.01808304525911808\n",
      "Surface training t=22609, loss=0.017073259688913822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=22610, loss=0.02268418576568365\n",
      "Surface training t=22611, loss=0.027506045997142792\n",
      "Surface training t=22612, loss=0.029001709073781967\n",
      "Surface training t=22613, loss=0.02414606511592865\n",
      "Surface training t=22614, loss=0.028123469091951847\n",
      "Surface training t=22615, loss=0.030154350213706493\n",
      "Surface training t=22616, loss=0.032515715807676315\n",
      "Surface training t=22617, loss=0.034979457035660744\n",
      "Surface training t=22618, loss=0.03145505581051111\n",
      "Surface training t=22619, loss=0.030213323421776295\n",
      "Surface training t=22620, loss=0.03691496979445219\n",
      "Surface training t=22621, loss=0.02421264350414276\n",
      "Surface training t=22622, loss=0.03783976845443249\n",
      "Surface training t=22623, loss=0.02596462331712246\n",
      "Surface training t=22624, loss=0.02231818437576294\n",
      "Surface training t=22625, loss=0.03128228150308132\n",
      "Surface training t=22626, loss=0.02767948340624571\n",
      "Surface training t=22627, loss=0.03309761732816696\n",
      "Surface training t=22628, loss=0.030662326142191887\n",
      "Surface training t=22629, loss=0.027777470648288727\n",
      "Surface training t=22630, loss=0.0323594119399786\n",
      "Surface training t=22631, loss=0.025098771788179874\n",
      "Surface training t=22632, loss=0.024432888254523277\n",
      "Surface training t=22633, loss=0.030737271532416344\n",
      "Surface training t=22634, loss=0.025444273836910725\n",
      "Surface training t=22635, loss=0.0270999763160944\n",
      "Surface training t=22636, loss=0.027676408179104328\n",
      "Surface training t=22637, loss=0.02188208047300577\n",
      "Surface training t=22638, loss=0.028870214708149433\n",
      "Surface training t=22639, loss=0.03440001606941223\n",
      "Surface training t=22640, loss=0.027792003005743027\n",
      "Surface training t=22641, loss=0.02957082074135542\n",
      "Surface training t=22642, loss=0.023207458667457104\n",
      "Surface training t=22643, loss=0.026225022040307522\n",
      "Surface training t=22644, loss=0.021947388537228107\n",
      "Surface training t=22645, loss=0.020650923252105713\n",
      "Surface training t=22646, loss=0.02872251160442829\n",
      "Surface training t=22647, loss=0.024686299264431\n",
      "Surface training t=22648, loss=0.025622498244047165\n",
      "Surface training t=22649, loss=0.026023010723292828\n",
      "Surface training t=22650, loss=0.019618410617113113\n",
      "Surface training t=22651, loss=0.028501160442829132\n",
      "Surface training t=22652, loss=0.021101408638060093\n",
      "Surface training t=22653, loss=0.028494305908679962\n",
      "Surface training t=22654, loss=0.022516383789479733\n",
      "Surface training t=22655, loss=0.024720024317502975\n",
      "Surface training t=22656, loss=0.021530586294829845\n",
      "Surface training t=22657, loss=0.0331551693379879\n",
      "Surface training t=22658, loss=0.026059724390506744\n",
      "Surface training t=22659, loss=0.02595922164618969\n",
      "Surface training t=22660, loss=0.032407136633992195\n",
      "Surface training t=22661, loss=0.028573316521942616\n",
      "Surface training t=22662, loss=0.02866237796843052\n",
      "Surface training t=22663, loss=0.02410672791302204\n",
      "Surface training t=22664, loss=0.03769958205521107\n",
      "Surface training t=22665, loss=0.031050704419612885\n",
      "Surface training t=22666, loss=0.028029541485011578\n",
      "Surface training t=22667, loss=0.03351046796888113\n",
      "Surface training t=22668, loss=0.04350098967552185\n",
      "Surface training t=22669, loss=0.029516124166548252\n",
      "Surface training t=22670, loss=0.032224384136497974\n",
      "Surface training t=22671, loss=0.04553176090121269\n",
      "Surface training t=22672, loss=0.038797320798039436\n",
      "Surface training t=22673, loss=0.04086120147258043\n",
      "Surface training t=22674, loss=0.04789944738149643\n",
      "Surface training t=22675, loss=0.05023886449635029\n",
      "Surface training t=22676, loss=0.03627696633338928\n",
      "Surface training t=22677, loss=0.030760856345295906\n",
      "Surface training t=22678, loss=0.03629159368574619\n",
      "Surface training t=22679, loss=0.026313778944313526\n",
      "Surface training t=22680, loss=0.03476736880838871\n",
      "Surface training t=22681, loss=0.03231590986251831\n",
      "Surface training t=22682, loss=0.033345622941851616\n",
      "Surface training t=22683, loss=0.0527243297547102\n",
      "Surface training t=22684, loss=0.04067177698016167\n",
      "Surface training t=22685, loss=0.03981238044798374\n",
      "Surface training t=22686, loss=0.043043846264481544\n",
      "Surface training t=22687, loss=0.03723311610519886\n",
      "Surface training t=22688, loss=0.02984411921352148\n",
      "Surface training t=22689, loss=0.03945373743772507\n",
      "Surface training t=22690, loss=0.032090007327497005\n",
      "Surface training t=22691, loss=0.05196047201752663\n",
      "Surface training t=22692, loss=0.04074695706367493\n",
      "Surface training t=22693, loss=0.04730236530303955\n",
      "Surface training t=22694, loss=0.04233673959970474\n",
      "Surface training t=22695, loss=0.038020421750843525\n",
      "Surface training t=22696, loss=0.05046873912215233\n",
      "Surface training t=22697, loss=0.04702611453831196\n",
      "Surface training t=22698, loss=0.03406370431184769\n",
      "Surface training t=22699, loss=0.0483661163598299\n",
      "Surface training t=22700, loss=0.03575569670647383\n",
      "Surface training t=22701, loss=0.04069233871996403\n",
      "Surface training t=22702, loss=0.03568683844059706\n",
      "Surface training t=22703, loss=0.03764335438609123\n",
      "Surface training t=22704, loss=0.02597129438072443\n",
      "Surface training t=22705, loss=0.030869370326399803\n",
      "Surface training t=22706, loss=0.032956818118691444\n",
      "Surface training t=22707, loss=0.027071951888501644\n",
      "Surface training t=22708, loss=0.031225244514644146\n",
      "Surface training t=22709, loss=0.027815306559205055\n",
      "Surface training t=22710, loss=0.02934231236577034\n",
      "Surface training t=22711, loss=0.03382481634616852\n",
      "Surface training t=22712, loss=0.02671628911048174\n",
      "Surface training t=22713, loss=0.020816036500036716\n",
      "Surface training t=22714, loss=0.02402295172214508\n",
      "Surface training t=22715, loss=0.020405868999660015\n",
      "Surface training t=22716, loss=0.020320948213338852\n",
      "Surface training t=22717, loss=0.01752736046910286\n",
      "Surface training t=22718, loss=0.020582537166774273\n",
      "Surface training t=22719, loss=0.016490054316818714\n",
      "Surface training t=22720, loss=0.01806231401860714\n",
      "Surface training t=22721, loss=0.01999809592962265\n",
      "Surface training t=22722, loss=0.015601781196892262\n",
      "Surface training t=22723, loss=0.026857743971049786\n",
      "Surface training t=22724, loss=0.018219638615846634\n",
      "Surface training t=22725, loss=0.03455127589404583\n",
      "Surface training t=22726, loss=0.037755267694592476\n",
      "Surface training t=22727, loss=0.035737511701881886\n",
      "Surface training t=22728, loss=0.042128621600568295\n",
      "Surface training t=22729, loss=0.058706894516944885\n",
      "Surface training t=22730, loss=0.0358202513307333\n",
      "Surface training t=22731, loss=0.043576059862971306\n",
      "Surface training t=22732, loss=0.046234844252467155\n",
      "Surface training t=22733, loss=0.037175887264311314\n",
      "Surface training t=22734, loss=0.04732632450759411\n",
      "Surface training t=22735, loss=0.04156533069908619\n",
      "Surface training t=22736, loss=0.027387382462620735\n",
      "Surface training t=22737, loss=0.029035072773694992\n",
      "Surface training t=22738, loss=0.030230043455958366\n",
      "Surface training t=22739, loss=0.04118568263947964\n",
      "Surface training t=22740, loss=0.02630016952753067\n",
      "Surface training t=22741, loss=0.028805255889892578\n",
      "Surface training t=22742, loss=0.02847798727452755\n",
      "Surface training t=22743, loss=0.026891501620411873\n",
      "Surface training t=22744, loss=0.02945962455123663\n",
      "Surface training t=22745, loss=0.028995591215789318\n",
      "Surface training t=22746, loss=0.025059456937015057\n",
      "Surface training t=22747, loss=0.034057460725307465\n",
      "Surface training t=22748, loss=0.0337783582508564\n",
      "Surface training t=22749, loss=0.022547323256731033\n",
      "Surface training t=22750, loss=0.018946140073239803\n",
      "Surface training t=22751, loss=0.02666857372969389\n",
      "Surface training t=22752, loss=0.0222040805965662\n",
      "Surface training t=22753, loss=0.014337572269141674\n",
      "Surface training t=22754, loss=0.018798284232616425\n",
      "Surface training t=22755, loss=0.018492942675948143\n",
      "Surface training t=22756, loss=0.022746117785573006\n",
      "Surface training t=22757, loss=0.017784709110856056\n",
      "Surface training t=22758, loss=0.01814162451773882\n",
      "Surface training t=22759, loss=0.01777857542037964\n",
      "Surface training t=22760, loss=0.017712516710162163\n",
      "Surface training t=22761, loss=0.017106642480939627\n",
      "Surface training t=22762, loss=0.017916414886713028\n",
      "Surface training t=22763, loss=0.016635803505778313\n",
      "Surface training t=22764, loss=0.01975026074796915\n",
      "Surface training t=22765, loss=0.017845874652266502\n",
      "Surface training t=22766, loss=0.01714298687875271\n",
      "Surface training t=22767, loss=0.01799296960234642\n",
      "Surface training t=22768, loss=0.019885637797415257\n",
      "Surface training t=22769, loss=0.020983771421015263\n",
      "Surface training t=22770, loss=0.02227356843650341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=22771, loss=0.026696047745645046\n",
      "Surface training t=22772, loss=0.02399462927132845\n",
      "Surface training t=22773, loss=0.022782555781304836\n",
      "Surface training t=22774, loss=0.024708754383027554\n",
      "Surface training t=22775, loss=0.020033431239426136\n",
      "Surface training t=22776, loss=0.018115611746907234\n",
      "Surface training t=22777, loss=0.01746706198900938\n",
      "Surface training t=22778, loss=0.01717735454440117\n",
      "Surface training t=22779, loss=0.026347951032221317\n",
      "Surface training t=22780, loss=0.0258311303332448\n",
      "Surface training t=22781, loss=0.024952120147645473\n",
      "Surface training t=22782, loss=0.024091510102152824\n",
      "Surface training t=22783, loss=0.027070796117186546\n",
      "Surface training t=22784, loss=0.023986770771443844\n",
      "Surface training t=22785, loss=0.020103059709072113\n",
      "Surface training t=22786, loss=0.02077085990458727\n",
      "Surface training t=22787, loss=0.01901121996343136\n",
      "Surface training t=22788, loss=0.015941699035465717\n",
      "Surface training t=22789, loss=0.017569522373378277\n",
      "Surface training t=22790, loss=0.013740325346589088\n",
      "Surface training t=22791, loss=0.017816306091845036\n",
      "Surface training t=22792, loss=0.025404788553714752\n",
      "Surface training t=22793, loss=0.024193312041461468\n",
      "Surface training t=22794, loss=0.028833742253482342\n",
      "Surface training t=22795, loss=0.02327269408851862\n",
      "Surface training t=22796, loss=0.018940377980470657\n",
      "Surface training t=22797, loss=0.02307505626231432\n",
      "Surface training t=22798, loss=0.01800351170822978\n",
      "Surface training t=22799, loss=0.019864829257130623\n",
      "Surface training t=22800, loss=0.01855380181223154\n",
      "Surface training t=22801, loss=0.025690915063023567\n",
      "Surface training t=22802, loss=0.028676125220954418\n",
      "Surface training t=22803, loss=0.028295581229031086\n",
      "Surface training t=22804, loss=0.015737399458885193\n",
      "Surface training t=22805, loss=0.02212607115507126\n",
      "Surface training t=22806, loss=0.020166666246950626\n",
      "Surface training t=22807, loss=0.016704311594367027\n",
      "Surface training t=22808, loss=0.02370729949325323\n",
      "Surface training t=22809, loss=0.029651016928255558\n",
      "Surface training t=22810, loss=0.028738588094711304\n",
      "Surface training t=22811, loss=0.02877451665699482\n",
      "Surface training t=22812, loss=0.028801207430660725\n",
      "Surface training t=22813, loss=0.029641144908964634\n",
      "Surface training t=22814, loss=0.030347133055329323\n",
      "Surface training t=22815, loss=0.02399691753089428\n",
      "Surface training t=22816, loss=0.023651085793972015\n",
      "Surface training t=22817, loss=0.02783382497727871\n",
      "Surface training t=22818, loss=0.03621098771691322\n",
      "Surface training t=22819, loss=0.03279465530067682\n",
      "Surface training t=22820, loss=0.03540278039872646\n",
      "Surface training t=22821, loss=0.031537264585494995\n",
      "Surface training t=22822, loss=0.03291630558669567\n",
      "Surface training t=22823, loss=0.026874669827520847\n",
      "Surface training t=22824, loss=0.022634195163846016\n",
      "Surface training t=22825, loss=0.026387177407741547\n",
      "Surface training t=22826, loss=0.02030871156603098\n",
      "Surface training t=22827, loss=0.015954923816025257\n",
      "Surface training t=22828, loss=0.01920056715607643\n",
      "Surface training t=22829, loss=0.015832615550607443\n",
      "Surface training t=22830, loss=0.014656399376690388\n",
      "Surface training t=22831, loss=0.02047786768525839\n",
      "Surface training t=22832, loss=0.01631587091833353\n",
      "Surface training t=22833, loss=0.0203029727563262\n",
      "Surface training t=22834, loss=0.02003179956227541\n",
      "Surface training t=22835, loss=0.02250771503895521\n",
      "Surface training t=22836, loss=0.024771008640527725\n",
      "Surface training t=22837, loss=0.03502988442778587\n",
      "Surface training t=22838, loss=0.03225509636104107\n",
      "Surface training t=22839, loss=0.03281544055789709\n",
      "Surface training t=22840, loss=0.03217726945877075\n",
      "Surface training t=22841, loss=0.027700007893145084\n",
      "Surface training t=22842, loss=0.02735261619091034\n",
      "Surface training t=22843, loss=0.029034790582954884\n",
      "Surface training t=22844, loss=0.027779879048466682\n",
      "Surface training t=22845, loss=0.020009329542517662\n",
      "Surface training t=22846, loss=0.028097398579120636\n",
      "Surface training t=22847, loss=0.02695667650550604\n",
      "Surface training t=22848, loss=0.02882428001612425\n",
      "Surface training t=22849, loss=0.034248560667037964\n",
      "Surface training t=22850, loss=0.02788608707487583\n",
      "Surface training t=22851, loss=0.02627443615347147\n",
      "Surface training t=22852, loss=0.020896032452583313\n",
      "Surface training t=22853, loss=0.021562006324529648\n",
      "Surface training t=22854, loss=0.018840277567505836\n",
      "Surface training t=22855, loss=0.02192068099975586\n",
      "Surface training t=22856, loss=0.016869695857167244\n",
      "Surface training t=22857, loss=0.02001605648547411\n",
      "Surface training t=22858, loss=0.015060756355524063\n",
      "Surface training t=22859, loss=0.020661397837102413\n",
      "Surface training t=22860, loss=0.020682898350059986\n",
      "Surface training t=22861, loss=0.01675664260983467\n",
      "Surface training t=22862, loss=0.01889043115079403\n",
      "Surface training t=22863, loss=0.021259384229779243\n",
      "Surface training t=22864, loss=0.016446887515485287\n",
      "Surface training t=22865, loss=0.022378585301339626\n",
      "Surface training t=22866, loss=0.02007357869297266\n",
      "Surface training t=22867, loss=0.017078464850783348\n",
      "Surface training t=22868, loss=0.01753619872033596\n",
      "Surface training t=22869, loss=0.01649693213403225\n",
      "Surface training t=22870, loss=0.019948464818298817\n",
      "Surface training t=22871, loss=0.014325946103781462\n",
      "Surface training t=22872, loss=0.019319025799632072\n",
      "Surface training t=22873, loss=0.020699551329016685\n",
      "Surface training t=22874, loss=0.015946295112371445\n",
      "Surface training t=22875, loss=0.02235835138708353\n",
      "Surface training t=22876, loss=0.02331687416881323\n",
      "Surface training t=22877, loss=0.03161579184234142\n",
      "Surface training t=22878, loss=0.031172492541372776\n",
      "Surface training t=22879, loss=0.022224169224500656\n",
      "Surface training t=22880, loss=0.03126116283237934\n",
      "Surface training t=22881, loss=0.019265485927462578\n",
      "Surface training t=22882, loss=0.024263651110231876\n",
      "Surface training t=22883, loss=0.02340090274810791\n",
      "Surface training t=22884, loss=0.02367774210870266\n",
      "Surface training t=22885, loss=0.018324854783713818\n",
      "Surface training t=22886, loss=0.023540649563074112\n",
      "Surface training t=22887, loss=0.023077789694070816\n",
      "Surface training t=22888, loss=0.02560554165393114\n",
      "Surface training t=22889, loss=0.027957488782703876\n",
      "Surface training t=22890, loss=0.03220486082136631\n",
      "Surface training t=22891, loss=0.022574787959456444\n",
      "Surface training t=22892, loss=0.029147337190806866\n",
      "Surface training t=22893, loss=0.0266070868819952\n",
      "Surface training t=22894, loss=0.028711343184113503\n",
      "Surface training t=22895, loss=0.024732607416808605\n",
      "Surface training t=22896, loss=0.02507853414863348\n",
      "Surface training t=22897, loss=0.026106588542461395\n",
      "Surface training t=22898, loss=0.034985775128006935\n",
      "Surface training t=22899, loss=0.02583459671586752\n",
      "Surface training t=22900, loss=0.021549432072788477\n",
      "Surface training t=22901, loss=0.029733690433204174\n",
      "Surface training t=22902, loss=0.024735026992857456\n",
      "Surface training t=22903, loss=0.02534643653780222\n",
      "Surface training t=22904, loss=0.028654325753450394\n",
      "Surface training t=22905, loss=0.02548166550695896\n",
      "Surface training t=22906, loss=0.02057568170130253\n",
      "Surface training t=22907, loss=0.0226244879886508\n",
      "Surface training t=22908, loss=0.019778229296207428\n",
      "Surface training t=22909, loss=0.017599365673959255\n",
      "Surface training t=22910, loss=0.019033554941415787\n",
      "Surface training t=22911, loss=0.017523063346743584\n",
      "Surface training t=22912, loss=0.021129626780748367\n",
      "Surface training t=22913, loss=0.02207003440707922\n",
      "Surface training t=22914, loss=0.016562895383685827\n",
      "Surface training t=22915, loss=0.021362990140914917\n",
      "Surface training t=22916, loss=0.021137493662536144\n",
      "Surface training t=22917, loss=0.023651269264519215\n",
      "Surface training t=22918, loss=0.030601290054619312\n",
      "Surface training t=22919, loss=0.029950729571282864\n",
      "Surface training t=22920, loss=0.023131873458623886\n",
      "Surface training t=22921, loss=0.02825964242219925\n",
      "Surface training t=22922, loss=0.02198271919041872\n",
      "Surface training t=22923, loss=0.01997694279998541\n",
      "Surface training t=22924, loss=0.019976377487182617\n",
      "Surface training t=22925, loss=0.018159231171011925\n",
      "Surface training t=22926, loss=0.018017632886767387\n",
      "Surface training t=22927, loss=0.018514967523515224\n",
      "Surface training t=22928, loss=0.020078214816749096\n",
      "Surface training t=22929, loss=0.020123008638620377\n",
      "Surface training t=22930, loss=0.021278321743011475\n",
      "Surface training t=22931, loss=0.020723004825413227\n",
      "Surface training t=22932, loss=0.020180830731987953\n",
      "Surface training t=22933, loss=0.028885374777019024\n",
      "Surface training t=22934, loss=0.026899158023297787\n",
      "Surface training t=22935, loss=0.030066750943660736\n",
      "Surface training t=22936, loss=0.025506037287414074\n",
      "Surface training t=22937, loss=0.021455674432218075\n",
      "Surface training t=22938, loss=0.021492344327270985\n",
      "Surface training t=22939, loss=0.030193353071808815\n",
      "Surface training t=22940, loss=0.03050162084400654\n",
      "Surface training t=22941, loss=0.022769492119550705\n",
      "Surface training t=22942, loss=0.020402222871780396\n",
      "Surface training t=22943, loss=0.02350038383156061\n",
      "Surface training t=22944, loss=0.021083710715174675\n",
      "Surface training t=22945, loss=0.021947077475488186\n",
      "Surface training t=22946, loss=0.01968113798648119\n",
      "Surface training t=22947, loss=0.013290409930050373\n",
      "Surface training t=22948, loss=0.013302762061357498\n",
      "Surface training t=22949, loss=0.01684264186769724\n",
      "Surface training t=22950, loss=0.015948932617902756\n",
      "Surface training t=22951, loss=0.0171932615339756\n",
      "Surface training t=22952, loss=0.019116740208119154\n",
      "Surface training t=22953, loss=0.030812821350991726\n",
      "Surface training t=22954, loss=0.028163578361272812\n",
      "Surface training t=22955, loss=0.040820250287652016\n",
      "Surface training t=22956, loss=0.02549532800912857\n",
      "Surface training t=22957, loss=0.022922969423234463\n",
      "Surface training t=22958, loss=0.0331702996045351\n",
      "Surface training t=22959, loss=0.029447253793478012\n",
      "Surface training t=22960, loss=0.04272705316543579\n",
      "Surface training t=22961, loss=0.032994452863931656\n",
      "Surface training t=22962, loss=0.02769674640148878\n",
      "Surface training t=22963, loss=0.024573056027293205\n",
      "Surface training t=22964, loss=0.024342410266399384\n",
      "Surface training t=22965, loss=0.01858609914779663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=22966, loss=0.020810185931622982\n",
      "Surface training t=22967, loss=0.022420604713261127\n",
      "Surface training t=22968, loss=0.019890956580638885\n",
      "Surface training t=22969, loss=0.018306064419448376\n",
      "Surface training t=22970, loss=0.01846898067742586\n",
      "Surface training t=22971, loss=0.01793952565640211\n",
      "Surface training t=22972, loss=0.01898689940571785\n",
      "Surface training t=22973, loss=0.018811007030308247\n",
      "Surface training t=22974, loss=0.018990831449627876\n",
      "Surface training t=22975, loss=0.022754240781068802\n",
      "Surface training t=22976, loss=0.03060944564640522\n",
      "Surface training t=22977, loss=0.021636389661580324\n",
      "Surface training t=22978, loss=0.02091937232762575\n",
      "Surface training t=22979, loss=0.019454226829111576\n",
      "Surface training t=22980, loss=0.020196945406496525\n",
      "Surface training t=22981, loss=0.021786266937851906\n",
      "Surface training t=22982, loss=0.01889200508594513\n",
      "Surface training t=22983, loss=0.019903107546269894\n",
      "Surface training t=22984, loss=0.023331118747591972\n",
      "Surface training t=22985, loss=0.03090725839138031\n",
      "Surface training t=22986, loss=0.03182671777904034\n",
      "Surface training t=22987, loss=0.03095889464020729\n",
      "Surface training t=22988, loss=0.027602847665548325\n",
      "Surface training t=22989, loss=0.03089391253888607\n",
      "Surface training t=22990, loss=0.02430675644427538\n",
      "Surface training t=22991, loss=0.021167946979403496\n",
      "Surface training t=22992, loss=0.018998978659510612\n",
      "Surface training t=22993, loss=0.02284327521920204\n",
      "Surface training t=22994, loss=0.029309479519724846\n",
      "Surface training t=22995, loss=0.03283564932644367\n",
      "Surface training t=22996, loss=0.029913638718426228\n",
      "Surface training t=22997, loss=0.029608486220240593\n",
      "Surface training t=22998, loss=0.031161745078861713\n",
      "Surface training t=22999, loss=0.03442222531884909\n",
      "Surface training t=23000, loss=0.03416651766747236\n",
      "Surface training t=23001, loss=0.04429230839014053\n",
      "Surface training t=23002, loss=0.03445982374250889\n",
      "Surface training t=23003, loss=0.03534477297216654\n",
      "Surface training t=23004, loss=0.0358018446713686\n",
      "Surface training t=23005, loss=0.046246374025940895\n",
      "Surface training t=23006, loss=0.03878795728087425\n",
      "Surface training t=23007, loss=0.02962454780936241\n",
      "Surface training t=23008, loss=0.030593259260058403\n",
      "Surface training t=23009, loss=0.03867574408650398\n",
      "Surface training t=23010, loss=0.027232258580625057\n",
      "Surface training t=23011, loss=0.029122788459062576\n",
      "Surface training t=23012, loss=0.025460204109549522\n",
      "Surface training t=23013, loss=0.021777117624878883\n",
      "Surface training t=23014, loss=0.020769139286130667\n",
      "Surface training t=23015, loss=0.022428099066019058\n",
      "Surface training t=23016, loss=0.025872960686683655\n",
      "Surface training t=23017, loss=0.018817814998328686\n",
      "Surface training t=23018, loss=0.024417441338300705\n",
      "Surface training t=23019, loss=0.027030213735997677\n",
      "Surface training t=23020, loss=0.0291155893355608\n",
      "Surface training t=23021, loss=0.020487336441874504\n",
      "Surface training t=23022, loss=0.021921824663877487\n",
      "Surface training t=23023, loss=0.029026424512267113\n",
      "Surface training t=23024, loss=0.019373321905732155\n",
      "Surface training t=23025, loss=0.01702388096600771\n",
      "Surface training t=23026, loss=0.02210531197488308\n",
      "Surface training t=23027, loss=0.019246570765972137\n",
      "Surface training t=23028, loss=0.017511649057269096\n",
      "Surface training t=23029, loss=0.020889670588076115\n",
      "Surface training t=23030, loss=0.014135616831481457\n",
      "Surface training t=23031, loss=0.014771365094929934\n",
      "Surface training t=23032, loss=0.022952549159526825\n",
      "Surface training t=23033, loss=0.01945347525179386\n",
      "Surface training t=23034, loss=0.017876995261758566\n",
      "Surface training t=23035, loss=0.025575620122253895\n",
      "Surface training t=23036, loss=0.023050344549119473\n",
      "Surface training t=23037, loss=0.024714931845664978\n",
      "Surface training t=23038, loss=0.025169490836560726\n",
      "Surface training t=23039, loss=0.022945307195186615\n",
      "Surface training t=23040, loss=0.027304179035127163\n",
      "Surface training t=23041, loss=0.026845471933484077\n",
      "Surface training t=23042, loss=0.025545697659254074\n",
      "Surface training t=23043, loss=0.021501407027244568\n",
      "Surface training t=23044, loss=0.020649454556405544\n",
      "Surface training t=23045, loss=0.016040187794715166\n",
      "Surface training t=23046, loss=0.028874034993350506\n",
      "Surface training t=23047, loss=0.022121483460068703\n",
      "Surface training t=23048, loss=0.018858304247260094\n",
      "Surface training t=23049, loss=0.022460016421973705\n",
      "Surface training t=23050, loss=0.024592269212007523\n",
      "Surface training t=23051, loss=0.022588316351175308\n",
      "Surface training t=23052, loss=0.023827219381928444\n",
      "Surface training t=23053, loss=0.021394765004515648\n",
      "Surface training t=23054, loss=0.016148113645613194\n",
      "Surface training t=23055, loss=0.02175137121230364\n",
      "Surface training t=23056, loss=0.020367255434393883\n",
      "Surface training t=23057, loss=0.024531444534659386\n",
      "Surface training t=23058, loss=0.02436983399093151\n",
      "Surface training t=23059, loss=0.02445884421467781\n",
      "Surface training t=23060, loss=0.0336702112108469\n",
      "Surface training t=23061, loss=0.025012295693159103\n",
      "Surface training t=23062, loss=0.019076145254075527\n",
      "Surface training t=23063, loss=0.028849076479673386\n",
      "Surface training t=23064, loss=0.024804122745990753\n",
      "Surface training t=23065, loss=0.024703534319996834\n",
      "Surface training t=23066, loss=0.02503760252147913\n",
      "Surface training t=23067, loss=0.018088387325406075\n",
      "Surface training t=23068, loss=0.02180431131273508\n",
      "Surface training t=23069, loss=0.024300680495798588\n",
      "Surface training t=23070, loss=0.024827024899423122\n",
      "Surface training t=23071, loss=0.024166317656636238\n",
      "Surface training t=23072, loss=0.02660258486866951\n",
      "Surface training t=23073, loss=0.024463790468871593\n",
      "Surface training t=23074, loss=0.02606711257249117\n",
      "Surface training t=23075, loss=0.023501058109104633\n",
      "Surface training t=23076, loss=0.01780781801789999\n",
      "Surface training t=23077, loss=0.021754959598183632\n",
      "Surface training t=23078, loss=0.02355797588825226\n",
      "Surface training t=23079, loss=0.020151659846305847\n",
      "Surface training t=23080, loss=0.01532899122685194\n",
      "Surface training t=23081, loss=0.017118548043072224\n",
      "Surface training t=23082, loss=0.02032193634659052\n",
      "Surface training t=23083, loss=0.018634134903550148\n",
      "Surface training t=23084, loss=0.017181046307086945\n",
      "Surface training t=23085, loss=0.016961200162768364\n",
      "Surface training t=23086, loss=0.021523932926356792\n",
      "Surface training t=23087, loss=0.023894071578979492\n",
      "Surface training t=23088, loss=0.02570805698633194\n",
      "Surface training t=23089, loss=0.019761540461331606\n",
      "Surface training t=23090, loss=0.01798206754028797\n",
      "Surface training t=23091, loss=0.02273884415626526\n",
      "Surface training t=23092, loss=0.023958592675626278\n",
      "Surface training t=23093, loss=0.0235699275508523\n",
      "Surface training t=23094, loss=0.0199443306773901\n",
      "Surface training t=23095, loss=0.01945909857749939\n",
      "Surface training t=23096, loss=0.02458925172686577\n",
      "Surface training t=23097, loss=0.014673774130642414\n",
      "Surface training t=23098, loss=0.02077208925038576\n",
      "Surface training t=23099, loss=0.01574787963181734\n",
      "Surface training t=23100, loss=0.021980788558721542\n",
      "Surface training t=23101, loss=0.016332700848579407\n",
      "Surface training t=23102, loss=0.025572625920176506\n",
      "Surface training t=23103, loss=0.024305099621415138\n",
      "Surface training t=23104, loss=0.019270870834589005\n",
      "Surface training t=23105, loss=0.02016657032072544\n",
      "Surface training t=23106, loss=0.02179652266204357\n",
      "Surface training t=23107, loss=0.016084984876215458\n",
      "Surface training t=23108, loss=0.021268324926495552\n",
      "Surface training t=23109, loss=0.018560798838734627\n",
      "Surface training t=23110, loss=0.022403755225241184\n",
      "Surface training t=23111, loss=0.028986627236008644\n",
      "Surface training t=23112, loss=0.03325813449919224\n",
      "Surface training t=23113, loss=0.024867895059287548\n",
      "Surface training t=23114, loss=0.028234686702489853\n",
      "Surface training t=23115, loss=0.025861941277980804\n",
      "Surface training t=23116, loss=0.0288078086450696\n",
      "Surface training t=23117, loss=0.03174450248479843\n",
      "Surface training t=23118, loss=0.023942535743117332\n",
      "Surface training t=23119, loss=0.025973854586482048\n",
      "Surface training t=23120, loss=0.03229693230241537\n",
      "Surface training t=23121, loss=0.027935718186199665\n",
      "Surface training t=23122, loss=0.033890943974256516\n",
      "Surface training t=23123, loss=0.03250407055020332\n",
      "Surface training t=23124, loss=0.02791687101125717\n",
      "Surface training t=23125, loss=0.035477256402373314\n",
      "Surface training t=23126, loss=0.025623268447816372\n",
      "Surface training t=23127, loss=0.02294040098786354\n",
      "Surface training t=23128, loss=0.029345670714974403\n",
      "Surface training t=23129, loss=0.028728390112519264\n",
      "Surface training t=23130, loss=0.029323258437216282\n",
      "Surface training t=23131, loss=0.03129299730062485\n",
      "Surface training t=23132, loss=0.03174035158008337\n",
      "Surface training t=23133, loss=0.02837025374174118\n",
      "Surface training t=23134, loss=0.03756197541952133\n",
      "Surface training t=23135, loss=0.024093437008559704\n",
      "Surface training t=23136, loss=0.03284675069153309\n",
      "Surface training t=23137, loss=0.029778405092656612\n",
      "Surface training t=23138, loss=0.023786373902112246\n",
      "Surface training t=23139, loss=0.02285951469093561\n",
      "Surface training t=23140, loss=0.024315943010151386\n",
      "Surface training t=23141, loss=0.032352020032703876\n",
      "Surface training t=23142, loss=0.022215518169105053\n",
      "Surface training t=23143, loss=0.017960055265575647\n",
      "Surface training t=23144, loss=0.03224565647542477\n",
      "Surface training t=23145, loss=0.024140363559126854\n",
      "Surface training t=23146, loss=0.024163994938135147\n",
      "Surface training t=23147, loss=0.03046676516532898\n",
      "Surface training t=23148, loss=0.02411932684481144\n",
      "Surface training t=23149, loss=0.026800379157066345\n",
      "Surface training t=23150, loss=0.028060972690582275\n",
      "Surface training t=23151, loss=0.03866007551550865\n",
      "Surface training t=23152, loss=0.03196517750620842\n",
      "Surface training t=23153, loss=0.02601313591003418\n",
      "Surface training t=23154, loss=0.021716615185141563\n",
      "Surface training t=23155, loss=0.02629654761403799\n",
      "Surface training t=23156, loss=0.022400901652872562\n",
      "Surface training t=23157, loss=0.028313877061009407\n",
      "Surface training t=23158, loss=0.022582807578146458\n",
      "Surface training t=23159, loss=0.019339729100465775\n",
      "Surface training t=23160, loss=0.018801217898726463\n",
      "Surface training t=23161, loss=0.01949762413278222\n",
      "Surface training t=23162, loss=0.021856694482266903\n",
      "Surface training t=23163, loss=0.021923329681158066\n",
      "Surface training t=23164, loss=0.017119644209742546\n",
      "Surface training t=23165, loss=0.024753740057349205\n",
      "Surface training t=23166, loss=0.02114837523549795\n",
      "Surface training t=23167, loss=0.019217681139707565\n",
      "Surface training t=23168, loss=0.021602883003652096\n",
      "Surface training t=23169, loss=0.027323422022163868\n",
      "Surface training t=23170, loss=0.01986233051866293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=23171, loss=0.02185877040028572\n",
      "Surface training t=23172, loss=0.021522635594010353\n",
      "Surface training t=23173, loss=0.028858658857643604\n",
      "Surface training t=23174, loss=0.032233826816082\n",
      "Surface training t=23175, loss=0.03331300523132086\n",
      "Surface training t=23176, loss=0.025094857439398766\n",
      "Surface training t=23177, loss=0.021409827284514904\n",
      "Surface training t=23178, loss=0.021513493731617928\n",
      "Surface training t=23179, loss=0.019655576907098293\n",
      "Surface training t=23180, loss=0.026804630644619465\n",
      "Surface training t=23181, loss=0.02342487219721079\n",
      "Surface training t=23182, loss=0.023406133987009525\n",
      "Surface training t=23183, loss=0.021254414692521095\n",
      "Surface training t=23184, loss=0.019422142300754786\n",
      "Surface training t=23185, loss=0.02748110517859459\n",
      "Surface training t=23186, loss=0.031565114855766296\n",
      "Surface training t=23187, loss=0.028903922997415066\n",
      "Surface training t=23188, loss=0.03821338526904583\n",
      "Surface training t=23189, loss=0.024490720592439175\n",
      "Surface training t=23190, loss=0.022167320363223553\n",
      "Surface training t=23191, loss=0.02405381202697754\n",
      "Surface training t=23192, loss=0.020726943388581276\n",
      "Surface training t=23193, loss=0.019795347936451435\n",
      "Surface training t=23194, loss=0.022146673873066902\n",
      "Surface training t=23195, loss=0.023170925676822662\n",
      "Surface training t=23196, loss=0.02046298887580633\n",
      "Surface training t=23197, loss=0.017628994770348072\n",
      "Surface training t=23198, loss=0.020685351453721523\n",
      "Surface training t=23199, loss=0.02034070063382387\n",
      "Surface training t=23200, loss=0.02372357528656721\n",
      "Surface training t=23201, loss=0.019329371862113476\n",
      "Surface training t=23202, loss=0.017133550718426704\n",
      "Surface training t=23203, loss=0.020402790047228336\n",
      "Surface training t=23204, loss=0.01963403820991516\n",
      "Surface training t=23205, loss=0.019663344137370586\n",
      "Surface training t=23206, loss=0.026078549213707447\n",
      "Surface training t=23207, loss=0.027215056121349335\n",
      "Surface training t=23208, loss=0.023921675980091095\n",
      "Surface training t=23209, loss=0.027192377485334873\n",
      "Surface training t=23210, loss=0.03195499628782272\n",
      "Surface training t=23211, loss=0.03156762570142746\n",
      "Surface training t=23212, loss=0.026244167238473892\n",
      "Surface training t=23213, loss=0.028728967532515526\n",
      "Surface training t=23214, loss=0.020810415968298912\n",
      "Surface training t=23215, loss=0.024321299977600574\n",
      "Surface training t=23216, loss=0.025641942396759987\n",
      "Surface training t=23217, loss=0.025738859549164772\n",
      "Surface training t=23218, loss=0.02493704203516245\n",
      "Surface training t=23219, loss=0.02668660320341587\n",
      "Surface training t=23220, loss=0.02884845808148384\n",
      "Surface training t=23221, loss=0.026682274416089058\n",
      "Surface training t=23222, loss=0.026906081475317478\n",
      "Surface training t=23223, loss=0.020735032856464386\n",
      "Surface training t=23224, loss=0.020976562052965164\n",
      "Surface training t=23225, loss=0.02066629659384489\n",
      "Surface training t=23226, loss=0.03374416381120682\n",
      "Surface training t=23227, loss=0.02809277828782797\n",
      "Surface training t=23228, loss=0.019937907345592976\n",
      "Surface training t=23229, loss=0.025002005510032177\n",
      "Surface training t=23230, loss=0.030436117202043533\n",
      "Surface training t=23231, loss=0.016304221004247665\n",
      "Surface training t=23232, loss=0.02507366891950369\n",
      "Surface training t=23233, loss=0.021355903707444668\n",
      "Surface training t=23234, loss=0.02122977189719677\n",
      "Surface training t=23235, loss=0.016427663154900074\n",
      "Surface training t=23236, loss=0.02553558349609375\n",
      "Surface training t=23237, loss=0.03149818256497383\n",
      "Surface training t=23238, loss=0.047218287363648415\n",
      "Surface training t=23239, loss=0.03611228335648775\n",
      "Surface training t=23240, loss=0.029520371928811073\n",
      "Surface training t=23241, loss=0.022283130325376987\n",
      "Surface training t=23242, loss=0.034960679709911346\n",
      "Surface training t=23243, loss=0.03192464541643858\n",
      "Surface training t=23244, loss=0.03519116621464491\n",
      "Surface training t=23245, loss=0.026057783514261246\n",
      "Surface training t=23246, loss=0.02780937682837248\n",
      "Surface training t=23247, loss=0.028127672150731087\n",
      "Surface training t=23248, loss=0.02404923364520073\n",
      "Surface training t=23249, loss=0.016138048842549324\n",
      "Surface training t=23250, loss=0.017908000387251377\n",
      "Surface training t=23251, loss=0.01971401274204254\n",
      "Surface training t=23252, loss=0.019239196553826332\n",
      "Surface training t=23253, loss=0.019117744639515877\n",
      "Surface training t=23254, loss=0.02151612378656864\n",
      "Surface training t=23255, loss=0.021718185395002365\n",
      "Surface training t=23256, loss=0.020445559173822403\n",
      "Surface training t=23257, loss=0.025868529453873634\n",
      "Surface training t=23258, loss=0.02857372723519802\n",
      "Surface training t=23259, loss=0.024006351828575134\n",
      "Surface training t=23260, loss=0.02005138900130987\n",
      "Surface training t=23261, loss=0.019521146081387997\n",
      "Surface training t=23262, loss=0.01902696955949068\n",
      "Surface training t=23263, loss=0.022015871480107307\n",
      "Surface training t=23264, loss=0.018113447353243828\n",
      "Surface training t=23265, loss=0.01996792759746313\n",
      "Surface training t=23266, loss=0.020722071640193462\n",
      "Surface training t=23267, loss=0.0213732635602355\n",
      "Surface training t=23268, loss=0.019363606348633766\n",
      "Surface training t=23269, loss=0.02232264168560505\n",
      "Surface training t=23270, loss=0.030458640307188034\n",
      "Surface training t=23271, loss=0.023526144213974476\n",
      "Surface training t=23272, loss=0.028998871333897114\n",
      "Surface training t=23273, loss=0.033200254663825035\n",
      "Surface training t=23274, loss=0.03766159899532795\n",
      "Surface training t=23275, loss=0.03255739249289036\n",
      "Surface training t=23276, loss=0.026439914479851723\n",
      "Surface training t=23277, loss=0.01852928288280964\n",
      "Surface training t=23278, loss=0.020562225952744484\n",
      "Surface training t=23279, loss=0.02299086283892393\n",
      "Surface training t=23280, loss=0.023730999790132046\n",
      "Surface training t=23281, loss=0.03653527796268463\n",
      "Surface training t=23282, loss=0.041782746091485023\n",
      "Surface training t=23283, loss=0.034245869144797325\n",
      "Surface training t=23284, loss=0.038992918096482754\n",
      "Surface training t=23285, loss=0.03292234614491463\n",
      "Surface training t=23286, loss=0.023947957903146744\n",
      "Surface training t=23287, loss=0.02341119386255741\n",
      "Surface training t=23288, loss=0.029620193876326084\n",
      "Surface training t=23289, loss=0.025847258046269417\n",
      "Surface training t=23290, loss=0.03095034696161747\n",
      "Surface training t=23291, loss=0.0342219565063715\n",
      "Surface training t=23292, loss=0.022817956283688545\n",
      "Surface training t=23293, loss=0.021624435670673847\n",
      "Surface training t=23294, loss=0.023284479044377804\n",
      "Surface training t=23295, loss=0.02621051575988531\n",
      "Surface training t=23296, loss=0.02029884047806263\n",
      "Surface training t=23297, loss=0.023166473023593426\n",
      "Surface training t=23298, loss=0.02403123490512371\n",
      "Surface training t=23299, loss=0.03284483030438423\n",
      "Surface training t=23300, loss=0.02547683473676443\n",
      "Surface training t=23301, loss=0.032528446055948734\n",
      "Surface training t=23302, loss=0.03183265868574381\n",
      "Surface training t=23303, loss=0.021363118663430214\n",
      "Surface training t=23304, loss=0.02235238766297698\n",
      "Surface training t=23305, loss=0.03455622307956219\n",
      "Surface training t=23306, loss=0.022208855487406254\n",
      "Surface training t=23307, loss=0.031906397081911564\n",
      "Surface training t=23308, loss=0.02175991889089346\n",
      "Surface training t=23309, loss=0.031085120514035225\n",
      "Surface training t=23310, loss=0.023278072476387024\n",
      "Surface training t=23311, loss=0.021214826963841915\n",
      "Surface training t=23312, loss=0.017451763153076172\n",
      "Surface training t=23313, loss=0.020403483882546425\n",
      "Surface training t=23314, loss=0.01851736195385456\n",
      "Surface training t=23315, loss=0.01880366075783968\n",
      "Surface training t=23316, loss=0.0175437293946743\n",
      "Surface training t=23317, loss=0.01897067204117775\n",
      "Surface training t=23318, loss=0.02569814957678318\n",
      "Surface training t=23319, loss=0.025662586092948914\n",
      "Surface training t=23320, loss=0.029756424017250538\n",
      "Surface training t=23321, loss=0.022728556767106056\n",
      "Surface training t=23322, loss=0.023910864256322384\n",
      "Surface training t=23323, loss=0.025657296180725098\n",
      "Surface training t=23324, loss=0.023686238564550877\n",
      "Surface training t=23325, loss=0.022097712382674217\n",
      "Surface training t=23326, loss=0.020125240087509155\n",
      "Surface training t=23327, loss=0.017356744036078453\n",
      "Surface training t=23328, loss=0.017147612757980824\n",
      "Surface training t=23329, loss=0.0226743184030056\n",
      "Surface training t=23330, loss=0.02995341829955578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=23331, loss=0.02479317458346486\n",
      "Surface training t=23332, loss=0.020763498730957508\n",
      "Surface training t=23333, loss=0.018533642403781414\n",
      "Surface training t=23334, loss=0.02522372081875801\n",
      "Surface training t=23335, loss=0.026024318765848875\n",
      "Surface training t=23336, loss=0.02504814974963665\n",
      "Surface training t=23337, loss=0.024746809154748917\n",
      "Surface training t=23338, loss=0.022996045649051666\n",
      "Surface training t=23339, loss=0.026020117104053497\n",
      "Surface training t=23340, loss=0.02577590849250555\n",
      "Surface training t=23341, loss=0.022854062728583813\n",
      "Surface training t=23342, loss=0.02756225038319826\n",
      "Surface training t=23343, loss=0.02344699390232563\n",
      "Surface training t=23344, loss=0.021900678053498268\n",
      "Surface training t=23345, loss=0.023320600390434265\n",
      "Surface training t=23346, loss=0.029027347452938557\n",
      "Surface training t=23347, loss=0.025100531987845898\n",
      "Surface training t=23348, loss=0.032823086716234684\n",
      "Surface training t=23349, loss=0.02152380719780922\n",
      "Surface training t=23350, loss=0.02335314592346549\n",
      "Surface training t=23351, loss=0.023827578872442245\n",
      "Surface training t=23352, loss=0.024813303723931313\n",
      "Surface training t=23353, loss=0.024686583317816257\n",
      "Surface training t=23354, loss=0.03551473468542099\n",
      "Surface training t=23355, loss=0.027182008139789104\n",
      "Surface training t=23356, loss=0.0293714739382267\n",
      "Surface training t=23357, loss=0.04105146788060665\n",
      "Surface training t=23358, loss=0.029171557165682316\n",
      "Surface training t=23359, loss=0.035785309970378876\n",
      "Surface training t=23360, loss=0.028289180248975754\n",
      "Surface training t=23361, loss=0.03011910431087017\n",
      "Surface training t=23362, loss=0.04393971338868141\n",
      "Surface training t=23363, loss=0.032762862741947174\n",
      "Surface training t=23364, loss=0.02897025365382433\n",
      "Surface training t=23365, loss=0.03477114625275135\n",
      "Surface training t=23366, loss=0.03819335624575615\n",
      "Surface training t=23367, loss=0.0261400006711483\n",
      "Surface training t=23368, loss=0.023455089889466763\n",
      "Surface training t=23369, loss=0.023288868367671967\n",
      "Surface training t=23370, loss=0.0187325244769454\n",
      "Surface training t=23371, loss=0.031076960265636444\n",
      "Surface training t=23372, loss=0.029859078116714954\n",
      "Surface training t=23373, loss=0.021516910754144192\n",
      "Surface training t=23374, loss=0.023590470664203167\n",
      "Surface training t=23375, loss=0.021698784083127975\n",
      "Surface training t=23376, loss=0.023140071891248226\n",
      "Surface training t=23377, loss=0.020933245308697224\n",
      "Surface training t=23378, loss=0.02636039350181818\n",
      "Surface training t=23379, loss=0.02865874581038952\n",
      "Surface training t=23380, loss=0.025857748463749886\n",
      "Surface training t=23381, loss=0.028206873685121536\n",
      "Surface training t=23382, loss=0.021400834433734417\n",
      "Surface training t=23383, loss=0.019449755549430847\n",
      "Surface training t=23384, loss=0.019155016168951988\n",
      "Surface training t=23385, loss=0.018515233881771564\n",
      "Surface training t=23386, loss=0.01903616636991501\n",
      "Surface training t=23387, loss=0.02147543989121914\n",
      "Surface training t=23388, loss=0.021960311569273472\n",
      "Surface training t=23389, loss=0.022580501157790422\n",
      "Surface training t=23390, loss=0.019670026376843452\n",
      "Surface training t=23391, loss=0.01984657160937786\n",
      "Surface training t=23392, loss=0.023882124572992325\n",
      "Surface training t=23393, loss=0.020965960808098316\n",
      "Surface training t=23394, loss=0.025755001232028008\n",
      "Surface training t=23395, loss=0.02413350623100996\n",
      "Surface training t=23396, loss=0.024074556306004524\n",
      "Surface training t=23397, loss=0.029002427123486996\n",
      "Surface training t=23398, loss=0.02664385922253132\n",
      "Surface training t=23399, loss=0.020026396960020065\n",
      "Surface training t=23400, loss=0.03527821972966194\n",
      "Surface training t=23401, loss=0.01987580955028534\n",
      "Surface training t=23402, loss=0.016172828618437052\n",
      "Surface training t=23403, loss=0.01929635740816593\n",
      "Surface training t=23404, loss=0.021121722646057606\n",
      "Surface training t=23405, loss=0.01755037158727646\n",
      "Surface training t=23406, loss=0.0173154566437006\n",
      "Surface training t=23407, loss=0.015438181348145008\n",
      "Surface training t=23408, loss=0.017605011351406574\n",
      "Surface training t=23409, loss=0.015702959150075912\n",
      "Surface training t=23410, loss=0.017230127938091755\n",
      "Surface training t=23411, loss=0.02275510597974062\n",
      "Surface training t=23412, loss=0.024145412258803844\n",
      "Surface training t=23413, loss=0.028642456978559494\n",
      "Surface training t=23414, loss=0.028867660090327263\n",
      "Surface training t=23415, loss=0.01947722490876913\n",
      "Surface training t=23416, loss=0.02451299224048853\n",
      "Surface training t=23417, loss=0.026288246735930443\n",
      "Surface training t=23418, loss=0.024383515119552612\n",
      "Surface training t=23419, loss=0.020530881360173225\n",
      "Surface training t=23420, loss=0.02419718075543642\n",
      "Surface training t=23421, loss=0.02957678586244583\n",
      "Surface training t=23422, loss=0.024354323744773865\n",
      "Surface training t=23423, loss=0.019626307301223278\n",
      "Surface training t=23424, loss=0.015446734614670277\n",
      "Surface training t=23425, loss=0.02220419328659773\n",
      "Surface training t=23426, loss=0.016137059777975082\n",
      "Surface training t=23427, loss=0.02137209288775921\n",
      "Surface training t=23428, loss=0.02124390471726656\n",
      "Surface training t=23429, loss=0.019431285560131073\n",
      "Surface training t=23430, loss=0.019675762858241796\n",
      "Surface training t=23431, loss=0.021125500090420246\n",
      "Surface training t=23432, loss=0.018250396475195885\n",
      "Surface training t=23433, loss=0.022214218974113464\n",
      "Surface training t=23434, loss=0.0204799585044384\n",
      "Surface training t=23435, loss=0.018755493685603142\n",
      "Surface training t=23436, loss=0.017809227108955383\n",
      "Surface training t=23437, loss=0.022833186201751232\n",
      "Surface training t=23438, loss=0.028749486431479454\n",
      "Surface training t=23439, loss=0.025791170075535774\n",
      "Surface training t=23440, loss=0.023750796914100647\n",
      "Surface training t=23441, loss=0.027842633426189423\n",
      "Surface training t=23442, loss=0.023059850558638573\n",
      "Surface training t=23443, loss=0.022618172224611044\n",
      "Surface training t=23444, loss=0.021105737425386906\n",
      "Surface training t=23445, loss=0.033070825040340424\n",
      "Surface training t=23446, loss=0.025894599966704845\n",
      "Surface training t=23447, loss=0.03663904778659344\n",
      "Surface training t=23448, loss=0.03377251606434584\n",
      "Surface training t=23449, loss=0.026327311992645264\n",
      "Surface training t=23450, loss=0.01743443263694644\n",
      "Surface training t=23451, loss=0.01775344740599394\n",
      "Surface training t=23452, loss=0.015080845914781094\n",
      "Surface training t=23453, loss=0.01989607699215412\n",
      "Surface training t=23454, loss=0.017630111426115036\n",
      "Surface training t=23455, loss=0.017355136573314667\n",
      "Surface training t=23456, loss=0.021196061745285988\n",
      "Surface training t=23457, loss=0.019416775554418564\n",
      "Surface training t=23458, loss=0.015286494046449661\n",
      "Surface training t=23459, loss=0.02105474192649126\n",
      "Surface training t=23460, loss=0.029111651703715324\n",
      "Surface training t=23461, loss=0.03153366129845381\n",
      "Surface training t=23462, loss=0.03847695700824261\n",
      "Surface training t=23463, loss=0.02449604868888855\n",
      "Surface training t=23464, loss=0.027555035427212715\n",
      "Surface training t=23465, loss=0.03190810792148113\n",
      "Surface training t=23466, loss=0.03538217302411795\n",
      "Surface training t=23467, loss=0.03359710983932018\n",
      "Surface training t=23468, loss=0.033207548782229424\n",
      "Surface training t=23469, loss=0.03607994690537453\n",
      "Surface training t=23470, loss=0.04520828276872635\n",
      "Surface training t=23471, loss=0.04152160696685314\n",
      "Surface training t=23472, loss=0.03853919915854931\n",
      "Surface training t=23473, loss=0.038185840472579\n",
      "Surface training t=23474, loss=0.029365376569330692\n",
      "Surface training t=23475, loss=0.046509988605976105\n",
      "Surface training t=23476, loss=0.04015609622001648\n",
      "Surface training t=23477, loss=0.033378930762410164\n",
      "Surface training t=23478, loss=0.03506769984960556\n",
      "Surface training t=23479, loss=0.04457950219511986\n",
      "Surface training t=23480, loss=0.033468637615442276\n",
      "Surface training t=23481, loss=0.026817046105861664\n",
      "Surface training t=23482, loss=0.030815185979008675\n",
      "Surface training t=23483, loss=0.027232706546783447\n",
      "Surface training t=23484, loss=0.022439611610025167\n",
      "Surface training t=23485, loss=0.032078973948955536\n",
      "Surface training t=23486, loss=0.02736726962029934\n",
      "Surface training t=23487, loss=0.03784852661192417\n",
      "Surface training t=23488, loss=0.032084157690405846\n",
      "Surface training t=23489, loss=0.03855193965137005\n",
      "Surface training t=23490, loss=0.042518250644207\n",
      "Surface training t=23491, loss=0.03828423377126455\n",
      "Surface training t=23492, loss=0.033475483767688274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=23493, loss=0.055427027866244316\n",
      "Surface training t=23494, loss=0.040791542269289494\n",
      "Surface training t=23495, loss=0.05062909983098507\n",
      "Surface training t=23496, loss=0.045418862253427505\n",
      "Surface training t=23497, loss=0.05255349725484848\n",
      "Surface training t=23498, loss=0.03914847411215305\n",
      "Surface training t=23499, loss=0.03140517696738243\n",
      "Surface training t=23500, loss=0.021623247303068638\n",
      "Surface training t=23501, loss=0.01991522591561079\n",
      "Surface training t=23502, loss=0.02328023500740528\n",
      "Surface training t=23503, loss=0.03149035945534706\n",
      "Surface training t=23504, loss=0.029404347762465477\n",
      "Surface training t=23505, loss=0.028267022222280502\n",
      "Surface training t=23506, loss=0.03135021589696407\n",
      "Surface training t=23507, loss=0.03284649737179279\n",
      "Surface training t=23508, loss=0.04399744234979153\n",
      "Surface training t=23509, loss=0.03288703877478838\n",
      "Surface training t=23510, loss=0.03653874807059765\n",
      "Surface training t=23511, loss=0.025972740724682808\n",
      "Surface training t=23512, loss=0.025734572671353817\n",
      "Surface training t=23513, loss=0.024638373404741287\n",
      "Surface training t=23514, loss=0.024480419233441353\n",
      "Surface training t=23515, loss=0.026391902938485146\n",
      "Surface training t=23516, loss=0.031713674776256084\n",
      "Surface training t=23517, loss=0.02422836795449257\n",
      "Surface training t=23518, loss=0.02188394032418728\n",
      "Surface training t=23519, loss=0.02046989742666483\n",
      "Surface training t=23520, loss=0.021330779418349266\n",
      "Surface training t=23521, loss=0.029330575838685036\n",
      "Surface training t=23522, loss=0.026010854169726372\n",
      "Surface training t=23523, loss=0.023165633901953697\n",
      "Surface training t=23524, loss=0.036219941452145576\n",
      "Surface training t=23525, loss=0.02755069825798273\n",
      "Surface training t=23526, loss=0.030943142250180244\n",
      "Surface training t=23527, loss=0.023655912838876247\n",
      "Surface training t=23528, loss=0.02022387459874153\n",
      "Surface training t=23529, loss=0.025286145508289337\n",
      "Surface training t=23530, loss=0.025502740405499935\n",
      "Surface training t=23531, loss=0.022045082412660122\n",
      "Surface training t=23532, loss=0.01991075649857521\n",
      "Surface training t=23533, loss=0.035861698910593987\n",
      "Surface training t=23534, loss=0.03286212496459484\n",
      "Surface training t=23535, loss=0.023091751150786877\n",
      "Surface training t=23536, loss=0.026143459603190422\n",
      "Surface training t=23537, loss=0.025862740352749825\n",
      "Surface training t=23538, loss=0.03026433289051056\n",
      "Surface training t=23539, loss=0.028567306697368622\n",
      "Surface training t=23540, loss=0.029192445799708366\n",
      "Surface training t=23541, loss=0.0386394876986742\n",
      "Surface training t=23542, loss=0.02901976928114891\n",
      "Surface training t=23543, loss=0.02857169881463051\n",
      "Surface training t=23544, loss=0.028774472884833813\n",
      "Surface training t=23545, loss=0.028958739712834358\n",
      "Surface training t=23546, loss=0.04240043833851814\n",
      "Surface training t=23547, loss=0.03264937736093998\n",
      "Surface training t=23548, loss=0.03287384007126093\n",
      "Surface training t=23549, loss=0.03052646853029728\n",
      "Surface training t=23550, loss=0.030511487275362015\n",
      "Surface training t=23551, loss=0.034251997247338295\n",
      "Surface training t=23552, loss=0.03274659439921379\n",
      "Surface training t=23553, loss=0.029144037514925003\n",
      "Surface training t=23554, loss=0.028291945345699787\n",
      "Surface training t=23555, loss=0.03425130061805248\n",
      "Surface training t=23556, loss=0.02928513754159212\n",
      "Surface training t=23557, loss=0.03507532738149166\n",
      "Surface training t=23558, loss=0.03421641979366541\n",
      "Surface training t=23559, loss=0.03030022606253624\n",
      "Surface training t=23560, loss=0.026405805721879005\n",
      "Surface training t=23561, loss=0.02720945980399847\n",
      "Surface training t=23562, loss=0.02014261484146118\n",
      "Surface training t=23563, loss=0.025232858955860138\n",
      "Surface training t=23564, loss=0.023999215103685856\n",
      "Surface training t=23565, loss=0.025199413299560547\n",
      "Surface training t=23566, loss=0.02556647453457117\n",
      "Surface training t=23567, loss=0.02638288028538227\n",
      "Surface training t=23568, loss=0.023078657686710358\n",
      "Surface training t=23569, loss=0.021669195033609867\n",
      "Surface training t=23570, loss=0.020044546574354172\n",
      "Surface training t=23571, loss=0.024944485165178776\n",
      "Surface training t=23572, loss=0.02300167828798294\n",
      "Surface training t=23573, loss=0.024612476117908955\n",
      "Surface training t=23574, loss=0.030654583126306534\n",
      "Surface training t=23575, loss=0.024717316031455994\n",
      "Surface training t=23576, loss=0.02362103946506977\n",
      "Surface training t=23577, loss=0.026454851031303406\n",
      "Surface training t=23578, loss=0.02345424611121416\n",
      "Surface training t=23579, loss=0.032187518663704395\n",
      "Surface training t=23580, loss=0.01960866805166006\n",
      "Surface training t=23581, loss=0.028465164825320244\n",
      "Surface training t=23582, loss=0.02870920766144991\n",
      "Surface training t=23583, loss=0.023702009581029415\n",
      "Surface training t=23584, loss=0.022108503617346287\n",
      "Surface training t=23585, loss=0.027671503834426403\n",
      "Surface training t=23586, loss=0.025305273942649364\n",
      "Surface training t=23587, loss=0.016941430047154427\n",
      "Surface training t=23588, loss=0.020243016071617603\n",
      "Surface training t=23589, loss=0.02162999100983143\n",
      "Surface training t=23590, loss=0.022707410156726837\n",
      "Surface training t=23591, loss=0.017398326192051172\n",
      "Surface training t=23592, loss=0.01932370476424694\n",
      "Surface training t=23593, loss=0.022217020392417908\n",
      "Surface training t=23594, loss=0.03656799532473087\n",
      "Surface training t=23595, loss=0.02829224243760109\n",
      "Surface training t=23596, loss=0.033405386842787266\n",
      "Surface training t=23597, loss=0.0277737807482481\n",
      "Surface training t=23598, loss=0.02633110899478197\n",
      "Surface training t=23599, loss=0.024657935835421085\n",
      "Surface training t=23600, loss=0.02250022254884243\n",
      "Surface training t=23601, loss=0.01592212077230215\n",
      "Surface training t=23602, loss=0.015824919566512108\n",
      "Surface training t=23603, loss=0.016661236062645912\n",
      "Surface training t=23604, loss=0.01698865182697773\n",
      "Surface training t=23605, loss=0.018039457499980927\n",
      "Surface training t=23606, loss=0.016623561270534992\n",
      "Surface training t=23607, loss=0.018207954242825508\n",
      "Surface training t=23608, loss=0.024925739504396915\n",
      "Surface training t=23609, loss=0.019841407425701618\n",
      "Surface training t=23610, loss=0.031661782413721085\n",
      "Surface training t=23611, loss=0.03839608281850815\n",
      "Surface training t=23612, loss=0.029561727307736874\n",
      "Surface training t=23613, loss=0.030210287310183048\n",
      "Surface training t=23614, loss=0.026549361646175385\n",
      "Surface training t=23615, loss=0.02365761250257492\n",
      "Surface training t=23616, loss=0.030646358616650105\n",
      "Surface training t=23617, loss=0.031241513788700104\n",
      "Surface training t=23618, loss=0.023033468052744865\n",
      "Surface training t=23619, loss=0.02782402280718088\n",
      "Surface training t=23620, loss=0.029729328118264675\n",
      "Surface training t=23621, loss=0.026666775345802307\n",
      "Surface training t=23622, loss=0.036035263910889626\n",
      "Surface training t=23623, loss=0.036502547562122345\n",
      "Surface training t=23624, loss=0.024246089160442352\n",
      "Surface training t=23625, loss=0.026104074902832508\n",
      "Surface training t=23626, loss=0.029702224768698215\n",
      "Surface training t=23627, loss=0.029471098445355892\n",
      "Surface training t=23628, loss=0.023226357996463776\n",
      "Surface training t=23629, loss=0.028746962547302246\n",
      "Surface training t=23630, loss=0.028830124996602535\n",
      "Surface training t=23631, loss=0.026238284073770046\n",
      "Surface training t=23632, loss=0.024033035151660442\n",
      "Surface training t=23633, loss=0.019998296163976192\n",
      "Surface training t=23634, loss=0.024234643206000328\n",
      "Surface training t=23635, loss=0.026170819997787476\n",
      "Surface training t=23636, loss=0.028080391697585583\n",
      "Surface training t=23637, loss=0.0318838506937027\n",
      "Surface training t=23638, loss=0.026121455244719982\n",
      "Surface training t=23639, loss=0.028733573853969574\n",
      "Surface training t=23640, loss=0.021265928633511066\n",
      "Surface training t=23641, loss=0.02400832623243332\n",
      "Surface training t=23642, loss=0.020181632600724697\n",
      "Surface training t=23643, loss=0.029096882790327072\n",
      "Surface training t=23644, loss=0.026912719942629337\n",
      "Surface training t=23645, loss=0.03190780244767666\n",
      "Surface training t=23646, loss=0.03169453330338001\n",
      "Surface training t=23647, loss=0.03937350772321224\n",
      "Surface training t=23648, loss=0.02559081930667162\n",
      "Surface training t=23649, loss=0.024684127420186996\n",
      "Surface training t=23650, loss=0.03473486751317978\n",
      "Surface training t=23651, loss=0.025233098305761814\n",
      "Surface training t=23652, loss=0.029799741692841053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=23653, loss=0.027030743658542633\n",
      "Surface training t=23654, loss=0.02778300642967224\n",
      "Surface training t=23655, loss=0.029834473505616188\n",
      "Surface training t=23656, loss=0.023635723628103733\n",
      "Surface training t=23657, loss=0.01868475042283535\n",
      "Surface training t=23658, loss=0.023073244839906693\n",
      "Surface training t=23659, loss=0.02164463046938181\n",
      "Surface training t=23660, loss=0.028778203763067722\n",
      "Surface training t=23661, loss=0.01960688177496195\n",
      "Surface training t=23662, loss=0.025060582906007767\n",
      "Surface training t=23663, loss=0.020480399951338768\n",
      "Surface training t=23664, loss=0.020359236747026443\n",
      "Surface training t=23665, loss=0.01971256360411644\n",
      "Surface training t=23666, loss=0.022409881465137005\n",
      "Surface training t=23667, loss=0.01712147891521454\n",
      "Surface training t=23668, loss=0.015432432293891907\n",
      "Surface training t=23669, loss=0.013606593012809753\n",
      "Surface training t=23670, loss=0.016927563585340977\n",
      "Surface training t=23671, loss=0.017459805589169264\n",
      "Surface training t=23672, loss=0.014459885191172361\n",
      "Surface training t=23673, loss=0.018218674696981907\n",
      "Surface training t=23674, loss=0.0180191183462739\n",
      "Surface training t=23675, loss=0.012968584895133972\n",
      "Surface training t=23676, loss=0.016039030626416206\n",
      "Surface training t=23677, loss=0.019929238595068455\n",
      "Surface training t=23678, loss=0.021597066894173622\n",
      "Surface training t=23679, loss=0.019465740770101547\n",
      "Surface training t=23680, loss=0.019279024563729763\n",
      "Surface training t=23681, loss=0.024209288880228996\n",
      "Surface training t=23682, loss=0.029330501332879066\n",
      "Surface training t=23683, loss=0.02373940870165825\n",
      "Surface training t=23684, loss=0.024647468701004982\n",
      "Surface training t=23685, loss=0.018323122523725033\n",
      "Surface training t=23686, loss=0.018991151824593544\n",
      "Surface training t=23687, loss=0.0229779789224267\n",
      "Surface training t=23688, loss=0.022253609262406826\n",
      "Surface training t=23689, loss=0.023792964406311512\n",
      "Surface training t=23690, loss=0.018229668028652668\n",
      "Surface training t=23691, loss=0.01992736803367734\n",
      "Surface training t=23692, loss=0.021170747466385365\n",
      "Surface training t=23693, loss=0.01706693647429347\n",
      "Surface training t=23694, loss=0.026317493990063667\n",
      "Surface training t=23695, loss=0.017739323899149895\n",
      "Surface training t=23696, loss=0.01859431527554989\n",
      "Surface training t=23697, loss=0.01419068081304431\n",
      "Surface training t=23698, loss=0.019044047221541405\n",
      "Surface training t=23699, loss=0.01482229633256793\n",
      "Surface training t=23700, loss=0.018794759176671505\n",
      "Surface training t=23701, loss=0.024928899481892586\n",
      "Surface training t=23702, loss=0.01828186586499214\n",
      "Surface training t=23703, loss=0.02266431227326393\n",
      "Surface training t=23704, loss=0.029608347453176975\n",
      "Surface training t=23705, loss=0.022366030141711235\n",
      "Surface training t=23706, loss=0.02390029840171337\n",
      "Surface training t=23707, loss=0.022931539453566074\n",
      "Surface training t=23708, loss=0.023076684214174747\n",
      "Surface training t=23709, loss=0.023331278935074806\n",
      "Surface training t=23710, loss=0.024920394644141197\n",
      "Surface training t=23711, loss=0.02460301388055086\n",
      "Surface training t=23712, loss=0.03145258128643036\n",
      "Surface training t=23713, loss=0.025344830006361008\n",
      "Surface training t=23714, loss=0.02440347708761692\n",
      "Surface training t=23715, loss=0.03150648809969425\n",
      "Surface training t=23716, loss=0.022665993310511112\n",
      "Surface training t=23717, loss=0.03330790903419256\n",
      "Surface training t=23718, loss=0.020898034796118736\n",
      "Surface training t=23719, loss=0.022731679491698742\n",
      "Surface training t=23720, loss=0.024714468978345394\n",
      "Surface training t=23721, loss=0.02583344653248787\n",
      "Surface training t=23722, loss=0.016155574470758438\n",
      "Surface training t=23723, loss=0.02133171074092388\n",
      "Surface training t=23724, loss=0.022568109445273876\n",
      "Surface training t=23725, loss=0.019890311174094677\n",
      "Surface training t=23726, loss=0.016628206707537174\n",
      "Surface training t=23727, loss=0.021947670727968216\n",
      "Surface training t=23728, loss=0.025766829028725624\n",
      "Surface training t=23729, loss=0.02000632928684354\n",
      "Surface training t=23730, loss=0.0209744181483984\n",
      "Surface training t=23731, loss=0.01736885029822588\n",
      "Surface training t=23732, loss=0.020092211198061705\n",
      "Surface training t=23733, loss=0.014094098471105099\n",
      "Surface training t=23734, loss=0.016642852686345577\n",
      "Surface training t=23735, loss=0.016567429527640343\n",
      "Surface training t=23736, loss=0.016323570162057877\n",
      "Surface training t=23737, loss=0.02326312381774187\n",
      "Surface training t=23738, loss=0.019765262492001057\n",
      "Surface training t=23739, loss=0.018437574617564678\n",
      "Surface training t=23740, loss=0.014643467031419277\n",
      "Surface training t=23741, loss=0.0184996509924531\n",
      "Surface training t=23742, loss=0.015448745340108871\n",
      "Surface training t=23743, loss=0.023645748384296894\n",
      "Surface training t=23744, loss=0.03363283537328243\n",
      "Surface training t=23745, loss=0.03417823649942875\n",
      "Surface training t=23746, loss=0.03387232590466738\n",
      "Surface training t=23747, loss=0.049611328169703484\n",
      "Surface training t=23748, loss=0.0434954259544611\n",
      "Surface training t=23749, loss=0.036845484748482704\n",
      "Surface training t=23750, loss=0.04493635147809982\n",
      "Surface training t=23751, loss=0.04587579891085625\n",
      "Surface training t=23752, loss=0.03609337005764246\n",
      "Surface training t=23753, loss=0.034546250477433205\n",
      "Surface training t=23754, loss=0.03108236286789179\n",
      "Surface training t=23755, loss=0.0317680174484849\n",
      "Surface training t=23756, loss=0.02911088988184929\n",
      "Surface training t=23757, loss=0.028685709461569786\n",
      "Surface training t=23758, loss=0.02890244498848915\n",
      "Surface training t=23759, loss=0.02980031445622444\n",
      "Surface training t=23760, loss=0.024243319407105446\n",
      "Surface training t=23761, loss=0.028455485589802265\n",
      "Surface training t=23762, loss=0.02887000236660242\n",
      "Surface training t=23763, loss=0.02205041330307722\n",
      "Surface training t=23764, loss=0.02516128681600094\n",
      "Surface training t=23765, loss=0.026265683583915234\n",
      "Surface training t=23766, loss=0.018920287489891052\n",
      "Surface training t=23767, loss=0.020372452680021524\n",
      "Surface training t=23768, loss=0.013936765491962433\n",
      "Surface training t=23769, loss=0.01794961979612708\n",
      "Surface training t=23770, loss=0.017645190469920635\n",
      "Surface training t=23771, loss=0.01534958090633154\n",
      "Surface training t=23772, loss=0.017493772320449352\n",
      "Surface training t=23773, loss=0.026442689821124077\n",
      "Surface training t=23774, loss=0.02838108502328396\n",
      "Surface training t=23775, loss=0.0242025014013052\n",
      "Surface training t=23776, loss=0.025612199679017067\n",
      "Surface training t=23777, loss=0.022892635315656662\n",
      "Surface training t=23778, loss=0.02406386472284794\n",
      "Surface training t=23779, loss=0.01735875802114606\n",
      "Surface training t=23780, loss=0.02277999371290207\n",
      "Surface training t=23781, loss=0.021972027607262135\n",
      "Surface training t=23782, loss=0.024874690920114517\n",
      "Surface training t=23783, loss=0.030471653677523136\n",
      "Surface training t=23784, loss=0.02644809614866972\n",
      "Surface training t=23785, loss=0.036992816254496574\n",
      "Surface training t=23786, loss=0.021032363176345825\n",
      "Surface training t=23787, loss=0.027486290782690048\n",
      "Surface training t=23788, loss=0.02699306048452854\n",
      "Surface training t=23789, loss=0.02340146992355585\n",
      "Surface training t=23790, loss=0.02405219804495573\n",
      "Surface training t=23791, loss=0.026173196732997894\n",
      "Surface training t=23792, loss=0.017133643850684166\n",
      "Surface training t=23793, loss=0.01767836231738329\n",
      "Surface training t=23794, loss=0.018133617006242275\n",
      "Surface training t=23795, loss=0.020568926818668842\n",
      "Surface training t=23796, loss=0.017935609444975853\n",
      "Surface training t=23797, loss=0.021092993207275867\n",
      "Surface training t=23798, loss=0.019641499035060406\n",
      "Surface training t=23799, loss=0.017796404659748077\n",
      "Surface training t=23800, loss=0.015584879089146852\n",
      "Surface training t=23801, loss=0.016194675117731094\n",
      "Surface training t=23802, loss=0.01710453350096941\n",
      "Surface training t=23803, loss=0.014929444063454866\n",
      "Surface training t=23804, loss=0.021348051726818085\n",
      "Surface training t=23805, loss=0.020085246302187443\n",
      "Surface training t=23806, loss=0.01882150024175644\n",
      "Surface training t=23807, loss=0.022980067878961563\n",
      "Surface training t=23808, loss=0.027568158693611622\n",
      "Surface training t=23809, loss=0.023269275203347206\n",
      "Surface training t=23810, loss=0.02314777672290802\n",
      "Surface training t=23811, loss=0.021867183968424797\n",
      "Surface training t=23812, loss=0.021370683796703815\n",
      "Surface training t=23813, loss=0.020562785677611828\n",
      "Surface training t=23814, loss=0.019930595066398382\n",
      "Surface training t=23815, loss=0.0252804858610034\n",
      "Surface training t=23816, loss=0.02291283942759037\n",
      "Surface training t=23817, loss=0.017306958325207233\n",
      "Surface training t=23818, loss=0.020965203642845154\n",
      "Surface training t=23819, loss=0.01522746216505766\n",
      "Surface training t=23820, loss=0.02038631495088339\n",
      "Surface training t=23821, loss=0.02060072124004364\n",
      "Surface training t=23822, loss=0.01920344028621912\n",
      "Surface training t=23823, loss=0.018323122523725033\n",
      "Surface training t=23824, loss=0.020426036790013313\n",
      "Surface training t=23825, loss=0.021532798185944557\n",
      "Surface training t=23826, loss=0.01764401886612177\n",
      "Surface training t=23827, loss=0.020774158649146557\n",
      "Surface training t=23828, loss=0.02358590066432953\n",
      "Surface training t=23829, loss=0.030697423964738846\n",
      "Surface training t=23830, loss=0.028116931207478046\n",
      "Surface training t=23831, loss=0.026917876675724983\n",
      "Surface training t=23832, loss=0.023135224357247353\n",
      "Surface training t=23833, loss=0.023997925221920013\n",
      "Surface training t=23834, loss=0.01655254326760769\n",
      "Surface training t=23835, loss=0.017064256593585014\n",
      "Surface training t=23836, loss=0.018607332836836576\n",
      "Surface training t=23837, loss=0.016915615182369947\n",
      "Surface training t=23838, loss=0.022829881869256496\n",
      "Surface training t=23839, loss=0.01710368972271681\n",
      "Surface training t=23840, loss=0.017548952251672745\n",
      "Surface training t=23841, loss=0.020778296515345573\n",
      "Surface training t=23842, loss=0.02972070872783661\n",
      "Surface training t=23843, loss=0.025713663548231125\n",
      "Surface training t=23844, loss=0.02046295627951622\n",
      "Surface training t=23845, loss=0.029916911385953426\n",
      "Surface training t=23846, loss=0.023654223419725895\n",
      "Surface training t=23847, loss=0.02538934350013733\n",
      "Surface training t=23848, loss=0.02252848632633686\n",
      "Surface training t=23849, loss=0.029124373570084572\n",
      "Surface training t=23850, loss=0.029676628299057484\n",
      "Surface training t=23851, loss=0.024154641665518284\n",
      "Surface training t=23852, loss=0.01732125971466303\n",
      "Surface training t=23853, loss=0.02504779677838087\n",
      "Surface training t=23854, loss=0.02076296415179968\n",
      "Surface training t=23855, loss=0.022672809660434723\n",
      "Surface training t=23856, loss=0.04016690142452717\n",
      "Surface training t=23857, loss=0.02682401891797781\n",
      "Surface training t=23858, loss=0.04072926007211208\n",
      "Surface training t=23859, loss=0.03914570715278387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=23860, loss=0.03508002124726772\n",
      "Surface training t=23861, loss=0.039055110886693\n",
      "Surface training t=23862, loss=0.030381455086171627\n",
      "Surface training t=23863, loss=0.029810900799930096\n",
      "Surface training t=23864, loss=0.038490286096930504\n",
      "Surface training t=23865, loss=0.0250209653750062\n",
      "Surface training t=23866, loss=0.048862285912036896\n",
      "Surface training t=23867, loss=0.03527831006795168\n",
      "Surface training t=23868, loss=0.05283898487687111\n",
      "Surface training t=23869, loss=0.03712929226458073\n",
      "Surface training t=23870, loss=0.03564193658530712\n",
      "Surface training t=23871, loss=0.028633346781134605\n",
      "Surface training t=23872, loss=0.045320065692067146\n",
      "Surface training t=23873, loss=0.03699982538819313\n",
      "Surface training t=23874, loss=0.04028995335102081\n",
      "Surface training t=23875, loss=0.029908147640526295\n",
      "Surface training t=23876, loss=0.026734775863587856\n",
      "Surface training t=23877, loss=0.029188125394284725\n",
      "Surface training t=23878, loss=0.024613388814032078\n",
      "Surface training t=23879, loss=0.018701076041907072\n",
      "Surface training t=23880, loss=0.02394176460802555\n",
      "Surface training t=23881, loss=0.024522271938621998\n",
      "Surface training t=23882, loss=0.01873468328267336\n",
      "Surface training t=23883, loss=0.017898951657116413\n",
      "Surface training t=23884, loss=0.022626031190156937\n",
      "Surface training t=23885, loss=0.018449287861585617\n",
      "Surface training t=23886, loss=0.02046248782426119\n",
      "Surface training t=23887, loss=0.019035776611417532\n",
      "Surface training t=23888, loss=0.026994734071195126\n",
      "Surface training t=23889, loss=0.024386429227888584\n",
      "Surface training t=23890, loss=0.021570656448602676\n",
      "Surface training t=23891, loss=0.023164953105151653\n",
      "Surface training t=23892, loss=0.02046556118875742\n",
      "Surface training t=23893, loss=0.02124151773750782\n",
      "Surface training t=23894, loss=0.018651064485311508\n",
      "Surface training t=23895, loss=0.01994914561510086\n",
      "Surface training t=23896, loss=0.027878825552761555\n",
      "Surface training t=23897, loss=0.026888566091656685\n",
      "Surface training t=23898, loss=0.020930327475070953\n",
      "Surface training t=23899, loss=0.016368322540074587\n",
      "Surface training t=23900, loss=0.019532451406121254\n",
      "Surface training t=23901, loss=0.016032644547522068\n",
      "Surface training t=23902, loss=0.01812341809272766\n",
      "Surface training t=23903, loss=0.022804051637649536\n",
      "Surface training t=23904, loss=0.02117680385708809\n",
      "Surface training t=23905, loss=0.03944307938218117\n",
      "Surface training t=23906, loss=0.03193271905183792\n",
      "Surface training t=23907, loss=0.02723050769418478\n",
      "Surface training t=23908, loss=0.03209011070430279\n",
      "Surface training t=23909, loss=0.04060693643987179\n",
      "Surface training t=23910, loss=0.035654387436807156\n",
      "Surface training t=23911, loss=0.028642368502914906\n",
      "Surface training t=23912, loss=0.029734225943684578\n",
      "Surface training t=23913, loss=0.02643914893269539\n",
      "Surface training t=23914, loss=0.029090045019984245\n",
      "Surface training t=23915, loss=0.02491890639066696\n",
      "Surface training t=23916, loss=0.024271915666759014\n",
      "Surface training t=23917, loss=0.025133785791695118\n",
      "Surface training t=23918, loss=0.02233101148158312\n",
      "Surface training t=23919, loss=0.029675332829356194\n",
      "Surface training t=23920, loss=0.027425430715084076\n",
      "Surface training t=23921, loss=0.02879667654633522\n",
      "Surface training t=23922, loss=0.028886955231428146\n",
      "Surface training t=23923, loss=0.021088894456624985\n",
      "Surface training t=23924, loss=0.022936235181987286\n",
      "Surface training t=23925, loss=0.018392333760857582\n",
      "Surface training t=23926, loss=0.023906138725578785\n",
      "Surface training t=23927, loss=0.020822491496801376\n",
      "Surface training t=23928, loss=0.02257853839546442\n",
      "Surface training t=23929, loss=0.03183992579579353\n",
      "Surface training t=23930, loss=0.02177009917795658\n",
      "Surface training t=23931, loss=0.016892410349100828\n",
      "Surface training t=23932, loss=0.018423867411911488\n",
      "Surface training t=23933, loss=0.0326184406876564\n",
      "Surface training t=23934, loss=0.02432321012020111\n",
      "Surface training t=23935, loss=0.021322202868759632\n",
      "Surface training t=23936, loss=0.020443957298994064\n",
      "Surface training t=23937, loss=0.020040078088641167\n",
      "Surface training t=23938, loss=0.02645850833505392\n",
      "Surface training t=23939, loss=0.03246927633881569\n",
      "Surface training t=23940, loss=0.03449936676770449\n",
      "Surface training t=23941, loss=0.04791809245944023\n",
      "Surface training t=23942, loss=0.03273216634988785\n",
      "Surface training t=23943, loss=0.034472880885005\n",
      "Surface training t=23944, loss=0.03727286122739315\n",
      "Surface training t=23945, loss=0.04162879101932049\n",
      "Surface training t=23946, loss=0.03184441290795803\n",
      "Surface training t=23947, loss=0.03294198773801327\n",
      "Surface training t=23948, loss=0.051393331959843636\n",
      "Surface training t=23949, loss=0.04097073432058096\n",
      "Surface training t=23950, loss=0.03269065171480179\n",
      "Surface training t=23951, loss=0.046188315376639366\n",
      "Surface training t=23952, loss=0.03962214570492506\n",
      "Surface training t=23953, loss=0.03661109507083893\n",
      "Surface training t=23954, loss=0.04549826681613922\n",
      "Surface training t=23955, loss=0.06058629788458347\n",
      "Surface training t=23956, loss=0.039170099422335625\n",
      "Surface training t=23957, loss=0.04359846189618111\n",
      "Surface training t=23958, loss=0.05748901329934597\n",
      "Surface training t=23959, loss=0.038688259199261665\n",
      "Surface training t=23960, loss=0.03744478337466717\n",
      "Surface training t=23961, loss=0.04599969834089279\n",
      "Surface training t=23962, loss=0.03365834429860115\n",
      "Surface training t=23963, loss=0.025814230553805828\n",
      "Surface training t=23964, loss=0.024943478405475616\n",
      "Surface training t=23965, loss=0.020895483903586864\n",
      "Surface training t=23966, loss=0.020152870565652847\n",
      "Surface training t=23967, loss=0.018684104084968567\n",
      "Surface training t=23968, loss=0.01777506759390235\n",
      "Surface training t=23969, loss=0.023230016231536865\n",
      "Surface training t=23970, loss=0.03149883821606636\n",
      "Surface training t=23971, loss=0.04031679406762123\n",
      "Surface training t=23972, loss=0.031857569701969624\n",
      "Surface training t=23973, loss=0.03098261170089245\n",
      "Surface training t=23974, loss=0.03364037722349167\n",
      "Surface training t=23975, loss=0.028006598353385925\n",
      "Surface training t=23976, loss=0.030253874137997627\n",
      "Surface training t=23977, loss=0.035097829066216946\n",
      "Surface training t=23978, loss=0.028697297908365726\n",
      "Surface training t=23979, loss=0.027351459488272667\n",
      "Surface training t=23980, loss=0.02425482776015997\n",
      "Surface training t=23981, loss=0.027714116498827934\n",
      "Surface training t=23982, loss=0.023387745954096317\n",
      "Surface training t=23983, loss=0.029428365640342236\n",
      "Surface training t=23984, loss=0.018069341778755188\n",
      "Surface training t=23985, loss=0.021071651950478554\n",
      "Surface training t=23986, loss=0.01725451834499836\n",
      "Surface training t=23987, loss=0.021527234464883804\n",
      "Surface training t=23988, loss=0.019044716842472553\n",
      "Surface training t=23989, loss=0.022311494685709476\n",
      "Surface training t=23990, loss=0.022641419433057308\n",
      "Surface training t=23991, loss=0.021193576976656914\n",
      "Surface training t=23992, loss=0.022256365045905113\n",
      "Surface training t=23993, loss=0.027944156900048256\n",
      "Surface training t=23994, loss=0.016859923489391804\n",
      "Surface training t=23995, loss=0.019723997451364994\n",
      "Surface training t=23996, loss=0.016468544490635395\n",
      "Surface training t=23997, loss=0.01881803385913372\n",
      "Surface training t=23998, loss=0.021525840274989605\n",
      "Surface training t=23999, loss=0.021507572382688522\n",
      "Surface training t=24000, loss=0.026565564796328545\n",
      "Surface training t=24001, loss=0.023118837736546993\n",
      "Surface training t=24002, loss=0.023477360606193542\n",
      "Surface training t=24003, loss=0.02553329337388277\n",
      "Surface training t=24004, loss=0.026097041554749012\n",
      "Surface training t=24005, loss=0.027732789516448975\n",
      "Surface training t=24006, loss=0.024717335123568773\n",
      "Surface training t=24007, loss=0.03981667384505272\n",
      "Surface training t=24008, loss=0.0337945893406868\n",
      "Surface training t=24009, loss=0.03828629292547703\n",
      "Surface training t=24010, loss=0.03087077010422945\n",
      "Surface training t=24011, loss=0.029822196811437607\n",
      "Surface training t=24012, loss=0.03417731635272503\n",
      "Surface training t=24013, loss=0.028354719281196594\n",
      "Surface training t=24014, loss=0.031443201936781406\n",
      "Surface training t=24015, loss=0.035472478717565536\n",
      "Surface training t=24016, loss=0.02335258387029171\n",
      "Surface training t=24017, loss=0.03380269464105368\n",
      "Surface training t=24018, loss=0.02926311083137989\n",
      "Surface training t=24019, loss=0.02315056324005127\n",
      "Surface training t=24020, loss=0.028401542454957962\n",
      "Surface training t=24021, loss=0.02883324772119522\n",
      "Surface training t=24022, loss=0.030104284174740314\n",
      "Surface training t=24023, loss=0.03173305373638868\n",
      "Surface training t=24024, loss=0.02343927137553692\n",
      "Surface training t=24025, loss=0.026000590063631535\n",
      "Surface training t=24026, loss=0.027393667958676815\n",
      "Surface training t=24027, loss=0.020672827027738094\n",
      "Surface training t=24028, loss=0.022399033419787884\n",
      "Surface training t=24029, loss=0.02726426161825657\n",
      "Surface training t=24030, loss=0.026008933782577515\n",
      "Surface training t=24031, loss=0.025019021704792976\n",
      "Surface training t=24032, loss=0.028699728660285473\n",
      "Surface training t=24033, loss=0.02963721938431263\n",
      "Surface training t=24034, loss=0.01938446005806327\n",
      "Surface training t=24035, loss=0.021966406144201756\n",
      "Surface training t=24036, loss=0.023115482181310654\n",
      "Surface training t=24037, loss=0.026107722893357277\n",
      "Surface training t=24038, loss=0.02299553155899048\n",
      "Surface training t=24039, loss=0.028217139653861523\n",
      "Surface training t=24040, loss=0.024225122295320034\n",
      "Surface training t=24041, loss=0.017504564486443996\n",
      "Surface training t=24042, loss=0.019272311590611935\n",
      "Surface training t=24043, loss=0.01751226931810379\n",
      "Surface training t=24044, loss=0.017338212113827467\n",
      "Surface training t=24045, loss=0.015871389769017696\n",
      "Surface training t=24046, loss=0.02227693609893322\n",
      "Surface training t=24047, loss=0.023302584886550903\n",
      "Surface training t=24048, loss=0.02366452105343342\n",
      "Surface training t=24049, loss=0.027030768804252148\n",
      "Surface training t=24050, loss=0.029205829836428165\n",
      "Surface training t=24051, loss=0.026782541535794735\n",
      "Surface training t=24052, loss=0.02591296099126339\n",
      "Surface training t=24053, loss=0.019730377476662397\n",
      "Surface training t=24054, loss=0.017152302898466587\n",
      "Surface training t=24055, loss=0.017629866488277912\n",
      "Surface training t=24056, loss=0.023387562483549118\n",
      "Surface training t=24057, loss=0.015082762110978365\n",
      "Surface training t=24058, loss=0.02297549694776535\n",
      "Surface training t=24059, loss=0.02143122861161828\n",
      "Surface training t=24060, loss=0.027359742671251297\n",
      "Surface training t=24061, loss=0.022920074872672558\n",
      "Surface training t=24062, loss=0.01935040857642889\n",
      "Surface training t=24063, loss=0.01914039347320795\n",
      "Surface training t=24064, loss=0.017954373732209206\n",
      "Surface training t=24065, loss=0.019082603976130486\n",
      "Surface training t=24066, loss=0.024052314460277557\n",
      "Surface training t=24067, loss=0.019923877902328968\n",
      "Surface training t=24068, loss=0.019764218013733625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=24069, loss=0.018456708639860153\n",
      "Surface training t=24070, loss=0.018967899959534407\n",
      "Surface training t=24071, loss=0.018609724938869476\n",
      "Surface training t=24072, loss=0.015668293461203575\n",
      "Surface training t=24073, loss=0.021623844746500254\n",
      "Surface training t=24074, loss=0.0290896138176322\n",
      "Surface training t=24075, loss=0.028510880656540394\n",
      "Surface training t=24076, loss=0.028877031058073044\n",
      "Surface training t=24077, loss=0.029490215703845024\n",
      "Surface training t=24078, loss=0.023739867843687534\n",
      "Surface training t=24079, loss=0.019847549498081207\n",
      "Surface training t=24080, loss=0.014894234947860241\n",
      "Surface training t=24081, loss=0.02299910970032215\n",
      "Surface training t=24082, loss=0.02204595785588026\n",
      "Surface training t=24083, loss=0.0203066635876894\n",
      "Surface training t=24084, loss=0.01955382339656353\n",
      "Surface training t=24085, loss=0.0179265015758574\n",
      "Surface training t=24086, loss=0.017331514041870832\n",
      "Surface training t=24087, loss=0.01779606007039547\n",
      "Surface training t=24088, loss=0.01807573065161705\n",
      "Surface training t=24089, loss=0.0189517205581069\n",
      "Surface training t=24090, loss=0.020219755358994007\n",
      "Surface training t=24091, loss=0.023287134245038033\n",
      "Surface training t=24092, loss=0.014990216121077538\n",
      "Surface training t=24093, loss=0.02337639592587948\n",
      "Surface training t=24094, loss=0.029773044399917126\n",
      "Surface training t=24095, loss=0.035607077181339264\n",
      "Surface training t=24096, loss=0.026850898750126362\n",
      "Surface training t=24097, loss=0.021894952282309532\n",
      "Surface training t=24098, loss=0.03937356173992157\n",
      "Surface training t=24099, loss=0.030809713527560234\n",
      "Surface training t=24100, loss=0.030247933231294155\n",
      "Surface training t=24101, loss=0.03652751259505749\n",
      "Surface training t=24102, loss=0.028516092337667942\n",
      "Surface training t=24103, loss=0.046124087646603584\n",
      "Surface training t=24104, loss=0.027308322489261627\n",
      "Surface training t=24105, loss=0.03426409512758255\n",
      "Surface training t=24106, loss=0.024447236210107803\n",
      "Surface training t=24107, loss=0.027939253486692905\n",
      "Surface training t=24108, loss=0.034637706354260445\n",
      "Surface training t=24109, loss=0.03470133524388075\n",
      "Surface training t=24110, loss=0.028333834372460842\n",
      "Surface training t=24111, loss=0.02656617294996977\n",
      "Surface training t=24112, loss=0.02801360934972763\n",
      "Surface training t=24113, loss=0.03094431571662426\n",
      "Surface training t=24114, loss=0.019291198812425137\n",
      "Surface training t=24115, loss=0.02373772580176592\n",
      "Surface training t=24116, loss=0.020768272690474987\n",
      "Surface training t=24117, loss=0.01885025855153799\n",
      "Surface training t=24118, loss=0.019629978574812412\n",
      "Surface training t=24119, loss=0.018339691683650017\n",
      "Surface training t=24120, loss=0.018813075497746468\n",
      "Surface training t=24121, loss=0.017028331756591797\n",
      "Surface training t=24122, loss=0.020617106929421425\n",
      "Surface training t=24123, loss=0.020579738542437553\n",
      "Surface training t=24124, loss=0.020365658216178417\n",
      "Surface training t=24125, loss=0.024331754073500633\n",
      "Surface training t=24126, loss=0.028529802337288857\n",
      "Surface training t=24127, loss=0.03428547829389572\n",
      "Surface training t=24128, loss=0.024618654511868954\n",
      "Surface training t=24129, loss=0.0319839958101511\n",
      "Surface training t=24130, loss=0.032163359224796295\n",
      "Surface training t=24131, loss=0.023841903544962406\n",
      "Surface training t=24132, loss=0.03055589087307453\n",
      "Surface training t=24133, loss=0.026566488668322563\n",
      "Surface training t=24134, loss=0.025159728713333607\n",
      "Surface training t=24135, loss=0.020948982797563076\n",
      "Surface training t=24136, loss=0.02270231582224369\n",
      "Surface training t=24137, loss=0.02079377043992281\n",
      "Surface training t=24138, loss=0.02431449480354786\n",
      "Surface training t=24139, loss=0.034290675073862076\n",
      "Surface training t=24140, loss=0.02920118346810341\n",
      "Surface training t=24141, loss=0.02410818822681904\n",
      "Surface training t=24142, loss=0.024620712734758854\n",
      "Surface training t=24143, loss=0.027604623697698116\n",
      "Surface training t=24144, loss=0.02453127782791853\n",
      "Surface training t=24145, loss=0.017862774431705475\n",
      "Surface training t=24146, loss=0.018359174020588398\n",
      "Surface training t=24147, loss=0.01631457917392254\n",
      "Surface training t=24148, loss=0.012731597293168306\n",
      "Surface training t=24149, loss=0.014812758192420006\n",
      "Surface training t=24150, loss=0.022778392769396305\n",
      "Surface training t=24151, loss=0.020940487273037434\n",
      "Surface training t=24152, loss=0.024025222286581993\n",
      "Surface training t=24153, loss=0.025797984562814236\n",
      "Surface training t=24154, loss=0.03159750532358885\n",
      "Surface training t=24155, loss=0.027269579470157623\n",
      "Surface training t=24156, loss=0.029502899385988712\n",
      "Surface training t=24157, loss=0.025043443776667118\n",
      "Surface training t=24158, loss=0.027932659722864628\n",
      "Surface training t=24159, loss=0.03474484756588936\n",
      "Surface training t=24160, loss=0.03395727835595608\n",
      "Surface training t=24161, loss=0.02890571393072605\n",
      "Surface training t=24162, loss=0.02343921083956957\n",
      "Surface training t=24163, loss=0.023082640953361988\n",
      "Surface training t=24164, loss=0.02852203417569399\n",
      "Surface training t=24165, loss=0.02220343053340912\n",
      "Surface training t=24166, loss=0.024886040948331356\n",
      "Surface training t=24167, loss=0.02253977209329605\n",
      "Surface training t=24168, loss=0.02129892259836197\n",
      "Surface training t=24169, loss=0.022231843322515488\n",
      "Surface training t=24170, loss=0.017683498561382294\n",
      "Surface training t=24171, loss=0.02196275908499956\n",
      "Surface training t=24172, loss=0.018554038368165493\n",
      "Surface training t=24173, loss=0.019768825732171535\n",
      "Surface training t=24174, loss=0.02169213443994522\n",
      "Surface training t=24175, loss=0.021025839261710644\n",
      "Surface training t=24176, loss=0.02272786572575569\n",
      "Surface training t=24177, loss=0.025752953253686428\n",
      "Surface training t=24178, loss=0.02350522018969059\n",
      "Surface training t=24179, loss=0.023659895174205303\n",
      "Surface training t=24180, loss=0.02780338004231453\n",
      "Surface training t=24181, loss=0.025324327871203423\n",
      "Surface training t=24182, loss=0.02250317856669426\n",
      "Surface training t=24183, loss=0.019903342705219984\n",
      "Surface training t=24184, loss=0.022086692973971367\n",
      "Surface training t=24185, loss=0.02589159458875656\n",
      "Surface training t=24186, loss=0.018652746453881264\n",
      "Surface training t=24187, loss=0.030231988057494164\n",
      "Surface training t=24188, loss=0.03198078740388155\n",
      "Surface training t=24189, loss=0.022716492414474487\n",
      "Surface training t=24190, loss=0.02178516797721386\n",
      "Surface training t=24191, loss=0.023940935730934143\n",
      "Surface training t=24192, loss=0.01636439375579357\n",
      "Surface training t=24193, loss=0.01734157744795084\n",
      "Surface training t=24194, loss=0.020954030565917492\n",
      "Surface training t=24195, loss=0.0257868729531765\n",
      "Surface training t=24196, loss=0.017492933198809624\n",
      "Surface training t=24197, loss=0.01920816581696272\n",
      "Surface training t=24198, loss=0.01789349876344204\n",
      "Surface training t=24199, loss=0.019606863148510456\n",
      "Surface training t=24200, loss=0.021012666635215282\n",
      "Surface training t=24201, loss=0.02481306903064251\n",
      "Surface training t=24202, loss=0.038051022216677666\n",
      "Surface training t=24203, loss=0.032996824476867914\n",
      "Surface training t=24204, loss=0.029451431706547737\n",
      "Surface training t=24205, loss=0.03314622864127159\n",
      "Surface training t=24206, loss=0.0269938288256526\n",
      "Surface training t=24207, loss=0.045037051662802696\n",
      "Surface training t=24208, loss=0.036358634009957314\n",
      "Surface training t=24209, loss=0.05334201268851757\n",
      "Surface training t=24210, loss=0.04380165599286556\n",
      "Surface training t=24211, loss=0.038880977779626846\n",
      "Surface training t=24212, loss=0.027595842722803354\n",
      "Surface training t=24213, loss=0.03444468695670366\n",
      "Surface training t=24214, loss=0.024021917954087257\n",
      "Surface training t=24215, loss=0.021509294398128986\n",
      "Surface training t=24216, loss=0.022646593861281872\n",
      "Surface training t=24217, loss=0.018081129528582096\n",
      "Surface training t=24218, loss=0.027647736482322216\n",
      "Surface training t=24219, loss=0.024912936612963676\n",
      "Surface training t=24220, loss=0.02866862528026104\n",
      "Surface training t=24221, loss=0.02155833411961794\n",
      "Surface training t=24222, loss=0.02103340905159712\n",
      "Surface training t=24223, loss=0.02120968885719776\n",
      "Surface training t=24224, loss=0.02033099578693509\n",
      "Surface training t=24225, loss=0.019757643342018127\n",
      "Surface training t=24226, loss=0.018431679345667362\n",
      "Surface training t=24227, loss=0.014241628348827362\n",
      "Surface training t=24228, loss=0.022052823565900326\n",
      "Surface training t=24229, loss=0.020172061398625374\n",
      "Surface training t=24230, loss=0.017476567067205906\n",
      "Surface training t=24231, loss=0.01688275532796979\n",
      "Surface training t=24232, loss=0.018980202730745077\n",
      "Surface training t=24233, loss=0.035704899579286575\n",
      "Surface training t=24234, loss=0.02836638782173395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=24235, loss=0.029431117698550224\n",
      "Surface training t=24236, loss=0.030764632858335972\n",
      "Surface training t=24237, loss=0.029017888940870762\n",
      "Surface training t=24238, loss=0.02230396680533886\n",
      "Surface training t=24239, loss=0.024568399414420128\n",
      "Surface training t=24240, loss=0.035365914925932884\n",
      "Surface training t=24241, loss=0.025149904191493988\n",
      "Surface training t=24242, loss=0.028360729105770588\n",
      "Surface training t=24243, loss=0.024866316467523575\n",
      "Surface training t=24244, loss=0.026829511858522892\n",
      "Surface training t=24245, loss=0.031952972523868084\n",
      "Surface training t=24246, loss=0.030071319080889225\n",
      "Surface training t=24247, loss=0.03308090474456549\n",
      "Surface training t=24248, loss=0.025366585701704025\n",
      "Surface training t=24249, loss=0.026512790471315384\n",
      "Surface training t=24250, loss=0.026920937933027744\n",
      "Surface training t=24251, loss=0.024449496529996395\n",
      "Surface training t=24252, loss=0.02720729075372219\n",
      "Surface training t=24253, loss=0.027464100159704685\n",
      "Surface training t=24254, loss=0.02243716735392809\n",
      "Surface training t=24255, loss=0.023340496234595776\n",
      "Surface training t=24256, loss=0.022082967683672905\n",
      "Surface training t=24257, loss=0.023151453584432602\n",
      "Surface training t=24258, loss=0.016120227053761482\n",
      "Surface training t=24259, loss=0.02153079491108656\n",
      "Surface training t=24260, loss=0.01582588069140911\n",
      "Surface training t=24261, loss=0.020507248118519783\n",
      "Surface training t=24262, loss=0.026454927399754524\n",
      "Surface training t=24263, loss=0.027522971853613853\n",
      "Surface training t=24264, loss=0.023758266121149063\n",
      "Surface training t=24265, loss=0.02546065580099821\n",
      "Surface training t=24266, loss=0.024591093882918358\n",
      "Surface training t=24267, loss=0.0393141433596611\n",
      "Surface training t=24268, loss=0.03132497798651457\n",
      "Surface training t=24269, loss=0.02420995570719242\n",
      "Surface training t=24270, loss=0.022166544571518898\n",
      "Surface training t=24271, loss=0.021705674938857555\n",
      "Surface training t=24272, loss=0.01942136138677597\n",
      "Surface training t=24273, loss=0.01503969356417656\n",
      "Surface training t=24274, loss=0.016587150283157825\n",
      "Surface training t=24275, loss=0.02347560878843069\n",
      "Surface training t=24276, loss=0.021005267277359962\n",
      "Surface training t=24277, loss=0.03021604474633932\n",
      "Surface training t=24278, loss=0.03857902251183987\n",
      "Surface training t=24279, loss=0.028093503788113594\n",
      "Surface training t=24280, loss=0.018942469730973244\n",
      "Surface training t=24281, loss=0.0203732717782259\n",
      "Surface training t=24282, loss=0.021005885675549507\n",
      "Surface training t=24283, loss=0.022018952295184135\n",
      "Surface training t=24284, loss=0.017208196222782135\n",
      "Surface training t=24285, loss=0.01839314214885235\n",
      "Surface training t=24286, loss=0.022016601637005806\n",
      "Surface training t=24287, loss=0.022630237974226475\n",
      "Surface training t=24288, loss=0.024393831379711628\n",
      "Surface training t=24289, loss=0.028208745643496513\n",
      "Surface training t=24290, loss=0.025738894939422607\n",
      "Surface training t=24291, loss=0.02504750806838274\n",
      "Surface training t=24292, loss=0.018722230568528175\n",
      "Surface training t=24293, loss=0.01686588115990162\n",
      "Surface training t=24294, loss=0.018054364249110222\n",
      "Surface training t=24295, loss=0.02008084300905466\n",
      "Surface training t=24296, loss=0.023287992924451828\n",
      "Surface training t=24297, loss=0.02301853708922863\n",
      "Surface training t=24298, loss=0.01697339490056038\n",
      "Surface training t=24299, loss=0.02095738146454096\n",
      "Surface training t=24300, loss=0.02671127673238516\n",
      "Surface training t=24301, loss=0.016241856385022402\n",
      "Surface training t=24302, loss=0.02019959595054388\n",
      "Surface training t=24303, loss=0.018778659403324127\n",
      "Surface training t=24304, loss=0.021920213475823402\n",
      "Surface training t=24305, loss=0.016880127601325512\n",
      "Surface training t=24306, loss=0.01638636225834489\n",
      "Surface training t=24307, loss=0.019662643782794476\n",
      "Surface training t=24308, loss=0.02224690280854702\n",
      "Surface training t=24309, loss=0.021936757490038872\n",
      "Surface training t=24310, loss=0.021317587234079838\n",
      "Surface training t=24311, loss=0.016841608099639416\n",
      "Surface training t=24312, loss=0.019330201670527458\n",
      "Surface training t=24313, loss=0.019830819219350815\n",
      "Surface training t=24314, loss=0.015623167622834444\n",
      "Surface training t=24315, loss=0.022045116871595383\n",
      "Surface training t=24316, loss=0.017282942309975624\n",
      "Surface training t=24317, loss=0.015789379831403494\n",
      "Surface training t=24318, loss=0.018994132988154888\n",
      "Surface training t=24319, loss=0.014497527852654457\n",
      "Surface training t=24320, loss=0.016621286049485207\n",
      "Surface training t=24321, loss=0.01807365659624338\n",
      "Surface training t=24322, loss=0.0255258958786726\n",
      "Surface training t=24323, loss=0.025975855998694897\n",
      "Surface training t=24324, loss=0.02583344653248787\n",
      "Surface training t=24325, loss=0.028685085475444794\n",
      "Surface training t=24326, loss=0.03931574150919914\n",
      "Surface training t=24327, loss=0.028978198766708374\n",
      "Surface training t=24328, loss=0.030003994703292847\n",
      "Surface training t=24329, loss=0.03561544418334961\n",
      "Surface training t=24330, loss=0.03803864307701588\n",
      "Surface training t=24331, loss=0.03421092312783003\n",
      "Surface training t=24332, loss=0.022375128231942654\n",
      "Surface training t=24333, loss=0.02902670670300722\n",
      "Surface training t=24334, loss=0.025707777589559555\n",
      "Surface training t=24335, loss=0.02333742380142212\n",
      "Surface training t=24336, loss=0.019978549797087908\n",
      "Surface training t=24337, loss=0.020821508951485157\n",
      "Surface training t=24338, loss=0.015785296447575092\n",
      "Surface training t=24339, loss=0.024046171456575394\n",
      "Surface training t=24340, loss=0.021181712858378887\n",
      "Surface training t=24341, loss=0.02889133896678686\n",
      "Surface training t=24342, loss=0.03209903836250305\n",
      "Surface training t=24343, loss=0.03781003877520561\n",
      "Surface training t=24344, loss=0.0384582094848156\n",
      "Surface training t=24345, loss=0.042992278933525085\n",
      "Surface training t=24346, loss=0.05713377892971039\n",
      "Surface training t=24347, loss=0.03880435973405838\n",
      "Surface training t=24348, loss=0.04291091486811638\n",
      "Surface training t=24349, loss=0.05045563355088234\n",
      "Surface training t=24350, loss=0.03745260275900364\n",
      "Surface training t=24351, loss=0.04827527888119221\n",
      "Surface training t=24352, loss=0.03445811569690704\n",
      "Surface training t=24353, loss=0.045347146689891815\n",
      "Surface training t=24354, loss=0.026431042701005936\n",
      "Surface training t=24355, loss=0.034139495342969894\n",
      "Surface training t=24356, loss=0.03956049121916294\n",
      "Surface training t=24357, loss=0.02828859258443117\n",
      "Surface training t=24358, loss=0.02974858507514\n",
      "Surface training t=24359, loss=0.023430241271853447\n",
      "Surface training t=24360, loss=0.024130485020577908\n",
      "Surface training t=24361, loss=0.02363589685410261\n",
      "Surface training t=24362, loss=0.017484692856669426\n",
      "Surface training t=24363, loss=0.02024171594530344\n",
      "Surface training t=24364, loss=0.01653517410159111\n",
      "Surface training t=24365, loss=0.028260103426873684\n",
      "Surface training t=24366, loss=0.020749004557728767\n",
      "Surface training t=24367, loss=0.02437371388077736\n",
      "Surface training t=24368, loss=0.022784415632486343\n",
      "Surface training t=24369, loss=0.02116508549079299\n",
      "Surface training t=24370, loss=0.023707237094640732\n",
      "Surface training t=24371, loss=0.016096909996122122\n",
      "Surface training t=24372, loss=0.019618852995336056\n",
      "Surface training t=24373, loss=0.020088788121938705\n",
      "Surface training t=24374, loss=0.019880881533026695\n",
      "Surface training t=24375, loss=0.019733119755983353\n",
      "Surface training t=24376, loss=0.03099615778774023\n",
      "Surface training t=24377, loss=0.03414437361061573\n",
      "Surface training t=24378, loss=0.026509910821914673\n",
      "Surface training t=24379, loss=0.03192860074341297\n",
      "Surface training t=24380, loss=0.028530134819447994\n",
      "Surface training t=24381, loss=0.03238315600901842\n",
      "Surface training t=24382, loss=0.023293571546673775\n",
      "Surface training t=24383, loss=0.016434292774647474\n",
      "Surface training t=24384, loss=0.02109675481915474\n",
      "Surface training t=24385, loss=0.017652915325015783\n",
      "Surface training t=24386, loss=0.019688774831593037\n",
      "Surface training t=24387, loss=0.016208921559154987\n",
      "Surface training t=24388, loss=0.017854743637144566\n",
      "Surface training t=24389, loss=0.02190714329481125\n",
      "Surface training t=24390, loss=0.02526597213000059\n",
      "Surface training t=24391, loss=0.023786470293998718\n",
      "Surface training t=24392, loss=0.024337726645171642\n",
      "Surface training t=24393, loss=0.030707004480063915\n",
      "Surface training t=24394, loss=0.02360427100211382\n",
      "Surface training t=24395, loss=0.02446001023054123\n",
      "Surface training t=24396, loss=0.027048563584685326\n",
      "Surface training t=24397, loss=0.025794735178351402\n",
      "Surface training t=24398, loss=0.02235567383468151\n",
      "Surface training t=24399, loss=0.024359796196222305\n",
      "Surface training t=24400, loss=0.031100787222385406\n",
      "Surface training t=24401, loss=0.021783754229545593\n",
      "Surface training t=24402, loss=0.02349412627518177\n",
      "Surface training t=24403, loss=0.025792664848268032\n",
      "Surface training t=24404, loss=0.02973511815071106\n",
      "Surface training t=24405, loss=0.022362560033798218\n",
      "Surface training t=24406, loss=0.021433047018945217\n",
      "Surface training t=24407, loss=0.018104800023138523\n",
      "Surface training t=24408, loss=0.019139419309794903\n",
      "Surface training t=24409, loss=0.025358572602272034\n",
      "Surface training t=24410, loss=0.026935288682579994\n",
      "Surface training t=24411, loss=0.02667329553514719\n",
      "Surface training t=24412, loss=0.03962009400129318\n",
      "Surface training t=24413, loss=0.02548518031835556\n",
      "Surface training t=24414, loss=0.02623449359089136\n",
      "Surface training t=24415, loss=0.02483230922371149\n",
      "Surface training t=24416, loss=0.02472069300711155\n",
      "Surface training t=24417, loss=0.023617872036993504\n",
      "Surface training t=24418, loss=0.02365717664361\n",
      "Surface training t=24419, loss=0.02090450469404459\n",
      "Surface training t=24420, loss=0.025258376263082027\n",
      "Surface training t=24421, loss=0.022562346421182156\n",
      "Surface training t=24422, loss=0.023372291587293148\n",
      "Surface training t=24423, loss=0.02229615766555071\n",
      "Surface training t=24424, loss=0.02416175790131092\n",
      "Surface training t=24425, loss=0.02336957398802042\n",
      "Surface training t=24426, loss=0.025042681489139795\n",
      "Surface training t=24427, loss=0.025639685802161694\n",
      "Surface training t=24428, loss=0.02423565648496151\n",
      "Surface training t=24429, loss=0.020403550937771797\n",
      "Surface training t=24430, loss=0.019735798239707947\n",
      "Surface training t=24431, loss=0.023433327674865723\n",
      "Surface training t=24432, loss=0.030197424814105034\n",
      "Surface training t=24433, loss=0.02713035698980093\n",
      "Surface training t=24434, loss=0.0207053255289793\n",
      "Surface training t=24435, loss=0.025830545462667942\n",
      "Surface training t=24436, loss=0.025975153781473637\n",
      "Surface training t=24437, loss=0.023979326710104942\n",
      "Surface training t=24438, loss=0.030530767515301704\n",
      "Surface training t=24439, loss=0.024889066815376282\n",
      "Surface training t=24440, loss=0.03668628539890051\n",
      "Surface training t=24441, loss=0.028919867239892483\n",
      "Surface training t=24442, loss=0.021652339957654476\n",
      "Surface training t=24443, loss=0.0245128208771348\n",
      "Surface training t=24444, loss=0.02750615868717432\n",
      "Surface training t=24445, loss=0.023379080928862095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=24446, loss=0.019326655194163322\n",
      "Surface training t=24447, loss=0.017700647469609976\n",
      "Surface training t=24448, loss=0.018600571434944868\n",
      "Surface training t=24449, loss=0.02038352284580469\n",
      "Surface training t=24450, loss=0.019864547066390514\n",
      "Surface training t=24451, loss=0.02413209807127714\n",
      "Surface training t=24452, loss=0.023868044838309288\n",
      "Surface training t=24453, loss=0.01942525990307331\n",
      "Surface training t=24454, loss=0.021224738098680973\n",
      "Surface training t=24455, loss=0.013295837212353945\n",
      "Surface training t=24456, loss=0.01827707327902317\n",
      "Surface training t=24457, loss=0.013202410191297531\n",
      "Surface training t=24458, loss=0.015887568704783916\n",
      "Surface training t=24459, loss=0.01617238065227866\n",
      "Surface training t=24460, loss=0.01513050775974989\n",
      "Surface training t=24461, loss=0.013651833403855562\n",
      "Surface training t=24462, loss=0.01491389237344265\n",
      "Surface training t=24463, loss=0.017613745294511318\n",
      "Surface training t=24464, loss=0.014255138579756021\n",
      "Surface training t=24465, loss=0.01511358143761754\n",
      "Surface training t=24466, loss=0.019216026179492474\n",
      "Surface training t=24467, loss=0.01537136035040021\n",
      "Surface training t=24468, loss=0.020156382583081722\n",
      "Surface training t=24469, loss=0.03361455164849758\n",
      "Surface training t=24470, loss=0.03142819833010435\n",
      "Surface training t=24471, loss=0.02315574698150158\n",
      "Surface training t=24472, loss=0.02501042652875185\n",
      "Surface training t=24473, loss=0.01866584736853838\n",
      "Surface training t=24474, loss=0.02517116628587246\n",
      "Surface training t=24475, loss=0.0186933777295053\n",
      "Surface training t=24476, loss=0.02255596872419119\n",
      "Surface training t=24477, loss=0.018626117147505283\n",
      "Surface training t=24478, loss=0.018475660122931004\n",
      "Surface training t=24479, loss=0.020192078314721584\n",
      "Surface training t=24480, loss=0.017826254479587078\n",
      "Surface training t=24481, loss=0.0210864394903183\n",
      "Surface training t=24482, loss=0.020412730984389782\n",
      "Surface training t=24483, loss=0.026109442114830017\n",
      "Surface training t=24484, loss=0.02209686115384102\n",
      "Surface training t=24485, loss=0.0307996841147542\n",
      "Surface training t=24486, loss=0.03174459561705589\n",
      "Surface training t=24487, loss=0.030961436219513416\n",
      "Surface training t=24488, loss=0.022370247170329094\n",
      "Surface training t=24489, loss=0.02469549886882305\n",
      "Surface training t=24490, loss=0.025900719687342644\n",
      "Surface training t=24491, loss=0.025074469856917858\n",
      "Surface training t=24492, loss=0.022784770466387272\n",
      "Surface training t=24493, loss=0.02289571985602379\n",
      "Surface training t=24494, loss=0.020046920515596867\n",
      "Surface training t=24495, loss=0.018347113393247128\n",
      "Surface training t=24496, loss=0.02300132066011429\n",
      "Surface training t=24497, loss=0.01946395169943571\n",
      "Surface training t=24498, loss=0.023392713628709316\n",
      "Surface training t=24499, loss=0.030327207408845425\n",
      "Surface training t=24500, loss=0.025872155092656612\n",
      "Surface training t=24501, loss=0.017444989178329706\n",
      "Surface training t=24502, loss=0.022582062520086765\n",
      "Surface training t=24503, loss=0.02230458240956068\n",
      "Surface training t=24504, loss=0.019435014575719833\n",
      "Surface training t=24505, loss=0.018970857374370098\n",
      "Surface training t=24506, loss=0.02969285100698471\n",
      "Surface training t=24507, loss=0.020455091260373592\n",
      "Surface training t=24508, loss=0.021730824373662472\n",
      "Surface training t=24509, loss=0.02719734888523817\n",
      "Surface training t=24510, loss=0.019799048081040382\n",
      "Surface training t=24511, loss=0.017330261878669262\n",
      "Surface training t=24512, loss=0.020370683632791042\n",
      "Surface training t=24513, loss=0.018974384292960167\n",
      "Surface training t=24514, loss=0.019129587337374687\n",
      "Surface training t=24515, loss=0.017614914570003748\n",
      "Surface training t=24516, loss=0.02200247347354889\n",
      "Surface training t=24517, loss=0.02030332013964653\n",
      "Surface training t=24518, loss=0.019079305231571198\n",
      "Surface training t=24519, loss=0.0326828807592392\n",
      "Surface training t=24520, loss=0.021945557557046413\n",
      "Surface training t=24521, loss=0.019834193401038647\n",
      "Surface training t=24522, loss=0.018246997613459826\n",
      "Surface training t=24523, loss=0.018849214538931847\n",
      "Surface training t=24524, loss=0.017704802565276623\n",
      "Surface training t=24525, loss=0.018318203277885914\n",
      "Surface training t=24526, loss=0.019454190507531166\n",
      "Surface training t=24527, loss=0.019616232253611088\n",
      "Surface training t=24528, loss=0.021115890704095364\n",
      "Surface training t=24529, loss=0.01745190192013979\n",
      "Surface training t=24530, loss=0.018561082892119884\n",
      "Surface training t=24531, loss=0.02109838742762804\n",
      "Surface training t=24532, loss=0.019475210458040237\n",
      "Surface training t=24533, loss=0.019728603772819042\n",
      "Surface training t=24534, loss=0.01787187810987234\n",
      "Surface training t=24535, loss=0.020052803680300713\n",
      "Surface training t=24536, loss=0.019903970882296562\n",
      "Surface training t=24537, loss=0.024730393663048744\n",
      "Surface training t=24538, loss=0.02011914737522602\n",
      "Surface training t=24539, loss=0.022491988725960255\n",
      "Surface training t=24540, loss=0.02677441854029894\n",
      "Surface training t=24541, loss=0.020905332639813423\n",
      "Surface training t=24542, loss=0.014201279263943434\n",
      "Surface training t=24543, loss=0.01301009813323617\n",
      "Surface training t=24544, loss=0.014649382792413235\n",
      "Surface training t=24545, loss=0.015274304430931807\n",
      "Surface training t=24546, loss=0.013892509508877993\n",
      "Surface training t=24547, loss=0.022486543282866478\n",
      "Surface training t=24548, loss=0.026455117389559746\n",
      "Surface training t=24549, loss=0.02807141561061144\n",
      "Surface training t=24550, loss=0.027765902690589428\n",
      "Surface training t=24551, loss=0.030370441265404224\n",
      "Surface training t=24552, loss=0.030976194888353348\n",
      "Surface training t=24553, loss=0.042077112942934036\n",
      "Surface training t=24554, loss=0.035709718242287636\n",
      "Surface training t=24555, loss=0.03393140621483326\n",
      "Surface training t=24556, loss=0.06197355315089226\n",
      "Surface training t=24557, loss=0.0439567044377327\n",
      "Surface training t=24558, loss=0.04726744815707207\n",
      "Surface training t=24559, loss=0.05401712469756603\n",
      "Surface training t=24560, loss=0.03387601301074028\n",
      "Surface training t=24561, loss=0.041131677106022835\n",
      "Surface training t=24562, loss=0.03417373262345791\n",
      "Surface training t=24563, loss=0.03523594792932272\n",
      "Surface training t=24564, loss=0.03006610833108425\n",
      "Surface training t=24565, loss=0.03473059367388487\n",
      "Surface training t=24566, loss=0.025109093636274338\n",
      "Surface training t=24567, loss=0.025696356780827045\n",
      "Surface training t=24568, loss=0.03317727893590927\n",
      "Surface training t=24569, loss=0.02655978314578533\n",
      "Surface training t=24570, loss=0.028519372455775738\n",
      "Surface training t=24571, loss=0.03478716127574444\n",
      "Surface training t=24572, loss=0.027518518269062042\n",
      "Surface training t=24573, loss=0.02714358549565077\n",
      "Surface training t=24574, loss=0.024185274727642536\n",
      "Surface training t=24575, loss=0.025016906671226025\n",
      "Surface training t=24576, loss=0.02102421037852764\n",
      "Surface training t=24577, loss=0.01955303829163313\n",
      "Surface training t=24578, loss=0.01885862834751606\n",
      "Surface training t=24579, loss=0.02044948237016797\n",
      "Surface training t=24580, loss=0.019342033192515373\n",
      "Surface training t=24581, loss=0.019819509238004684\n",
      "Surface training t=24582, loss=0.015520778018981218\n",
      "Surface training t=24583, loss=0.015056094154715538\n",
      "Surface training t=24584, loss=0.013355834409594536\n",
      "Surface training t=24585, loss=0.01289444975554943\n",
      "Surface training t=24586, loss=0.020040846429765224\n",
      "Surface training t=24587, loss=0.02606348693370819\n",
      "Surface training t=24588, loss=0.025066626258194447\n",
      "Surface training t=24589, loss=0.03902066871523857\n",
      "Surface training t=24590, loss=0.028064703568816185\n",
      "Surface training t=24591, loss=0.0370326042175293\n",
      "Surface training t=24592, loss=0.030936324037611485\n",
      "Surface training t=24593, loss=0.029160739853978157\n",
      "Surface training t=24594, loss=0.03839077614247799\n",
      "Surface training t=24595, loss=0.03215864859521389\n",
      "Surface training t=24596, loss=0.03392723482102156\n",
      "Surface training t=24597, loss=0.030585854314267635\n",
      "Surface training t=24598, loss=0.037171514704823494\n",
      "Surface training t=24599, loss=0.039547648280858994\n",
      "Surface training t=24600, loss=0.028763524256646633\n",
      "Surface training t=24601, loss=0.023891850374639034\n",
      "Surface training t=24602, loss=0.034291947260499\n",
      "Surface training t=24603, loss=0.027057082392275333\n",
      "Surface training t=24604, loss=0.036454951390624046\n",
      "Surface training t=24605, loss=0.030760533176362514\n",
      "Surface training t=24606, loss=0.05120840482413769\n",
      "Surface training t=24607, loss=0.037133761681616306\n",
      "Surface training t=24608, loss=0.037997154518961906\n",
      "Surface training t=24609, loss=0.027239272370934486\n",
      "Surface training t=24610, loss=0.027863947674632072\n",
      "Surface training t=24611, loss=0.036384232342243195\n",
      "Surface training t=24612, loss=0.031338038854300976\n",
      "Surface training t=24613, loss=0.029949277639389038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=24614, loss=0.027186334133148193\n",
      "Surface training t=24615, loss=0.021678593009710312\n",
      "Surface training t=24616, loss=0.02327617723494768\n",
      "Surface training t=24617, loss=0.02926935814321041\n",
      "Surface training t=24618, loss=0.023597086779773235\n",
      "Surface training t=24619, loss=0.02266837190836668\n",
      "Surface training t=24620, loss=0.046285491436719894\n",
      "Surface training t=24621, loss=0.036155592650175095\n",
      "Surface training t=24622, loss=0.038519540801644325\n",
      "Surface training t=24623, loss=0.05837083421647549\n",
      "Surface training t=24624, loss=0.04314818046987057\n",
      "Surface training t=24625, loss=0.05011076666414738\n",
      "Surface training t=24626, loss=0.03925767168402672\n",
      "Surface training t=24627, loss=0.02936523500829935\n",
      "Surface training t=24628, loss=0.028231754899024963\n",
      "Surface training t=24629, loss=0.03563481383025646\n",
      "Surface training t=24630, loss=0.043626539409160614\n",
      "Surface training t=24631, loss=0.03549885377287865\n",
      "Surface training t=24632, loss=0.03500443138182163\n",
      "Surface training t=24633, loss=0.029184071347117424\n",
      "Surface training t=24634, loss=0.02638370357453823\n",
      "Surface training t=24635, loss=0.02228625863790512\n",
      "Surface training t=24636, loss=0.0301862433552742\n",
      "Surface training t=24637, loss=0.03308489918708801\n",
      "Surface training t=24638, loss=0.031954726204276085\n",
      "Surface training t=24639, loss=0.02822640910744667\n",
      "Surface training t=24640, loss=0.026276497170329094\n",
      "Surface training t=24641, loss=0.036384472623467445\n",
      "Surface training t=24642, loss=0.039903296157717705\n",
      "Surface training t=24643, loss=0.02772387769073248\n",
      "Surface training t=24644, loss=0.03563457727432251\n",
      "Surface training t=24645, loss=0.03003850858658552\n",
      "Surface training t=24646, loss=0.03223149944096804\n",
      "Surface training t=24647, loss=0.03227218985557556\n",
      "Surface training t=24648, loss=0.029985131695866585\n",
      "Surface training t=24649, loss=0.028350336477160454\n",
      "Surface training t=24650, loss=0.030002402141690254\n",
      "Surface training t=24651, loss=0.031481245532631874\n",
      "Surface training t=24652, loss=0.029688398353755474\n",
      "Surface training t=24653, loss=0.030201954767107964\n",
      "Surface training t=24654, loss=0.028582477942109108\n",
      "Surface training t=24655, loss=0.025922742672264576\n",
      "Surface training t=24656, loss=0.022261913865804672\n",
      "Surface training t=24657, loss=0.020666957832872868\n",
      "Surface training t=24658, loss=0.029665490612387657\n",
      "Surface training t=24659, loss=0.03161126561462879\n",
      "Surface training t=24660, loss=0.03378203697502613\n",
      "Surface training t=24661, loss=0.022789902985095978\n",
      "Surface training t=24662, loss=0.021723506040871143\n",
      "Surface training t=24663, loss=0.029291468672454357\n",
      "Surface training t=24664, loss=0.02390007022768259\n",
      "Surface training t=24665, loss=0.023983745835721493\n",
      "Surface training t=24666, loss=0.026495952159166336\n",
      "Surface training t=24667, loss=0.01955211767926812\n",
      "Surface training t=24668, loss=0.03707221336662769\n",
      "Surface training t=24669, loss=0.04569254815578461\n",
      "Surface training t=24670, loss=0.035283828154206276\n",
      "Surface training t=24671, loss=0.03875001519918442\n",
      "Surface training t=24672, loss=0.04042832925915718\n",
      "Surface training t=24673, loss=0.03140588290989399\n",
      "Surface training t=24674, loss=0.030816231854259968\n",
      "Surface training t=24675, loss=0.02688337303698063\n",
      "Surface training t=24676, loss=0.03708232194185257\n",
      "Surface training t=24677, loss=0.0332245621830225\n",
      "Surface training t=24678, loss=0.030664090998470783\n",
      "Surface training t=24679, loss=0.02963544800877571\n",
      "Surface training t=24680, loss=0.029518562369048595\n",
      "Surface training t=24681, loss=0.03672577440738678\n",
      "Surface training t=24682, loss=0.029129193630069494\n",
      "Surface training t=24683, loss=0.030478017404675484\n",
      "Surface training t=24684, loss=0.027441917918622494\n",
      "Surface training t=24685, loss=0.030859138816595078\n",
      "Surface training t=24686, loss=0.0187684940174222\n",
      "Surface training t=24687, loss=0.021407919004559517\n",
      "Surface training t=24688, loss=0.020725032314658165\n",
      "Surface training t=24689, loss=0.01764658698812127\n",
      "Surface training t=24690, loss=0.02518519852310419\n",
      "Surface training t=24691, loss=0.025002731941640377\n",
      "Surface training t=24692, loss=0.02100531943142414\n",
      "Surface training t=24693, loss=0.024109916761517525\n",
      "Surface training t=24694, loss=0.02310088649392128\n",
      "Surface training t=24695, loss=0.021791997365653515\n",
      "Surface training t=24696, loss=0.01784101827070117\n",
      "Surface training t=24697, loss=0.023449506610631943\n",
      "Surface training t=24698, loss=0.023587009869515896\n",
      "Surface training t=24699, loss=0.027888072654604912\n",
      "Surface training t=24700, loss=0.021687700413167477\n",
      "Surface training t=24701, loss=0.022662164643406868\n",
      "Surface training t=24702, loss=0.024636970832943916\n",
      "Surface training t=24703, loss=0.023435452952980995\n",
      "Surface training t=24704, loss=0.026634756475687027\n",
      "Surface training t=24705, loss=0.029589390382170677\n",
      "Surface training t=24706, loss=0.024476589635014534\n",
      "Surface training t=24707, loss=0.02150743454694748\n",
      "Surface training t=24708, loss=0.02227618359029293\n",
      "Surface training t=24709, loss=0.02175186015665531\n",
      "Surface training t=24710, loss=0.01791747659444809\n",
      "Surface training t=24711, loss=0.02219502069056034\n",
      "Surface training t=24712, loss=0.019340284168720245\n",
      "Surface training t=24713, loss=0.025700544007122517\n",
      "Surface training t=24714, loss=0.02065717987716198\n",
      "Surface training t=24715, loss=0.023407254368066788\n",
      "Surface training t=24716, loss=0.02310365531593561\n",
      "Surface training t=24717, loss=0.021155912429094315\n",
      "Surface training t=24718, loss=0.017661787569522858\n",
      "Surface training t=24719, loss=0.02254288550466299\n",
      "Surface training t=24720, loss=0.020225192420184612\n",
      "Surface training t=24721, loss=0.018668601289391518\n",
      "Surface training t=24722, loss=0.017286257818341255\n",
      "Surface training t=24723, loss=0.030169925652444363\n",
      "Surface training t=24724, loss=0.01838577864691615\n",
      "Surface training t=24725, loss=0.021954025141894817\n",
      "Surface training t=24726, loss=0.029650459066033363\n",
      "Surface training t=24727, loss=0.03039564471691847\n",
      "Surface training t=24728, loss=0.024000519886612892\n",
      "Surface training t=24729, loss=0.0331959817558527\n",
      "Surface training t=24730, loss=0.0258789686486125\n",
      "Surface training t=24731, loss=0.02308228611946106\n",
      "Surface training t=24732, loss=0.019915452226996422\n",
      "Surface training t=24733, loss=0.02026959042996168\n",
      "Surface training t=24734, loss=0.0198289193212986\n",
      "Surface training t=24735, loss=0.017059347592294216\n",
      "Surface training t=24736, loss=0.018004189245402813\n",
      "Surface training t=24737, loss=0.0174268358387053\n",
      "Surface training t=24738, loss=0.017552402801811695\n",
      "Surface training t=24739, loss=0.018502073362469673\n",
      "Surface training t=24740, loss=0.02078610472381115\n",
      "Surface training t=24741, loss=0.0315677747130394\n",
      "Surface training t=24742, loss=0.024285221472382545\n",
      "Surface training t=24743, loss=0.029510275460779667\n",
      "Surface training t=24744, loss=0.017001288942992687\n",
      "Surface training t=24745, loss=0.019585011061280966\n",
      "Surface training t=24746, loss=0.024363349191844463\n",
      "Surface training t=24747, loss=0.025036116130650043\n",
      "Surface training t=24748, loss=0.03005098458379507\n",
      "Surface training t=24749, loss=0.02219038177281618\n",
      "Surface training t=24750, loss=0.01928936503827572\n",
      "Surface training t=24751, loss=0.02193350624293089\n",
      "Surface training t=24752, loss=0.024533899500966072\n",
      "Surface training t=24753, loss=0.021104000974446535\n",
      "Surface training t=24754, loss=0.02462043520063162\n",
      "Surface training t=24755, loss=0.021629715338349342\n",
      "Surface training t=24756, loss=0.017675841227173805\n",
      "Surface training t=24757, loss=0.01937356125563383\n",
      "Surface training t=24758, loss=0.019486960023641586\n",
      "Surface training t=24759, loss=0.01894121104851365\n",
      "Surface training t=24760, loss=0.019452339969575405\n",
      "Surface training t=24761, loss=0.0251253480091691\n",
      "Surface training t=24762, loss=0.025567300617694855\n",
      "Surface training t=24763, loss=0.027761250734329224\n",
      "Surface training t=24764, loss=0.02965497598052025\n",
      "Surface training t=24765, loss=0.02398786600679159\n",
      "Surface training t=24766, loss=0.016961215995252132\n",
      "Surface training t=24767, loss=0.027962234802544117\n",
      "Surface training t=24768, loss=0.022387034259736538\n",
      "Surface training t=24769, loss=0.02879498526453972\n",
      "Surface training t=24770, loss=0.029400288127362728\n",
      "Surface training t=24771, loss=0.028954009525477886\n",
      "Surface training t=24772, loss=0.02574250102043152\n",
      "Surface training t=24773, loss=0.024376469664275646\n",
      "Surface training t=24774, loss=0.020791977643966675\n",
      "Surface training t=24775, loss=0.023686829954385757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=24776, loss=0.0234497906640172\n",
      "Surface training t=24777, loss=0.02543217223137617\n",
      "Surface training t=24778, loss=0.025116094388067722\n",
      "Surface training t=24779, loss=0.02117135375738144\n",
      "Surface training t=24780, loss=0.021295434795320034\n",
      "Surface training t=24781, loss=0.026742832735180855\n",
      "Surface training t=24782, loss=0.022362682968378067\n",
      "Surface training t=24783, loss=0.01936657540500164\n",
      "Surface training t=24784, loss=0.02140447311103344\n",
      "Surface training t=24785, loss=0.025563166476786137\n",
      "Surface training t=24786, loss=0.02647928148508072\n",
      "Surface training t=24787, loss=0.021168389357626438\n",
      "Surface training t=24788, loss=0.016929167322814465\n",
      "Surface training t=24789, loss=0.026067666709423065\n",
      "Surface training t=24790, loss=0.02557314559817314\n",
      "Surface training t=24791, loss=0.018449955619871616\n",
      "Surface training t=24792, loss=0.02291901223361492\n",
      "Surface training t=24793, loss=0.022017589770257473\n",
      "Surface training t=24794, loss=0.021123915910720825\n",
      "Surface training t=24795, loss=0.028244088403880596\n",
      "Surface training t=24796, loss=0.022930840030312538\n",
      "Surface training t=24797, loss=0.022916074842214584\n",
      "Surface training t=24798, loss=0.023884876631200314\n",
      "Surface training t=24799, loss=0.02459654677659273\n",
      "Surface training t=24800, loss=0.02014997322112322\n",
      "Surface training t=24801, loss=0.020556063391268253\n",
      "Surface training t=24802, loss=0.015992307104170322\n",
      "Surface training t=24803, loss=0.015301617328077555\n",
      "Surface training t=24804, loss=0.01649028016254306\n",
      "Surface training t=24805, loss=0.014589030761271715\n",
      "Surface training t=24806, loss=0.016883578151464462\n",
      "Surface training t=24807, loss=0.016105125658214092\n",
      "Surface training t=24808, loss=0.018948741257190704\n",
      "Surface training t=24809, loss=0.013334173709154129\n",
      "Surface training t=24810, loss=0.018111486919224262\n",
      "Surface training t=24811, loss=0.01683998527005315\n",
      "Surface training t=24812, loss=0.028618765994906425\n",
      "Surface training t=24813, loss=0.03004281036555767\n",
      "Surface training t=24814, loss=0.03187442012131214\n",
      "Surface training t=24815, loss=0.020829313900321722\n",
      "Surface training t=24816, loss=0.015554610639810562\n",
      "Surface training t=24817, loss=0.016662550158798695\n",
      "Surface training t=24818, loss=0.01940213330090046\n",
      "Surface training t=24819, loss=0.02503162994980812\n",
      "Surface training t=24820, loss=0.02456788159906864\n",
      "Surface training t=24821, loss=0.029749508947134018\n",
      "Surface training t=24822, loss=0.035719141364097595\n",
      "Surface training t=24823, loss=0.02371475100517273\n",
      "Surface training t=24824, loss=0.03019625972956419\n",
      "Surface training t=24825, loss=0.019873454235494137\n",
      "Surface training t=24826, loss=0.024078321643173695\n",
      "Surface training t=24827, loss=0.02196818683296442\n",
      "Surface training t=24828, loss=0.02949175238609314\n",
      "Surface training t=24829, loss=0.026450559496879578\n",
      "Surface training t=24830, loss=0.023921264335513115\n",
      "Surface training t=24831, loss=0.02544342540204525\n",
      "Surface training t=24832, loss=0.03786570392549038\n",
      "Surface training t=24833, loss=0.01983922068029642\n",
      "Surface training t=24834, loss=0.02976209856569767\n",
      "Surface training t=24835, loss=0.03770826198160648\n",
      "Surface training t=24836, loss=0.03504731133580208\n",
      "Surface training t=24837, loss=0.028775643557310104\n",
      "Surface training t=24838, loss=0.03863954916596413\n",
      "Surface training t=24839, loss=0.04009263403713703\n",
      "Surface training t=24840, loss=0.02978503704071045\n",
      "Surface training t=24841, loss=0.02575088432058692\n",
      "Surface training t=24842, loss=0.024848484434187412\n",
      "Surface training t=24843, loss=0.034660251811146736\n",
      "Surface training t=24844, loss=0.03927295282483101\n",
      "Surface training t=24845, loss=0.03422961011528969\n",
      "Surface training t=24846, loss=0.03880068473517895\n",
      "Surface training t=24847, loss=0.04496571235358715\n",
      "Surface training t=24848, loss=0.03266526572406292\n",
      "Surface training t=24849, loss=0.03593851998448372\n",
      "Surface training t=24850, loss=0.050609463825821877\n",
      "Surface training t=24851, loss=0.035855747759342194\n",
      "Surface training t=24852, loss=0.034511033445596695\n",
      "Surface training t=24853, loss=0.03002011403441429\n",
      "Surface training t=24854, loss=0.03236511442810297\n",
      "Surface training t=24855, loss=0.03212125785648823\n",
      "Surface training t=24856, loss=0.023019191808998585\n",
      "Surface training t=24857, loss=0.02109126839786768\n",
      "Surface training t=24858, loss=0.02767473179847002\n",
      "Surface training t=24859, loss=0.03476087749004364\n",
      "Surface training t=24860, loss=0.022606716491281986\n",
      "Surface training t=24861, loss=0.021335361525416374\n",
      "Surface training t=24862, loss=0.01815170794725418\n",
      "Surface training t=24863, loss=0.018244552426040173\n",
      "Surface training t=24864, loss=0.026720646768808365\n",
      "Surface training t=24865, loss=0.023932214826345444\n",
      "Surface training t=24866, loss=0.020289700478315353\n",
      "Surface training t=24867, loss=0.021841559559106827\n",
      "Surface training t=24868, loss=0.019745071418583393\n",
      "Surface training t=24869, loss=0.017811487428843975\n",
      "Surface training t=24870, loss=0.025107719004154205\n",
      "Surface training t=24871, loss=0.024846943095326424\n",
      "Surface training t=24872, loss=0.019182780757546425\n",
      "Surface training t=24873, loss=0.023253183811903\n",
      "Surface training t=24874, loss=0.021627248264849186\n",
      "Surface training t=24875, loss=0.026119337417185307\n",
      "Surface training t=24876, loss=0.019190200604498386\n",
      "Surface training t=24877, loss=0.021990060806274414\n",
      "Surface training t=24878, loss=0.0237968061119318\n",
      "Surface training t=24879, loss=0.021081543527543545\n",
      "Surface training t=24880, loss=0.020021517761051655\n",
      "Surface training t=24881, loss=0.01758600678294897\n",
      "Surface training t=24882, loss=0.020332514308393\n",
      "Surface training t=24883, loss=0.02225561160594225\n",
      "Surface training t=24884, loss=0.020474780350923538\n",
      "Surface training t=24885, loss=0.01993782352656126\n",
      "Surface training t=24886, loss=0.016342919785529375\n",
      "Surface training t=24887, loss=0.034221114590764046\n",
      "Surface training t=24888, loss=0.0243205763399601\n",
      "Surface training t=24889, loss=0.020046881400048733\n",
      "Surface training t=24890, loss=0.023426412604749203\n",
      "Surface training t=24891, loss=0.02047646837309003\n",
      "Surface training t=24892, loss=0.0212984262034297\n",
      "Surface training t=24893, loss=0.024860290810465813\n",
      "Surface training t=24894, loss=0.025788641534745693\n",
      "Surface training t=24895, loss=0.02276515681296587\n",
      "Surface training t=24896, loss=0.024280966259539127\n",
      "Surface training t=24897, loss=0.01742746215313673\n",
      "Surface training t=24898, loss=0.016000673174858093\n",
      "Surface training t=24899, loss=0.01592878671362996\n",
      "Surface training t=24900, loss=0.015113485045731068\n",
      "Surface training t=24901, loss=0.01747591607272625\n",
      "Surface training t=24902, loss=0.01746713276952505\n",
      "Surface training t=24903, loss=0.012725756503641605\n",
      "Surface training t=24904, loss=0.01570793939754367\n",
      "Surface training t=24905, loss=0.015334578696638346\n",
      "Surface training t=24906, loss=0.014570512808859348\n",
      "Surface training t=24907, loss=0.022354211658239365\n",
      "Surface training t=24908, loss=0.02333452831953764\n",
      "Surface training t=24909, loss=0.021848278120160103\n",
      "Surface training t=24910, loss=0.022570747882127762\n",
      "Surface training t=24911, loss=0.01714305579662323\n",
      "Surface training t=24912, loss=0.019824840128421783\n",
      "Surface training t=24913, loss=0.024795496836304665\n",
      "Surface training t=24914, loss=0.018650724552571774\n",
      "Surface training t=24915, loss=0.02477736584842205\n",
      "Surface training t=24916, loss=0.028712528757750988\n",
      "Surface training t=24917, loss=0.024815388955175877\n",
      "Surface training t=24918, loss=0.023818531073629856\n",
      "Surface training t=24919, loss=0.028850032947957516\n",
      "Surface training t=24920, loss=0.026553185656666756\n",
      "Surface training t=24921, loss=0.03116922453045845\n",
      "Surface training t=24922, loss=0.023037568666040897\n",
      "Surface training t=24923, loss=0.027521376498043537\n",
      "Surface training t=24924, loss=0.022392078302800655\n",
      "Surface training t=24925, loss=0.031014501117169857\n",
      "Surface training t=24926, loss=0.030149228870868683\n",
      "Surface training t=24927, loss=0.028759625740349293\n",
      "Surface training t=24928, loss=0.028086830861866474\n",
      "Surface training t=24929, loss=0.02912208065390587\n",
      "Surface training t=24930, loss=0.019334330689162016\n",
      "Surface training t=24931, loss=0.024110897444188595\n",
      "Surface training t=24932, loss=0.029766688123345375\n",
      "Surface training t=24933, loss=0.021000304259359837\n",
      "Surface training t=24934, loss=0.026416616514325142\n",
      "Surface training t=24935, loss=0.032673549838364124\n",
      "Surface training t=24936, loss=0.02332491148263216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=24937, loss=0.027642227709293365\n",
      "Surface training t=24938, loss=0.029370224103331566\n",
      "Surface training t=24939, loss=0.020792610943317413\n",
      "Surface training t=24940, loss=0.02703062817454338\n",
      "Surface training t=24941, loss=0.017312389332801104\n",
      "Surface training t=24942, loss=0.02668983768671751\n",
      "Surface training t=24943, loss=0.02274408843368292\n",
      "Surface training t=24944, loss=0.026066064834594727\n",
      "Surface training t=24945, loss=0.018148934468626976\n",
      "Surface training t=24946, loss=0.025959407910704613\n",
      "Surface training t=24947, loss=0.0247995276004076\n",
      "Surface training t=24948, loss=0.02142307348549366\n",
      "Surface training t=24949, loss=0.031914626248180866\n",
      "Surface training t=24950, loss=0.02346357051283121\n",
      "Surface training t=24951, loss=0.01820026570931077\n",
      "Surface training t=24952, loss=0.019767852500081062\n",
      "Surface training t=24953, loss=0.018744084052741528\n",
      "Surface training t=24954, loss=0.02132601384073496\n",
      "Surface training t=24955, loss=0.023453806526958942\n",
      "Surface training t=24956, loss=0.016728718765079975\n",
      "Surface training t=24957, loss=0.01609613560140133\n",
      "Surface training t=24958, loss=0.014495160896331072\n",
      "Surface training t=24959, loss=0.01621580682694912\n",
      "Surface training t=24960, loss=0.017747086938470602\n",
      "Surface training t=24961, loss=0.019983284175395966\n",
      "Surface training t=24962, loss=0.021051708608865738\n",
      "Surface training t=24963, loss=0.025193754583597183\n",
      "Surface training t=24964, loss=0.022622416727244854\n",
      "Surface training t=24965, loss=0.022189089562743902\n",
      "Surface training t=24966, loss=0.01524408534169197\n",
      "Surface training t=24967, loss=0.026843564584851265\n",
      "Surface training t=24968, loss=0.03672259487211704\n",
      "Surface training t=24969, loss=0.031084402464330196\n",
      "Surface training t=24970, loss=0.027328495867550373\n",
      "Surface training t=24971, loss=0.02606387622654438\n",
      "Surface training t=24972, loss=0.025380133651196957\n",
      "Surface training t=24973, loss=0.023717068135738373\n",
      "Surface training t=24974, loss=0.02255861461162567\n",
      "Surface training t=24975, loss=0.021030833944678307\n",
      "Surface training t=24976, loss=0.02650914154946804\n",
      "Surface training t=24977, loss=0.021890530362725258\n",
      "Surface training t=24978, loss=0.03455406241118908\n",
      "Surface training t=24979, loss=0.031227678060531616\n",
      "Surface training t=24980, loss=0.02864706516265869\n",
      "Surface training t=24981, loss=0.02592475153505802\n",
      "Surface training t=24982, loss=0.02103268261998892\n",
      "Surface training t=24983, loss=0.024426324293017387\n",
      "Surface training t=24984, loss=0.03099494706839323\n",
      "Surface training t=24985, loss=0.026425874792039394\n",
      "Surface training t=24986, loss=0.032365525141358376\n",
      "Surface training t=24987, loss=0.02898641023784876\n",
      "Surface training t=24988, loss=0.03328433632850647\n",
      "Surface training t=24989, loss=0.02423693146556616\n",
      "Surface training t=24990, loss=0.020626423880457878\n",
      "Surface training t=24991, loss=0.021265532821416855\n",
      "Surface training t=24992, loss=0.021460670046508312\n",
      "Surface training t=24993, loss=0.023537807166576385\n",
      "Surface training t=24994, loss=0.025786065496504307\n",
      "Surface training t=24995, loss=0.03237398341298103\n",
      "Surface training t=24996, loss=0.0453384593129158\n",
      "Surface training t=24997, loss=0.03701953403651714\n",
      "Surface training t=24998, loss=0.031039781868457794\n",
      "Surface training t=24999, loss=0.03789188526570797\n",
      "Surface training t=25000, loss=0.023646775633096695\n",
      "Surface training t=25001, loss=0.029194608330726624\n",
      "Surface training t=25002, loss=0.038666337728500366\n",
      "Surface training t=25003, loss=0.030752630904316902\n",
      "Surface training t=25004, loss=0.029747363179922104\n",
      "Surface training t=25005, loss=0.027739343233406544\n",
      "Surface training t=25006, loss=0.03302113339304924\n",
      "Surface training t=25007, loss=0.02409530058503151\n",
      "Surface training t=25008, loss=0.02743207849562168\n",
      "Surface training t=25009, loss=0.031365721486508846\n",
      "Surface training t=25010, loss=0.03858520835638046\n",
      "Surface training t=25011, loss=0.03313485532999039\n",
      "Surface training t=25012, loss=0.028273235075175762\n",
      "Surface training t=25013, loss=0.027024822309613228\n",
      "Surface training t=25014, loss=0.0194927379488945\n",
      "Surface training t=25015, loss=0.01703095156699419\n",
      "Surface training t=25016, loss=0.018778029829263687\n",
      "Surface training t=25017, loss=0.016759137623012066\n",
      "Surface training t=25018, loss=0.0133100226521492\n",
      "Surface training t=25019, loss=0.02780374325811863\n",
      "Surface training t=25020, loss=0.023879115469753742\n",
      "Surface training t=25021, loss=0.02261835616081953\n",
      "Surface training t=25022, loss=0.024562496691942215\n",
      "Surface training t=25023, loss=0.022751535288989544\n",
      "Surface training t=25024, loss=0.017781550996005535\n",
      "Surface training t=25025, loss=0.026232007890939713\n",
      "Surface training t=25026, loss=0.02064344985410571\n",
      "Surface training t=25027, loss=0.026099546812474728\n",
      "Surface training t=25028, loss=0.022493495605885983\n",
      "Surface training t=25029, loss=0.027916566468775272\n",
      "Surface training t=25030, loss=0.03235070966184139\n",
      "Surface training t=25031, loss=0.02020355686545372\n",
      "Surface training t=25032, loss=0.03022086527198553\n",
      "Surface training t=25033, loss=0.025647605769336224\n",
      "Surface training t=25034, loss=0.028145535849034786\n",
      "Surface training t=25035, loss=0.02473635971546173\n",
      "Surface training t=25036, loss=0.02241745311766863\n",
      "Surface training t=25037, loss=0.022105054929852486\n",
      "Surface training t=25038, loss=0.019564812071621418\n",
      "Surface training t=25039, loss=0.02266277652233839\n",
      "Surface training t=25040, loss=0.0239686518907547\n",
      "Surface training t=25041, loss=0.025860373862087727\n",
      "Surface training t=25042, loss=0.02651820983737707\n",
      "Surface training t=25043, loss=0.02268554549664259\n",
      "Surface training t=25044, loss=0.02757798694074154\n",
      "Surface training t=25045, loss=0.023668057285249233\n",
      "Surface training t=25046, loss=0.02608572691679001\n",
      "Surface training t=25047, loss=0.027757512405514717\n",
      "Surface training t=25048, loss=0.03083165269345045\n",
      "Surface training t=25049, loss=0.025089473463594913\n",
      "Surface training t=25050, loss=0.03388449735939503\n",
      "Surface training t=25051, loss=0.05007918365299702\n",
      "Surface training t=25052, loss=0.0335985217243433\n",
      "Surface training t=25053, loss=0.05062031373381615\n",
      "Surface training t=25054, loss=0.03695842996239662\n",
      "Surface training t=25055, loss=0.03749245963990688\n",
      "Surface training t=25056, loss=0.04354299604892731\n",
      "Surface training t=25057, loss=0.032476767897605896\n",
      "Surface training t=25058, loss=0.03226679004728794\n",
      "Surface training t=25059, loss=0.03210506774485111\n",
      "Surface training t=25060, loss=0.02326615620404482\n",
      "Surface training t=25061, loss=0.034689342603087425\n",
      "Surface training t=25062, loss=0.03159155696630478\n",
      "Surface training t=25063, loss=0.027453508228063583\n",
      "Surface training t=25064, loss=0.02578583173453808\n",
      "Surface training t=25065, loss=0.02625636663287878\n",
      "Surface training t=25066, loss=0.022066663019359112\n",
      "Surface training t=25067, loss=0.01948129665106535\n",
      "Surface training t=25068, loss=0.025719428434967995\n",
      "Surface training t=25069, loss=0.0242355614900589\n",
      "Surface training t=25070, loss=0.03127459064126015\n",
      "Surface training t=25071, loss=0.01827115472406149\n",
      "Surface training t=25072, loss=0.022814324125647545\n",
      "Surface training t=25073, loss=0.02342603076249361\n",
      "Surface training t=25074, loss=0.027155166491866112\n",
      "Surface training t=25075, loss=0.02857499849051237\n",
      "Surface training t=25076, loss=0.026440100744366646\n",
      "Surface training t=25077, loss=0.02864893339574337\n",
      "Surface training t=25078, loss=0.02393971011042595\n",
      "Surface training t=25079, loss=0.03288828115910292\n",
      "Surface training t=25080, loss=0.02758968435227871\n",
      "Surface training t=25081, loss=0.029808873310685158\n",
      "Surface training t=25082, loss=0.02468438632786274\n",
      "Surface training t=25083, loss=0.022554324474185705\n",
      "Surface training t=25084, loss=0.024415191262960434\n",
      "Surface training t=25085, loss=0.02646581642329693\n",
      "Surface training t=25086, loss=0.024281145073473454\n",
      "Surface training t=25087, loss=0.03246915340423584\n",
      "Surface training t=25088, loss=0.03381715901196003\n",
      "Surface training t=25089, loss=0.030959484167397022\n",
      "Surface training t=25090, loss=0.028785726986825466\n",
      "Surface training t=25091, loss=0.030943073332309723\n",
      "Surface training t=25092, loss=0.027346313931047916\n",
      "Surface training t=25093, loss=0.025938723236322403\n",
      "Surface training t=25094, loss=0.05435704067349434\n",
      "Surface training t=25095, loss=0.031135824508965015\n",
      "Surface training t=25096, loss=0.03549663908779621\n",
      "Surface training t=25097, loss=0.03210121486335993\n",
      "Surface training t=25098, loss=0.03408307023346424\n",
      "Surface training t=25099, loss=0.025904268492013216\n",
      "Surface training t=25100, loss=0.03092486597597599\n",
      "Surface training t=25101, loss=0.027059923857450485\n",
      "Surface training t=25102, loss=0.040535058826208115\n",
      "Surface training t=25103, loss=0.031131687574088573\n",
      "Surface training t=25104, loss=0.027980643324553967\n",
      "Surface training t=25105, loss=0.02433778438717127\n",
      "Surface training t=25106, loss=0.02528899908065796\n",
      "Surface training t=25107, loss=0.028588425368070602\n",
      "Surface training t=25108, loss=0.02041742205619812\n",
      "Surface training t=25109, loss=0.023711517453193665\n",
      "Surface training t=25110, loss=0.02566349506378174\n",
      "Surface training t=25111, loss=0.02074903529137373\n",
      "Surface training t=25112, loss=0.02270680759102106\n",
      "Surface training t=25113, loss=0.028710308484733105\n",
      "Surface training t=25114, loss=0.035100383684039116\n",
      "Surface training t=25115, loss=0.026772869750857353\n",
      "Surface training t=25116, loss=0.03143131174147129\n",
      "Surface training t=25117, loss=0.029604263603687286\n",
      "Surface training t=25118, loss=0.02050147671252489\n",
      "Surface training t=25119, loss=0.025786845944821835\n",
      "Surface training t=25120, loss=0.020932450890541077\n",
      "Surface training t=25121, loss=0.026610896922647953\n",
      "Surface training t=25122, loss=0.027362381108105183\n",
      "Surface training t=25123, loss=0.03274894692003727\n",
      "Surface training t=25124, loss=0.023075876757502556\n",
      "Surface training t=25125, loss=0.024369528517127037\n",
      "Surface training t=25126, loss=0.02151415031403303\n",
      "Surface training t=25127, loss=0.017823122441768646\n",
      "Surface training t=25128, loss=0.022927354089915752\n",
      "Surface training t=25129, loss=0.017772656865417957\n",
      "Surface training t=25130, loss=0.01667376048862934\n",
      "Surface training t=25131, loss=0.01734847715124488\n",
      "Surface training t=25132, loss=0.01440013526007533\n",
      "Surface training t=25133, loss=0.016505584120750427\n",
      "Surface training t=25134, loss=0.018274972215294838\n",
      "Surface training t=25135, loss=0.02183330524712801\n",
      "Surface training t=25136, loss=0.026046181097626686\n",
      "Surface training t=25137, loss=0.024348328821361065\n",
      "Surface training t=25138, loss=0.023297270759940147\n",
      "Surface training t=25139, loss=0.02063558530062437\n",
      "Surface training t=25140, loss=0.020862361416220665\n",
      "Surface training t=25141, loss=0.023330737836658955\n",
      "Surface training t=25142, loss=0.023110785521566868\n",
      "Surface training t=25143, loss=0.023100548423826694\n",
      "Surface training t=25144, loss=0.021222486160695553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=25145, loss=0.02532987203449011\n",
      "Surface training t=25146, loss=0.019822368398308754\n",
      "Surface training t=25147, loss=0.01885306043550372\n",
      "Surface training t=25148, loss=0.01676005218178034\n",
      "Surface training t=25149, loss=0.017611338756978512\n",
      "Surface training t=25150, loss=0.016554929316043854\n",
      "Surface training t=25151, loss=0.014521467499434948\n",
      "Surface training t=25152, loss=0.01869719848036766\n",
      "Surface training t=25153, loss=0.014104453381150961\n",
      "Surface training t=25154, loss=0.018364903517067432\n",
      "Surface training t=25155, loss=0.02524417359381914\n",
      "Surface training t=25156, loss=0.03302543982863426\n",
      "Surface training t=25157, loss=0.021026122383773327\n",
      "Surface training t=25158, loss=0.027764728292822838\n",
      "Surface training t=25159, loss=0.016263006255030632\n",
      "Surface training t=25160, loss=0.014805252198129892\n",
      "Surface training t=25161, loss=0.013374218717217445\n",
      "Surface training t=25162, loss=0.019733188673853874\n",
      "Surface training t=25163, loss=0.0195614043623209\n",
      "Surface training t=25164, loss=0.02099058125168085\n",
      "Surface training t=25165, loss=0.013459712266921997\n",
      "Surface training t=25166, loss=0.014133503660559654\n",
      "Surface training t=25167, loss=0.017674545757472515\n",
      "Surface training t=25168, loss=0.01801193505525589\n",
      "Surface training t=25169, loss=0.02597154676914215\n",
      "Surface training t=25170, loss=0.019472556188702583\n",
      "Surface training t=25171, loss=0.02398193534463644\n",
      "Surface training t=25172, loss=0.021104171872138977\n",
      "Surface training t=25173, loss=0.027444412000477314\n",
      "Surface training t=25174, loss=0.019687477499246597\n",
      "Surface training t=25175, loss=0.02394095901399851\n",
      "Surface training t=25176, loss=0.022330861538648605\n",
      "Surface training t=25177, loss=0.01633703149855137\n",
      "Surface training t=25178, loss=0.02211651299148798\n",
      "Surface training t=25179, loss=0.014879585709422827\n",
      "Surface training t=25180, loss=0.018353261053562164\n",
      "Surface training t=25181, loss=0.02025480940937996\n",
      "Surface training t=25182, loss=0.021841201931238174\n",
      "Surface training t=25183, loss=0.031146707013249397\n",
      "Surface training t=25184, loss=0.02867879718542099\n",
      "Surface training t=25185, loss=0.027785029262304306\n",
      "Surface training t=25186, loss=0.01868349127471447\n",
      "Surface training t=25187, loss=0.01786344638094306\n",
      "Surface training t=25188, loss=0.0230961125344038\n",
      "Surface training t=25189, loss=0.02373537514358759\n",
      "Surface training t=25190, loss=0.02413630299270153\n",
      "Surface training t=25191, loss=0.014962906017899513\n",
      "Surface training t=25192, loss=0.02021876722574234\n",
      "Surface training t=25193, loss=0.015369321219623089\n",
      "Surface training t=25194, loss=0.017535896971821785\n",
      "Surface training t=25195, loss=0.014454496093094349\n",
      "Surface training t=25196, loss=0.01961237285286188\n",
      "Surface training t=25197, loss=0.027037326246500015\n",
      "Surface training t=25198, loss=0.024677003733813763\n",
      "Surface training t=25199, loss=0.024150948971509933\n",
      "Surface training t=25200, loss=0.02401896845549345\n",
      "Surface training t=25201, loss=0.019427917897701263\n",
      "Surface training t=25202, loss=0.02543935924768448\n",
      "Surface training t=25203, loss=0.027842020615935326\n",
      "Surface training t=25204, loss=0.019219581969082355\n",
      "Surface training t=25205, loss=0.017239635810256004\n",
      "Surface training t=25206, loss=0.024148655124008656\n",
      "Surface training t=25207, loss=0.019103530794382095\n",
      "Surface training t=25208, loss=0.028645670041441917\n",
      "Surface training t=25209, loss=0.02549688797444105\n",
      "Surface training t=25210, loss=0.022293545305728912\n",
      "Surface training t=25211, loss=0.01856753882020712\n",
      "Surface training t=25212, loss=0.02094398532062769\n",
      "Surface training t=25213, loss=0.01716485619544983\n",
      "Surface training t=25214, loss=0.017659052275121212\n",
      "Surface training t=25215, loss=0.020261290483176708\n",
      "Surface training t=25216, loss=0.019375885371118784\n",
      "Surface training t=25217, loss=0.02757772896438837\n",
      "Surface training t=25218, loss=0.02818256989121437\n",
      "Surface training t=25219, loss=0.03891916759312153\n",
      "Surface training t=25220, loss=0.03304537013173103\n",
      "Surface training t=25221, loss=0.04486101306974888\n",
      "Surface training t=25222, loss=0.041394198313355446\n",
      "Surface training t=25223, loss=0.04618752747774124\n",
      "Surface training t=25224, loss=0.03345916699618101\n",
      "Surface training t=25225, loss=0.03629178926348686\n",
      "Surface training t=25226, loss=0.028070599772036076\n",
      "Surface training t=25227, loss=0.019968608394265175\n",
      "Surface training t=25228, loss=0.021526126191020012\n",
      "Surface training t=25229, loss=0.01651813741773367\n",
      "Surface training t=25230, loss=0.021337277255952358\n",
      "Surface training t=25231, loss=0.017825962509959936\n",
      "Surface training t=25232, loss=0.022247960790991783\n",
      "Surface training t=25233, loss=0.027851346880197525\n",
      "Surface training t=25234, loss=0.028211455792188644\n",
      "Surface training t=25235, loss=0.029446623288094997\n",
      "Surface training t=25236, loss=0.026458664797246456\n",
      "Surface training t=25237, loss=0.022013223730027676\n",
      "Surface training t=25238, loss=0.020748560316860676\n",
      "Surface training t=25239, loss=0.02474132739007473\n",
      "Surface training t=25240, loss=0.028772108256816864\n",
      "Surface training t=25241, loss=0.01813441328704357\n",
      "Surface training t=25242, loss=0.019636228680610657\n",
      "Surface training t=25243, loss=0.030304275453090668\n",
      "Surface training t=25244, loss=0.023656046018004417\n",
      "Surface training t=25245, loss=0.04097078554332256\n",
      "Surface training t=25246, loss=0.029218103736639023\n",
      "Surface training t=25247, loss=0.03517906367778778\n",
      "Surface training t=25248, loss=0.04239959083497524\n",
      "Surface training t=25249, loss=0.030565720051527023\n",
      "Surface training t=25250, loss=0.044447923079133034\n",
      "Surface training t=25251, loss=0.024040463380515575\n",
      "Surface training t=25252, loss=0.034559642896056175\n",
      "Surface training t=25253, loss=0.027864781208336353\n",
      "Surface training t=25254, loss=0.02457293588668108\n",
      "Surface training t=25255, loss=0.03084256872534752\n",
      "Surface training t=25256, loss=0.026973742060363293\n",
      "Surface training t=25257, loss=0.0215669022873044\n",
      "Surface training t=25258, loss=0.018912075087428093\n",
      "Surface training t=25259, loss=0.019365016371011734\n",
      "Surface training t=25260, loss=0.022187822498381138\n",
      "Surface training t=25261, loss=0.023584455251693726\n",
      "Surface training t=25262, loss=0.023112746886909008\n",
      "Surface training t=25263, loss=0.01939390879124403\n",
      "Surface training t=25264, loss=0.032768139615654945\n",
      "Surface training t=25265, loss=0.021837158128619194\n",
      "Surface training t=25266, loss=0.025895217433571815\n",
      "Surface training t=25267, loss=0.04097312502563\n",
      "Surface training t=25268, loss=0.027270134538412094\n",
      "Surface training t=25269, loss=0.03349324129521847\n",
      "Surface training t=25270, loss=0.027011231519281864\n",
      "Surface training t=25271, loss=0.026835624128580093\n",
      "Surface training t=25272, loss=0.02544598001986742\n",
      "Surface training t=25273, loss=0.025224176235496998\n",
      "Surface training t=25274, loss=0.02113727480173111\n",
      "Surface training t=25275, loss=0.024716327898204327\n",
      "Surface training t=25276, loss=0.03189728036522865\n",
      "Surface training t=25277, loss=0.03435036912560463\n",
      "Surface training t=25278, loss=0.025105595588684082\n",
      "Surface training t=25279, loss=0.02713841199874878\n",
      "Surface training t=25280, loss=0.023623740300536156\n",
      "Surface training t=25281, loss=0.02467618603259325\n",
      "Surface training t=25282, loss=0.018639013171195984\n",
      "Surface training t=25283, loss=0.022206539288163185\n",
      "Surface training t=25284, loss=0.04285930097103119\n",
      "Surface training t=25285, loss=0.03410189040005207\n",
      "Surface training t=25286, loss=0.025089802220463753\n",
      "Surface training t=25287, loss=0.02891758270561695\n",
      "Surface training t=25288, loss=0.03714473731815815\n",
      "Surface training t=25289, loss=0.02310357242822647\n",
      "Surface training t=25290, loss=0.028758099302649498\n",
      "Surface training t=25291, loss=0.02701034676283598\n",
      "Surface training t=25292, loss=0.028770742937922478\n",
      "Surface training t=25293, loss=0.030583580955863\n",
      "Surface training t=25294, loss=0.03168867714703083\n",
      "Surface training t=25295, loss=0.05614188313484192\n",
      "Surface training t=25296, loss=0.04087523929774761\n",
      "Surface training t=25297, loss=0.03444763086736202\n",
      "Surface training t=25298, loss=0.03887521103024483\n",
      "Surface training t=25299, loss=0.02595620509237051\n",
      "Surface training t=25300, loss=0.029369293712079525\n",
      "Surface training t=25301, loss=0.025886310264468193\n",
      "Surface training t=25302, loss=0.035361915826797485\n",
      "Surface training t=25303, loss=0.02418292174115777\n",
      "Surface training t=25304, loss=0.0398100521415472\n",
      "Surface training t=25305, loss=0.022211583331227303\n",
      "Surface training t=25306, loss=0.022218854166567326\n",
      "Surface training t=25307, loss=0.023898951709270477\n",
      "Surface training t=25308, loss=0.02071616519242525\n",
      "Surface training t=25309, loss=0.033226373605430126\n",
      "Surface training t=25310, loss=0.02822334785014391\n",
      "Surface training t=25311, loss=0.02745882421731949\n",
      "Surface training t=25312, loss=0.02448983769863844\n",
      "Surface training t=25313, loss=0.024676882661879063\n",
      "Surface training t=25314, loss=0.0207015136256814\n",
      "Surface training t=25315, loss=0.023466600105166435\n",
      "Surface training t=25316, loss=0.019991434179246426\n",
      "Surface training t=25317, loss=0.02026380691677332\n",
      "Surface training t=25318, loss=0.01864876039326191\n",
      "Surface training t=25319, loss=0.01706516556441784\n",
      "Surface training t=25320, loss=0.015618091449141502\n",
      "Surface training t=25321, loss=0.016106833703815937\n",
      "Surface training t=25322, loss=0.023858544416725636\n",
      "Surface training t=25323, loss=0.022884592413902283\n",
      "Surface training t=25324, loss=0.028765934519469738\n",
      "Surface training t=25325, loss=0.03312401659786701\n",
      "Surface training t=25326, loss=0.0296213086694479\n",
      "Surface training t=25327, loss=0.02425203938037157\n",
      "Surface training t=25328, loss=0.034160676412284374\n",
      "Surface training t=25329, loss=0.02242820430546999\n",
      "Surface training t=25330, loss=0.027040266431868076\n",
      "Surface training t=25331, loss=0.02337533514946699\n",
      "Surface training t=25332, loss=0.02109189983457327\n",
      "Surface training t=25333, loss=0.024128173477947712\n",
      "Surface training t=25334, loss=0.022954806685447693\n",
      "Surface training t=25335, loss=0.022748545743525028\n",
      "Surface training t=25336, loss=0.019846762530505657\n",
      "Surface training t=25337, loss=0.016054854728281498\n",
      "Surface training t=25338, loss=0.01996555458754301\n",
      "Surface training t=25339, loss=0.025384049862623215\n",
      "Surface training t=25340, loss=0.02546792011708021\n",
      "Surface training t=25341, loss=0.024518187157809734\n",
      "Surface training t=25342, loss=0.02837597392499447\n",
      "Surface training t=25343, loss=0.02596965618431568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=25344, loss=0.03298359178006649\n",
      "Surface training t=25345, loss=0.03824907820671797\n",
      "Surface training t=25346, loss=0.03316009324043989\n",
      "Surface training t=25347, loss=0.039894500747323036\n",
      "Surface training t=25348, loss=0.03208634816110134\n",
      "Surface training t=25349, loss=0.029599588364362717\n",
      "Surface training t=25350, loss=0.03098861314356327\n",
      "Surface training t=25351, loss=0.03544170502573252\n",
      "Surface training t=25352, loss=0.05059606768190861\n",
      "Surface training t=25353, loss=0.03600303269922733\n",
      "Surface training t=25354, loss=0.03560692444443703\n",
      "Surface training t=25355, loss=0.03565572388470173\n",
      "Surface training t=25356, loss=0.04150239750742912\n",
      "Surface training t=25357, loss=0.036693694069981575\n",
      "Surface training t=25358, loss=0.052349673584103584\n",
      "Surface training t=25359, loss=0.03353555127978325\n",
      "Surface training t=25360, loss=0.03690676391124725\n",
      "Surface training t=25361, loss=0.05987539142370224\n",
      "Surface training t=25362, loss=0.0360682737082243\n",
      "Surface training t=25363, loss=0.03507891297340393\n",
      "Surface training t=25364, loss=0.031086232513189316\n",
      "Surface training t=25365, loss=0.03125259932130575\n",
      "Surface training t=25366, loss=0.03324167989194393\n",
      "Surface training t=25367, loss=0.03968166187405586\n",
      "Surface training t=25368, loss=0.04318119026720524\n",
      "Surface training t=25369, loss=0.02999412827193737\n",
      "Surface training t=25370, loss=0.03282505087554455\n",
      "Surface training t=25371, loss=0.02926436811685562\n",
      "Surface training t=25372, loss=0.0225664796307683\n",
      "Surface training t=25373, loss=0.021740569733083248\n",
      "Surface training t=25374, loss=0.02327120676636696\n",
      "Surface training t=25375, loss=0.02084483439102769\n",
      "Surface training t=25376, loss=0.027673578821122646\n",
      "Surface training t=25377, loss=0.031822058372199535\n",
      "Surface training t=25378, loss=0.02548348531126976\n",
      "Surface training t=25379, loss=0.020743221044540405\n",
      "Surface training t=25380, loss=0.03557993099093437\n",
      "Surface training t=25381, loss=0.024896481074392796\n",
      "Surface training t=25382, loss=0.027888654731214046\n",
      "Surface training t=25383, loss=0.025239921174943447\n",
      "Surface training t=25384, loss=0.023156654089689255\n",
      "Surface training t=25385, loss=0.023656845092773438\n",
      "Surface training t=25386, loss=0.017361332662403584\n",
      "Surface training t=25387, loss=0.019802022259682417\n",
      "Surface training t=25388, loss=0.022469954565167427\n",
      "Surface training t=25389, loss=0.01881337631493807\n",
      "Surface training t=25390, loss=0.020369185134768486\n",
      "Surface training t=25391, loss=0.030222161673009396\n",
      "Surface training t=25392, loss=0.02187006175518036\n",
      "Surface training t=25393, loss=0.03257958870381117\n",
      "Surface training t=25394, loss=0.0236892718821764\n",
      "Surface training t=25395, loss=0.026694761589169502\n",
      "Surface training t=25396, loss=0.026097135618329048\n",
      "Surface training t=25397, loss=0.02906861249357462\n",
      "Surface training t=25398, loss=0.02899215929210186\n",
      "Surface training t=25399, loss=0.018240297213196754\n",
      "Surface training t=25400, loss=0.02274729125201702\n",
      "Surface training t=25401, loss=0.025622934103012085\n",
      "Surface training t=25402, loss=0.025635752826929092\n",
      "Surface training t=25403, loss=0.026149955578148365\n",
      "Surface training t=25404, loss=0.02687913691624999\n",
      "Surface training t=25405, loss=0.03738403879106045\n",
      "Surface training t=25406, loss=0.036269038915634155\n",
      "Surface training t=25407, loss=0.036629851907491684\n",
      "Surface training t=25408, loss=0.030756326392292976\n",
      "Surface training t=25409, loss=0.02965926006436348\n",
      "Surface training t=25410, loss=0.028808724135160446\n",
      "Surface training t=25411, loss=0.03624962270259857\n",
      "Surface training t=25412, loss=0.021395171992480755\n",
      "Surface training t=25413, loss=0.02090608887374401\n",
      "Surface training t=25414, loss=0.022198382765054703\n",
      "Surface training t=25415, loss=0.03199412859976292\n",
      "Surface training t=25416, loss=0.024884921498596668\n",
      "Surface training t=25417, loss=0.023118619807064533\n",
      "Surface training t=25418, loss=0.018014593049883842\n",
      "Surface training t=25419, loss=0.02495306357741356\n",
      "Surface training t=25420, loss=0.02793180476874113\n",
      "Surface training t=25421, loss=0.02792929206043482\n",
      "Surface training t=25422, loss=0.027494169771671295\n",
      "Surface training t=25423, loss=0.026479844003915787\n",
      "Surface training t=25424, loss=0.02598282601684332\n",
      "Surface training t=25425, loss=0.020427259616553783\n",
      "Surface training t=25426, loss=0.025436257012188435\n",
      "Surface training t=25427, loss=0.021094467490911484\n",
      "Surface training t=25428, loss=0.02750188671052456\n",
      "Surface training t=25429, loss=0.02390916272997856\n",
      "Surface training t=25430, loss=0.021736664697527885\n",
      "Surface training t=25431, loss=0.02232437115162611\n",
      "Surface training t=25432, loss=0.024306770414114\n",
      "Surface training t=25433, loss=0.021335316821932793\n",
      "Surface training t=25434, loss=0.02082967199385166\n",
      "Surface training t=25435, loss=0.01882214331999421\n",
      "Surface training t=25436, loss=0.022589463740587234\n",
      "Surface training t=25437, loss=0.02045197505503893\n",
      "Surface training t=25438, loss=0.017180009745061398\n",
      "Surface training t=25439, loss=0.02642266359180212\n",
      "Surface training t=25440, loss=0.018608233891427517\n",
      "Surface training t=25441, loss=0.02590202074497938\n",
      "Surface training t=25442, loss=0.02475468721240759\n",
      "Surface training t=25443, loss=0.022605844773352146\n",
      "Surface training t=25444, loss=0.025268860161304474\n",
      "Surface training t=25445, loss=0.027427264489233494\n",
      "Surface training t=25446, loss=0.03225110564380884\n",
      "Surface training t=25447, loss=0.02536468580365181\n",
      "Surface training t=25448, loss=0.022383825853466988\n",
      "Surface training t=25449, loss=0.02490629442036152\n",
      "Surface training t=25450, loss=0.02250690758228302\n",
      "Surface training t=25451, loss=0.029491757974028587\n",
      "Surface training t=25452, loss=0.02333789598196745\n",
      "Surface training t=25453, loss=0.02593341562896967\n",
      "Surface training t=25454, loss=0.022083666175603867\n",
      "Surface training t=25455, loss=0.023399380035698414\n",
      "Surface training t=25456, loss=0.02434423565864563\n",
      "Surface training t=25457, loss=0.02077268622815609\n",
      "Surface training t=25458, loss=0.021512404084205627\n",
      "Surface training t=25459, loss=0.02384368423372507\n",
      "Surface training t=25460, loss=0.0185659471899271\n",
      "Surface training t=25461, loss=0.015866960398852825\n",
      "Surface training t=25462, loss=0.017460248433053493\n",
      "Surface training t=25463, loss=0.023875465616583824\n",
      "Surface training t=25464, loss=0.019545862451195717\n",
      "Surface training t=25465, loss=0.02314182184636593\n",
      "Surface training t=25466, loss=0.01971027534455061\n",
      "Surface training t=25467, loss=0.024747868068516254\n",
      "Surface training t=25468, loss=0.018303081393241882\n",
      "Surface training t=25469, loss=0.01747992541640997\n",
      "Surface training t=25470, loss=0.021822000853717327\n",
      "Surface training t=25471, loss=0.022298868745565414\n",
      "Surface training t=25472, loss=0.023899272084236145\n",
      "Surface training t=25473, loss=0.019775761757045984\n",
      "Surface training t=25474, loss=0.023884018883109093\n",
      "Surface training t=25475, loss=0.018828387837857008\n",
      "Surface training t=25476, loss=0.021722599864006042\n",
      "Surface training t=25477, loss=0.017709466628730297\n",
      "Surface training t=25478, loss=0.015512581914663315\n",
      "Surface training t=25479, loss=0.026035979390144348\n",
      "Surface training t=25480, loss=0.02758148033171892\n",
      "Surface training t=25481, loss=0.021494846791028976\n",
      "Surface training t=25482, loss=0.029958650469779968\n",
      "Surface training t=25483, loss=0.026272490620613098\n",
      "Surface training t=25484, loss=0.019159489311277866\n",
      "Surface training t=25485, loss=0.02605881541967392\n",
      "Surface training t=25486, loss=0.023144054226577282\n",
      "Surface training t=25487, loss=0.02023387234658003\n",
      "Surface training t=25488, loss=0.020639758557081223\n",
      "Surface training t=25489, loss=0.01858506351709366\n",
      "Surface training t=25490, loss=0.02723813895136118\n",
      "Surface training t=25491, loss=0.02097827009856701\n",
      "Surface training t=25492, loss=0.019504106603562832\n",
      "Surface training t=25493, loss=0.021233647130429745\n",
      "Surface training t=25494, loss=0.021209697239100933\n",
      "Surface training t=25495, loss=0.023064153268933296\n",
      "Surface training t=25496, loss=0.017380270175635815\n",
      "Surface training t=25497, loss=0.021832664497196674\n",
      "Surface training t=25498, loss=0.01851908303797245\n",
      "Surface training t=25499, loss=0.02232729271054268\n",
      "Surface training t=25500, loss=0.028324385173618793\n",
      "Surface training t=25501, loss=0.040806327015161514\n",
      "Surface training t=25502, loss=0.030697082169353962\n",
      "Surface training t=25503, loss=0.0287933386862278\n",
      "Surface training t=25504, loss=0.02221822924911976\n",
      "Surface training t=25505, loss=0.017983097583055496\n",
      "Surface training t=25506, loss=0.022870387881994247\n",
      "Surface training t=25507, loss=0.022605673409998417\n",
      "Surface training t=25508, loss=0.024411250837147236\n",
      "Surface training t=25509, loss=0.02581708785146475\n",
      "Surface training t=25510, loss=0.025957980193197727\n",
      "Surface training t=25511, loss=0.023692253977060318\n",
      "Surface training t=25512, loss=0.0203227736055851\n",
      "Surface training t=25513, loss=0.01946657244116068\n",
      "Surface training t=25514, loss=0.01847468689084053\n",
      "Surface training t=25515, loss=0.016117084305733442\n",
      "Surface training t=25516, loss=0.018165182322263718\n",
      "Surface training t=25517, loss=0.021646492183208466\n",
      "Surface training t=25518, loss=0.01864696852862835\n",
      "Surface training t=25519, loss=0.019357748329639435\n",
      "Surface training t=25520, loss=0.018488282337784767\n",
      "Surface training t=25521, loss=0.01830611750483513\n",
      "Surface training t=25522, loss=0.02078997064381838\n",
      "Surface training t=25523, loss=0.01984356064349413\n",
      "Surface training t=25524, loss=0.02024283166974783\n",
      "Surface training t=25525, loss=0.02029405813664198\n",
      "Surface training t=25526, loss=0.020867187529802322\n",
      "Surface training t=25527, loss=0.02561183273792267\n",
      "Surface training t=25528, loss=0.03047313168644905\n",
      "Surface training t=25529, loss=0.023161543533205986\n",
      "Surface training t=25530, loss=0.024968989193439484\n",
      "Surface training t=25531, loss=0.02417956478893757\n",
      "Surface training t=25532, loss=0.01975651178508997\n",
      "Surface training t=25533, loss=0.02217080444097519\n",
      "Surface training t=25534, loss=0.029018192552030087\n",
      "Surface training t=25535, loss=0.022046837024390697\n",
      "Surface training t=25536, loss=0.02128061093389988\n",
      "Surface training t=25537, loss=0.02415867242962122\n",
      "Surface training t=25538, loss=0.040521303191781044\n",
      "Surface training t=25539, loss=0.022793828509747982\n",
      "Surface training t=25540, loss=0.036739081144332886\n",
      "Surface training t=25541, loss=0.024858810007572174\n",
      "Surface training t=25542, loss=0.02414438035339117\n",
      "Surface training t=25543, loss=0.029531102627515793\n",
      "Surface training t=25544, loss=0.029156535863876343\n",
      "Surface training t=25545, loss=0.02812040224671364\n",
      "Surface training t=25546, loss=0.029483048245310783\n",
      "Surface training t=25547, loss=0.026278321631252766\n",
      "Surface training t=25548, loss=0.02387564815580845\n",
      "Surface training t=25549, loss=0.018992948345839977\n",
      "Surface training t=25550, loss=0.020966523326933384\n",
      "Surface training t=25551, loss=0.020843081176280975\n",
      "Surface training t=25552, loss=0.020757521502673626\n",
      "Surface training t=25553, loss=0.021523275412619114\n",
      "Surface training t=25554, loss=0.020537178963422775\n",
      "Surface training t=25555, loss=0.02028775028884411\n",
      "Surface training t=25556, loss=0.015984296798706055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=25557, loss=0.014315573498606682\n",
      "Surface training t=25558, loss=0.01612668763846159\n",
      "Surface training t=25559, loss=0.021890953183174133\n",
      "Surface training t=25560, loss=0.01848017703741789\n",
      "Surface training t=25561, loss=0.021947551518678665\n",
      "Surface training t=25562, loss=0.022206070832908154\n",
      "Surface training t=25563, loss=0.0259588910266757\n",
      "Surface training t=25564, loss=0.02835606224834919\n",
      "Surface training t=25565, loss=0.023410013876855373\n",
      "Surface training t=25566, loss=0.023275159299373627\n",
      "Surface training t=25567, loss=0.02227958384901285\n",
      "Surface training t=25568, loss=0.020934651605784893\n",
      "Surface training t=25569, loss=0.01764390943571925\n",
      "Surface training t=25570, loss=0.023236106149852276\n",
      "Surface training t=25571, loss=0.01970903854817152\n",
      "Surface training t=25572, loss=0.020893815904855728\n",
      "Surface training t=25573, loss=0.017892081756144762\n",
      "Surface training t=25574, loss=0.017896133475005627\n",
      "Surface training t=25575, loss=0.01321687689051032\n",
      "Surface training t=25576, loss=0.017418858595192432\n",
      "Surface training t=25577, loss=0.01905258186161518\n",
      "Surface training t=25578, loss=0.01753958873450756\n",
      "Surface training t=25579, loss=0.01943167019635439\n",
      "Surface training t=25580, loss=0.01749364659190178\n",
      "Surface training t=25581, loss=0.016131241340190172\n",
      "Surface training t=25582, loss=0.015003554057329893\n",
      "Surface training t=25583, loss=0.01496450137346983\n",
      "Surface training t=25584, loss=0.02345181256532669\n",
      "Surface training t=25585, loss=0.02377036027610302\n",
      "Surface training t=25586, loss=0.022251233458518982\n",
      "Surface training t=25587, loss=0.021383737213909626\n",
      "Surface training t=25588, loss=0.027492529712617397\n",
      "Surface training t=25589, loss=0.02917314227670431\n",
      "Surface training t=25590, loss=0.03152989502996206\n",
      "Surface training t=25591, loss=0.028407875448465347\n",
      "Surface training t=25592, loss=0.026817694306373596\n",
      "Surface training t=25593, loss=0.03097974695265293\n",
      "Surface training t=25594, loss=0.022484284825623035\n",
      "Surface training t=25595, loss=0.028138594701886177\n",
      "Surface training t=25596, loss=0.02560642920434475\n",
      "Surface training t=25597, loss=0.025903555564582348\n",
      "Surface training t=25598, loss=0.020479360595345497\n",
      "Surface training t=25599, loss=0.018132697325199842\n",
      "Surface training t=25600, loss=0.02283062320202589\n",
      "Surface training t=25601, loss=0.02053021639585495\n",
      "Surface training t=25602, loss=0.025156592950224876\n",
      "Surface training t=25603, loss=0.022927354089915752\n",
      "Surface training t=25604, loss=0.028237721882760525\n",
      "Surface training t=25605, loss=0.02135831955820322\n",
      "Surface training t=25606, loss=0.0217271251603961\n",
      "Surface training t=25607, loss=0.017092383466660976\n",
      "Surface training t=25608, loss=0.022182360291481018\n",
      "Surface training t=25609, loss=0.023878020234405994\n",
      "Surface training t=25610, loss=0.021230305545032024\n",
      "Surface training t=25611, loss=0.021035240963101387\n",
      "Surface training t=25612, loss=0.02273527905344963\n",
      "Surface training t=25613, loss=0.01779570197686553\n",
      "Surface training t=25614, loss=0.018905431032180786\n",
      "Surface training t=25615, loss=0.01967084128409624\n",
      "Surface training t=25616, loss=0.016034326516091824\n",
      "Surface training t=25617, loss=0.020962018985301256\n",
      "Surface training t=25618, loss=0.01691754348576069\n",
      "Surface training t=25619, loss=0.019109838642179966\n",
      "Surface training t=25620, loss=0.0191449336707592\n",
      "Surface training t=25621, loss=0.020229751244187355\n",
      "Surface training t=25622, loss=0.018750272691249847\n",
      "Surface training t=25623, loss=0.02082290407270193\n",
      "Surface training t=25624, loss=0.015707463026046753\n",
      "Surface training t=25625, loss=0.021151194348931313\n",
      "Surface training t=25626, loss=0.022644270211458206\n",
      "Surface training t=25627, loss=0.014464202336966991\n",
      "Surface training t=25628, loss=0.020852629095315933\n",
      "Surface training t=25629, loss=0.01303118234500289\n",
      "Surface training t=25630, loss=0.017936937510967255\n",
      "Surface training t=25631, loss=0.016174550633877516\n",
      "Surface training t=25632, loss=0.015807458199560642\n",
      "Surface training t=25633, loss=0.01715722307562828\n",
      "Surface training t=25634, loss=0.0189004042185843\n",
      "Surface training t=25635, loss=0.026054115034639835\n",
      "Surface training t=25636, loss=0.02441966999322176\n",
      "Surface training t=25637, loss=0.023346315138041973\n",
      "Surface training t=25638, loss=0.03672834113240242\n",
      "Surface training t=25639, loss=0.028209397569298744\n",
      "Surface training t=25640, loss=0.032068828120827675\n",
      "Surface training t=25641, loss=0.03504300303757191\n",
      "Surface training t=25642, loss=0.023805980570614338\n",
      "Surface training t=25643, loss=0.042916858568787575\n",
      "Surface training t=25644, loss=0.026255209930241108\n",
      "Surface training t=25645, loss=0.0411812923848629\n",
      "Surface training t=25646, loss=0.027211026288568974\n",
      "Surface training t=25647, loss=0.03263949602842331\n",
      "Surface training t=25648, loss=0.030869358219206333\n",
      "Surface training t=25649, loss=0.026495045982301235\n",
      "Surface training t=25650, loss=0.027359060011804104\n",
      "Surface training t=25651, loss=0.03574367240071297\n",
      "Surface training t=25652, loss=0.03051568567752838\n",
      "Surface training t=25653, loss=0.03139944840222597\n",
      "Surface training t=25654, loss=0.028580354060977697\n",
      "Surface training t=25655, loss=0.034867098554968834\n",
      "Surface training t=25656, loss=0.031084110960364342\n",
      "Surface training t=25657, loss=0.027347720228135586\n",
      "Surface training t=25658, loss=0.018874807748943567\n",
      "Surface training t=25659, loss=0.019718648865818977\n",
      "Surface training t=25660, loss=0.016180839389562607\n",
      "Surface training t=25661, loss=0.020874689798802137\n",
      "Surface training t=25662, loss=0.01967284455895424\n",
      "Surface training t=25663, loss=0.016101860906928778\n",
      "Surface training t=25664, loss=0.02019859477877617\n",
      "Surface training t=25665, loss=0.015011515468358994\n",
      "Surface training t=25666, loss=0.016263131983578205\n",
      "Surface training t=25667, loss=0.014143619686365128\n",
      "Surface training t=25668, loss=0.024313931353390217\n",
      "Surface training t=25669, loss=0.027570840902626514\n",
      "Surface training t=25670, loss=0.036827925592660904\n",
      "Surface training t=25671, loss=0.021663335151970387\n",
      "Surface training t=25672, loss=0.029200338758528233\n",
      "Surface training t=25673, loss=0.031894344836473465\n",
      "Surface training t=25674, loss=0.03164403513073921\n",
      "Surface training t=25675, loss=0.028630714863538742\n",
      "Surface training t=25676, loss=0.0311949010938406\n",
      "Surface training t=25677, loss=0.021446376107633114\n",
      "Surface training t=25678, loss=0.028399810194969177\n",
      "Surface training t=25679, loss=0.02657295111566782\n",
      "Surface training t=25680, loss=0.023721981793642044\n",
      "Surface training t=25681, loss=0.029863247647881508\n",
      "Surface training t=25682, loss=0.02377215586602688\n",
      "Surface training t=25683, loss=0.029873887076973915\n",
      "Surface training t=25684, loss=0.028745674528181553\n",
      "Surface training t=25685, loss=0.03157408349215984\n",
      "Surface training t=25686, loss=0.03941971994936466\n",
      "Surface training t=25687, loss=0.03277873247861862\n",
      "Surface training t=25688, loss=0.03277645632624626\n",
      "Surface training t=25689, loss=0.025075328536331654\n",
      "Surface training t=25690, loss=0.02846369706094265\n",
      "Surface training t=25691, loss=0.02909095399081707\n",
      "Surface training t=25692, loss=0.028021630831062794\n",
      "Surface training t=25693, loss=0.032740107737481594\n",
      "Surface training t=25694, loss=0.024218272417783737\n",
      "Surface training t=25695, loss=0.02904611825942993\n",
      "Surface training t=25696, loss=0.03319159150123596\n",
      "Surface training t=25697, loss=0.024649355560541153\n",
      "Surface training t=25698, loss=0.027095087803900242\n",
      "Surface training t=25699, loss=0.02372085675597191\n",
      "Surface training t=25700, loss=0.026077366434037685\n",
      "Surface training t=25701, loss=0.02854209393262863\n",
      "Surface training t=25702, loss=0.03061431460082531\n",
      "Surface training t=25703, loss=0.025805439800024033\n",
      "Surface training t=25704, loss=0.019017992541193962\n",
      "Surface training t=25705, loss=0.015651815105229616\n",
      "Surface training t=25706, loss=0.018447767477482557\n",
      "Surface training t=25707, loss=0.020837496034801006\n",
      "Surface training t=25708, loss=0.017928894143551588\n",
      "Surface training t=25709, loss=0.018953148275613785\n",
      "Surface training t=25710, loss=0.031020470894873142\n",
      "Surface training t=25711, loss=0.027047475799918175\n",
      "Surface training t=25712, loss=0.0214694794267416\n",
      "Surface training t=25713, loss=0.028320617973804474\n",
      "Surface training t=25714, loss=0.02183082140982151\n",
      "Surface training t=25715, loss=0.022580676712095737\n",
      "Surface training t=25716, loss=0.04022762551903725\n",
      "Surface training t=25717, loss=0.02509088534861803\n",
      "Surface training t=25718, loss=0.024699688889086246\n",
      "Surface training t=25719, loss=0.028330682776868343\n",
      "Surface training t=25720, loss=0.025968116708099842\n",
      "Surface training t=25721, loss=0.029273267835378647\n",
      "Surface training t=25722, loss=0.033627305179834366\n",
      "Surface training t=25723, loss=0.023526650853455067\n",
      "Surface training t=25724, loss=0.026956576853990555\n",
      "Surface training t=25725, loss=0.04067613743245602\n",
      "Surface training t=25726, loss=0.033829646185040474\n",
      "Surface training t=25727, loss=0.03965052217245102\n",
      "Surface training t=25728, loss=0.02789386175572872\n",
      "Surface training t=25729, loss=0.02985723689198494\n",
      "Surface training t=25730, loss=0.028523455373942852\n",
      "Surface training t=25731, loss=0.041855594143271446\n",
      "Surface training t=25732, loss=0.029739927500486374\n",
      "Surface training t=25733, loss=0.034604890272021294\n",
      "Surface training t=25734, loss=0.03908023051917553\n",
      "Surface training t=25735, loss=0.047196704894304276\n",
      "Surface training t=25736, loss=0.03598032891750336\n",
      "Surface training t=25737, loss=0.02657417394220829\n",
      "Surface training t=25738, loss=0.027052052319049835\n",
      "Surface training t=25739, loss=0.028059616684913635\n",
      "Surface training t=25740, loss=0.0324283791705966\n",
      "Surface training t=25741, loss=0.049971867352724075\n",
      "Surface training t=25742, loss=0.02820081263780594\n",
      "Surface training t=25743, loss=0.02999392058700323\n",
      "Surface training t=25744, loss=0.03343770559877157\n",
      "Surface training t=25745, loss=0.03008808195590973\n",
      "Surface training t=25746, loss=0.027008467353880405\n",
      "Surface training t=25747, loss=0.02619200199842453\n",
      "Surface training t=25748, loss=0.024612804874777794\n",
      "Surface training t=25749, loss=0.023992321453988552\n",
      "Surface training t=25750, loss=0.027506329119205475\n",
      "Surface training t=25751, loss=0.0285413796082139\n",
      "Surface training t=25752, loss=0.034019202925264835\n",
      "Surface training t=25753, loss=0.04575919173657894\n",
      "Surface training t=25754, loss=0.03508821967989206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=25755, loss=0.042055217549204826\n",
      "Surface training t=25756, loss=0.04151364043354988\n",
      "Surface training t=25757, loss=0.03484439663589001\n",
      "Surface training t=25758, loss=0.0353170670568943\n",
      "Surface training t=25759, loss=0.033110687509179115\n",
      "Surface training t=25760, loss=0.03591453842818737\n",
      "Surface training t=25761, loss=0.025735403411090374\n",
      "Surface training t=25762, loss=0.040924934670329094\n",
      "Surface training t=25763, loss=0.025791621766984463\n",
      "Surface training t=25764, loss=0.031849407590925694\n",
      "Surface training t=25765, loss=0.023546290583908558\n",
      "Surface training t=25766, loss=0.018439004663378\n",
      "Surface training t=25767, loss=0.020161107182502747\n",
      "Surface training t=25768, loss=0.023678037337958813\n",
      "Surface training t=25769, loss=0.021492482163012028\n",
      "Surface training t=25770, loss=0.02138871606439352\n",
      "Surface training t=25771, loss=0.017157288268208504\n",
      "Surface training t=25772, loss=0.020328804850578308\n",
      "Surface training t=25773, loss=0.018006769940257072\n",
      "Surface training t=25774, loss=0.01502757053822279\n",
      "Surface training t=25775, loss=0.01688358746469021\n",
      "Surface training t=25776, loss=0.015414289198815823\n",
      "Surface training t=25777, loss=0.017363756895065308\n",
      "Surface training t=25778, loss=0.020932476967573166\n",
      "Surface training t=25779, loss=0.019721793942153454\n",
      "Surface training t=25780, loss=0.02064493205398321\n",
      "Surface training t=25781, loss=0.01707193534821272\n",
      "Surface training t=25782, loss=0.016764785163104534\n",
      "Surface training t=25783, loss=0.026269464753568172\n",
      "Surface training t=25784, loss=0.02419128641486168\n",
      "Surface training t=25785, loss=0.02101953886449337\n",
      "Surface training t=25786, loss=0.0218176431953907\n",
      "Surface training t=25787, loss=0.02575451973825693\n",
      "Surface training t=25788, loss=0.019716902635991573\n",
      "Surface training t=25789, loss=0.024115961510688066\n",
      "Surface training t=25790, loss=0.027180246077477932\n",
      "Surface training t=25791, loss=0.025179771706461906\n",
      "Surface training t=25792, loss=0.02491340134292841\n",
      "Surface training t=25793, loss=0.039168573915958405\n",
      "Surface training t=25794, loss=0.027209742926061153\n",
      "Surface training t=25795, loss=0.0351690910756588\n",
      "Surface training t=25796, loss=0.030768584460020065\n",
      "Surface training t=25797, loss=0.041939038783311844\n",
      "Surface training t=25798, loss=0.03255430981516838\n",
      "Surface training t=25799, loss=0.026534587144851685\n",
      "Surface training t=25800, loss=0.024229343980550766\n",
      "Surface training t=25801, loss=0.020519412122666836\n",
      "Surface training t=25802, loss=0.02352994680404663\n",
      "Surface training t=25803, loss=0.02590438351035118\n",
      "Surface training t=25804, loss=0.02471732720732689\n",
      "Surface training t=25805, loss=0.022421457804739475\n",
      "Surface training t=25806, loss=0.020411581732332706\n",
      "Surface training t=25807, loss=0.019580595195293427\n",
      "Surface training t=25808, loss=0.02197412122040987\n",
      "Surface training t=25809, loss=0.02306864783167839\n",
      "Surface training t=25810, loss=0.02291968185454607\n",
      "Surface training t=25811, loss=0.028047138825058937\n",
      "Surface training t=25812, loss=0.030098695307970047\n",
      "Surface training t=25813, loss=0.025768358260393143\n",
      "Surface training t=25814, loss=0.023785254918038845\n",
      "Surface training t=25815, loss=0.026078219525516033\n",
      "Surface training t=25816, loss=0.027577068656682968\n",
      "Surface training t=25817, loss=0.024451488628983498\n",
      "Surface training t=25818, loss=0.029979621060192585\n",
      "Surface training t=25819, loss=0.026531429961323738\n",
      "Surface training t=25820, loss=0.025793377310037613\n",
      "Surface training t=25821, loss=0.0223050806671381\n",
      "Surface training t=25822, loss=0.028866608627140522\n",
      "Surface training t=25823, loss=0.0271600978448987\n",
      "Surface training t=25824, loss=0.027918011881411076\n",
      "Surface training t=25825, loss=0.023359508253633976\n",
      "Surface training t=25826, loss=0.01964739989489317\n",
      "Surface training t=25827, loss=0.016594878863543272\n",
      "Surface training t=25828, loss=0.015420346986502409\n",
      "Surface training t=25829, loss=0.017191519029438496\n",
      "Surface training t=25830, loss=0.016542044933885336\n",
      "Surface training t=25831, loss=0.01915183151140809\n",
      "Surface training t=25832, loss=0.016793442890048027\n",
      "Surface training t=25833, loss=0.020627806894481182\n",
      "Surface training t=25834, loss=0.01214326499029994\n",
      "Surface training t=25835, loss=0.016248243860900402\n",
      "Surface training t=25836, loss=0.015900337137281895\n",
      "Surface training t=25837, loss=0.01682691741734743\n",
      "Surface training t=25838, loss=0.0153785296715796\n",
      "Surface training t=25839, loss=0.013713293708860874\n",
      "Surface training t=25840, loss=0.017353211995214224\n",
      "Surface training t=25841, loss=0.02137681283056736\n",
      "Surface training t=25842, loss=0.025617815554142\n",
      "Surface training t=25843, loss=0.026692445389926434\n",
      "Surface training t=25844, loss=0.030260998755693436\n",
      "Surface training t=25845, loss=0.03345578908920288\n",
      "Surface training t=25846, loss=0.025400206446647644\n",
      "Surface training t=25847, loss=0.03410713002085686\n",
      "Surface training t=25848, loss=0.04467006400227547\n",
      "Surface training t=25849, loss=0.03199844900518656\n",
      "Surface training t=25850, loss=0.030027889646589756\n",
      "Surface training t=25851, loss=0.021163627970963717\n",
      "Surface training t=25852, loss=0.02290577907115221\n",
      "Surface training t=25853, loss=0.022346924059093\n",
      "Surface training t=25854, loss=0.022930320352315903\n",
      "Surface training t=25855, loss=0.027220742776989937\n",
      "Surface training t=25856, loss=0.022161815315485\n",
      "Surface training t=25857, loss=0.023030360229313374\n",
      "Surface training t=25858, loss=0.027997354045510292\n",
      "Surface training t=25859, loss=0.03440132178366184\n",
      "Surface training t=25860, loss=0.024673993699252605\n",
      "Surface training t=25861, loss=0.035555871203541756\n",
      "Surface training t=25862, loss=0.024024835787713528\n",
      "Surface training t=25863, loss=0.02922958228737116\n",
      "Surface training t=25864, loss=0.019671939313411713\n",
      "Surface training t=25865, loss=0.021093294024467468\n",
      "Surface training t=25866, loss=0.033230604603886604\n",
      "Surface training t=25867, loss=0.03038155473768711\n",
      "Surface training t=25868, loss=0.024577373638749123\n",
      "Surface training t=25869, loss=0.02634417451918125\n",
      "Surface training t=25870, loss=0.026971131563186646\n",
      "Surface training t=25871, loss=0.024054111912846565\n",
      "Surface training t=25872, loss=0.03470396250486374\n",
      "Surface training t=25873, loss=0.024436277337372303\n",
      "Surface training t=25874, loss=0.028557831421494484\n",
      "Surface training t=25875, loss=0.021174801513552666\n",
      "Surface training t=25876, loss=0.021003500558435917\n",
      "Surface training t=25877, loss=0.023579152300953865\n",
      "Surface training t=25878, loss=0.017483381554484367\n",
      "Surface training t=25879, loss=0.01855431403964758\n",
      "Surface training t=25880, loss=0.015838665422052145\n",
      "Surface training t=25881, loss=0.01472994964569807\n",
      "Surface training t=25882, loss=0.015838949009776115\n",
      "Surface training t=25883, loss=0.018843717873096466\n",
      "Surface training t=25884, loss=0.01794319087639451\n",
      "Surface training t=25885, loss=0.023054111748933792\n",
      "Surface training t=25886, loss=0.025272672064602375\n",
      "Surface training t=25887, loss=0.014591682236641645\n",
      "Surface training t=25888, loss=0.01580412918701768\n",
      "Surface training t=25889, loss=0.020290233194828033\n",
      "Surface training t=25890, loss=0.021193443797528744\n",
      "Surface training t=25891, loss=0.022514047101140022\n",
      "Surface training t=25892, loss=0.02886094432324171\n",
      "Surface training t=25893, loss=0.02190515212714672\n",
      "Surface training t=25894, loss=0.029947366565465927\n",
      "Surface training t=25895, loss=0.029167314991354942\n",
      "Surface training t=25896, loss=0.03379753604531288\n",
      "Surface training t=25897, loss=0.03229862358421087\n",
      "Surface training t=25898, loss=0.02779268939048052\n",
      "Surface training t=25899, loss=0.02172977663576603\n",
      "Surface training t=25900, loss=0.029271992854773998\n",
      "Surface training t=25901, loss=0.02707899734377861\n",
      "Surface training t=25902, loss=0.04383968561887741\n",
      "Surface training t=25903, loss=0.028450962156057358\n",
      "Surface training t=25904, loss=0.02657162956893444\n",
      "Surface training t=25905, loss=0.03224153257906437\n",
      "Surface training t=25906, loss=0.034783635288476944\n",
      "Surface training t=25907, loss=0.027616648003458977\n",
      "Surface training t=25908, loss=0.03565682843327522\n",
      "Surface training t=25909, loss=0.04193788766860962\n",
      "Surface training t=25910, loss=0.03955176658928394\n",
      "Surface training t=25911, loss=0.036827901378273964\n",
      "Surface training t=25912, loss=0.03650677762925625\n",
      "Surface training t=25913, loss=0.03152444027364254\n",
      "Surface training t=25914, loss=0.028620329685509205\n",
      "Surface training t=25915, loss=0.027594372630119324\n",
      "Surface training t=25916, loss=0.02635864820331335\n",
      "Surface training t=25917, loss=0.04526039771735668\n",
      "Surface training t=25918, loss=0.025847584940493107\n",
      "Surface training t=25919, loss=0.024319520220160484\n",
      "Surface training t=25920, loss=0.02910755481570959\n",
      "Surface training t=25921, loss=0.02818125579506159\n",
      "Surface training t=25922, loss=0.023617202416062355\n",
      "Surface training t=25923, loss=0.024659311398863792\n",
      "Surface training t=25924, loss=0.03897004574537277\n",
      "Surface training t=25925, loss=0.030221796594560146\n",
      "Surface training t=25926, loss=0.0251213563606143\n",
      "Surface training t=25927, loss=0.024479208514094353\n",
      "Surface training t=25928, loss=0.018052772618830204\n",
      "Surface training t=25929, loss=0.016779527068138123\n",
      "Surface training t=25930, loss=0.023820807226002216\n",
      "Surface training t=25931, loss=0.03652952425181866\n",
      "Surface training t=25932, loss=0.024695693515241146\n",
      "Surface training t=25933, loss=0.021255414001643658\n",
      "Surface training t=25934, loss=0.020749898627400398\n",
      "Surface training t=25935, loss=0.022791799157857895\n",
      "Surface training t=25936, loss=0.024015425704419613\n",
      "Surface training t=25937, loss=0.021082903258502483\n",
      "Surface training t=25938, loss=0.021439913660287857\n",
      "Surface training t=25939, loss=0.021515244618058205\n",
      "Surface training t=25940, loss=0.023001331835985184\n",
      "Surface training t=25941, loss=0.03349905647337437\n",
      "Surface training t=25942, loss=0.03669280931353569\n",
      "Surface training t=25943, loss=0.034573767334222794\n",
      "Surface training t=25944, loss=0.030221091583371162\n",
      "Surface training t=25945, loss=0.02489964012056589\n",
      "Surface training t=25946, loss=0.02602876629680395\n",
      "Surface training t=25947, loss=0.023059185594320297\n",
      "Surface training t=25948, loss=0.02070318441838026\n",
      "Surface training t=25949, loss=0.024357701651751995\n",
      "Surface training t=25950, loss=0.025536376982927322\n",
      "Surface training t=25951, loss=0.022336112335324287\n",
      "Surface training t=25952, loss=0.02452170941978693\n",
      "Surface training t=25953, loss=0.02286478877067566\n",
      "Surface training t=25954, loss=0.02169984020292759\n",
      "Surface training t=25955, loss=0.020604705438017845\n",
      "Surface training t=25956, loss=0.022503022104501724\n",
      "Surface training t=25957, loss=0.02395383734256029\n",
      "Surface training t=25958, loss=0.016126585192978382\n",
      "Surface training t=25959, loss=0.017522153444588184\n",
      "Surface training t=25960, loss=0.022990419529378414\n",
      "Surface training t=25961, loss=0.025805474258959293\n",
      "Surface training t=25962, loss=0.022512848488986492\n",
      "Surface training t=25963, loss=0.024467501789331436\n",
      "Surface training t=25964, loss=0.04132164269685745\n",
      "Surface training t=25965, loss=0.02698496263474226\n",
      "Surface training t=25966, loss=0.03389879781752825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=25967, loss=0.041485495865345\n",
      "Surface training t=25968, loss=0.03018825128674507\n",
      "Surface training t=25969, loss=0.02581965085119009\n",
      "Surface training t=25970, loss=0.022541998885571957\n",
      "Surface training t=25971, loss=0.0246038930490613\n",
      "Surface training t=25972, loss=0.02123875916004181\n",
      "Surface training t=25973, loss=0.02256530523300171\n",
      "Surface training t=25974, loss=0.0292646624147892\n",
      "Surface training t=25975, loss=0.031104610301554203\n",
      "Surface training t=25976, loss=0.030800025910139084\n",
      "Surface training t=25977, loss=0.026296991854906082\n",
      "Surface training t=25978, loss=0.029306160286068916\n",
      "Surface training t=25979, loss=0.03549128118902445\n",
      "Surface training t=25980, loss=0.032729946076869965\n",
      "Surface training t=25981, loss=0.034646145068109035\n",
      "Surface training t=25982, loss=0.04605505429208279\n",
      "Surface training t=25983, loss=0.03413880057632923\n",
      "Surface training t=25984, loss=0.030999724753201008\n",
      "Surface training t=25985, loss=0.02919068932533264\n",
      "Surface training t=25986, loss=0.0371194239705801\n",
      "Surface training t=25987, loss=0.027719834819436073\n",
      "Surface training t=25988, loss=0.023578675463795662\n",
      "Surface training t=25989, loss=0.027002576738595963\n",
      "Surface training t=25990, loss=0.025105122476816177\n",
      "Surface training t=25991, loss=0.03999052196741104\n",
      "Surface training t=25992, loss=0.031408848240971565\n",
      "Surface training t=25993, loss=0.04978235438466072\n",
      "Surface training t=25994, loss=0.030274552293121815\n",
      "Surface training t=25995, loss=0.04751739650964737\n",
      "Surface training t=25996, loss=0.026200142689049244\n",
      "Surface training t=25997, loss=0.037063343450427055\n",
      "Surface training t=25998, loss=0.03499120473861694\n",
      "Surface training t=25999, loss=0.02580953761935234\n",
      "Surface training t=26000, loss=0.030888468027114868\n",
      "Surface training t=26001, loss=0.02515417616814375\n",
      "Surface training t=26002, loss=0.032098494470119476\n",
      "Surface training t=26003, loss=0.03067849576473236\n",
      "Surface training t=26004, loss=0.027410926297307014\n",
      "Surface training t=26005, loss=0.036268580704927444\n",
      "Surface training t=26006, loss=0.03299019206315279\n",
      "Surface training t=26007, loss=0.028137550689280033\n",
      "Surface training t=26008, loss=0.02856370247900486\n",
      "Surface training t=26009, loss=0.02265931386500597\n",
      "Surface training t=26010, loss=0.020108919590711594\n",
      "Surface training t=26011, loss=0.030498345382511616\n",
      "Surface training t=26012, loss=0.01944753434509039\n",
      "Surface training t=26013, loss=0.025982131250202656\n",
      "Surface training t=26014, loss=0.02470137272030115\n",
      "Surface training t=26015, loss=0.02493621315807104\n",
      "Surface training t=26016, loss=0.019616618752479553\n",
      "Surface training t=26017, loss=0.020182104781270027\n",
      "Surface training t=26018, loss=0.024076425470411777\n",
      "Surface training t=26019, loss=0.030216756276786327\n",
      "Surface training t=26020, loss=0.03284091874957085\n",
      "Surface training t=26021, loss=0.026933561079204082\n",
      "Surface training t=26022, loss=0.02617962844669819\n",
      "Surface training t=26023, loss=0.02852494642138481\n",
      "Surface training t=26024, loss=0.018443490844219923\n",
      "Surface training t=26025, loss=0.018934258725494146\n",
      "Surface training t=26026, loss=0.020495809614658356\n",
      "Surface training t=26027, loss=0.019815044477581978\n",
      "Surface training t=26028, loss=0.01569673791527748\n",
      "Surface training t=26029, loss=0.022157324478030205\n",
      "Surface training t=26030, loss=0.0276905819773674\n",
      "Surface training t=26031, loss=0.029928884468972683\n",
      "Surface training t=26032, loss=0.028311758302152157\n",
      "Surface training t=26033, loss=0.028116789646446705\n",
      "Surface training t=26034, loss=0.025072124786674976\n",
      "Surface training t=26035, loss=0.029267728328704834\n",
      "Surface training t=26036, loss=0.031952458433806896\n",
      "Surface training t=26037, loss=0.026443556882441044\n",
      "Surface training t=26038, loss=0.02763673011213541\n",
      "Surface training t=26039, loss=0.028134359046816826\n",
      "Surface training t=26040, loss=0.018787794280797243\n",
      "Surface training t=26041, loss=0.025297287851572037\n",
      "Surface training t=26042, loss=0.02166332071647048\n",
      "Surface training t=26043, loss=0.027305825613439083\n",
      "Surface training t=26044, loss=0.019775171764194965\n",
      "Surface training t=26045, loss=0.019587963819503784\n",
      "Surface training t=26046, loss=0.02240313868969679\n",
      "Surface training t=26047, loss=0.02166043594479561\n",
      "Surface training t=26048, loss=0.017183764837682247\n",
      "Surface training t=26049, loss=0.01886746846139431\n",
      "Surface training t=26050, loss=0.020034813322126865\n",
      "Surface training t=26051, loss=0.017487010918557644\n",
      "Surface training t=26052, loss=0.019677428528666496\n",
      "Surface training t=26053, loss=0.021147850900888443\n",
      "Surface training t=26054, loss=0.01804992649704218\n",
      "Surface training t=26055, loss=0.017742732539772987\n",
      "Surface training t=26056, loss=0.025312134996056557\n",
      "Surface training t=26057, loss=0.01861137617379427\n",
      "Surface training t=26058, loss=0.020095370709896088\n",
      "Surface training t=26059, loss=0.026572409085929394\n",
      "Surface training t=26060, loss=0.02124401181936264\n",
      "Surface training t=26061, loss=0.024298175238072872\n",
      "Surface training t=26062, loss=0.026038086973130703\n",
      "Surface training t=26063, loss=0.029337928630411625\n",
      "Surface training t=26064, loss=0.031894419342279434\n",
      "Surface training t=26065, loss=0.025611717253923416\n",
      "Surface training t=26066, loss=0.02552824467420578\n",
      "Surface training t=26067, loss=0.027723648585379124\n",
      "Surface training t=26068, loss=0.024118042550981045\n",
      "Surface training t=26069, loss=0.023862622678279877\n",
      "Surface training t=26070, loss=0.023090096190571785\n",
      "Surface training t=26071, loss=0.030960483476519585\n",
      "Surface training t=26072, loss=0.02397553063929081\n",
      "Surface training t=26073, loss=0.02284051477909088\n",
      "Surface training t=26074, loss=0.020264589227735996\n",
      "Surface training t=26075, loss=0.018503887578845024\n",
      "Surface training t=26076, loss=0.023307526484131813\n",
      "Surface training t=26077, loss=0.01858167164027691\n",
      "Surface training t=26078, loss=0.020727982744574547\n",
      "Surface training t=26079, loss=0.016755249351263046\n",
      "Surface training t=26080, loss=0.018338733352720737\n",
      "Surface training t=26081, loss=0.018483576364815235\n",
      "Surface training t=26082, loss=0.015865349676460028\n",
      "Surface training t=26083, loss=0.01967438869178295\n",
      "Surface training t=26084, loss=0.017232194542884827\n",
      "Surface training t=26085, loss=0.01855662278831005\n",
      "Surface training t=26086, loss=0.019273238256573677\n",
      "Surface training t=26087, loss=0.023870735429227352\n",
      "Surface training t=26088, loss=0.03225962910801172\n",
      "Surface training t=26089, loss=0.021835139952600002\n",
      "Surface training t=26090, loss=0.016336981672793627\n",
      "Surface training t=26091, loss=0.017547840252518654\n",
      "Surface training t=26092, loss=0.023637225851416588\n",
      "Surface training t=26093, loss=0.01641277363523841\n",
      "Surface training t=26094, loss=0.021275357343256474\n",
      "Surface training t=26095, loss=0.021257642656564713\n",
      "Surface training t=26096, loss=0.023129040375351906\n",
      "Surface training t=26097, loss=0.019296811893582344\n",
      "Surface training t=26098, loss=0.016935770865529776\n",
      "Surface training t=26099, loss=0.02247443236410618\n",
      "Surface training t=26100, loss=0.032917171716690063\n",
      "Surface training t=26101, loss=0.024218063801527023\n",
      "Surface training t=26102, loss=0.025179877877235413\n",
      "Surface training t=26103, loss=0.023558584973216057\n",
      "Surface training t=26104, loss=0.016327836085110903\n",
      "Surface training t=26105, loss=0.020565807819366455\n",
      "Surface training t=26106, loss=0.017908163368701935\n",
      "Surface training t=26107, loss=0.019164569675922394\n",
      "Surface training t=26108, loss=0.013602171093225479\n",
      "Surface training t=26109, loss=0.018099477514624596\n",
      "Surface training t=26110, loss=0.01825984474271536\n",
      "Surface training t=26111, loss=0.015480856411159039\n",
      "Surface training t=26112, loss=0.015911592170596123\n",
      "Surface training t=26113, loss=0.01630955096334219\n",
      "Surface training t=26114, loss=0.018651618622243404\n",
      "Surface training t=26115, loss=0.020718427374958992\n",
      "Surface training t=26116, loss=0.020093602128326893\n",
      "Surface training t=26117, loss=0.017532016150653362\n",
      "Surface training t=26118, loss=0.016646770760416985\n",
      "Surface training t=26119, loss=0.018503975123167038\n",
      "Surface training t=26120, loss=0.02001294493675232\n",
      "Surface training t=26121, loss=0.025789296254515648\n",
      "Surface training t=26122, loss=0.027016742154955864\n",
      "Surface training t=26123, loss=0.023387485183775425\n",
      "Surface training t=26124, loss=0.01893198862671852\n",
      "Surface training t=26125, loss=0.015747863799333572\n",
      "Surface training t=26126, loss=0.024396736174821854\n",
      "Surface training t=26127, loss=0.021127793937921524\n",
      "Surface training t=26128, loss=0.015063536819070578\n",
      "Surface training t=26129, loss=0.016689272597432137\n",
      "Surface training t=26130, loss=0.014193673618137836\n",
      "Surface training t=26131, loss=0.016550608910620213\n",
      "Surface training t=26132, loss=0.012591094011440873\n",
      "Surface training t=26133, loss=0.0150428363122046\n",
      "Surface training t=26134, loss=0.01660901866853237\n",
      "Surface training t=26135, loss=0.015346936415880919\n",
      "Surface training t=26136, loss=0.01672738790512085\n",
      "Surface training t=26137, loss=0.016493823379278183\n",
      "Surface training t=26138, loss=0.015194869134575129\n",
      "Surface training t=26139, loss=0.016733381897211075\n",
      "Surface training t=26140, loss=0.025807649828493595\n",
      "Surface training t=26141, loss=0.030823852866888046\n",
      "Surface training t=26142, loss=0.03226177114993334\n",
      "Surface training t=26143, loss=0.027096252888441086\n",
      "Surface training t=26144, loss=0.029156556352972984\n",
      "Surface training t=26145, loss=0.031627961434423923\n",
      "Surface training t=26146, loss=0.024629171937704086\n",
      "Surface training t=26147, loss=0.023037638515233994\n",
      "Surface training t=26148, loss=0.03465734422206879\n",
      "Surface training t=26149, loss=0.0285214614123106\n",
      "Surface training t=26150, loss=0.0368034802377224\n",
      "Surface training t=26151, loss=0.025399071164429188\n",
      "Surface training t=26152, loss=0.032412284053862095\n",
      "Surface training t=26153, loss=0.028729742392897606\n",
      "Surface training t=26154, loss=0.03282188065350056\n",
      "Surface training t=26155, loss=0.04635328985750675\n",
      "Surface training t=26156, loss=0.04374667443335056\n",
      "Surface training t=26157, loss=0.030598534271121025\n",
      "Surface training t=26158, loss=0.03611373528838158\n",
      "Surface training t=26159, loss=0.03913610614836216\n",
      "Surface training t=26160, loss=0.03423363622277975\n",
      "Surface training t=26161, loss=0.043129630386829376\n",
      "Surface training t=26162, loss=0.03296850435435772\n",
      "Surface training t=26163, loss=0.04081113263964653\n",
      "Surface training t=26164, loss=0.03388629108667374\n",
      "Surface training t=26165, loss=0.032268837094306946\n",
      "Surface training t=26166, loss=0.027787848375737667\n",
      "Surface training t=26167, loss=0.029291899874806404\n",
      "Surface training t=26168, loss=0.029102565720677376\n",
      "Surface training t=26169, loss=0.02143957745283842\n",
      "Surface training t=26170, loss=0.02718271967023611\n",
      "Surface training t=26171, loss=0.023377221077680588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=26172, loss=0.0274362089112401\n",
      "Surface training t=26173, loss=0.02808745577931404\n",
      "Surface training t=26174, loss=0.0269825030118227\n",
      "Surface training t=26175, loss=0.02343880385160446\n",
      "Surface training t=26176, loss=0.028319566510617733\n",
      "Surface training t=26177, loss=0.02441344503313303\n",
      "Surface training t=26178, loss=0.02381577529013157\n",
      "Surface training t=26179, loss=0.03673143498599529\n",
      "Surface training t=26180, loss=0.028031070716679096\n",
      "Surface training t=26181, loss=0.022965963929891586\n",
      "Surface training t=26182, loss=0.023256991989910603\n",
      "Surface training t=26183, loss=0.027951001189649105\n",
      "Surface training t=26184, loss=0.03352932445704937\n",
      "Surface training t=26185, loss=0.04247371293604374\n",
      "Surface training t=26186, loss=0.025943304412066936\n",
      "Surface training t=26187, loss=0.024301250465214252\n",
      "Surface training t=26188, loss=0.01913839764893055\n",
      "Surface training t=26189, loss=0.013715160777792335\n",
      "Surface training t=26190, loss=0.01643272489309311\n",
      "Surface training t=26191, loss=0.019812597893178463\n",
      "Surface training t=26192, loss=0.018773924559354782\n",
      "Surface training t=26193, loss=0.020625753793865442\n",
      "Surface training t=26194, loss=0.018864176236093044\n",
      "Surface training t=26195, loss=0.02007421664893627\n",
      "Surface training t=26196, loss=0.042512090876698494\n",
      "Surface training t=26197, loss=0.03055736795067787\n",
      "Surface training t=26198, loss=0.033038681373000145\n",
      "Surface training t=26199, loss=0.03991004079580307\n",
      "Surface training t=26200, loss=0.03719283267855644\n",
      "Surface training t=26201, loss=0.032363600097596645\n",
      "Surface training t=26202, loss=0.0378748569637537\n",
      "Surface training t=26203, loss=0.027227467857301235\n",
      "Surface training t=26204, loss=0.038006946444511414\n",
      "Surface training t=26205, loss=0.026742983609437943\n",
      "Surface training t=26206, loss=0.0317991878837347\n",
      "Surface training t=26207, loss=0.027538497000932693\n",
      "Surface training t=26208, loss=0.02960983570665121\n",
      "Surface training t=26209, loss=0.03093637153506279\n",
      "Surface training t=26210, loss=0.024805406108498573\n",
      "Surface training t=26211, loss=0.02548354770988226\n",
      "Surface training t=26212, loss=0.021024326793849468\n",
      "Surface training t=26213, loss=0.02741492073982954\n",
      "Surface training t=26214, loss=0.02403977420181036\n",
      "Surface training t=26215, loss=0.02508060447871685\n",
      "Surface training t=26216, loss=0.031065145507454872\n",
      "Surface training t=26217, loss=0.021110590547323227\n",
      "Surface training t=26218, loss=0.026159677654504776\n",
      "Surface training t=26219, loss=0.021335408091545105\n",
      "Surface training t=26220, loss=0.020837790332734585\n",
      "Surface training t=26221, loss=0.020784580148756504\n",
      "Surface training t=26222, loss=0.021474070847034454\n",
      "Surface training t=26223, loss=0.02515301574021578\n",
      "Surface training t=26224, loss=0.02282662596553564\n",
      "Surface training t=26225, loss=0.020953773520886898\n",
      "Surface training t=26226, loss=0.015611288137733936\n",
      "Surface training t=26227, loss=0.020744934678077698\n",
      "Surface training t=26228, loss=0.02318945899605751\n",
      "Surface training t=26229, loss=0.023135501891374588\n",
      "Surface training t=26230, loss=0.022870698012411594\n",
      "Surface training t=26231, loss=0.02612286526709795\n",
      "Surface training t=26232, loss=0.015925105195492506\n",
      "Surface training t=26233, loss=0.02770654298365116\n",
      "Surface training t=26234, loss=0.03193986415863037\n",
      "Surface training t=26235, loss=0.025767052546143532\n",
      "Surface training t=26236, loss=0.04333525337278843\n",
      "Surface training t=26237, loss=0.032077861949801445\n",
      "Surface training t=26238, loss=0.03306352999061346\n",
      "Surface training t=26239, loss=0.030922003090381622\n",
      "Surface training t=26240, loss=0.03073118068277836\n",
      "Surface training t=26241, loss=0.022755444049835205\n",
      "Surface training t=26242, loss=0.030458714812994003\n",
      "Surface training t=26243, loss=0.02342166844755411\n",
      "Surface training t=26244, loss=0.022350591607391834\n",
      "Surface training t=26245, loss=0.01713522430509329\n",
      "Surface training t=26246, loss=0.016281728632748127\n",
      "Surface training t=26247, loss=0.017243691254407167\n",
      "Surface training t=26248, loss=0.020852682180702686\n",
      "Surface training t=26249, loss=0.021947667934000492\n",
      "Surface training t=26250, loss=0.01777948858216405\n",
      "Surface training t=26251, loss=0.0168420379050076\n",
      "Surface training t=26252, loss=0.013124500401318073\n",
      "Surface training t=26253, loss=0.021240821108222008\n",
      "Surface training t=26254, loss=0.018340652342885733\n",
      "Surface training t=26255, loss=0.020267068408429623\n",
      "Surface training t=26256, loss=0.030793660320341587\n",
      "Surface training t=26257, loss=0.02380772028118372\n",
      "Surface training t=26258, loss=0.02373103890568018\n",
      "Surface training t=26259, loss=0.02412388287484646\n",
      "Surface training t=26260, loss=0.030599103309214115\n",
      "Surface training t=26261, loss=0.03938772715628147\n",
      "Surface training t=26262, loss=0.0289909690618515\n",
      "Surface training t=26263, loss=0.023239401169121265\n",
      "Surface training t=26264, loss=0.019813209772109985\n",
      "Surface training t=26265, loss=0.016825486905872822\n",
      "Surface training t=26266, loss=0.017489315010607243\n",
      "Surface training t=26267, loss=0.02118444163352251\n",
      "Surface training t=26268, loss=0.021847997792065144\n",
      "Surface training t=26269, loss=0.02059367671608925\n",
      "Surface training t=26270, loss=0.02641132101416588\n",
      "Surface training t=26271, loss=0.019804304465651512\n",
      "Surface training t=26272, loss=0.0179441524669528\n",
      "Surface training t=26273, loss=0.015047557651996613\n",
      "Surface training t=26274, loss=0.01608858723193407\n",
      "Surface training t=26275, loss=0.015440443530678749\n",
      "Surface training t=26276, loss=0.017075875774025917\n",
      "Surface training t=26277, loss=0.014779378660023212\n",
      "Surface training t=26278, loss=0.01860620640218258\n",
      "Surface training t=26279, loss=0.0332297757267952\n",
      "Surface training t=26280, loss=0.03589933179318905\n",
      "Surface training t=26281, loss=0.03096108417958021\n",
      "Surface training t=26282, loss=0.03083263337612152\n",
      "Surface training t=26283, loss=0.05521527864038944\n",
      "Surface training t=26284, loss=0.03206310793757439\n",
      "Surface training t=26285, loss=0.03141343221068382\n",
      "Surface training t=26286, loss=0.043219927698373795\n",
      "Surface training t=26287, loss=0.02768191322684288\n",
      "Surface training t=26288, loss=0.03208843804895878\n",
      "Surface training t=26289, loss=0.0246423352509737\n",
      "Surface training t=26290, loss=0.035990720614790916\n",
      "Surface training t=26291, loss=0.03840065374970436\n",
      "Surface training t=26292, loss=0.03164629731327295\n",
      "Surface training t=26293, loss=0.04265440069139004\n",
      "Surface training t=26294, loss=0.052278921008110046\n",
      "Surface training t=26295, loss=0.035314274951815605\n",
      "Surface training t=26296, loss=0.03438760060817003\n",
      "Surface training t=26297, loss=0.02808874100446701\n",
      "Surface training t=26298, loss=0.03707348369061947\n",
      "Surface training t=26299, loss=0.033804142847657204\n",
      "Surface training t=26300, loss=0.032227128744125366\n",
      "Surface training t=26301, loss=0.041355062276124954\n",
      "Surface training t=26302, loss=0.031120413914322853\n",
      "Surface training t=26303, loss=0.03222855366766453\n",
      "Surface training t=26304, loss=0.03861246071755886\n",
      "Surface training t=26305, loss=0.0317526375874877\n",
      "Surface training t=26306, loss=0.019789421930909157\n",
      "Surface training t=26307, loss=0.019918635487556458\n",
      "Surface training t=26308, loss=0.021589848212897778\n",
      "Surface training t=26309, loss=0.026761403307318687\n",
      "Surface training t=26310, loss=0.019455166533589363\n",
      "Surface training t=26311, loss=0.01834691781550646\n",
      "Surface training t=26312, loss=0.022443984635174274\n",
      "Surface training t=26313, loss=0.01770273596048355\n",
      "Surface training t=26314, loss=0.016570320818573236\n",
      "Surface training t=26315, loss=0.01584568154066801\n",
      "Surface training t=26316, loss=0.015788815449923277\n",
      "Surface training t=26317, loss=0.014847047161310911\n",
      "Surface training t=26318, loss=0.014926274307072163\n",
      "Surface training t=26319, loss=0.01449147891253233\n",
      "Surface training t=26320, loss=0.01681850291788578\n",
      "Surface training t=26321, loss=0.020677845925092697\n",
      "Surface training t=26322, loss=0.01701033115386963\n",
      "Surface training t=26323, loss=0.020914197899401188\n",
      "Surface training t=26324, loss=0.02031954610720277\n",
      "Surface training t=26325, loss=0.017416090704500675\n",
      "Surface training t=26326, loss=0.018289029598236084\n",
      "Surface training t=26327, loss=0.022890171967446804\n",
      "Surface training t=26328, loss=0.03070229571312666\n",
      "Surface training t=26329, loss=0.023472623899579048\n",
      "Surface training t=26330, loss=0.01989648025482893\n",
      "Surface training t=26331, loss=0.01864475104957819\n",
      "Surface training t=26332, loss=0.021612228825688362\n",
      "Surface training t=26333, loss=0.023677421733736992\n",
      "Surface training t=26334, loss=0.034982748329639435\n",
      "Surface training t=26335, loss=0.02714993618428707\n",
      "Surface training t=26336, loss=0.02217474766075611\n",
      "Surface training t=26337, loss=0.02470549289137125\n",
      "Surface training t=26338, loss=0.02962635178118944\n",
      "Surface training t=26339, loss=0.019739845767617226\n",
      "Surface training t=26340, loss=0.02116046193987131\n",
      "Surface training t=26341, loss=0.023862059228122234\n",
      "Surface training t=26342, loss=0.022298059426248074\n",
      "Surface training t=26343, loss=0.024156086146831512\n",
      "Surface training t=26344, loss=0.024425869807600975\n",
      "Surface training t=26345, loss=0.02229452319443226\n",
      "Surface training t=26346, loss=0.022535132244229317\n",
      "Surface training t=26347, loss=0.01937384344637394\n",
      "Surface training t=26348, loss=0.011913965689018369\n",
      "Surface training t=26349, loss=0.018158168997615576\n",
      "Surface training t=26350, loss=0.019327126443386078\n",
      "Surface training t=26351, loss=0.021564441733062267\n",
      "Surface training t=26352, loss=0.018661571200937033\n",
      "Surface training t=26353, loss=0.01788339577615261\n",
      "Surface training t=26354, loss=0.025288010016083717\n",
      "Surface training t=26355, loss=0.022686856798827648\n",
      "Surface training t=26356, loss=0.023343040607869625\n",
      "Surface training t=26357, loss=0.022732414305210114\n",
      "Surface training t=26358, loss=0.028325271792709827\n",
      "Surface training t=26359, loss=0.025086007080972195\n",
      "Surface training t=26360, loss=0.018094846047461033\n",
      "Surface training t=26361, loss=0.01840823795646429\n",
      "Surface training t=26362, loss=0.014173836447298527\n",
      "Surface training t=26363, loss=0.01168375974521041\n",
      "Surface training t=26364, loss=0.02001511538401246\n",
      "Surface training t=26365, loss=0.02133831474930048\n",
      "Surface training t=26366, loss=0.01722479984164238\n",
      "Surface training t=26367, loss=0.021428611129522324\n",
      "Surface training t=26368, loss=0.01941702887415886\n",
      "Surface training t=26369, loss=0.01792287826538086\n",
      "Surface training t=26370, loss=0.020538607612252235\n",
      "Surface training t=26371, loss=0.02002779860049486\n",
      "Surface training t=26372, loss=0.018131064251065254\n",
      "Surface training t=26373, loss=0.025358575396239758\n",
      "Surface training t=26374, loss=0.015467016026377678\n",
      "Surface training t=26375, loss=0.022731264121830463\n",
      "Surface training t=26376, loss=0.024087288416922092\n",
      "Surface training t=26377, loss=0.019981992430984974\n",
      "Surface training t=26378, loss=0.01787528023123741\n",
      "Surface training t=26379, loss=0.018656035885214806\n",
      "Surface training t=26380, loss=0.01880842074751854\n",
      "Surface training t=26381, loss=0.021852903999388218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=26382, loss=0.01772178802639246\n",
      "Surface training t=26383, loss=0.021138501353561878\n",
      "Surface training t=26384, loss=0.01568891853094101\n",
      "Surface training t=26385, loss=0.01688370108604431\n",
      "Surface training t=26386, loss=0.01864886935800314\n",
      "Surface training t=26387, loss=0.0161757692694664\n",
      "Surface training t=26388, loss=0.01998677011579275\n",
      "Surface training t=26389, loss=0.017394770868122578\n",
      "Surface training t=26390, loss=0.015680798329412937\n",
      "Surface training t=26391, loss=0.022638332098722458\n",
      "Surface training t=26392, loss=0.017687685787677765\n",
      "Surface training t=26393, loss=0.016237560659646988\n",
      "Surface training t=26394, loss=0.019859778694808483\n",
      "Surface training t=26395, loss=0.014897832181304693\n",
      "Surface training t=26396, loss=0.01756350975483656\n",
      "Surface training t=26397, loss=0.0186051893979311\n",
      "Surface training t=26398, loss=0.023834173567593098\n",
      "Surface training t=26399, loss=0.025035434402525425\n",
      "Surface training t=26400, loss=0.01693770382553339\n",
      "Surface training t=26401, loss=0.018826859537512064\n",
      "Surface training t=26402, loss=0.02625653799623251\n",
      "Surface training t=26403, loss=0.026977838948369026\n",
      "Surface training t=26404, loss=0.0288610914722085\n",
      "Surface training t=26405, loss=0.027513885870575905\n",
      "Surface training t=26406, loss=0.02169169671833515\n",
      "Surface training t=26407, loss=0.0237621134147048\n",
      "Surface training t=26408, loss=0.023455428890883923\n",
      "Surface training t=26409, loss=0.019629908725619316\n",
      "Surface training t=26410, loss=0.02629148866981268\n",
      "Surface training t=26411, loss=0.030821208842098713\n",
      "Surface training t=26412, loss=0.020342824049293995\n",
      "Surface training t=26413, loss=0.019456111826002598\n",
      "Surface training t=26414, loss=0.020360760390758514\n",
      "Surface training t=26415, loss=0.01775537896901369\n",
      "Surface training t=26416, loss=0.01950740907341242\n",
      "Surface training t=26417, loss=0.020930870436131954\n",
      "Surface training t=26418, loss=0.028240262530744076\n",
      "Surface training t=26419, loss=0.021951009519398212\n",
      "Surface training t=26420, loss=0.022713455371558666\n",
      "Surface training t=26421, loss=0.026645994745194912\n",
      "Surface training t=26422, loss=0.02480925712734461\n",
      "Surface training t=26423, loss=0.020166514441370964\n",
      "Surface training t=26424, loss=0.024260375648736954\n",
      "Surface training t=26425, loss=0.030878376215696335\n",
      "Surface training t=26426, loss=0.020813808776438236\n",
      "Surface training t=26427, loss=0.029906662181019783\n",
      "Surface training t=26428, loss=0.02797720581293106\n",
      "Surface training t=26429, loss=0.02429191954433918\n",
      "Surface training t=26430, loss=0.029098790138959885\n",
      "Surface training t=26431, loss=0.017827002331614494\n",
      "Surface training t=26432, loss=0.0169070097617805\n",
      "Surface training t=26433, loss=0.015194885432720184\n",
      "Surface training t=26434, loss=0.016640322748571634\n",
      "Surface training t=26435, loss=0.014923983253538609\n",
      "Surface training t=26436, loss=0.025152767077088356\n",
      "Surface training t=26437, loss=0.03202824015170336\n",
      "Surface training t=26438, loss=0.028731833212077618\n",
      "Surface training t=26439, loss=0.021556073799729347\n",
      "Surface training t=26440, loss=0.026561329141259193\n",
      "Surface training t=26441, loss=0.02755869645625353\n",
      "Surface training t=26442, loss=0.02731235884130001\n",
      "Surface training t=26443, loss=0.028602933511137962\n",
      "Surface training t=26444, loss=0.03553450480103493\n",
      "Surface training t=26445, loss=0.04368898086249828\n",
      "Surface training t=26446, loss=0.04211168922483921\n",
      "Surface training t=26447, loss=0.03624376654624939\n",
      "Surface training t=26448, loss=0.038108253851532936\n",
      "Surface training t=26449, loss=0.03530805557966232\n",
      "Surface training t=26450, loss=0.03136494196951389\n",
      "Surface training t=26451, loss=0.04013100266456604\n",
      "Surface training t=26452, loss=0.031850568018853664\n",
      "Surface training t=26453, loss=0.03461090475320816\n",
      "Surface training t=26454, loss=0.031200464814901352\n",
      "Surface training t=26455, loss=0.042373886331915855\n",
      "Surface training t=26456, loss=0.04625370167195797\n",
      "Surface training t=26457, loss=0.03422923944890499\n",
      "Surface training t=26458, loss=0.03216412663459778\n",
      "Surface training t=26459, loss=0.030856752768158913\n",
      "Surface training t=26460, loss=0.043382553383708\n",
      "Surface training t=26461, loss=0.03290202282369137\n",
      "Surface training t=26462, loss=0.04898379184305668\n",
      "Surface training t=26463, loss=0.0372199472039938\n",
      "Surface training t=26464, loss=0.03647509776055813\n",
      "Surface training t=26465, loss=0.04622394032776356\n",
      "Surface training t=26466, loss=0.03093005996197462\n",
      "Surface training t=26467, loss=0.03205030411481857\n",
      "Surface training t=26468, loss=0.031203387305140495\n",
      "Surface training t=26469, loss=0.04349967837333679\n",
      "Surface training t=26470, loss=0.03650069609284401\n",
      "Surface training t=26471, loss=0.038521953858435154\n",
      "Surface training t=26472, loss=0.031699071638286114\n",
      "Surface training t=26473, loss=0.03156200610101223\n",
      "Surface training t=26474, loss=0.029098734259605408\n",
      "Surface training t=26475, loss=0.028422517701983452\n",
      "Surface training t=26476, loss=0.032543543726205826\n",
      "Surface training t=26477, loss=0.02548260986804962\n",
      "Surface training t=26478, loss=0.022829758003354073\n",
      "Surface training t=26479, loss=0.022904563695192337\n",
      "Surface training t=26480, loss=0.030848409980535507\n",
      "Surface training t=26481, loss=0.025448349304497242\n",
      "Surface training t=26482, loss=0.02678658440709114\n",
      "Surface training t=26483, loss=0.03159054554998875\n",
      "Surface training t=26484, loss=0.033073144033551216\n",
      "Surface training t=26485, loss=0.03450027108192444\n",
      "Surface training t=26486, loss=0.0321611100807786\n",
      "Surface training t=26487, loss=0.03851115517318249\n",
      "Surface training t=26488, loss=0.046523021534085274\n",
      "Surface training t=26489, loss=0.03201352804899216\n",
      "Surface training t=26490, loss=0.031144050881266594\n",
      "Surface training t=26491, loss=0.0340447723865509\n",
      "Surface training t=26492, loss=0.028406732715666294\n",
      "Surface training t=26493, loss=0.025439279153943062\n",
      "Surface training t=26494, loss=0.025822791270911694\n",
      "Surface training t=26495, loss=0.028753035701811314\n",
      "Surface training t=26496, loss=0.027929017320275307\n",
      "Surface training t=26497, loss=0.02334336657077074\n",
      "Surface training t=26498, loss=0.024778532795608044\n",
      "Surface training t=26499, loss=0.025842826813459396\n",
      "Surface training t=26500, loss=0.023682115599513054\n",
      "Surface training t=26501, loss=0.02126669231802225\n",
      "Surface training t=26502, loss=0.021354115568101406\n",
      "Surface training t=26503, loss=0.0237262356095016\n",
      "Surface training t=26504, loss=0.024442116729915142\n",
      "Surface training t=26505, loss=0.021215985529124737\n",
      "Surface training t=26506, loss=0.023529469966888428\n",
      "Surface training t=26507, loss=0.02110382355749607\n",
      "Surface training t=26508, loss=0.022569689899683\n",
      "Surface training t=26509, loss=0.015511035919189453\n",
      "Surface training t=26510, loss=0.02100569475442171\n",
      "Surface training t=26511, loss=0.020896613597869873\n",
      "Surface training t=26512, loss=0.0170460045337677\n",
      "Surface training t=26513, loss=0.023173958994448185\n",
      "Surface training t=26514, loss=0.021134909242391586\n",
      "Surface training t=26515, loss=0.022129323333501816\n",
      "Surface training t=26516, loss=0.021359330974519253\n",
      "Surface training t=26517, loss=0.033360473811626434\n",
      "Surface training t=26518, loss=0.022790423594415188\n",
      "Surface training t=26519, loss=0.02534913644194603\n",
      "Surface training t=26520, loss=0.02749838214367628\n",
      "Surface training t=26521, loss=0.022834484465420246\n",
      "Surface training t=26522, loss=0.020810523070394993\n",
      "Surface training t=26523, loss=0.023814181797206402\n",
      "Surface training t=26524, loss=0.02175193838775158\n",
      "Surface training t=26525, loss=0.023262994363904\n",
      "Surface training t=26526, loss=0.01929406262934208\n",
      "Surface training t=26527, loss=0.020729213021695614\n",
      "Surface training t=26528, loss=0.0153625444509089\n",
      "Surface training t=26529, loss=0.01982684899121523\n",
      "Surface training t=26530, loss=0.01694725127890706\n",
      "Surface training t=26531, loss=0.017189725302159786\n",
      "Surface training t=26532, loss=0.023694725707173347\n",
      "Surface training t=26533, loss=0.019004076719284058\n",
      "Surface training t=26534, loss=0.020488115958869457\n",
      "Surface training t=26535, loss=0.027610219083726406\n",
      "Surface training t=26536, loss=0.027378438040614128\n",
      "Surface training t=26537, loss=0.01845067646354437\n",
      "Surface training t=26538, loss=0.019808282144367695\n",
      "Surface training t=26539, loss=0.020794999785721302\n",
      "Surface training t=26540, loss=0.018482177518308163\n",
      "Surface training t=26541, loss=0.028944595716893673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=26542, loss=0.025613234378397465\n",
      "Surface training t=26543, loss=0.023383446037769318\n",
      "Surface training t=26544, loss=0.031175479292869568\n",
      "Surface training t=26545, loss=0.027449171990156174\n",
      "Surface training t=26546, loss=0.02050183154642582\n",
      "Surface training t=26547, loss=0.023344943299889565\n",
      "Surface training t=26548, loss=0.027441399171948433\n",
      "Surface training t=26549, loss=0.026018647477030754\n",
      "Surface training t=26550, loss=0.019875140860676765\n",
      "Surface training t=26551, loss=0.02224859967827797\n",
      "Surface training t=26552, loss=0.017149693332612514\n",
      "Surface training t=26553, loss=0.01804056577384472\n",
      "Surface training t=26554, loss=0.01883196458220482\n",
      "Surface training t=26555, loss=0.01905600167810917\n",
      "Surface training t=26556, loss=0.022848833352327347\n",
      "Surface training t=26557, loss=0.022937863133847713\n",
      "Surface training t=26558, loss=0.03221359755843878\n",
      "Surface training t=26559, loss=0.023082167841494083\n",
      "Surface training t=26560, loss=0.02723222505301237\n",
      "Surface training t=26561, loss=0.033279587514698505\n",
      "Surface training t=26562, loss=0.02379586733877659\n",
      "Surface training t=26563, loss=0.030998872593045235\n",
      "Surface training t=26564, loss=0.031295452266931534\n",
      "Surface training t=26565, loss=0.026994244195520878\n",
      "Surface training t=26566, loss=0.03389977477490902\n",
      "Surface training t=26567, loss=0.025939187966287136\n",
      "Surface training t=26568, loss=0.02698102965950966\n",
      "Surface training t=26569, loss=0.025189966894686222\n",
      "Surface training t=26570, loss=0.03147077187895775\n",
      "Surface training t=26571, loss=0.025553531013429165\n",
      "Surface training t=26572, loss=0.036530718207359314\n",
      "Surface training t=26573, loss=0.031258510425686836\n",
      "Surface training t=26574, loss=0.037664225324988365\n",
      "Surface training t=26575, loss=0.027774767018854618\n",
      "Surface training t=26576, loss=0.02911110408604145\n",
      "Surface training t=26577, loss=0.033694913610816\n",
      "Surface training t=26578, loss=0.025899769738316536\n",
      "Surface training t=26579, loss=0.03589335083961487\n",
      "Surface training t=26580, loss=0.025965100154280663\n",
      "Surface training t=26581, loss=0.025766558945178986\n",
      "Surface training t=26582, loss=0.038620725274086\n",
      "Surface training t=26583, loss=0.035326093435287476\n",
      "Surface training t=26584, loss=0.0347285196185112\n",
      "Surface training t=26585, loss=0.023673572577536106\n",
      "Surface training t=26586, loss=0.02856836747378111\n",
      "Surface training t=26587, loss=0.023380876518785954\n",
      "Surface training t=26588, loss=0.0275599155575037\n",
      "Surface training t=26589, loss=0.029705123975872993\n",
      "Surface training t=26590, loss=0.03121645376086235\n",
      "Surface training t=26591, loss=0.022188644856214523\n",
      "Surface training t=26592, loss=0.025120683945715427\n",
      "Surface training t=26593, loss=0.027344951406121254\n",
      "Surface training t=26594, loss=0.028826389461755753\n",
      "Surface training t=26595, loss=0.038159651681780815\n",
      "Surface training t=26596, loss=0.025786837562918663\n",
      "Surface training t=26597, loss=0.03066929057240486\n",
      "Surface training t=26598, loss=0.02853370551019907\n",
      "Surface training t=26599, loss=0.025086216628551483\n",
      "Surface training t=26600, loss=0.032945998944342136\n",
      "Surface training t=26601, loss=0.02948721870779991\n",
      "Surface training t=26602, loss=0.023563137277960777\n",
      "Surface training t=26603, loss=0.021396594122052193\n",
      "Surface training t=26604, loss=0.02396508865058422\n",
      "Surface training t=26605, loss=0.02059916127473116\n",
      "Surface training t=26606, loss=0.020130488090217113\n",
      "Surface training t=26607, loss=0.01984244491904974\n",
      "Surface training t=26608, loss=0.03293133992701769\n",
      "Surface training t=26609, loss=0.028171132318675518\n",
      "Surface training t=26610, loss=0.022827666252851486\n",
      "Surface training t=26611, loss=0.018813439644873142\n",
      "Surface training t=26612, loss=0.02158679161220789\n",
      "Surface training t=26613, loss=0.01938571408390999\n",
      "Surface training t=26614, loss=0.020398293156176805\n",
      "Surface training t=26615, loss=0.019678502343595028\n",
      "Surface training t=26616, loss=0.023975620046257973\n",
      "Surface training t=26617, loss=0.02415182627737522\n",
      "Surface training t=26618, loss=0.02066759392619133\n",
      "Surface training t=26619, loss=0.021455449052155018\n",
      "Surface training t=26620, loss=0.018061217851936817\n",
      "Surface training t=26621, loss=0.015556619968265295\n",
      "Surface training t=26622, loss=0.014771520160138607\n",
      "Surface training t=26623, loss=0.016387672629207373\n",
      "Surface training t=26624, loss=0.017705054953694344\n",
      "Surface training t=26625, loss=0.0159502774477005\n",
      "Surface training t=26626, loss=0.017289836890995502\n",
      "Surface training t=26627, loss=0.016944178845733404\n",
      "Surface training t=26628, loss=0.022152977995574474\n",
      "Surface training t=26629, loss=0.023009405471384525\n",
      "Surface training t=26630, loss=0.02378149703145027\n",
      "Surface training t=26631, loss=0.029667716473340988\n",
      "Surface training t=26632, loss=0.03156043775379658\n",
      "Surface training t=26633, loss=0.04161849059164524\n",
      "Surface training t=26634, loss=0.028676211833953857\n",
      "Surface training t=26635, loss=0.03461524751037359\n",
      "Surface training t=26636, loss=0.02322025503963232\n",
      "Surface training t=26637, loss=0.01887326454743743\n",
      "Surface training t=26638, loss=0.031189781613647938\n",
      "Surface training t=26639, loss=0.02638854831457138\n",
      "Surface training t=26640, loss=0.023950278759002686\n",
      "Surface training t=26641, loss=0.029011382721364498\n",
      "Surface training t=26642, loss=0.02599593624472618\n",
      "Surface training t=26643, loss=0.024225926958024502\n",
      "Surface training t=26644, loss=0.01844397746026516\n",
      "Surface training t=26645, loss=0.01955440081655979\n",
      "Surface training t=26646, loss=0.018105522729456425\n",
      "Surface training t=26647, loss=0.02333460096269846\n",
      "Surface training t=26648, loss=0.020004034973680973\n",
      "Surface training t=26649, loss=0.018818167503923178\n",
      "Surface training t=26650, loss=0.022671012207865715\n",
      "Surface training t=26651, loss=0.020044052973389626\n",
      "Surface training t=26652, loss=0.018425488844513893\n",
      "Surface training t=26653, loss=0.018143966794013977\n",
      "Surface training t=26654, loss=0.02031613141298294\n",
      "Surface training t=26655, loss=0.02291359193623066\n",
      "Surface training t=26656, loss=0.024956155568361282\n",
      "Surface training t=26657, loss=0.028857476077973843\n",
      "Surface training t=26658, loss=0.02480527851730585\n",
      "Surface training t=26659, loss=0.022209457121789455\n",
      "Surface training t=26660, loss=0.022262834012508392\n",
      "Surface training t=26661, loss=0.01643344108015299\n",
      "Surface training t=26662, loss=0.022598855197429657\n",
      "Surface training t=26663, loss=0.01881061401218176\n",
      "Surface training t=26664, loss=0.01941092498600483\n",
      "Surface training t=26665, loss=0.0263825049623847\n",
      "Surface training t=26666, loss=0.022169150412082672\n",
      "Surface training t=26667, loss=0.015575127676129341\n",
      "Surface training t=26668, loss=0.021646575070917606\n",
      "Surface training t=26669, loss=0.0165421012789011\n",
      "Surface training t=26670, loss=0.02154871355742216\n",
      "Surface training t=26671, loss=0.020118439570069313\n",
      "Surface training t=26672, loss=0.018153691664338112\n",
      "Surface training t=26673, loss=0.01786710135638714\n",
      "Surface training t=26674, loss=0.022472851909697056\n",
      "Surface training t=26675, loss=0.02584890043362975\n",
      "Surface training t=26676, loss=0.023601258173584938\n",
      "Surface training t=26677, loss=0.025826189666986465\n",
      "Surface training t=26678, loss=0.03850769065320492\n",
      "Surface training t=26679, loss=0.03101448155939579\n",
      "Surface training t=26680, loss=0.03268483653664589\n",
      "Surface training t=26681, loss=0.026188328862190247\n",
      "Surface training t=26682, loss=0.022143185138702393\n",
      "Surface training t=26683, loss=0.03195010218769312\n",
      "Surface training t=26684, loss=0.02825010195374489\n",
      "Surface training t=26685, loss=0.023985588923096657\n",
      "Surface training t=26686, loss=0.03285336773842573\n",
      "Surface training t=26687, loss=0.02473971899598837\n",
      "Surface training t=26688, loss=0.030707809142768383\n",
      "Surface training t=26689, loss=0.02917831763625145\n",
      "Surface training t=26690, loss=0.02377161756157875\n",
      "Surface training t=26691, loss=0.026008586399257183\n",
      "Surface training t=26692, loss=0.030046367086470127\n",
      "Surface training t=26693, loss=0.025268581695854664\n",
      "Surface training t=26694, loss=0.030426928773522377\n",
      "Surface training t=26695, loss=0.02839125134050846\n",
      "Surface training t=26696, loss=0.02287574578076601\n",
      "Surface training t=26697, loss=0.04245861619710922\n",
      "Surface training t=26698, loss=0.02833937667310238\n",
      "Surface training t=26699, loss=0.0424590390175581\n",
      "Surface training t=26700, loss=0.03132180403918028\n",
      "Surface training t=26701, loss=0.028634147718548775\n",
      "Surface training t=26702, loss=0.028821912594139576\n",
      "Surface training t=26703, loss=0.03364919312298298\n",
      "Surface training t=26704, loss=0.023230988532304764\n",
      "Surface training t=26705, loss=0.028966760262846947\n",
      "Surface training t=26706, loss=0.022248800843954086\n",
      "Surface training t=26707, loss=0.019483983516693115\n",
      "Surface training t=26708, loss=0.015550042036920786\n",
      "Surface training t=26709, loss=0.018074871972203255\n",
      "Surface training t=26710, loss=0.024915166199207306\n",
      "Surface training t=26711, loss=0.024262175895273685\n",
      "Surface training t=26712, loss=0.01446449477225542\n",
      "Surface training t=26713, loss=0.01788888592272997\n",
      "Surface training t=26714, loss=0.014154210221022367\n",
      "Surface training t=26715, loss=0.017966043204069138\n",
      "Surface training t=26716, loss=0.015540830790996552\n",
      "Surface training t=26717, loss=0.017993222922086716\n",
      "Surface training t=26718, loss=0.018708630464971066\n",
      "Surface training t=26719, loss=0.021819954738020897\n",
      "Surface training t=26720, loss=0.018114046193659306\n",
      "Surface training t=26721, loss=0.020738153718411922\n",
      "Surface training t=26722, loss=0.01854722760617733\n",
      "Surface training t=26723, loss=0.021013102494180202\n",
      "Surface training t=26724, loss=0.020730603486299515\n",
      "Surface training t=26725, loss=0.020982286892831326\n",
      "Surface training t=26726, loss=0.020968531258404255\n",
      "Surface training t=26727, loss=0.020724169444292784\n",
      "Surface training t=26728, loss=0.02089923992753029\n",
      "Surface training t=26729, loss=0.024655962362885475\n",
      "Surface training t=26730, loss=0.02178204618394375\n",
      "Surface training t=26731, loss=0.019787501078099012\n",
      "Surface training t=26732, loss=0.024214296601712704\n",
      "Surface training t=26733, loss=0.02856338955461979\n",
      "Surface training t=26734, loss=0.021429235115647316\n",
      "Surface training t=26735, loss=0.02455311454832554\n",
      "Surface training t=26736, loss=0.03389127738773823\n",
      "Surface training t=26737, loss=0.029656761325895786\n",
      "Surface training t=26738, loss=0.02093820832669735\n",
      "Surface training t=26739, loss=0.03580840863287449\n",
      "Surface training t=26740, loss=0.022348728962242603\n",
      "Surface training t=26741, loss=0.023320638574659824\n",
      "Surface training t=26742, loss=0.02843271940946579\n",
      "Surface training t=26743, loss=0.02445764746516943\n",
      "Surface training t=26744, loss=0.023456212133169174\n",
      "Surface training t=26745, loss=0.023459513671696186\n",
      "Surface training t=26746, loss=0.020846380852162838\n",
      "Surface training t=26747, loss=0.017315562814474106\n",
      "Surface training t=26748, loss=0.015837284736335278\n",
      "Surface training t=26749, loss=0.012348115677013993\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=26750, loss=0.027700232341885567\n",
      "Surface training t=26751, loss=0.02779199182987213\n",
      "Surface training t=26752, loss=0.028403736650943756\n",
      "Surface training t=26753, loss=0.03757094219326973\n",
      "Surface training t=26754, loss=0.027486024424433708\n",
      "Surface training t=26755, loss=0.03896253928542137\n",
      "Surface training t=26756, loss=0.03142682183533907\n",
      "Surface training t=26757, loss=0.026020338758826256\n",
      "Surface training t=26758, loss=0.029787395149469376\n",
      "Surface training t=26759, loss=0.025426638312637806\n",
      "Surface training t=26760, loss=0.027856169268488884\n",
      "Surface training t=26761, loss=0.03464309498667717\n",
      "Surface training t=26762, loss=0.027290313504636288\n",
      "Surface training t=26763, loss=0.028831264935433865\n",
      "Surface training t=26764, loss=0.030128229409456253\n",
      "Surface training t=26765, loss=0.026855308562517166\n",
      "Surface training t=26766, loss=0.052006255835294724\n",
      "Surface training t=26767, loss=0.0326827522367239\n",
      "Surface training t=26768, loss=0.03930904436856508\n",
      "Surface training t=26769, loss=0.05832775495946407\n",
      "Surface training t=26770, loss=0.03449162933975458\n",
      "Surface training t=26771, loss=0.047884657979011536\n",
      "Surface training t=26772, loss=0.026962127070873976\n",
      "Surface training t=26773, loss=0.02884641755372286\n",
      "Surface training t=26774, loss=0.026828736066818237\n",
      "Surface training t=26775, loss=0.028266864828765392\n",
      "Surface training t=26776, loss=0.029510308988392353\n",
      "Surface training t=26777, loss=0.022892155684530735\n",
      "Surface training t=26778, loss=0.02183162420988083\n",
      "Surface training t=26779, loss=0.022417588159441948\n",
      "Surface training t=26780, loss=0.019297434948384762\n",
      "Surface training t=26781, loss=0.028894826769828796\n",
      "Surface training t=26782, loss=0.0260475380346179\n",
      "Surface training t=26783, loss=0.02799784205853939\n",
      "Surface training t=26784, loss=0.02903833892196417\n",
      "Surface training t=26785, loss=0.025824550539255142\n",
      "Surface training t=26786, loss=0.021833069622516632\n",
      "Surface training t=26787, loss=0.02553131803870201\n",
      "Surface training t=26788, loss=0.024800618179142475\n",
      "Surface training t=26789, loss=0.026187860406935215\n",
      "Surface training t=26790, loss=0.022685707546770573\n",
      "Surface training t=26791, loss=0.01935255154967308\n",
      "Surface training t=26792, loss=0.022746878676116467\n",
      "Surface training t=26793, loss=0.019924402236938477\n",
      "Surface training t=26794, loss=0.019494352862238884\n",
      "Surface training t=26795, loss=0.021389364264905453\n",
      "Surface training t=26796, loss=0.01690651196986437\n",
      "Surface training t=26797, loss=0.018945956602692604\n",
      "Surface training t=26798, loss=0.020868735387921333\n",
      "Surface training t=26799, loss=0.016792865470051765\n",
      "Surface training t=26800, loss=0.023210582323372364\n",
      "Surface training t=26801, loss=0.019321264699101448\n",
      "Surface training t=26802, loss=0.022541682235896587\n",
      "Surface training t=26803, loss=0.021782664582133293\n",
      "Surface training t=26804, loss=0.019076106138527393\n",
      "Surface training t=26805, loss=0.014728138223290443\n",
      "Surface training t=26806, loss=0.017510061617940664\n",
      "Surface training t=26807, loss=0.01960840728133917\n",
      "Surface training t=26808, loss=0.020067677833139896\n",
      "Surface training t=26809, loss=0.019174817949533463\n",
      "Surface training t=26810, loss=0.02394074574112892\n",
      "Surface training t=26811, loss=0.027383269742131233\n",
      "Surface training t=26812, loss=0.022989804856479168\n",
      "Surface training t=26813, loss=0.019429026637226343\n",
      "Surface training t=26814, loss=0.019647376611828804\n",
      "Surface training t=26815, loss=0.022309179417788982\n",
      "Surface training t=26816, loss=0.01964022684842348\n",
      "Surface training t=26817, loss=0.01988658681511879\n",
      "Surface training t=26818, loss=0.01642149919643998\n",
      "Surface training t=26819, loss=0.020752943120896816\n",
      "Surface training t=26820, loss=0.02366103231906891\n",
      "Surface training t=26821, loss=0.024706323631107807\n",
      "Surface training t=26822, loss=0.020507565699517727\n",
      "Surface training t=26823, loss=0.017440106254070997\n",
      "Surface training t=26824, loss=0.022379210218787193\n",
      "Surface training t=26825, loss=0.016585303470492363\n",
      "Surface training t=26826, loss=0.015257998835295439\n",
      "Surface training t=26827, loss=0.018782027065753937\n",
      "Surface training t=26828, loss=0.017978177405893803\n",
      "Surface training t=26829, loss=0.01781861949712038\n",
      "Surface training t=26830, loss=0.01952225249260664\n",
      "Surface training t=26831, loss=0.01769338082522154\n",
      "Surface training t=26832, loss=0.020388559438288212\n",
      "Surface training t=26833, loss=0.022874865680933\n",
      "Surface training t=26834, loss=0.01807808643206954\n",
      "Surface training t=26835, loss=0.019420743919909\n",
      "Surface training t=26836, loss=0.015474895015358925\n",
      "Surface training t=26837, loss=0.01788436435163021\n",
      "Surface training t=26838, loss=0.021300971508026123\n",
      "Surface training t=26839, loss=0.02019376400858164\n",
      "Surface training t=26840, loss=0.0249840272590518\n",
      "Surface training t=26841, loss=0.02019552420824766\n",
      "Surface training t=26842, loss=0.021719645708799362\n",
      "Surface training t=26843, loss=0.021590281277894974\n",
      "Surface training t=26844, loss=0.019759451039135456\n",
      "Surface training t=26845, loss=0.015181050170212984\n",
      "Surface training t=26846, loss=0.020196544006466866\n",
      "Surface training t=26847, loss=0.015396709088236094\n",
      "Surface training t=26848, loss=0.017599827609956264\n",
      "Surface training t=26849, loss=0.01838413765653968\n",
      "Surface training t=26850, loss=0.019267029128968716\n",
      "Surface training t=26851, loss=0.01854108739644289\n",
      "Surface training t=26852, loss=0.015802673064172268\n",
      "Surface training t=26853, loss=0.01805241499096155\n",
      "Surface training t=26854, loss=0.017877810169011354\n",
      "Surface training t=26855, loss=0.016946290619671345\n",
      "Surface training t=26856, loss=0.018133276142179966\n",
      "Surface training t=26857, loss=0.023463976569473743\n",
      "Surface training t=26858, loss=0.031315505504608154\n",
      "Surface training t=26859, loss=0.026930110529065132\n",
      "Surface training t=26860, loss=0.03983984887599945\n",
      "Surface training t=26861, loss=0.028285525739192963\n",
      "Surface training t=26862, loss=0.02616290282458067\n",
      "Surface training t=26863, loss=0.03071887418627739\n",
      "Surface training t=26864, loss=0.0274196770042181\n",
      "Surface training t=26865, loss=0.03145894780755043\n",
      "Surface training t=26866, loss=0.027308091521263123\n",
      "Surface training t=26867, loss=0.02486612368375063\n",
      "Surface training t=26868, loss=0.02228622417896986\n",
      "Surface training t=26869, loss=0.03827211074531078\n",
      "Surface training t=26870, loss=0.027592744678258896\n",
      "Surface training t=26871, loss=0.04069912247359753\n",
      "Surface training t=26872, loss=0.0292810732498765\n",
      "Surface training t=26873, loss=0.029651041142642498\n",
      "Surface training t=26874, loss=0.03479443024843931\n",
      "Surface training t=26875, loss=0.02837185747921467\n",
      "Surface training t=26876, loss=0.03593826666474342\n",
      "Surface training t=26877, loss=0.026305794715881348\n",
      "Surface training t=26878, loss=0.03135838359594345\n",
      "Surface training t=26879, loss=0.02656231541186571\n",
      "Surface training t=26880, loss=0.019212340004742146\n",
      "Surface training t=26881, loss=0.01767652854323387\n",
      "Surface training t=26882, loss=0.016786916181445122\n",
      "Surface training t=26883, loss=0.015629241708666086\n",
      "Surface training t=26884, loss=0.016621719114482403\n",
      "Surface training t=26885, loss=0.013032801914960146\n",
      "Surface training t=26886, loss=0.0219039311632514\n",
      "Surface training t=26887, loss=0.018979030661284924\n",
      "Surface training t=26888, loss=0.018964088521897793\n",
      "Surface training t=26889, loss=0.01964524295181036\n",
      "Surface training t=26890, loss=0.02441919967532158\n",
      "Surface training t=26891, loss=0.02896455116569996\n",
      "Surface training t=26892, loss=0.03576071001589298\n",
      "Surface training t=26893, loss=0.02932604029774666\n",
      "Surface training t=26894, loss=0.023666825145483017\n",
      "Surface training t=26895, loss=0.03155449591577053\n",
      "Surface training t=26896, loss=0.03729736618697643\n",
      "Surface training t=26897, loss=0.029363062232732773\n",
      "Surface training t=26898, loss=0.03254939988255501\n",
      "Surface training t=26899, loss=0.032079579308629036\n",
      "Surface training t=26900, loss=0.045565035194158554\n",
      "Surface training t=26901, loss=0.041391029953956604\n",
      "Surface training t=26902, loss=0.03568083606660366\n",
      "Surface training t=26903, loss=0.039962053298950195\n",
      "Surface training t=26904, loss=0.02403427194803953\n",
      "Surface training t=26905, loss=0.022027921862900257\n",
      "Surface training t=26906, loss=0.021106433123350143\n",
      "Surface training t=26907, loss=0.018939943984150887\n",
      "Surface training t=26908, loss=0.01852208562195301\n",
      "Surface training t=26909, loss=0.015884158667176962\n",
      "Surface training t=26910, loss=0.01902661006897688\n",
      "Surface training t=26911, loss=0.024785947054624557\n",
      "Surface training t=26912, loss=0.03190728276968002\n",
      "Surface training t=26913, loss=0.021558833308517933\n",
      "Surface training t=26914, loss=0.023535712622106075\n",
      "Surface training t=26915, loss=0.02426900528371334\n",
      "Surface training t=26916, loss=0.028046071529388428\n",
      "Surface training t=26917, loss=0.03208498377352953\n",
      "Surface training t=26918, loss=0.028278778307139874\n",
      "Surface training t=26919, loss=0.02495462354272604\n",
      "Surface training t=26920, loss=0.028000012040138245\n",
      "Surface training t=26921, loss=0.022648312151432037\n",
      "Surface training t=26922, loss=0.034278806298971176\n",
      "Surface training t=26923, loss=0.03714725002646446\n",
      "Surface training t=26924, loss=0.031476644799113274\n",
      "Surface training t=26925, loss=0.05625907517969608\n",
      "Surface training t=26926, loss=0.03440315928310156\n",
      "Surface training t=26927, loss=0.03323919977992773\n",
      "Surface training t=26928, loss=0.028618372976779938\n",
      "Surface training t=26929, loss=0.03542625345289707\n",
      "Surface training t=26930, loss=0.02877007983624935\n",
      "Surface training t=26931, loss=0.028450494166463614\n",
      "Surface training t=26932, loss=0.0371562484651804\n",
      "Surface training t=26933, loss=0.036699273623526096\n",
      "Surface training t=26934, loss=0.028569347225129604\n",
      "Surface training t=26935, loss=0.021000326611101627\n",
      "Surface training t=26936, loss=0.027274079620838165\n",
      "Surface training t=26937, loss=0.0282681742683053\n",
      "Surface training t=26938, loss=0.028844216838479042\n",
      "Surface training t=26939, loss=0.020015674643218517\n",
      "Surface training t=26940, loss=0.023570368066430092\n",
      "Surface training t=26941, loss=0.02293423842638731\n",
      "Surface training t=26942, loss=0.024914494715631008\n",
      "Surface training t=26943, loss=0.02298123762011528\n",
      "Surface training t=26944, loss=0.014652057085186243\n",
      "Surface training t=26945, loss=0.02204087283462286\n",
      "Surface training t=26946, loss=0.02022639475762844\n",
      "Surface training t=26947, loss=0.027921966277062893\n",
      "Surface training t=26948, loss=0.020461044274270535\n",
      "Surface training t=26949, loss=0.018976610153913498\n",
      "Surface training t=26950, loss=0.020381650887429714\n",
      "Surface training t=26951, loss=0.02388478722423315\n",
      "Surface training t=26952, loss=0.023688077926635742\n",
      "Surface training t=26953, loss=0.024489798583090305\n",
      "Surface training t=26954, loss=0.026944074779748917\n",
      "Surface training t=26955, loss=0.020063217729330063\n",
      "Surface training t=26956, loss=0.019669603556394577\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=26957, loss=0.02044010069221258\n",
      "Surface training t=26958, loss=0.022144552320241928\n",
      "Surface training t=26959, loss=0.01783924736082554\n",
      "Surface training t=26960, loss=0.01770285051316023\n",
      "Surface training t=26961, loss=0.01905764266848564\n",
      "Surface training t=26962, loss=0.014347035437822342\n",
      "Surface training t=26963, loss=0.016150166280567646\n",
      "Surface training t=26964, loss=0.020389918237924576\n",
      "Surface training t=26965, loss=0.023114576004445553\n",
      "Surface training t=26966, loss=0.02292228676378727\n",
      "Surface training t=26967, loss=0.018466535955667496\n",
      "Surface training t=26968, loss=0.019288625568151474\n",
      "Surface training t=26969, loss=0.022691424936056137\n",
      "Surface training t=26970, loss=0.02040799241513014\n",
      "Surface training t=26971, loss=0.017686056904494762\n",
      "Surface training t=26972, loss=0.020667122676968575\n",
      "Surface training t=26973, loss=0.027422001585364342\n",
      "Surface training t=26974, loss=0.02492512669414282\n",
      "Surface training t=26975, loss=0.03172288369387388\n",
      "Surface training t=26976, loss=0.022751903161406517\n",
      "Surface training t=26977, loss=0.02822146099060774\n",
      "Surface training t=26978, loss=0.022142712958157063\n",
      "Surface training t=26979, loss=0.022369678132236004\n",
      "Surface training t=26980, loss=0.024851338006556034\n",
      "Surface training t=26981, loss=0.022855008020997047\n",
      "Surface training t=26982, loss=0.023150460794568062\n",
      "Surface training t=26983, loss=0.02484466414898634\n",
      "Surface training t=26984, loss=0.022644256241619587\n",
      "Surface training t=26985, loss=0.02202996565029025\n",
      "Surface training t=26986, loss=0.022605234757065773\n",
      "Surface training t=26987, loss=0.02485608123242855\n",
      "Surface training t=26988, loss=0.026065267622470856\n",
      "Surface training t=26989, loss=0.027814926579594612\n",
      "Surface training t=26990, loss=0.0398112740367651\n",
      "Surface training t=26991, loss=0.031780133955180645\n",
      "Surface training t=26992, loss=0.033145494759082794\n",
      "Surface training t=26993, loss=0.04822670668363571\n",
      "Surface training t=26994, loss=0.04253395274281502\n",
      "Surface training t=26995, loss=0.032361045479774475\n",
      "Surface training t=26996, loss=0.03610462974756956\n",
      "Surface training t=26997, loss=0.034372550435364246\n",
      "Surface training t=26998, loss=0.02387683466076851\n",
      "Surface training t=26999, loss=0.027907252311706543\n",
      "Surface training t=27000, loss=0.02620207704603672\n",
      "Surface training t=27001, loss=0.032321373000741005\n",
      "Surface training t=27002, loss=0.02617024350911379\n",
      "Surface training t=27003, loss=0.026484818197786808\n",
      "Surface training t=27004, loss=0.021778211928904057\n",
      "Surface training t=27005, loss=0.025911429896950722\n",
      "Surface training t=27006, loss=0.03118240926414728\n",
      "Surface training t=27007, loss=0.02637208253145218\n",
      "Surface training t=27008, loss=0.02583853341639042\n",
      "Surface training t=27009, loss=0.029050039127469063\n",
      "Surface training t=27010, loss=0.02060109842568636\n",
      "Surface training t=27011, loss=0.02348276972770691\n",
      "Surface training t=27012, loss=0.028155972249805927\n",
      "Surface training t=27013, loss=0.022608657367527485\n",
      "Surface training t=27014, loss=0.023961488157510757\n",
      "Surface training t=27015, loss=0.033929482102394104\n",
      "Surface training t=27016, loss=0.026558367535471916\n",
      "Surface training t=27017, loss=0.029513186775147915\n",
      "Surface training t=27018, loss=0.023428321816027164\n",
      "Surface training t=27019, loss=0.03256107680499554\n",
      "Surface training t=27020, loss=0.02228876668959856\n",
      "Surface training t=27021, loss=0.03467187285423279\n",
      "Surface training t=27022, loss=0.02292401622980833\n",
      "Surface training t=27023, loss=0.019344737753272057\n",
      "Surface training t=27024, loss=0.026057185605168343\n",
      "Surface training t=27025, loss=0.023262389935553074\n",
      "Surface training t=27026, loss=0.017498312518000603\n",
      "Surface training t=27027, loss=0.02364757750183344\n",
      "Surface training t=27028, loss=0.01613223645836115\n",
      "Surface training t=27029, loss=0.013855346944183111\n",
      "Surface training t=27030, loss=0.013934254180639982\n",
      "Surface training t=27031, loss=0.020824010483920574\n",
      "Surface training t=27032, loss=0.02081090211868286\n",
      "Surface training t=27033, loss=0.01959382463246584\n",
      "Surface training t=27034, loss=0.01933662872761488\n",
      "Surface training t=27035, loss=0.02049240656197071\n",
      "Surface training t=27036, loss=0.021772812120616436\n",
      "Surface training t=27037, loss=0.015293360222131014\n",
      "Surface training t=27038, loss=0.017817710526287556\n",
      "Surface training t=27039, loss=0.020683390088379383\n",
      "Surface training t=27040, loss=0.011894472409039736\n",
      "Surface training t=27041, loss=0.014179861173033714\n",
      "Surface training t=27042, loss=0.015509032178670168\n",
      "Surface training t=27043, loss=0.014726377557963133\n",
      "Surface training t=27044, loss=0.015276343561708927\n",
      "Surface training t=27045, loss=0.014872337691485882\n",
      "Surface training t=27046, loss=0.014371329452842474\n",
      "Surface training t=27047, loss=0.01928803324699402\n",
      "Surface training t=27048, loss=0.015009550377726555\n",
      "Surface training t=27049, loss=0.01681529451161623\n",
      "Surface training t=27050, loss=0.015774482395499945\n",
      "Surface training t=27051, loss=0.018179161474108696\n",
      "Surface training t=27052, loss=0.014297312125563622\n",
      "Surface training t=27053, loss=0.01908353576436639\n",
      "Surface training t=27054, loss=0.018305815756320953\n",
      "Surface training t=27055, loss=0.019181708805263042\n",
      "Surface training t=27056, loss=0.020290045998990536\n",
      "Surface training t=27057, loss=0.016865405719727278\n",
      "Surface training t=27058, loss=0.018155154306441545\n",
      "Surface training t=27059, loss=0.021398098208010197\n",
      "Surface training t=27060, loss=0.01847596187144518\n",
      "Surface training t=27061, loss=0.018638870678842068\n",
      "Surface training t=27062, loss=0.04303601570427418\n",
      "Surface training t=27063, loss=0.026714451611042023\n",
      "Surface training t=27064, loss=0.03357491921633482\n",
      "Surface training t=27065, loss=0.026287497021257877\n",
      "Surface training t=27066, loss=0.0250778216868639\n",
      "Surface training t=27067, loss=0.035193538293242455\n",
      "Surface training t=27068, loss=0.029820634983479977\n",
      "Surface training t=27069, loss=0.03018841426819563\n",
      "Surface training t=27070, loss=0.030520400032401085\n",
      "Surface training t=27071, loss=0.024671198800206184\n",
      "Surface training t=27072, loss=0.02429369557648897\n",
      "Surface training t=27073, loss=0.02210131846368313\n",
      "Surface training t=27074, loss=0.017680935095995665\n",
      "Surface training t=27075, loss=0.030225095339119434\n",
      "Surface training t=27076, loss=0.03804655931890011\n",
      "Surface training t=27077, loss=0.0305801834911108\n",
      "Surface training t=27078, loss=0.031876638531684875\n",
      "Surface training t=27079, loss=0.028271658346056938\n",
      "Surface training t=27080, loss=0.030723472125828266\n",
      "Surface training t=27081, loss=0.03103779163211584\n",
      "Surface training t=27082, loss=0.028065833263099194\n",
      "Surface training t=27083, loss=0.026748008094727993\n",
      "Surface training t=27084, loss=0.02330778446048498\n",
      "Surface training t=27085, loss=0.02440230082720518\n",
      "Surface training t=27086, loss=0.021260907873511314\n",
      "Surface training t=27087, loss=0.02259316574782133\n",
      "Surface training t=27088, loss=0.016890425700694323\n",
      "Surface training t=27089, loss=0.01494352426379919\n",
      "Surface training t=27090, loss=0.024302988313138485\n",
      "Surface training t=27091, loss=0.027154389768838882\n",
      "Surface training t=27092, loss=0.03153568785637617\n",
      "Surface training t=27093, loss=0.027839560993015766\n",
      "Surface training t=27094, loss=0.03252008371055126\n",
      "Surface training t=27095, loss=0.028361945413053036\n",
      "Surface training t=27096, loss=0.040811795741319656\n",
      "Surface training t=27097, loss=0.025924271903932095\n",
      "Surface training t=27098, loss=0.0282749580219388\n",
      "Surface training t=27099, loss=0.037456363439559937\n",
      "Surface training t=27100, loss=0.027986934408545494\n",
      "Surface training t=27101, loss=0.03501569479703903\n",
      "Surface training t=27102, loss=0.02799312397837639\n",
      "Surface training t=27103, loss=0.03271844983100891\n",
      "Surface training t=27104, loss=0.02528530079871416\n",
      "Surface training t=27105, loss=0.020120788365602493\n",
      "Surface training t=27106, loss=0.0219012051820755\n",
      "Surface training t=27107, loss=0.027334352023899555\n",
      "Surface training t=27108, loss=0.02329659927636385\n",
      "Surface training t=27109, loss=0.023254837840795517\n",
      "Surface training t=27110, loss=0.03433326259255409\n",
      "Surface training t=27111, loss=0.029059878550469875\n",
      "Surface training t=27112, loss=0.031052681617438793\n",
      "Surface training t=27113, loss=0.0446306299418211\n",
      "Surface training t=27114, loss=0.03436988778412342\n",
      "Surface training t=27115, loss=0.027454130351543427\n",
      "Surface training t=27116, loss=0.03841186873614788\n",
      "Surface training t=27117, loss=0.03187235537916422\n",
      "Surface training t=27118, loss=0.03545636776834726\n",
      "Surface training t=27119, loss=0.03729366697371006\n",
      "Surface training t=27120, loss=0.028787448070943356\n",
      "Surface training t=27121, loss=0.03356114216148853\n",
      "Surface training t=27122, loss=0.026156472973525524\n",
      "Surface training t=27123, loss=0.01724256668239832\n",
      "Surface training t=27124, loss=0.020594080910086632\n",
      "Surface training t=27125, loss=0.015040540602058172\n",
      "Surface training t=27126, loss=0.024322877638041973\n",
      "Surface training t=27127, loss=0.025065231136977673\n",
      "Surface training t=27128, loss=0.02271584328263998\n",
      "Surface training t=27129, loss=0.021236108615994453\n",
      "Surface training t=27130, loss=0.03880828991532326\n",
      "Surface training t=27131, loss=0.02937759179621935\n",
      "Surface training t=27132, loss=0.03899109363555908\n",
      "Surface training t=27133, loss=0.0299272695556283\n",
      "Surface training t=27134, loss=0.04532433860003948\n",
      "Surface training t=27135, loss=0.03536786325275898\n",
      "Surface training t=27136, loss=0.031695401296019554\n",
      "Surface training t=27137, loss=0.037193670868873596\n",
      "Surface training t=27138, loss=0.0247508492320776\n",
      "Surface training t=27139, loss=0.018700867891311646\n",
      "Surface training t=27140, loss=0.018715535290539265\n",
      "Surface training t=27141, loss=0.020883994176983833\n",
      "Surface training t=27142, loss=0.027546076104044914\n",
      "Surface training t=27143, loss=0.028597352094948292\n",
      "Surface training t=27144, loss=0.03090602532029152\n",
      "Surface training t=27145, loss=0.021527649834752083\n",
      "Surface training t=27146, loss=0.026892557740211487\n",
      "Surface training t=27147, loss=0.023398863151669502\n",
      "Surface training t=27148, loss=0.02547221351414919\n",
      "Surface training t=27149, loss=0.029645448550581932\n",
      "Surface training t=27150, loss=0.03139836527407169\n",
      "Surface training t=27151, loss=0.02762995008379221\n",
      "Surface training t=27152, loss=0.025986523367464542\n",
      "Surface training t=27153, loss=0.02494119293987751\n",
      "Surface training t=27154, loss=0.024186287075281143\n",
      "Surface training t=27155, loss=0.028870667330920696\n",
      "Surface training t=27156, loss=0.026030157692730427\n",
      "Surface training t=27157, loss=0.02265645330771804\n",
      "Surface training t=27158, loss=0.018651656806468964\n",
      "Surface training t=27159, loss=0.02050084713846445\n",
      "Surface training t=27160, loss=0.019209500402212143\n",
      "Surface training t=27161, loss=0.019660294987261295\n",
      "Surface training t=27162, loss=0.02125915139913559\n",
      "Surface training t=27163, loss=0.022261057514697313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=27164, loss=0.02848134282976389\n",
      "Surface training t=27165, loss=0.028764483518898487\n",
      "Surface training t=27166, loss=0.03587385453283787\n",
      "Surface training t=27167, loss=0.028652844950556755\n",
      "Surface training t=27168, loss=0.019369883462786674\n",
      "Surface training t=27169, loss=0.019704698584973812\n",
      "Surface training t=27170, loss=0.01916253799572587\n",
      "Surface training t=27171, loss=0.027680352330207825\n",
      "Surface training t=27172, loss=0.023251130245625973\n",
      "Surface training t=27173, loss=0.028645575046539307\n",
      "Surface training t=27174, loss=0.022379654459655285\n",
      "Surface training t=27175, loss=0.02024602424353361\n",
      "Surface training t=27176, loss=0.019002240151166916\n",
      "Surface training t=27177, loss=0.020743229426443577\n",
      "Surface training t=27178, loss=0.022181913256645203\n",
      "Surface training t=27179, loss=0.016228842083364725\n",
      "Surface training t=27180, loss=0.020283252000808716\n",
      "Surface training t=27181, loss=0.018246960826218128\n",
      "Surface training t=27182, loss=0.018539351411163807\n",
      "Surface training t=27183, loss=0.017706511542201042\n",
      "Surface training t=27184, loss=0.01580321416258812\n",
      "Surface training t=27185, loss=0.015268273651599884\n",
      "Surface training t=27186, loss=0.01712551061064005\n",
      "Surface training t=27187, loss=0.017703401390463114\n",
      "Surface training t=27188, loss=0.018042533192783594\n",
      "Surface training t=27189, loss=0.014855818822979927\n",
      "Surface training t=27190, loss=0.016944500152021646\n",
      "Surface training t=27191, loss=0.016670582350343466\n",
      "Surface training t=27192, loss=0.018234664108604193\n",
      "Surface training t=27193, loss=0.013560605235397816\n",
      "Surface training t=27194, loss=0.015300026163458824\n",
      "Surface training t=27195, loss=0.018271364271640778\n",
      "Surface training t=27196, loss=0.015464586671441793\n",
      "Surface training t=27197, loss=0.013088114559650421\n",
      "Surface training t=27198, loss=0.01757175801321864\n",
      "Surface training t=27199, loss=0.012342299334704876\n",
      "Surface training t=27200, loss=0.014360636938363314\n",
      "Surface training t=27201, loss=0.014784800354391336\n",
      "Surface training t=27202, loss=0.015297492500394583\n",
      "Surface training t=27203, loss=0.014882257208228111\n",
      "Surface training t=27204, loss=0.019424714613705873\n",
      "Surface training t=27205, loss=0.021571682766079903\n",
      "Surface training t=27206, loss=0.01866669626906514\n",
      "Surface training t=27207, loss=0.016732463147491217\n",
      "Surface training t=27208, loss=0.018209893256425858\n",
      "Surface training t=27209, loss=0.01611726824194193\n",
      "Surface training t=27210, loss=0.017265483736991882\n",
      "Surface training t=27211, loss=0.01512649841606617\n",
      "Surface training t=27212, loss=0.01800432987511158\n",
      "Surface training t=27213, loss=0.01794601511210203\n",
      "Surface training t=27214, loss=0.019734550267457962\n",
      "Surface training t=27215, loss=0.013187226839363575\n",
      "Surface training t=27216, loss=0.018080600537359715\n",
      "Surface training t=27217, loss=0.016337723471224308\n",
      "Surface training t=27218, loss=0.016613516956567764\n",
      "Surface training t=27219, loss=0.021035038866102695\n",
      "Surface training t=27220, loss=0.021095504984259605\n",
      "Surface training t=27221, loss=0.026356272399425507\n",
      "Surface training t=27222, loss=0.022814905270934105\n",
      "Surface training t=27223, loss=0.024025672115385532\n",
      "Surface training t=27224, loss=0.022673881612718105\n",
      "Surface training t=27225, loss=0.019073612056672573\n",
      "Surface training t=27226, loss=0.019040411338210106\n",
      "Surface training t=27227, loss=0.015529044903814793\n",
      "Surface training t=27228, loss=0.017277750186622143\n",
      "Surface training t=27229, loss=0.017972830682992935\n",
      "Surface training t=27230, loss=0.014868848491460085\n",
      "Surface training t=27231, loss=0.014374821912497282\n",
      "Surface training t=27232, loss=0.01523851789534092\n",
      "Surface training t=27233, loss=0.018266598228365183\n",
      "Surface training t=27234, loss=0.0240761898458004\n",
      "Surface training t=27235, loss=0.03446296229958534\n",
      "Surface training t=27236, loss=0.02699813712388277\n",
      "Surface training t=27237, loss=0.025961737148463726\n",
      "Surface training t=27238, loss=0.028582402504980564\n",
      "Surface training t=27239, loss=0.021734373178333044\n",
      "Surface training t=27240, loss=0.0197625532746315\n",
      "Surface training t=27241, loss=0.02516904566437006\n",
      "Surface training t=27242, loss=0.02620484121143818\n",
      "Surface training t=27243, loss=0.03290400467813015\n",
      "Surface training t=27244, loss=0.028720738366246223\n",
      "Surface training t=27245, loss=0.02979352604597807\n",
      "Surface training t=27246, loss=0.037352483719587326\n",
      "Surface training t=27247, loss=0.032813175581395626\n",
      "Surface training t=27248, loss=0.036735476925969124\n",
      "Surface training t=27249, loss=0.03734312020242214\n",
      "Surface training t=27250, loss=0.032896898686885834\n",
      "Surface training t=27251, loss=0.03910777345299721\n",
      "Surface training t=27252, loss=0.019638829864561558\n",
      "Surface training t=27253, loss=0.035289518535137177\n",
      "Surface training t=27254, loss=0.03807055205106735\n",
      "Surface training t=27255, loss=0.022276100236922503\n",
      "Surface training t=27256, loss=0.030137816444039345\n",
      "Surface training t=27257, loss=0.025025762617588043\n",
      "Surface training t=27258, loss=0.01998894475400448\n",
      "Surface training t=27259, loss=0.021341828629374504\n",
      "Surface training t=27260, loss=0.017620479222387075\n",
      "Surface training t=27261, loss=0.025737214833498\n",
      "Surface training t=27262, loss=0.029222617857158184\n",
      "Surface training t=27263, loss=0.030136089771986008\n",
      "Surface training t=27264, loss=0.025469188578426838\n",
      "Surface training t=27265, loss=0.028582319617271423\n",
      "Surface training t=27266, loss=0.030864007771015167\n",
      "Surface training t=27267, loss=0.030758455395698547\n",
      "Surface training t=27268, loss=0.028175896033644676\n",
      "Surface training t=27269, loss=0.026287703774869442\n",
      "Surface training t=27270, loss=0.026644466444849968\n",
      "Surface training t=27271, loss=0.029320908710360527\n",
      "Surface training t=27272, loss=0.025981377810239792\n",
      "Surface training t=27273, loss=0.032636442221701145\n",
      "Surface training t=27274, loss=0.03187006711959839\n",
      "Surface training t=27275, loss=0.028221014887094498\n",
      "Surface training t=27276, loss=0.026707978919148445\n",
      "Surface training t=27277, loss=0.02341547980904579\n",
      "Surface training t=27278, loss=0.017713225446641445\n",
      "Surface training t=27279, loss=0.024811169132590294\n",
      "Surface training t=27280, loss=0.02298583835363388\n",
      "Surface training t=27281, loss=0.016271570697426796\n",
      "Surface training t=27282, loss=0.014211400877684355\n",
      "Surface training t=27283, loss=0.01691223355010152\n",
      "Surface training t=27284, loss=0.015449874568730593\n",
      "Surface training t=27285, loss=0.01979330275207758\n",
      "Surface training t=27286, loss=0.01705285534262657\n",
      "Surface training t=27287, loss=0.014552588574588299\n",
      "Surface training t=27288, loss=0.015431524254381657\n",
      "Surface training t=27289, loss=0.020025338511914015\n",
      "Surface training t=27290, loss=0.02098738681524992\n",
      "Surface training t=27291, loss=0.019786175806075335\n",
      "Surface training t=27292, loss=0.023373683914542198\n",
      "Surface training t=27293, loss=0.020261642523109913\n",
      "Surface training t=27294, loss=0.025283189490437508\n",
      "Surface training t=27295, loss=0.02041088044643402\n",
      "Surface training t=27296, loss=0.03531981445848942\n",
      "Surface training t=27297, loss=0.02708583977073431\n",
      "Surface training t=27298, loss=0.023427161388099194\n",
      "Surface training t=27299, loss=0.02357475832104683\n",
      "Surface training t=27300, loss=0.02376740053296089\n",
      "Surface training t=27301, loss=0.02591311652213335\n",
      "Surface training t=27302, loss=0.024023442529141903\n",
      "Surface training t=27303, loss=0.027867245487868786\n",
      "Surface training t=27304, loss=0.02035303460434079\n",
      "Surface training t=27305, loss=0.020118266344070435\n",
      "Surface training t=27306, loss=0.027550656348466873\n",
      "Surface training t=27307, loss=0.024988661520183086\n",
      "Surface training t=27308, loss=0.0376509428024292\n",
      "Surface training t=27309, loss=0.025312436744570732\n",
      "Surface training t=27310, loss=0.024828720837831497\n",
      "Surface training t=27311, loss=0.021590312477201223\n",
      "Surface training t=27312, loss=0.02659896295517683\n",
      "Surface training t=27313, loss=0.020634731277823448\n",
      "Surface training t=27314, loss=0.02231459692120552\n",
      "Surface training t=27315, loss=0.016823233105242252\n",
      "Surface training t=27316, loss=0.018848552368581295\n",
      "Surface training t=27317, loss=0.024995163083076477\n",
      "Surface training t=27318, loss=0.017108998261392117\n",
      "Surface training t=27319, loss=0.015591546893119812\n",
      "Surface training t=27320, loss=0.022314743138849735\n",
      "Surface training t=27321, loss=0.019374564290046692\n",
      "Surface training t=27322, loss=0.022079508751630783\n",
      "Surface training t=27323, loss=0.019384409300982952\n",
      "Surface training t=27324, loss=0.021130884066224098\n",
      "Surface training t=27325, loss=0.017984973266720772\n",
      "Surface training t=27326, loss=0.017655299976468086\n",
      "Surface training t=27327, loss=0.012954581528902054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=27328, loss=0.01619875244796276\n",
      "Surface training t=27329, loss=0.017839260399341583\n",
      "Surface training t=27330, loss=0.020781535655260086\n",
      "Surface training t=27331, loss=0.02943706139922142\n",
      "Surface training t=27332, loss=0.02405325509607792\n",
      "Surface training t=27333, loss=0.01794891245663166\n",
      "Surface training t=27334, loss=0.02379966713488102\n",
      "Surface training t=27335, loss=0.02370339259505272\n",
      "Surface training t=27336, loss=0.018234044313430786\n",
      "Surface training t=27337, loss=0.021983017213642597\n",
      "Surface training t=27338, loss=0.01302828686311841\n",
      "Surface training t=27339, loss=0.017740804702043533\n",
      "Surface training t=27340, loss=0.015965106431394815\n",
      "Surface training t=27341, loss=0.021206701174378395\n",
      "Surface training t=27342, loss=0.02883146796375513\n",
      "Surface training t=27343, loss=0.021403702907264233\n",
      "Surface training t=27344, loss=0.02284698188304901\n",
      "Surface training t=27345, loss=0.025926075875759125\n",
      "Surface training t=27346, loss=0.02639947645366192\n",
      "Surface training t=27347, loss=0.02951642870903015\n",
      "Surface training t=27348, loss=0.026278088800609112\n",
      "Surface training t=27349, loss=0.029002871364355087\n",
      "Surface training t=27350, loss=0.027995468117296696\n",
      "Surface training t=27351, loss=0.031407617032527924\n",
      "Surface training t=27352, loss=0.03394969366490841\n",
      "Surface training t=27353, loss=0.03193740826100111\n",
      "Surface training t=27354, loss=0.0259700370952487\n",
      "Surface training t=27355, loss=0.028207422234117985\n",
      "Surface training t=27356, loss=0.03465267736464739\n",
      "Surface training t=27357, loss=0.029271120205521584\n",
      "Surface training t=27358, loss=0.03198446333408356\n",
      "Surface training t=27359, loss=0.038131202571094036\n",
      "Surface training t=27360, loss=0.04713445343077183\n",
      "Surface training t=27361, loss=0.041956715285778046\n",
      "Surface training t=27362, loss=0.044734152033925056\n",
      "Surface training t=27363, loss=0.02837182767689228\n",
      "Surface training t=27364, loss=0.026464328169822693\n",
      "Surface training t=27365, loss=0.02731501031666994\n",
      "Surface training t=27366, loss=0.02794897835701704\n",
      "Surface training t=27367, loss=0.030832702293992043\n",
      "Surface training t=27368, loss=0.04302582889795303\n",
      "Surface training t=27369, loss=0.026802537962794304\n",
      "Surface training t=27370, loss=0.028618287295103073\n",
      "Surface training t=27371, loss=0.024073190055787563\n",
      "Surface training t=27372, loss=0.02582646068185568\n",
      "Surface training t=27373, loss=0.021744253113865852\n",
      "Surface training t=27374, loss=0.024660624563694\n",
      "Surface training t=27375, loss=0.029963587410748005\n",
      "Surface training t=27376, loss=0.023640201427042484\n",
      "Surface training t=27377, loss=0.02879601437598467\n",
      "Surface training t=27378, loss=0.026252874173223972\n",
      "Surface training t=27379, loss=0.031232940033078194\n",
      "Surface training t=27380, loss=0.031637043692171574\n",
      "Surface training t=27381, loss=0.02723439410328865\n",
      "Surface training t=27382, loss=0.031021817587316036\n",
      "Surface training t=27383, loss=0.028660654090344906\n",
      "Surface training t=27384, loss=0.020626908168196678\n",
      "Surface training t=27385, loss=0.022111801430583\n",
      "Surface training t=27386, loss=0.018803033977746964\n",
      "Surface training t=27387, loss=0.020127364434301853\n",
      "Surface training t=27388, loss=0.0248128529638052\n",
      "Surface training t=27389, loss=0.0212382385507226\n",
      "Surface training t=27390, loss=0.0154878506436944\n",
      "Surface training t=27391, loss=0.021384721621870995\n",
      "Surface training t=27392, loss=0.02297370508313179\n",
      "Surface training t=27393, loss=0.021173156797885895\n",
      "Surface training t=27394, loss=0.016623562667518854\n",
      "Surface training t=27395, loss=0.021635886281728745\n",
      "Surface training t=27396, loss=0.021656861528754234\n",
      "Surface training t=27397, loss=0.022594140842556953\n",
      "Surface training t=27398, loss=0.02175927720963955\n",
      "Surface training t=27399, loss=0.024952242150902748\n",
      "Surface training t=27400, loss=0.023481992073357105\n",
      "Surface training t=27401, loss=0.029020175337791443\n",
      "Surface training t=27402, loss=0.04553719609975815\n",
      "Surface training t=27403, loss=0.03365664556622505\n",
      "Surface training t=27404, loss=0.038867329247295856\n",
      "Surface training t=27405, loss=0.05265112780034542\n",
      "Surface training t=27406, loss=0.038602931424975395\n",
      "Surface training t=27407, loss=0.042015815153717995\n",
      "Surface training t=27408, loss=0.03297707438468933\n",
      "Surface training t=27409, loss=0.024606063030660152\n",
      "Surface training t=27410, loss=0.03064462821930647\n",
      "Surface training t=27411, loss=0.024987089447677135\n",
      "Surface training t=27412, loss=0.02683134377002716\n",
      "Surface training t=27413, loss=0.024533954448997974\n",
      "Surface training t=27414, loss=0.03144642245024443\n",
      "Surface training t=27415, loss=0.0343654602766037\n",
      "Surface training t=27416, loss=0.027632268145680428\n",
      "Surface training t=27417, loss=0.03730032220482826\n",
      "Surface training t=27418, loss=0.03257011342793703\n",
      "Surface training t=27419, loss=0.03500285930931568\n",
      "Surface training t=27420, loss=0.03847227990627289\n",
      "Surface training t=27421, loss=0.029498745687305927\n",
      "Surface training t=27422, loss=0.026977738365530968\n",
      "Surface training t=27423, loss=0.027716683223843575\n",
      "Surface training t=27424, loss=0.024860107339918613\n",
      "Surface training t=27425, loss=0.02341560460627079\n",
      "Surface training t=27426, loss=0.031268646009266376\n",
      "Surface training t=27427, loss=0.026612896472215652\n",
      "Surface training t=27428, loss=0.030398515053093433\n",
      "Surface training t=27429, loss=0.031718840822577477\n",
      "Surface training t=27430, loss=0.02487902995198965\n",
      "Surface training t=27431, loss=0.03257869090884924\n",
      "Surface training t=27432, loss=0.026884869672358036\n",
      "Surface training t=27433, loss=0.028188947588205338\n",
      "Surface training t=27434, loss=0.02367432415485382\n",
      "Surface training t=27435, loss=0.021913782693445683\n",
      "Surface training t=27436, loss=0.019847994670271873\n",
      "Surface training t=27437, loss=0.0176002262160182\n",
      "Surface training t=27438, loss=0.020040458999574184\n",
      "Surface training t=27439, loss=0.018240579403936863\n",
      "Surface training t=27440, loss=0.02564875688403845\n",
      "Surface training t=27441, loss=0.017583515495061874\n",
      "Surface training t=27442, loss=0.024038922041654587\n",
      "Surface training t=27443, loss=0.023785293102264404\n",
      "Surface training t=27444, loss=0.02155377622693777\n",
      "Surface training t=27445, loss=0.023713963106274605\n",
      "Surface training t=27446, loss=0.019811255857348442\n",
      "Surface training t=27447, loss=0.01912139868363738\n",
      "Surface training t=27448, loss=0.021409770473837852\n",
      "Surface training t=27449, loss=0.025285822339355946\n",
      "Surface training t=27450, loss=0.016642923932522535\n",
      "Surface training t=27451, loss=0.019622843712568283\n",
      "Surface training t=27452, loss=0.022115147672593594\n",
      "Surface training t=27453, loss=0.018512561917304993\n",
      "Surface training t=27454, loss=0.01748052053153515\n",
      "Surface training t=27455, loss=0.016586003825068474\n",
      "Surface training t=27456, loss=0.023657958023250103\n",
      "Surface training t=27457, loss=0.018181036226451397\n",
      "Surface training t=27458, loss=0.023413430899381638\n",
      "Surface training t=27459, loss=0.025644666515290737\n",
      "Surface training t=27460, loss=0.03149003908038139\n",
      "Surface training t=27461, loss=0.03415648452937603\n",
      "Surface training t=27462, loss=0.028202246874570847\n",
      "Surface training t=27463, loss=0.03186020255088806\n",
      "Surface training t=27464, loss=0.03967735730111599\n",
      "Surface training t=27465, loss=0.03148399665951729\n",
      "Surface training t=27466, loss=0.028442558832466602\n",
      "Surface training t=27467, loss=0.03315340355038643\n",
      "Surface training t=27468, loss=0.031116507947444916\n",
      "Surface training t=27469, loss=0.027708529494702816\n",
      "Surface training t=27470, loss=0.024378793314099312\n",
      "Surface training t=27471, loss=0.023650528863072395\n",
      "Surface training t=27472, loss=0.020041923969984055\n",
      "Surface training t=27473, loss=0.01886661071330309\n",
      "Surface training t=27474, loss=0.018708757124841213\n",
      "Surface training t=27475, loss=0.01681705843657255\n",
      "Surface training t=27476, loss=0.015970204025506973\n",
      "Surface training t=27477, loss=0.017453642562031746\n",
      "Surface training t=27478, loss=0.016639928799122572\n",
      "Surface training t=27479, loss=0.017300275154411793\n",
      "Surface training t=27480, loss=0.017789042554795742\n",
      "Surface training t=27481, loss=0.01909061335027218\n",
      "Surface training t=27482, loss=0.024962020106613636\n",
      "Surface training t=27483, loss=0.026021823287010193\n",
      "Surface training t=27484, loss=0.02077129576355219\n",
      "Surface training t=27485, loss=0.021209019236266613\n",
      "Surface training t=27486, loss=0.020011325366795063\n",
      "Surface training t=27487, loss=0.01860957033932209\n",
      "Surface training t=27488, loss=0.02091360744088888\n",
      "Surface training t=27489, loss=0.024840116500854492\n",
      "Surface training t=27490, loss=0.026633317582309246\n",
      "Surface training t=27491, loss=0.02464572060853243\n",
      "Surface training t=27492, loss=0.02485833689570427\n",
      "Surface training t=27493, loss=0.03659561835229397\n",
      "Surface training t=27494, loss=0.02783789113163948\n",
      "Surface training t=27495, loss=0.026542527601122856\n",
      "Surface training t=27496, loss=0.022600959055125713\n",
      "Surface training t=27497, loss=0.0464529674500227\n",
      "Surface training t=27498, loss=0.0341974887996912\n",
      "Surface training t=27499, loss=0.03245009761303663\n",
      "Surface training t=27500, loss=0.031664587557315826\n",
      "Surface training t=27501, loss=0.03128077834844589\n",
      "Surface training t=27502, loss=0.03326734434813261\n",
      "Surface training t=27503, loss=0.02980763278901577\n",
      "Surface training t=27504, loss=0.02598385140299797\n",
      "Surface training t=27505, loss=0.023463495075702667\n",
      "Surface training t=27506, loss=0.02483850996941328\n",
      "Surface training t=27507, loss=0.027637934312224388\n",
      "Surface training t=27508, loss=0.03720096871256828\n",
      "Surface training t=27509, loss=0.030885386280715466\n",
      "Surface training t=27510, loss=0.028961360454559326\n",
      "Surface training t=27511, loss=0.029011116363108158\n",
      "Surface training t=27512, loss=0.03293787222355604\n",
      "Surface training t=27513, loss=0.04232562705874443\n",
      "Surface training t=27514, loss=0.03225140366703272\n",
      "Surface training t=27515, loss=0.0351030332967639\n",
      "Surface training t=27516, loss=0.043824756518006325\n",
      "Surface training t=27517, loss=0.03847781755030155\n",
      "Surface training t=27518, loss=0.043053628876805305\n",
      "Surface training t=27519, loss=0.04301728308200836\n",
      "Surface training t=27520, loss=0.03651002049446106\n",
      "Surface training t=27521, loss=0.054348548874258995\n",
      "Surface training t=27522, loss=0.035922364331781864\n",
      "Surface training t=27523, loss=0.0344202509149909\n",
      "Surface training t=27524, loss=0.04320163652300835\n",
      "Surface training t=27525, loss=0.03493589907884598\n",
      "Surface training t=27526, loss=0.031055202707648277\n",
      "Surface training t=27527, loss=0.037316739559173584\n",
      "Surface training t=27528, loss=0.039421225897967815\n",
      "Surface training t=27529, loss=0.030514923855662346\n",
      "Surface training t=27530, loss=0.030446825549006462\n",
      "Surface training t=27531, loss=0.03101487271487713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=27532, loss=0.033823675476014614\n",
      "Surface training t=27533, loss=0.025845607742667198\n",
      "Surface training t=27534, loss=0.02556120976805687\n",
      "Surface training t=27535, loss=0.021181855350732803\n",
      "Surface training t=27536, loss=0.022724458947777748\n",
      "Surface training t=27537, loss=0.0272762319073081\n",
      "Surface training t=27538, loss=0.02494935132563114\n",
      "Surface training t=27539, loss=0.01769753359258175\n",
      "Surface training t=27540, loss=0.022037711925804615\n",
      "Surface training t=27541, loss=0.025599914602935314\n",
      "Surface training t=27542, loss=0.025105783715844154\n",
      "Surface training t=27543, loss=0.02205847203731537\n",
      "Surface training t=27544, loss=0.01929630059748888\n",
      "Surface training t=27545, loss=0.02563906740397215\n",
      "Surface training t=27546, loss=0.021886282600462437\n",
      "Surface training t=27547, loss=0.014200064353644848\n",
      "Surface training t=27548, loss=0.017213867511600256\n",
      "Surface training t=27549, loss=0.014055718667805195\n",
      "Surface training t=27550, loss=0.01817764900624752\n",
      "Surface training t=27551, loss=0.018420962616801262\n",
      "Surface training t=27552, loss=0.015113579109311104\n",
      "Surface training t=27553, loss=0.01907999347895384\n",
      "Surface training t=27554, loss=0.01640621153637767\n",
      "Surface training t=27555, loss=0.017783456947654486\n",
      "Surface training t=27556, loss=0.02114791516214609\n",
      "Surface training t=27557, loss=0.018827810883522034\n",
      "Surface training t=27558, loss=0.014946897979825735\n",
      "Surface training t=27559, loss=0.014845522586256266\n",
      "Surface training t=27560, loss=0.01250487333163619\n",
      "Surface training t=27561, loss=0.014103967230767012\n",
      "Surface training t=27562, loss=0.016436648555099964\n",
      "Surface training t=27563, loss=0.01728043332695961\n",
      "Surface training t=27564, loss=0.01574015012010932\n",
      "Surface training t=27565, loss=0.01938761491328478\n",
      "Surface training t=27566, loss=0.016293358523398638\n",
      "Surface training t=27567, loss=0.016342798247933388\n",
      "Surface training t=27568, loss=0.01566800568252802\n",
      "Surface training t=27569, loss=0.01943073607981205\n",
      "Surface training t=27570, loss=0.02506289817392826\n",
      "Surface training t=27571, loss=0.02413194254040718\n",
      "Surface training t=27572, loss=0.020073245279490948\n",
      "Surface training t=27573, loss=0.022681422531604767\n",
      "Surface training t=27574, loss=0.034473735839128494\n",
      "Surface training t=27575, loss=0.022014685906469822\n",
      "Surface training t=27576, loss=0.02011918742209673\n",
      "Surface training t=27577, loss=0.02965667936950922\n",
      "Surface training t=27578, loss=0.019982969388365746\n",
      "Surface training t=27579, loss=0.020910647697746754\n",
      "Surface training t=27580, loss=0.0198250412940979\n",
      "Surface training t=27581, loss=0.01730494387447834\n",
      "Surface training t=27582, loss=0.018935075029730797\n",
      "Surface training t=27583, loss=0.018761964049190283\n",
      "Surface training t=27584, loss=0.023100978694856167\n",
      "Surface training t=27585, loss=0.01866268366575241\n",
      "Surface training t=27586, loss=0.016381744295358658\n",
      "Surface training t=27587, loss=0.022266699001193047\n",
      "Surface training t=27588, loss=0.020988398231565952\n",
      "Surface training t=27589, loss=0.026298335753381252\n",
      "Surface training t=27590, loss=0.022148409858345985\n",
      "Surface training t=27591, loss=0.019156454131007195\n",
      "Surface training t=27592, loss=0.021106651052832603\n",
      "Surface training t=27593, loss=0.021456480957567692\n",
      "Surface training t=27594, loss=0.022961697541177273\n",
      "Surface training t=27595, loss=0.023631194606423378\n",
      "Surface training t=27596, loss=0.028424755670130253\n",
      "Surface training t=27597, loss=0.023623822256922722\n",
      "Surface training t=27598, loss=0.02102866768836975\n",
      "Surface training t=27599, loss=0.021031079813838005\n",
      "Surface training t=27600, loss=0.026027205400168896\n",
      "Surface training t=27601, loss=0.020000260323286057\n",
      "Surface training t=27602, loss=0.027808508835732937\n",
      "Surface training t=27603, loss=0.03310745768249035\n",
      "Surface training t=27604, loss=0.02504836954176426\n",
      "Surface training t=27605, loss=0.026228894479572773\n",
      "Surface training t=27606, loss=0.029531802982091904\n",
      "Surface training t=27607, loss=0.030450335703790188\n",
      "Surface training t=27608, loss=0.037206098437309265\n",
      "Surface training t=27609, loss=0.025908704847097397\n",
      "Surface training t=27610, loss=0.02871674206107855\n",
      "Surface training t=27611, loss=0.03519104793667793\n",
      "Surface training t=27612, loss=0.03664008900523186\n",
      "Surface training t=27613, loss=0.03178275469690561\n",
      "Surface training t=27614, loss=0.036289600655436516\n",
      "Surface training t=27615, loss=0.04071152210235596\n",
      "Surface training t=27616, loss=0.024769517593085766\n",
      "Surface training t=27617, loss=0.027220592834055424\n",
      "Surface training t=27618, loss=0.024196342565119267\n",
      "Surface training t=27619, loss=0.024138161912560463\n",
      "Surface training t=27620, loss=0.02139168232679367\n",
      "Surface training t=27621, loss=0.022809339687228203\n",
      "Surface training t=27622, loss=0.025728371925652027\n",
      "Surface training t=27623, loss=0.027219736017286777\n",
      "Surface training t=27624, loss=0.03278158977627754\n",
      "Surface training t=27625, loss=0.03836419619619846\n",
      "Surface training t=27626, loss=0.030689483508467674\n",
      "Surface training t=27627, loss=0.02965718973428011\n",
      "Surface training t=27628, loss=0.027692988514900208\n",
      "Surface training t=27629, loss=0.04197176545858383\n",
      "Surface training t=27630, loss=0.028978480957448483\n",
      "Surface training t=27631, loss=0.028860002756118774\n",
      "Surface training t=27632, loss=0.02871001325547695\n",
      "Surface training t=27633, loss=0.03319589979946613\n",
      "Surface training t=27634, loss=0.022108733654022217\n",
      "Surface training t=27635, loss=0.019987761974334717\n",
      "Surface training t=27636, loss=0.02278977632522583\n",
      "Surface training t=27637, loss=0.022153626196086407\n",
      "Surface training t=27638, loss=0.023700066842138767\n",
      "Surface training t=27639, loss=0.020334268920123577\n",
      "Surface training t=27640, loss=0.023726667277514935\n",
      "Surface training t=27641, loss=0.03066575899720192\n",
      "Surface training t=27642, loss=0.029322735033929348\n",
      "Surface training t=27643, loss=0.026069742627441883\n",
      "Surface training t=27644, loss=0.02846788614988327\n",
      "Surface training t=27645, loss=0.036650851368904114\n",
      "Surface training t=27646, loss=0.035925730131566525\n",
      "Surface training t=27647, loss=0.03102933708578348\n",
      "Surface training t=27648, loss=0.03729112446308136\n",
      "Surface training t=27649, loss=0.03453810699284077\n",
      "Surface training t=27650, loss=0.025815204717218876\n",
      "Surface training t=27651, loss=0.033330341801047325\n",
      "Surface training t=27652, loss=0.023055095225572586\n",
      "Surface training t=27653, loss=0.024371705949306488\n",
      "Surface training t=27654, loss=0.027410367503762245\n",
      "Surface training t=27655, loss=0.03116979543119669\n",
      "Surface training t=27656, loss=0.03198301047086716\n",
      "Surface training t=27657, loss=0.028803536668419838\n",
      "Surface training t=27658, loss=0.020138714462518692\n",
      "Surface training t=27659, loss=0.02656687516719103\n",
      "Surface training t=27660, loss=0.020704824943095446\n",
      "Surface training t=27661, loss=0.02166650164872408\n",
      "Surface training t=27662, loss=0.020406333729624748\n",
      "Surface training t=27663, loss=0.01719912327826023\n",
      "Surface training t=27664, loss=0.021952235139906406\n",
      "Surface training t=27665, loss=0.018605020828545094\n",
      "Surface training t=27666, loss=0.016489296220242977\n",
      "Surface training t=27667, loss=0.013884998857975006\n",
      "Surface training t=27668, loss=0.022592135705053806\n",
      "Surface training t=27669, loss=0.016752548050135374\n",
      "Surface training t=27670, loss=0.01654177764430642\n",
      "Surface training t=27671, loss=0.018464921042323112\n",
      "Surface training t=27672, loss=0.01688528200611472\n",
      "Surface training t=27673, loss=0.015693020075559616\n",
      "Surface training t=27674, loss=0.01931553054600954\n",
      "Surface training t=27675, loss=0.020270667038857937\n",
      "Surface training t=27676, loss=0.020072161220014095\n",
      "Surface training t=27677, loss=0.020112419500947\n",
      "Surface training t=27678, loss=0.018245533108711243\n",
      "Surface training t=27679, loss=0.02290021162480116\n",
      "Surface training t=27680, loss=0.02305900678038597\n",
      "Surface training t=27681, loss=0.025617968291044235\n",
      "Surface training t=27682, loss=0.036051319912075996\n",
      "Surface training t=27683, loss=0.02457005623728037\n",
      "Surface training t=27684, loss=0.029672074131667614\n",
      "Surface training t=27685, loss=0.043041354045271873\n",
      "Surface training t=27686, loss=0.0258781174197793\n",
      "Surface training t=27687, loss=0.04686926677823067\n",
      "Surface training t=27688, loss=0.034935591742396355\n",
      "Surface training t=27689, loss=0.03867172822356224\n",
      "Surface training t=27690, loss=0.03415774833410978\n",
      "Surface training t=27691, loss=0.0519380122423172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=27692, loss=0.03967718593776226\n",
      "Surface training t=27693, loss=0.05771147459745407\n",
      "Surface training t=27694, loss=0.038613857701420784\n",
      "Surface training t=27695, loss=0.05227992683649063\n",
      "Surface training t=27696, loss=0.04634195379912853\n",
      "Surface training t=27697, loss=0.036512838676571846\n",
      "Surface training t=27698, loss=0.039096809923648834\n",
      "Surface training t=27699, loss=0.034955878742039204\n",
      "Surface training t=27700, loss=0.03890870697796345\n",
      "Surface training t=27701, loss=0.032815489917993546\n",
      "Surface training t=27702, loss=0.0301731638610363\n",
      "Surface training t=27703, loss=0.03554338030517101\n",
      "Surface training t=27704, loss=0.022958800196647644\n",
      "Surface training t=27705, loss=0.026659897528588772\n",
      "Surface training t=27706, loss=0.025862937793135643\n",
      "Surface training t=27707, loss=0.01953030563890934\n",
      "Surface training t=27708, loss=0.024419800378382206\n",
      "Surface training t=27709, loss=0.0238149706274271\n",
      "Surface training t=27710, loss=0.02517937868833542\n",
      "Surface training t=27711, loss=0.02467372454702854\n",
      "Surface training t=27712, loss=0.022227453999221325\n",
      "Surface training t=27713, loss=0.023088273592293262\n",
      "Surface training t=27714, loss=0.024269741028547287\n",
      "Surface training t=27715, loss=0.025420943275094032\n",
      "Surface training t=27716, loss=0.018930944614112377\n",
      "Surface training t=27717, loss=0.024259960278868675\n",
      "Surface training t=27718, loss=0.021916531957685947\n",
      "Surface training t=27719, loss=0.021238491870462894\n",
      "Surface training t=27720, loss=0.026083238422870636\n",
      "Surface training t=27721, loss=0.0173262981697917\n",
      "Surface training t=27722, loss=0.02028175164014101\n",
      "Surface training t=27723, loss=0.015611376147717237\n",
      "Surface training t=27724, loss=0.016986634116619825\n",
      "Surface training t=27725, loss=0.018363346345722675\n",
      "Surface training t=27726, loss=0.019854577258229256\n",
      "Surface training t=27727, loss=0.015876919962465763\n",
      "Surface training t=27728, loss=0.01831453340128064\n",
      "Surface training t=27729, loss=0.017765552271157503\n",
      "Surface training t=27730, loss=0.02132310252636671\n",
      "Surface training t=27731, loss=0.019529791548848152\n",
      "Surface training t=27732, loss=0.018379258923232555\n",
      "Surface training t=27733, loss=0.01881435513496399\n",
      "Surface training t=27734, loss=0.017917927354574203\n",
      "Surface training t=27735, loss=0.020649433135986328\n",
      "Surface training t=27736, loss=0.017176369205117226\n",
      "Surface training t=27737, loss=0.02745706867426634\n",
      "Surface training t=27738, loss=0.03550564870238304\n",
      "Surface training t=27739, loss=0.02766087558120489\n",
      "Surface training t=27740, loss=0.025548603385686874\n",
      "Surface training t=27741, loss=0.02634145226329565\n",
      "Surface training t=27742, loss=0.030413244850933552\n",
      "Surface training t=27743, loss=0.022587095387279987\n",
      "Surface training t=27744, loss=0.016776185482740402\n",
      "Surface training t=27745, loss=0.018879457842558622\n",
      "Surface training t=27746, loss=0.021272472105920315\n",
      "Surface training t=27747, loss=0.02584797516465187\n",
      "Surface training t=27748, loss=0.02585239615291357\n",
      "Surface training t=27749, loss=0.02290162444114685\n",
      "Surface training t=27750, loss=0.01696810405701399\n",
      "Surface training t=27751, loss=0.02141878567636013\n",
      "Surface training t=27752, loss=0.019860584288835526\n",
      "Surface training t=27753, loss=0.020084958523511887\n",
      "Surface training t=27754, loss=0.01819978840649128\n",
      "Surface training t=27755, loss=0.01772918738424778\n",
      "Surface training t=27756, loss=0.015797445084899664\n",
      "Surface training t=27757, loss=0.016391413286328316\n",
      "Surface training t=27758, loss=0.015203487128019333\n",
      "Surface training t=27759, loss=0.012730329297482967\n",
      "Surface training t=27760, loss=0.016211941838264465\n",
      "Surface training t=27761, loss=0.01586012588813901\n",
      "Surface training t=27762, loss=0.011451228987425566\n",
      "Surface training t=27763, loss=0.012670733965933323\n",
      "Surface training t=27764, loss=0.012006219942122698\n",
      "Surface training t=27765, loss=0.013614431023597717\n",
      "Surface training t=27766, loss=0.01922037359327078\n",
      "Surface training t=27767, loss=0.019605416804552078\n",
      "Surface training t=27768, loss=0.02229171898216009\n",
      "Surface training t=27769, loss=0.018930611200630665\n",
      "Surface training t=27770, loss=0.021808608435094357\n",
      "Surface training t=27771, loss=0.019729805178940296\n",
      "Surface training t=27772, loss=0.020530777983367443\n",
      "Surface training t=27773, loss=0.022723968140780926\n",
      "Surface training t=27774, loss=0.019933415576815605\n",
      "Surface training t=27775, loss=0.01565652433782816\n",
      "Surface training t=27776, loss=0.022424189373850822\n",
      "Surface training t=27777, loss=0.01942208968102932\n",
      "Surface training t=27778, loss=0.015225164126604795\n",
      "Surface training t=27779, loss=0.018882863223552704\n",
      "Surface training t=27780, loss=0.013297637924551964\n",
      "Surface training t=27781, loss=0.011899566277861595\n",
      "Surface training t=27782, loss=0.017691995948553085\n",
      "Surface training t=27783, loss=0.017862148582935333\n",
      "Surface training t=27784, loss=0.018384930677711964\n",
      "Surface training t=27785, loss=0.01380149181932211\n",
      "Surface training t=27786, loss=0.016068127937614918\n",
      "Surface training t=27787, loss=0.019159282092005014\n",
      "Surface training t=27788, loss=0.02095959521830082\n",
      "Surface training t=27789, loss=0.022713077254593372\n",
      "Surface training t=27790, loss=0.01899679657071829\n",
      "Surface training t=27791, loss=0.02094663865864277\n",
      "Surface training t=27792, loss=0.023240447975695133\n",
      "Surface training t=27793, loss=0.02262937743216753\n",
      "Surface training t=27794, loss=0.02049626223742962\n",
      "Surface training t=27795, loss=0.0245577497407794\n",
      "Surface training t=27796, loss=0.030881470069289207\n",
      "Surface training t=27797, loss=0.022994245402514935\n",
      "Surface training t=27798, loss=0.01737096207216382\n",
      "Surface training t=27799, loss=0.014996539801359177\n",
      "Surface training t=27800, loss=0.015538581181317568\n",
      "Surface training t=27801, loss=0.01868976652622223\n",
      "Surface training t=27802, loss=0.01757709216326475\n",
      "Surface training t=27803, loss=0.0186653733253479\n",
      "Surface training t=27804, loss=0.016629527788609266\n",
      "Surface training t=27805, loss=0.017661916092038155\n",
      "Surface training t=27806, loss=0.01923803798854351\n",
      "Surface training t=27807, loss=0.01800133939832449\n",
      "Surface training t=27808, loss=0.020657941699028015\n",
      "Surface training t=27809, loss=0.02278857585042715\n",
      "Surface training t=27810, loss=0.01802858244627714\n",
      "Surface training t=27811, loss=0.018208280205726624\n",
      "Surface training t=27812, loss=0.023249113000929356\n",
      "Surface training t=27813, loss=0.026061560958623886\n",
      "Surface training t=27814, loss=0.0309145487844944\n",
      "Surface training t=27815, loss=0.029599986970424652\n",
      "Surface training t=27816, loss=0.037378303706645966\n",
      "Surface training t=27817, loss=0.029855075292289257\n",
      "Surface training t=27818, loss=0.028797468170523643\n",
      "Surface training t=27819, loss=0.02543602790683508\n",
      "Surface training t=27820, loss=0.02611719351261854\n",
      "Surface training t=27821, loss=0.032617468386888504\n",
      "Surface training t=27822, loss=0.04049724526703358\n",
      "Surface training t=27823, loss=0.032838018611073494\n",
      "Surface training t=27824, loss=0.026548249647021294\n",
      "Surface training t=27825, loss=0.02524299919605255\n",
      "Surface training t=27826, loss=0.019839409738779068\n",
      "Surface training t=27827, loss=0.02158559113740921\n",
      "Surface training t=27828, loss=0.018674843478947878\n",
      "Surface training t=27829, loss=0.018878964707255363\n",
      "Surface training t=27830, loss=0.020347709767520428\n",
      "Surface training t=27831, loss=0.022860906086862087\n",
      "Surface training t=27832, loss=0.02234868612140417\n",
      "Surface training t=27833, loss=0.02193572837859392\n",
      "Surface training t=27834, loss=0.023288926109671593\n",
      "Surface training t=27835, loss=0.01957537606358528\n",
      "Surface training t=27836, loss=0.013686774298548698\n",
      "Surface training t=27837, loss=0.014135031495243311\n",
      "Surface training t=27838, loss=0.02292358409613371\n",
      "Surface training t=27839, loss=0.016993090510368347\n",
      "Surface training t=27840, loss=0.017466534860432148\n",
      "Surface training t=27841, loss=0.023673677816987038\n",
      "Surface training t=27842, loss=0.027330839075148106\n",
      "Surface training t=27843, loss=0.02736519742757082\n",
      "Surface training t=27844, loss=0.03427213430404663\n",
      "Surface training t=27845, loss=0.023047962225973606\n",
      "Surface training t=27846, loss=0.027602079324424267\n",
      "Surface training t=27847, loss=0.027719199657440186\n",
      "Surface training t=27848, loss=0.02523938100785017\n",
      "Surface training t=27849, loss=0.01903735101222992\n",
      "Surface training t=27850, loss=0.02011894714087248\n",
      "Surface training t=27851, loss=0.0182129954919219\n",
      "Surface training t=27852, loss=0.01984396670013666\n",
      "Surface training t=27853, loss=0.02153566386550665\n",
      "Surface training t=27854, loss=0.021983183920383453\n",
      "Surface training t=27855, loss=0.024035131558775902\n",
      "Surface training t=27856, loss=0.03576613962650299\n",
      "Surface training t=27857, loss=0.02572616096585989\n",
      "Surface training t=27858, loss=0.024806346744298935\n",
      "Surface training t=27859, loss=0.024001714773476124\n",
      "Surface training t=27860, loss=0.02614598535001278\n",
      "Surface training t=27861, loss=0.029970331117510796\n",
      "Surface training t=27862, loss=0.024565255269408226\n",
      "Surface training t=27863, loss=0.024011804722249508\n",
      "Surface training t=27864, loss=0.029707045294344425\n",
      "Surface training t=27865, loss=0.02130914945155382\n",
      "Surface training t=27866, loss=0.030886315740644932\n",
      "Surface training t=27867, loss=0.029231992550194263\n",
      "Surface training t=27868, loss=0.02021510200574994\n",
      "Surface training t=27869, loss=0.02578588481992483\n",
      "Surface training t=27870, loss=0.02700077835470438\n",
      "Surface training t=27871, loss=0.019786246120929718\n",
      "Surface training t=27872, loss=0.0206136591732502\n",
      "Surface training t=27873, loss=0.026339896023273468\n",
      "Surface training t=27874, loss=0.022096898406744003\n",
      "Surface training t=27875, loss=0.020751692354679108\n",
      "Surface training t=27876, loss=0.02635880745947361\n",
      "Surface training t=27877, loss=0.019852428697049618\n",
      "Surface training t=27878, loss=0.020567418076097965\n",
      "Surface training t=27879, loss=0.021975496783852577\n",
      "Surface training t=27880, loss=0.02636505290865898\n",
      "Surface training t=27881, loss=0.03130409959703684\n",
      "Surface training t=27882, loss=0.023068467155098915\n",
      "Surface training t=27883, loss=0.029785198159515858\n",
      "Surface training t=27884, loss=0.025547053664922714\n",
      "Surface training t=27885, loss=0.028697548434138298\n",
      "Surface training t=27886, loss=0.02800230961292982\n",
      "Surface training t=27887, loss=0.02995684463530779\n",
      "Surface training t=27888, loss=0.02924669999629259\n",
      "Surface training t=27889, loss=0.022009071428328753\n",
      "Surface training t=27890, loss=0.026006869971752167\n",
      "Surface training t=27891, loss=0.027753964066505432\n",
      "Surface training t=27892, loss=0.02979167178273201\n",
      "Surface training t=27893, loss=0.023088298738002777\n",
      "Surface training t=27894, loss=0.028509536758065224\n",
      "Surface training t=27895, loss=0.02254902385175228\n",
      "Surface training t=27896, loss=0.02179639134556055\n",
      "Surface training t=27897, loss=0.01934468001127243\n",
      "Surface training t=27898, loss=0.01681079575791955\n",
      "Surface training t=27899, loss=0.020569047890603542\n",
      "Surface training t=27900, loss=0.01470192614942789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=27901, loss=0.021372588351368904\n",
      "Surface training t=27902, loss=0.021837199572473764\n",
      "Surface training t=27903, loss=0.02075171936303377\n",
      "Surface training t=27904, loss=0.02700035460293293\n",
      "Surface training t=27905, loss=0.01880104187875986\n",
      "Surface training t=27906, loss=0.022816898301243782\n",
      "Surface training t=27907, loss=0.018662788905203342\n",
      "Surface training t=27908, loss=0.024421505630016327\n",
      "Surface training t=27909, loss=0.02235241886228323\n",
      "Surface training t=27910, loss=0.024072719737887383\n",
      "Surface training t=27911, loss=0.03099672496318817\n",
      "Surface training t=27912, loss=0.023998428136110306\n",
      "Surface training t=27913, loss=0.03409311827272177\n",
      "Surface training t=27914, loss=0.02389940246939659\n",
      "Surface training t=27915, loss=0.022460326552391052\n",
      "Surface training t=27916, loss=0.015497073531150818\n",
      "Surface training t=27917, loss=0.02567758411169052\n",
      "Surface training t=27918, loss=0.02816982287913561\n",
      "Surface training t=27919, loss=0.028643546625971794\n",
      "Surface training t=27920, loss=0.028104216791689396\n",
      "Surface training t=27921, loss=0.02242510300129652\n",
      "Surface training t=27922, loss=0.026224260218441486\n",
      "Surface training t=27923, loss=0.028800517320632935\n",
      "Surface training t=27924, loss=0.0309563297778368\n",
      "Surface training t=27925, loss=0.03823411837220192\n",
      "Surface training t=27926, loss=0.027575738728046417\n",
      "Surface training t=27927, loss=0.030506417155265808\n",
      "Surface training t=27928, loss=0.020754418335855007\n",
      "Surface training t=27929, loss=0.023714538663625717\n",
      "Surface training t=27930, loss=0.022410983219742775\n",
      "Surface training t=27931, loss=0.02062212210148573\n",
      "Surface training t=27932, loss=0.024884073995053768\n",
      "Surface training t=27933, loss=0.018782068975269794\n",
      "Surface training t=27934, loss=0.025331120938062668\n",
      "Surface training t=27935, loss=0.023727376013994217\n",
      "Surface training t=27936, loss=0.024141091853380203\n",
      "Surface training t=27937, loss=0.024910394102334976\n",
      "Surface training t=27938, loss=0.022720681503415108\n",
      "Surface training t=27939, loss=0.018510231748223305\n",
      "Surface training t=27940, loss=0.02481363620609045\n",
      "Surface training t=27941, loss=0.031308574602007866\n",
      "Surface training t=27942, loss=0.023144565522670746\n",
      "Surface training t=27943, loss=0.02558143436908722\n",
      "Surface training t=27944, loss=0.02433053497225046\n",
      "Surface training t=27945, loss=0.02457443531602621\n",
      "Surface training t=27946, loss=0.023628666065633297\n",
      "Surface training t=27947, loss=0.02032705768942833\n",
      "Surface training t=27948, loss=0.02381414081901312\n",
      "Surface training t=27949, loss=0.022939239628612995\n",
      "Surface training t=27950, loss=0.02276193629950285\n",
      "Surface training t=27951, loss=0.024547766894102097\n",
      "Surface training t=27952, loss=0.016321507282555103\n",
      "Surface training t=27953, loss=0.021261987276375294\n",
      "Surface training t=27954, loss=0.022769734263420105\n",
      "Surface training t=27955, loss=0.01672629965469241\n",
      "Surface training t=27956, loss=0.017359154298901558\n",
      "Surface training t=27957, loss=0.016236302442848682\n",
      "Surface training t=27958, loss=0.015082235913723707\n",
      "Surface training t=27959, loss=0.018910993821918964\n",
      "Surface training t=27960, loss=0.016840368509292603\n",
      "Surface training t=27961, loss=0.019546015188097954\n",
      "Surface training t=27962, loss=0.018057281151413918\n",
      "Surface training t=27963, loss=0.019603434018790722\n",
      "Surface training t=27964, loss=0.02013322990387678\n",
      "Surface training t=27965, loss=0.01885530725121498\n",
      "Surface training t=27966, loss=0.01576521061360836\n",
      "Surface training t=27967, loss=0.018640348687767982\n",
      "Surface training t=27968, loss=0.02077093906700611\n",
      "Surface training t=27969, loss=0.016065950505435467\n",
      "Surface training t=27970, loss=0.01891932962462306\n",
      "Surface training t=27971, loss=0.02038305252790451\n",
      "Surface training t=27972, loss=0.02202140912413597\n",
      "Surface training t=27973, loss=0.023661119863390923\n",
      "Surface training t=27974, loss=0.01841630879789591\n",
      "Surface training t=27975, loss=0.015567161608487368\n",
      "Surface training t=27976, loss=0.017491887789219618\n",
      "Surface training t=27977, loss=0.018183264415711164\n",
      "Surface training t=27978, loss=0.01736588589847088\n",
      "Surface training t=27979, loss=0.024119596928358078\n",
      "Surface training t=27980, loss=0.025150740519165993\n",
      "Surface training t=27981, loss=0.02589964959770441\n",
      "Surface training t=27982, loss=0.016735355369746685\n",
      "Surface training t=27983, loss=0.01874187681823969\n",
      "Surface training t=27984, loss=0.02102035004645586\n",
      "Surface training t=27985, loss=0.018520635552704334\n",
      "Surface training t=27986, loss=0.018941730260849\n",
      "Surface training t=27987, loss=0.018794888630509377\n",
      "Surface training t=27988, loss=0.015439957845956087\n",
      "Surface training t=27989, loss=0.01726646814495325\n",
      "Surface training t=27990, loss=0.020062658935785294\n",
      "Surface training t=27991, loss=0.02372359298169613\n",
      "Surface training t=27992, loss=0.023202046751976013\n",
      "Surface training t=27993, loss=0.018617709167301655\n",
      "Surface training t=27994, loss=0.015895187854766846\n",
      "Surface training t=27995, loss=0.03239543177187443\n",
      "Surface training t=27996, loss=0.022249859757721424\n",
      "Surface training t=27997, loss=0.03310737106949091\n",
      "Surface training t=27998, loss=0.02785121463239193\n",
      "Surface training t=27999, loss=0.021570284850895405\n",
      "Surface training t=28000, loss=0.0265297656878829\n",
      "Surface training t=28001, loss=0.02042776346206665\n",
      "Surface training t=28002, loss=0.016167878173291683\n",
      "Surface training t=28003, loss=0.023497212678194046\n",
      "Surface training t=28004, loss=0.01564285345375538\n",
      "Surface training t=28005, loss=0.016470883507281542\n",
      "Surface training t=28006, loss=0.01830616220831871\n",
      "Surface training t=28007, loss=0.019521400332450867\n",
      "Surface training t=28008, loss=0.021055053919553757\n",
      "Surface training t=28009, loss=0.01618221588432789\n",
      "Surface training t=28010, loss=0.018612460233271122\n",
      "Surface training t=28011, loss=0.01718649361282587\n",
      "Surface training t=28012, loss=0.01819052780047059\n",
      "Surface training t=28013, loss=0.022571129724383354\n",
      "Surface training t=28014, loss=0.0196685753762722\n",
      "Surface training t=28015, loss=0.016814561560750008\n",
      "Surface training t=28016, loss=0.018147965893149376\n",
      "Surface training t=28017, loss=0.01622430980205536\n",
      "Surface training t=28018, loss=0.01776453945785761\n",
      "Surface training t=28019, loss=0.01376212714239955\n",
      "Surface training t=28020, loss=0.01883571967482567\n",
      "Surface training t=28021, loss=0.01357000321149826\n",
      "Surface training t=28022, loss=0.017126412130892277\n",
      "Surface training t=28023, loss=0.013385205529630184\n",
      "Surface training t=28024, loss=0.01585989212617278\n",
      "Surface training t=28025, loss=0.015552324242889881\n",
      "Surface training t=28026, loss=0.014473853167146444\n",
      "Surface training t=28027, loss=0.018577589187771082\n",
      "Surface training t=28028, loss=0.024148722179234028\n",
      "Surface training t=28029, loss=0.020700505003333092\n",
      "Surface training t=28030, loss=0.02886057458817959\n",
      "Surface training t=28031, loss=0.023215366527438164\n",
      "Surface training t=28032, loss=0.026268478482961655\n",
      "Surface training t=28033, loss=0.022147140465676785\n",
      "Surface training t=28034, loss=0.020969060249626637\n",
      "Surface training t=28035, loss=0.020190025214105844\n",
      "Surface training t=28036, loss=0.01844798680394888\n",
      "Surface training t=28037, loss=0.017175418324768543\n",
      "Surface training t=28038, loss=0.01609402010217309\n",
      "Surface training t=28039, loss=0.01609998708590865\n",
      "Surface training t=28040, loss=0.017733708955347538\n",
      "Surface training t=28041, loss=0.018564574420452118\n",
      "Surface training t=28042, loss=0.020420389249920845\n",
      "Surface training t=28043, loss=0.02127788122743368\n",
      "Surface training t=28044, loss=0.022689444944262505\n",
      "Surface training t=28045, loss=0.027643934823572636\n",
      "Surface training t=28046, loss=0.01634242618456483\n",
      "Surface training t=28047, loss=0.014944156631827354\n",
      "Surface training t=28048, loss=0.01915309438481927\n",
      "Surface training t=28049, loss=0.01920431014150381\n",
      "Surface training t=28050, loss=0.022452587261795998\n",
      "Surface training t=28051, loss=0.021896890364587307\n",
      "Surface training t=28052, loss=0.021823612973093987\n",
      "Surface training t=28053, loss=0.025489318184554577\n",
      "Surface training t=28054, loss=0.025960519909858704\n",
      "Surface training t=28055, loss=0.021306712180376053\n",
      "Surface training t=28056, loss=0.02601318061351776\n",
      "Surface training t=28057, loss=0.02761092595756054\n",
      "Surface training t=28058, loss=0.029860854148864746\n",
      "Surface training t=28059, loss=0.01957038789987564\n",
      "Surface training t=28060, loss=0.014952694531530142\n",
      "Surface training t=28061, loss=0.030159653164446354\n",
      "Surface training t=28062, loss=0.01767493039369583\n",
      "Surface training t=28063, loss=0.03217852860689163\n",
      "Surface training t=28064, loss=0.02621220238506794\n",
      "Surface training t=28065, loss=0.022872203961014748\n",
      "Surface training t=28066, loss=0.026969491504132748\n",
      "Surface training t=28067, loss=0.024647634476423264\n",
      "Surface training t=28068, loss=0.028709392063319683\n",
      "Surface training t=28069, loss=0.022466945461928844\n",
      "Surface training t=28070, loss=0.018365848809480667\n",
      "Surface training t=28071, loss=0.018521395977586508\n",
      "Surface training t=28072, loss=0.024968464858829975\n",
      "Surface training t=28073, loss=0.025260137394070625\n",
      "Surface training t=28074, loss=0.017263724468648434\n",
      "Surface training t=28075, loss=0.03004630282521248\n",
      "Surface training t=28076, loss=0.0236955713480711\n",
      "Surface training t=28077, loss=0.026025516912341118\n",
      "Surface training t=28078, loss=0.032169096171855927\n",
      "Surface training t=28079, loss=0.026851993054151535\n",
      "Surface training t=28080, loss=0.023049998097121716\n",
      "Surface training t=28081, loss=0.025508644059300423\n",
      "Surface training t=28082, loss=0.018811258487403393\n",
      "Surface training t=28083, loss=0.016634239815175533\n",
      "Surface training t=28084, loss=0.01707124477252364\n",
      "Surface training t=28085, loss=0.02126905508339405\n",
      "Surface training t=28086, loss=0.01512190792709589\n",
      "Surface training t=28087, loss=0.020063786767423153\n",
      "Surface training t=28088, loss=0.013716401997953653\n",
      "Surface training t=28089, loss=0.014413375873118639\n",
      "Surface training t=28090, loss=0.013938922435045242\n",
      "Surface training t=28091, loss=0.01256435178220272\n",
      "Surface training t=28092, loss=0.016947058960795403\n",
      "Surface training t=28093, loss=0.02118176966905594\n",
      "Surface training t=28094, loss=0.021239078603684902\n",
      "Surface training t=28095, loss=0.02429702877998352\n",
      "Surface training t=28096, loss=0.023042109794914722\n",
      "Surface training t=28097, loss=0.018230533227324486\n",
      "Surface training t=28098, loss=0.018979689106345177\n",
      "Surface training t=28099, loss=0.01908909808844328\n",
      "Surface training t=28100, loss=0.020316521637141705\n",
      "Surface training t=28101, loss=0.024352609179913998\n",
      "Surface training t=28102, loss=0.01852116920053959\n",
      "Surface training t=28103, loss=0.024838997051119804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=28104, loss=0.027995833195745945\n",
      "Surface training t=28105, loss=0.0197359761223197\n",
      "Surface training t=28106, loss=0.021594191901385784\n",
      "Surface training t=28107, loss=0.03271692059934139\n",
      "Surface training t=28108, loss=0.025151751935482025\n",
      "Surface training t=28109, loss=0.02992895618081093\n",
      "Surface training t=28110, loss=0.028547008521854877\n",
      "Surface training t=28111, loss=0.02806591149419546\n",
      "Surface training t=28112, loss=0.025960834696888924\n",
      "Surface training t=28113, loss=0.0170338349416852\n",
      "Surface training t=28114, loss=0.020370975136756897\n",
      "Surface training t=28115, loss=0.023282003588974476\n",
      "Surface training t=28116, loss=0.023841272108256817\n",
      "Surface training t=28117, loss=0.02364002913236618\n",
      "Surface training t=28118, loss=0.018891059793531895\n",
      "Surface training t=28119, loss=0.023810015991330147\n",
      "Surface training t=28120, loss=0.023730644024908543\n",
      "Surface training t=28121, loss=0.024589729495346546\n",
      "Surface training t=28122, loss=0.020814866758883\n",
      "Surface training t=28123, loss=0.017050032503902912\n",
      "Surface training t=28124, loss=0.02341173868626356\n",
      "Surface training t=28125, loss=0.01774443220347166\n",
      "Surface training t=28126, loss=0.020867803134024143\n",
      "Surface training t=28127, loss=0.023996224626898766\n",
      "Surface training t=28128, loss=0.020503845997154713\n",
      "Surface training t=28129, loss=0.02252245880663395\n",
      "Surface training t=28130, loss=0.024879745207726955\n",
      "Surface training t=28131, loss=0.01876120176166296\n",
      "Surface training t=28132, loss=0.021689457818865776\n",
      "Surface training t=28133, loss=0.017327788285911083\n",
      "Surface training t=28134, loss=0.02245241217315197\n",
      "Surface training t=28135, loss=0.020535052753984928\n",
      "Surface training t=28136, loss=0.024670463986694813\n",
      "Surface training t=28137, loss=0.023853576742112637\n",
      "Surface training t=28138, loss=0.02535056695342064\n",
      "Surface training t=28139, loss=0.02636262122541666\n",
      "Surface training t=28140, loss=0.020669869147241116\n",
      "Surface training t=28141, loss=0.020775304175913334\n",
      "Surface training t=28142, loss=0.02051382791250944\n",
      "Surface training t=28143, loss=0.019117919728159904\n",
      "Surface training t=28144, loss=0.02065801341086626\n",
      "Surface training t=28145, loss=0.023633331060409546\n",
      "Surface training t=28146, loss=0.018086097203195095\n",
      "Surface training t=28147, loss=0.022616485133767128\n",
      "Surface training t=28148, loss=0.023486946243792772\n",
      "Surface training t=28149, loss=0.02288295328617096\n",
      "Surface training t=28150, loss=0.027353052981197834\n",
      "Surface training t=28151, loss=0.02375134266912937\n",
      "Surface training t=28152, loss=0.02272589225322008\n",
      "Surface training t=28153, loss=0.02574241068214178\n",
      "Surface training t=28154, loss=0.026494179852306843\n",
      "Surface training t=28155, loss=0.0237100999802351\n",
      "Surface training t=28156, loss=0.037691861391067505\n",
      "Surface training t=28157, loss=0.03156551253050566\n",
      "Surface training t=28158, loss=0.02690695319324732\n",
      "Surface training t=28159, loss=0.027132312767207623\n",
      "Surface training t=28160, loss=0.024567341431975365\n",
      "Surface training t=28161, loss=0.02487615030258894\n",
      "Surface training t=28162, loss=0.022380066569894552\n",
      "Surface training t=28163, loss=0.024743408896028996\n",
      "Surface training t=28164, loss=0.02945595234632492\n",
      "Surface training t=28165, loss=0.02939656563103199\n",
      "Surface training t=28166, loss=0.02444621082395315\n",
      "Surface training t=28167, loss=0.02688341774046421\n",
      "Surface training t=28168, loss=0.02947745006531477\n",
      "Surface training t=28169, loss=0.019859887193888426\n",
      "Surface training t=28170, loss=0.018018831498920918\n",
      "Surface training t=28171, loss=0.018415392376482487\n",
      "Surface training t=28172, loss=0.017179626505821943\n",
      "Surface training t=28173, loss=0.016640642657876015\n",
      "Surface training t=28174, loss=0.021891221404075623\n",
      "Surface training t=28175, loss=0.023913436569273472\n",
      "Surface training t=28176, loss=0.027102832682430744\n",
      "Surface training t=28177, loss=0.019629701040685177\n",
      "Surface training t=28178, loss=0.019295452162623405\n",
      "Surface training t=28179, loss=0.017922330647706985\n",
      "Surface training t=28180, loss=0.022546750493347645\n",
      "Surface training t=28181, loss=0.020599855110049248\n",
      "Surface training t=28182, loss=0.01804165542125702\n",
      "Surface training t=28183, loss=0.016503070946782827\n",
      "Surface training t=28184, loss=0.017115900292992592\n",
      "Surface training t=28185, loss=0.016290881671011448\n",
      "Surface training t=28186, loss=0.017270312178879976\n",
      "Surface training t=28187, loss=0.015757259912788868\n",
      "Surface training t=28188, loss=0.019739937037229538\n",
      "Surface training t=28189, loss=0.020511317066848278\n",
      "Surface training t=28190, loss=0.020907907746732235\n",
      "Surface training t=28191, loss=0.02925360295921564\n",
      "Surface training t=28192, loss=0.018241723999381065\n",
      "Surface training t=28193, loss=0.024071697145700455\n",
      "Surface training t=28194, loss=0.021145280450582504\n",
      "Surface training t=28195, loss=0.022580300457775593\n",
      "Surface training t=28196, loss=0.02550849039107561\n",
      "Surface training t=28197, loss=0.023904599249362946\n",
      "Surface training t=28198, loss=0.032997941598296165\n",
      "Surface training t=28199, loss=0.023662755265831947\n",
      "Surface training t=28200, loss=0.026420666836202145\n",
      "Surface training t=28201, loss=0.02640717849135399\n",
      "Surface training t=28202, loss=0.023916181176900864\n",
      "Surface training t=28203, loss=0.02639122772961855\n",
      "Surface training t=28204, loss=0.03007841855287552\n",
      "Surface training t=28205, loss=0.02122162375599146\n",
      "Surface training t=28206, loss=0.02246780227869749\n",
      "Surface training t=28207, loss=0.019811205565929413\n",
      "Surface training t=28208, loss=0.030322558246552944\n",
      "Surface training t=28209, loss=0.027979369275271893\n",
      "Surface training t=28210, loss=0.03781324811279774\n",
      "Surface training t=28211, loss=0.03417033702135086\n",
      "Surface training t=28212, loss=0.0296733183786273\n",
      "Surface training t=28213, loss=0.026142138056457043\n",
      "Surface training t=28214, loss=0.02516242116689682\n",
      "Surface training t=28215, loss=0.02740784827619791\n",
      "Surface training t=28216, loss=0.024917916394770145\n",
      "Surface training t=28217, loss=0.02158785704523325\n",
      "Surface training t=28218, loss=0.023965032771229744\n",
      "Surface training t=28219, loss=0.022669944912195206\n",
      "Surface training t=28220, loss=0.03313405439257622\n",
      "Surface training t=28221, loss=0.02428832557052374\n",
      "Surface training t=28222, loss=0.024595143273472786\n",
      "Surface training t=28223, loss=0.027442898601293564\n",
      "Surface training t=28224, loss=0.02324995305389166\n",
      "Surface training t=28225, loss=0.02271004021167755\n",
      "Surface training t=28226, loss=0.02275152876973152\n",
      "Surface training t=28227, loss=0.025943662971258163\n",
      "Surface training t=28228, loss=0.022821866907179356\n",
      "Surface training t=28229, loss=0.017311913892626762\n",
      "Surface training t=28230, loss=0.016861540731042624\n",
      "Surface training t=28231, loss=0.018593238666653633\n",
      "Surface training t=28232, loss=0.01563074579462409\n",
      "Surface training t=28233, loss=0.017083076760172844\n",
      "Surface training t=28234, loss=0.015301642008125782\n",
      "Surface training t=28235, loss=0.01551839616149664\n",
      "Surface training t=28236, loss=0.017418865114450455\n",
      "Surface training t=28237, loss=0.013620796613395214\n",
      "Surface training t=28238, loss=0.018174374476075172\n",
      "Surface training t=28239, loss=0.021613141521811485\n",
      "Surface training t=28240, loss=0.02847928460687399\n",
      "Surface training t=28241, loss=0.021394817624241114\n",
      "Surface training t=28242, loss=0.020798776298761368\n",
      "Surface training t=28243, loss=0.021884772926568985\n",
      "Surface training t=28244, loss=0.0141027239151299\n",
      "Surface training t=28245, loss=0.018639614339917898\n",
      "Surface training t=28246, loss=0.018356505781412125\n",
      "Surface training t=28247, loss=0.019615270663052797\n",
      "Surface training t=28248, loss=0.01795291854068637\n",
      "Surface training t=28249, loss=0.021584040485322475\n",
      "Surface training t=28250, loss=0.018486863002181053\n",
      "Surface training t=28251, loss=0.029080609790980816\n",
      "Surface training t=28252, loss=0.029224743135273457\n",
      "Surface training t=28253, loss=0.020623603835701942\n",
      "Surface training t=28254, loss=0.023917404003441334\n",
      "Surface training t=28255, loss=0.02775616943836212\n",
      "Surface training t=28256, loss=0.02354886755347252\n",
      "Surface training t=28257, loss=0.019818244501948357\n",
      "Surface training t=28258, loss=0.01770879700779915\n",
      "Surface training t=28259, loss=0.016252863686531782\n",
      "Surface training t=28260, loss=0.02396919671446085\n",
      "Surface training t=28261, loss=0.02341694012284279\n",
      "Surface training t=28262, loss=0.022892745211720467\n",
      "Surface training t=28263, loss=0.02322474494576454\n",
      "Surface training t=28264, loss=0.023904613219201565\n",
      "Surface training t=28265, loss=0.028092974796891212\n",
      "Surface training t=28266, loss=0.034445302560925484\n",
      "Surface training t=28267, loss=0.031526118516922\n",
      "Surface training t=28268, loss=0.03795139957219362\n",
      "Surface training t=28269, loss=0.04704124853014946\n",
      "Surface training t=28270, loss=0.03564880043268204\n",
      "Surface training t=28271, loss=0.028221523389220238\n",
      "Surface training t=28272, loss=0.035746727138757706\n",
      "Surface training t=28273, loss=0.028167356736958027\n",
      "Surface training t=28274, loss=0.029134382493793964\n",
      "Surface training t=28275, loss=0.02508711814880371\n",
      "Surface training t=28276, loss=0.020355047658085823\n",
      "Surface training t=28277, loss=0.024309348315000534\n",
      "Surface training t=28278, loss=0.021073988638818264\n",
      "Surface training t=28279, loss=0.02119533997029066\n",
      "Surface training t=28280, loss=0.020784342661499977\n",
      "Surface training t=28281, loss=0.02104559075087309\n",
      "Surface training t=28282, loss=0.0209931256249547\n",
      "Surface training t=28283, loss=0.023279568180441856\n",
      "Surface training t=28284, loss=0.03295241296291351\n",
      "Surface training t=28285, loss=0.02729714848101139\n",
      "Surface training t=28286, loss=0.024835925549268723\n",
      "Surface training t=28287, loss=0.02614910714328289\n",
      "Surface training t=28288, loss=0.021951597183942795\n",
      "Surface training t=28289, loss=0.02044228231534362\n",
      "Surface training t=28290, loss=0.023631593212485313\n",
      "Surface training t=28291, loss=0.01769786700606346\n",
      "Surface training t=28292, loss=0.013889702968299389\n",
      "Surface training t=28293, loss=0.013485280331224203\n",
      "Surface training t=28294, loss=0.015465395990759134\n",
      "Surface training t=28295, loss=0.01780630461871624\n",
      "Surface training t=28296, loss=0.020150166004896164\n",
      "Surface training t=28297, loss=0.016390109434723854\n",
      "Surface training t=28298, loss=0.014321242459118366\n",
      "Surface training t=28299, loss=0.020191308110952377\n",
      "Surface training t=28300, loss=0.026871761307120323\n",
      "Surface training t=28301, loss=0.021234050393104553\n",
      "Surface training t=28302, loss=0.02377972099930048\n",
      "Surface training t=28303, loss=0.020441566593945026\n",
      "Surface training t=28304, loss=0.022100976668298244\n",
      "Surface training t=28305, loss=0.015168569050729275\n",
      "Surface training t=28306, loss=0.013039056677371264\n",
      "Surface training t=28307, loss=0.01672506984323263\n",
      "Surface training t=28308, loss=0.019537736661732197\n",
      "Surface training t=28309, loss=0.02082937676459551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=28310, loss=0.01853476744145155\n",
      "Surface training t=28311, loss=0.015484113246202469\n",
      "Surface training t=28312, loss=0.018447628244757652\n",
      "Surface training t=28313, loss=0.01683131232857704\n",
      "Surface training t=28314, loss=0.025900905951857567\n",
      "Surface training t=28315, loss=0.021516570821404457\n",
      "Surface training t=28316, loss=0.017614400014281273\n",
      "Surface training t=28317, loss=0.019401113502681255\n",
      "Surface training t=28318, loss=0.017683109734207392\n",
      "Surface training t=28319, loss=0.020971516147255898\n",
      "Surface training t=28320, loss=0.028836197219789028\n",
      "Surface training t=28321, loss=0.028484473936259747\n",
      "Surface training t=28322, loss=0.03944903239607811\n",
      "Surface training t=28323, loss=0.034464627504348755\n",
      "Surface training t=28324, loss=0.0520276203751564\n",
      "Surface training t=28325, loss=0.035158151760697365\n",
      "Surface training t=28326, loss=0.03842976875603199\n",
      "Surface training t=28327, loss=0.05736688897013664\n",
      "Surface training t=28328, loss=0.04727020673453808\n",
      "Surface training t=28329, loss=0.031967622227966785\n",
      "Surface training t=28330, loss=0.036672431975603104\n",
      "Surface training t=28331, loss=0.038985397666692734\n",
      "Surface training t=28332, loss=0.034161802381277084\n",
      "Surface training t=28333, loss=0.049616387113928795\n",
      "Surface training t=28334, loss=0.0343667846173048\n",
      "Surface training t=28335, loss=0.03342633694410324\n",
      "Surface training t=28336, loss=0.026420440524816513\n",
      "Surface training t=28337, loss=0.030497911386191845\n",
      "Surface training t=28338, loss=0.02424096129834652\n",
      "Surface training t=28339, loss=0.0252313781529665\n",
      "Surface training t=28340, loss=0.02851146273314953\n",
      "Surface training t=28341, loss=0.02484966441988945\n",
      "Surface training t=28342, loss=0.025908883661031723\n",
      "Surface training t=28343, loss=0.019592037424445152\n",
      "Surface training t=28344, loss=0.019822724629193544\n",
      "Surface training t=28345, loss=0.015553733799606562\n",
      "Surface training t=28346, loss=0.015212832018733025\n",
      "Surface training t=28347, loss=0.016231045126914978\n",
      "Surface training t=28348, loss=0.017451481893658638\n",
      "Surface training t=28349, loss=0.02012163493782282\n",
      "Surface training t=28350, loss=0.016116506420075893\n",
      "Surface training t=28351, loss=0.01592866936698556\n",
      "Surface training t=28352, loss=0.017186612356454134\n",
      "Surface training t=28353, loss=0.013694771565496922\n",
      "Surface training t=28354, loss=0.01209374750033021\n",
      "Surface training t=28355, loss=0.02003301400691271\n",
      "Surface training t=28356, loss=0.02193448133766651\n",
      "Surface training t=28357, loss=0.02438763529062271\n",
      "Surface training t=28358, loss=0.021111884154379368\n",
      "Surface training t=28359, loss=0.023429126478731632\n",
      "Surface training t=28360, loss=0.02010315004736185\n",
      "Surface training t=28361, loss=0.025761726312339306\n",
      "Surface training t=28362, loss=0.022360894829034805\n",
      "Surface training t=28363, loss=0.017595461569726467\n",
      "Surface training t=28364, loss=0.02323502954095602\n",
      "Surface training t=28365, loss=0.023563041351735592\n",
      "Surface training t=28366, loss=0.02034826949238777\n",
      "Surface training t=28367, loss=0.029294014908373356\n",
      "Surface training t=28368, loss=0.02224088180810213\n",
      "Surface training t=28369, loss=0.020473352633416653\n",
      "Surface training t=28370, loss=0.019538485445082188\n",
      "Surface training t=28371, loss=0.019229162950068712\n",
      "Surface training t=28372, loss=0.01753155142068863\n",
      "Surface training t=28373, loss=0.019435344263911247\n",
      "Surface training t=28374, loss=0.017254925332963467\n",
      "Surface training t=28375, loss=0.023951304145157337\n",
      "Surface training t=28376, loss=0.022679870948195457\n",
      "Surface training t=28377, loss=0.019451061263680458\n",
      "Surface training t=28378, loss=0.017549989745020866\n",
      "Surface training t=28379, loss=0.021719125099480152\n",
      "Surface training t=28380, loss=0.026478036306798458\n",
      "Surface training t=28381, loss=0.027992148883640766\n",
      "Surface training t=28382, loss=0.02203649841248989\n",
      "Surface training t=28383, loss=0.023393561132252216\n",
      "Surface training t=28384, loss=0.018031837418675423\n",
      "Surface training t=28385, loss=0.020123645663261414\n",
      "Surface training t=28386, loss=0.021633898839354515\n",
      "Surface training t=28387, loss=0.020388804376125336\n",
      "Surface training t=28388, loss=0.01746124681085348\n",
      "Surface training t=28389, loss=0.02139161340892315\n",
      "Surface training t=28390, loss=0.017567714676260948\n",
      "Surface training t=28391, loss=0.01947413943707943\n",
      "Surface training t=28392, loss=0.018898287788033485\n",
      "Surface training t=28393, loss=0.02057700604200363\n",
      "Surface training t=28394, loss=0.031547486782073975\n",
      "Surface training t=28395, loss=0.022971787489950657\n",
      "Surface training t=28396, loss=0.020293679554015398\n",
      "Surface training t=28397, loss=0.026006050407886505\n",
      "Surface training t=28398, loss=0.025181107223033905\n",
      "Surface training t=28399, loss=0.023808257654309273\n",
      "Surface training t=28400, loss=0.022136163897812366\n",
      "Surface training t=28401, loss=0.023510907776653767\n",
      "Surface training t=28402, loss=0.02082705870270729\n",
      "Surface training t=28403, loss=0.022484260611236095\n",
      "Surface training t=28404, loss=0.024911698885262012\n",
      "Surface training t=28405, loss=0.02039658185094595\n",
      "Surface training t=28406, loss=0.030866393819451332\n",
      "Surface training t=28407, loss=0.025756445713341236\n",
      "Surface training t=28408, loss=0.026477201841771603\n",
      "Surface training t=28409, loss=0.0403993409126997\n",
      "Surface training t=28410, loss=0.032350026071071625\n",
      "Surface training t=28411, loss=0.047213878482580185\n",
      "Surface training t=28412, loss=0.026812908239662647\n",
      "Surface training t=28413, loss=0.03049214743077755\n",
      "Surface training t=28414, loss=0.021233958192169666\n",
      "Surface training t=28415, loss=0.028693951666355133\n",
      "Surface training t=28416, loss=0.03596762474626303\n",
      "Surface training t=28417, loss=0.031121359206736088\n",
      "Surface training t=28418, loss=0.03108762949705124\n",
      "Surface training t=28419, loss=0.029840661212801933\n",
      "Surface training t=28420, loss=0.02647740300744772\n",
      "Surface training t=28421, loss=0.025032101199030876\n",
      "Surface training t=28422, loss=0.04249463975429535\n",
      "Surface training t=28423, loss=0.02792319282889366\n",
      "Surface training t=28424, loss=0.031828269362449646\n",
      "Surface training t=28425, loss=0.033901821821928024\n",
      "Surface training t=28426, loss=0.04365067556500435\n",
      "Surface training t=28427, loss=0.032970258966088295\n",
      "Surface training t=28428, loss=0.05222243256866932\n",
      "Surface training t=28429, loss=0.035592799074947834\n",
      "Surface training t=28430, loss=0.040998052805662155\n",
      "Surface training t=28431, loss=0.026851296424865723\n",
      "Surface training t=28432, loss=0.026315913535654545\n",
      "Surface training t=28433, loss=0.029218324460089207\n",
      "Surface training t=28434, loss=0.023172828368842602\n",
      "Surface training t=28435, loss=0.02276693657040596\n",
      "Surface training t=28436, loss=0.03364086244255304\n",
      "Surface training t=28437, loss=0.021542957052588463\n",
      "Surface training t=28438, loss=0.02221496682614088\n",
      "Surface training t=28439, loss=0.024185599759221077\n",
      "Surface training t=28440, loss=0.020242784172296524\n",
      "Surface training t=28441, loss=0.022695918567478657\n",
      "Surface training t=28442, loss=0.030872659757733345\n",
      "Surface training t=28443, loss=0.034587275236845016\n",
      "Surface training t=28444, loss=0.029810702428221703\n",
      "Surface training t=28445, loss=0.03792236186563969\n",
      "Surface training t=28446, loss=0.03363322652876377\n",
      "Surface training t=28447, loss=0.024043606594204903\n",
      "Surface training t=28448, loss=0.02098038326948881\n",
      "Surface training t=28449, loss=0.01496011484414339\n",
      "Surface training t=28450, loss=0.01869233138859272\n",
      "Surface training t=28451, loss=0.020047961734235287\n",
      "Surface training t=28452, loss=0.017869205214083195\n",
      "Surface training t=28453, loss=0.020451738499104977\n",
      "Surface training t=28454, loss=0.022306066006422043\n",
      "Surface training t=28455, loss=0.0475953072309494\n",
      "Surface training t=28456, loss=0.03373897913843393\n",
      "Surface training t=28457, loss=0.04875354841351509\n",
      "Surface training t=28458, loss=0.03990701027214527\n",
      "Surface training t=28459, loss=0.029155105352401733\n",
      "Surface training t=28460, loss=0.033434610813856125\n",
      "Surface training t=28461, loss=0.025922427885234356\n",
      "Surface training t=28462, loss=0.02268373593688011\n",
      "Surface training t=28463, loss=0.018957090564072132\n",
      "Surface training t=28464, loss=0.03410220704972744\n",
      "Surface training t=28465, loss=0.029672810807824135\n",
      "Surface training t=28466, loss=0.02935538161545992\n",
      "Surface training t=28467, loss=0.026053478941321373\n",
      "Surface training t=28468, loss=0.021578785963356495\n",
      "Surface training t=28469, loss=0.04150952026247978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=28470, loss=0.02973866742104292\n",
      "Surface training t=28471, loss=0.03323042020201683\n",
      "Surface training t=28472, loss=0.05920827388763428\n",
      "Surface training t=28473, loss=0.0406525619328022\n",
      "Surface training t=28474, loss=0.054452164098620415\n",
      "Surface training t=28475, loss=0.032258568331599236\n",
      "Surface training t=28476, loss=0.03173556178808212\n",
      "Surface training t=28477, loss=0.025854192674160004\n",
      "Surface training t=28478, loss=0.027741936966776848\n",
      "Surface training t=28479, loss=0.03215243015438318\n",
      "Surface training t=28480, loss=0.027224287390708923\n",
      "Surface training t=28481, loss=0.02324963826686144\n",
      "Surface training t=28482, loss=0.0371532142162323\n",
      "Surface training t=28483, loss=0.027585445903241634\n",
      "Surface training t=28484, loss=0.03119373507797718\n",
      "Surface training t=28485, loss=0.03478255867958069\n",
      "Surface training t=28486, loss=0.03882699832320213\n",
      "Surface training t=28487, loss=0.03317393362522125\n",
      "Surface training t=28488, loss=0.036253771744668484\n",
      "Surface training t=28489, loss=0.04110804758965969\n",
      "Surface training t=28490, loss=0.03278140351176262\n",
      "Surface training t=28491, loss=0.040772564709186554\n",
      "Surface training t=28492, loss=0.03139848727732897\n",
      "Surface training t=28493, loss=0.03068238403648138\n",
      "Surface training t=28494, loss=0.02796781901270151\n",
      "Surface training t=28495, loss=0.029759323224425316\n",
      "Surface training t=28496, loss=0.053461525589227676\n",
      "Surface training t=28497, loss=0.042220307514071465\n",
      "Surface training t=28498, loss=0.048304324969649315\n",
      "Surface training t=28499, loss=0.042813559994101524\n",
      "Surface training t=28500, loss=0.028513044118881226\n",
      "Surface training t=28501, loss=0.027209972962737083\n",
      "Surface training t=28502, loss=0.029701429419219494\n",
      "Surface training t=28503, loss=0.023431084118783474\n",
      "Surface training t=28504, loss=0.03174269385635853\n",
      "Surface training t=28505, loss=0.02779859211295843\n",
      "Surface training t=28506, loss=0.028369909152388573\n",
      "Surface training t=28507, loss=0.03185711428523064\n",
      "Surface training t=28508, loss=0.02775478083640337\n",
      "Surface training t=28509, loss=0.02927605714648962\n",
      "Surface training t=28510, loss=0.02513639535754919\n",
      "Surface training t=28511, loss=0.026238450780510902\n",
      "Surface training t=28512, loss=0.030567463487386703\n",
      "Surface training t=28513, loss=0.030356640927493572\n",
      "Surface training t=28514, loss=0.024444714188575745\n",
      "Surface training t=28515, loss=0.03085981123149395\n",
      "Surface training t=28516, loss=0.030199742875993252\n",
      "Surface training t=28517, loss=0.028270638547837734\n",
      "Surface training t=28518, loss=0.025052032433450222\n",
      "Surface training t=28519, loss=0.02197373565286398\n",
      "Surface training t=28520, loss=0.019929182715713978\n",
      "Surface training t=28521, loss=0.02022435422986746\n",
      "Surface training t=28522, loss=0.02186806220561266\n",
      "Surface training t=28523, loss=0.021020307205617428\n",
      "Surface training t=28524, loss=0.016429031267762184\n",
      "Surface training t=28525, loss=0.021671727299690247\n",
      "Surface training t=28526, loss=0.02042313478887081\n",
      "Surface training t=28527, loss=0.021625771187245846\n",
      "Surface training t=28528, loss=0.017847249284386635\n",
      "Surface training t=28529, loss=0.015594058204442263\n",
      "Surface training t=28530, loss=0.021118237636983395\n",
      "Surface training t=28531, loss=0.01661355746909976\n",
      "Surface training t=28532, loss=0.018498631194233894\n",
      "Surface training t=28533, loss=0.01798613928258419\n",
      "Surface training t=28534, loss=0.02036702260375023\n",
      "Surface training t=28535, loss=0.02073767501860857\n",
      "Surface training t=28536, loss=0.02285726275295019\n",
      "Surface training t=28537, loss=0.015189331024885178\n",
      "Surface training t=28538, loss=0.01918434351682663\n",
      "Surface training t=28539, loss=0.020818124525249004\n",
      "Surface training t=28540, loss=0.019976016134023666\n",
      "Surface training t=28541, loss=0.020545051433146\n",
      "Surface training t=28542, loss=0.022189217619597912\n",
      "Surface training t=28543, loss=0.020230007357895374\n",
      "Surface training t=28544, loss=0.02016584388911724\n",
      "Surface training t=28545, loss=0.03164015430957079\n",
      "Surface training t=28546, loss=0.019974862225353718\n",
      "Surface training t=28547, loss=0.01942575629800558\n",
      "Surface training t=28548, loss=0.02288288064301014\n",
      "Surface training t=28549, loss=0.025820634327828884\n",
      "Surface training t=28550, loss=0.027019910514354706\n",
      "Surface training t=28551, loss=0.03919630404561758\n",
      "Surface training t=28552, loss=0.026218934915959835\n",
      "Surface training t=28553, loss=0.0319405235350132\n",
      "Surface training t=28554, loss=0.027740041725337505\n",
      "Surface training t=28555, loss=0.028557488694787025\n",
      "Surface training t=28556, loss=0.024416528642177582\n",
      "Surface training t=28557, loss=0.0239772479981184\n",
      "Surface training t=28558, loss=0.02914977353066206\n",
      "Surface training t=28559, loss=0.02500003296881914\n",
      "Surface training t=28560, loss=0.02558781113475561\n",
      "Surface training t=28561, loss=0.0339966993778944\n",
      "Surface training t=28562, loss=0.030509168282151222\n",
      "Surface training t=28563, loss=0.031129310838878155\n",
      "Surface training t=28564, loss=0.02564880345016718\n",
      "Surface training t=28565, loss=0.02661379985511303\n",
      "Surface training t=28566, loss=0.03243883699178696\n",
      "Surface training t=28567, loss=0.02421450801193714\n",
      "Surface training t=28568, loss=0.027796950191259384\n",
      "Surface training t=28569, loss=0.028433951549232006\n",
      "Surface training t=28570, loss=0.023204000666737556\n",
      "Surface training t=28571, loss=0.023034507408738136\n",
      "Surface training t=28572, loss=0.02219441719353199\n",
      "Surface training t=28573, loss=0.01756592048332095\n",
      "Surface training t=28574, loss=0.02428367454558611\n",
      "Surface training t=28575, loss=0.01735421922057867\n",
      "Surface training t=28576, loss=0.02197299338877201\n",
      "Surface training t=28577, loss=0.016332055442035198\n",
      "Surface training t=28578, loss=0.021317433565855026\n",
      "Surface training t=28579, loss=0.01427910290658474\n",
      "Surface training t=28580, loss=0.021110319532454014\n",
      "Surface training t=28581, loss=0.01852534618228674\n",
      "Surface training t=28582, loss=0.023227084428071976\n",
      "Surface training t=28583, loss=0.017871562391519547\n",
      "Surface training t=28584, loss=0.020420292858034372\n",
      "Surface training t=28585, loss=0.018073623068630695\n",
      "Surface training t=28586, loss=0.02296322863548994\n",
      "Surface training t=28587, loss=0.03229255694895983\n",
      "Surface training t=28588, loss=0.02404477633535862\n",
      "Surface training t=28589, loss=0.030169066973030567\n",
      "Surface training t=28590, loss=0.02055539097636938\n",
      "Surface training t=28591, loss=0.024304254911839962\n",
      "Surface training t=28592, loss=0.02904967125505209\n",
      "Surface training t=28593, loss=0.023440267890691757\n",
      "Surface training t=28594, loss=0.024114505387842655\n",
      "Surface training t=28595, loss=0.014867340214550495\n",
      "Surface training t=28596, loss=0.01700026262551546\n",
      "Surface training t=28597, loss=0.014462689869105816\n",
      "Surface training t=28598, loss=0.019206794910132885\n",
      "Surface training t=28599, loss=0.022754136472940445\n",
      "Surface training t=28600, loss=0.024139302782714367\n",
      "Surface training t=28601, loss=0.023968755267560482\n",
      "Surface training t=28602, loss=0.03482868615537882\n",
      "Surface training t=28603, loss=0.028522025793790817\n",
      "Surface training t=28604, loss=0.028863387182354927\n",
      "Surface training t=28605, loss=0.036619171500205994\n",
      "Surface training t=28606, loss=0.03537929058074951\n",
      "Surface training t=28607, loss=0.029027795419096947\n",
      "Surface training t=28608, loss=0.03130309842526913\n",
      "Surface training t=28609, loss=0.041990939527750015\n",
      "Surface training t=28610, loss=0.026738861575722694\n",
      "Surface training t=28611, loss=0.03772137127816677\n",
      "Surface training t=28612, loss=0.028590538538992405\n",
      "Surface training t=28613, loss=0.03318948484957218\n",
      "Surface training t=28614, loss=0.025553010404109955\n",
      "Surface training t=28615, loss=0.028574266470968723\n",
      "Surface training t=28616, loss=0.024182969704270363\n",
      "Surface training t=28617, loss=0.021238891407847404\n",
      "Surface training t=28618, loss=0.022120701149106026\n",
      "Surface training t=28619, loss=0.024586222134530544\n",
      "Surface training t=28620, loss=0.023149192333221436\n",
      "Surface training t=28621, loss=0.01817223709076643\n",
      "Surface training t=28622, loss=0.021354641765356064\n",
      "Surface training t=28623, loss=0.020012281835079193\n",
      "Surface training t=28624, loss=0.015164298936724663\n",
      "Surface training t=28625, loss=0.018923611380159855\n",
      "Surface training t=28626, loss=0.020310314372181892\n",
      "Surface training t=28627, loss=0.015328174456954002\n",
      "Surface training t=28628, loss=0.018873419612646103\n",
      "Surface training t=28629, loss=0.015090267639607191\n",
      "Surface training t=28630, loss=0.018158809281885624\n",
      "Surface training t=28631, loss=0.016571802087128162\n",
      "Surface training t=28632, loss=0.01736746635288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=28633, loss=0.01472760597243905\n",
      "Surface training t=28634, loss=0.013897622935473919\n",
      "Surface training t=28635, loss=0.014809418469667435\n",
      "Surface training t=28636, loss=0.018754920922219753\n",
      "Surface training t=28637, loss=0.01642677653580904\n",
      "Surface training t=28638, loss=0.019275803118944168\n",
      "Surface training t=28639, loss=0.027603772468864918\n",
      "Surface training t=28640, loss=0.022278508637100458\n",
      "Surface training t=28641, loss=0.02277978789061308\n",
      "Surface training t=28642, loss=0.026520922780036926\n",
      "Surface training t=28643, loss=0.02151307463645935\n",
      "Surface training t=28644, loss=0.019120008684694767\n",
      "Surface training t=28645, loss=0.02167478296905756\n",
      "Surface training t=28646, loss=0.019467527978122234\n",
      "Surface training t=28647, loss=0.017892442643642426\n",
      "Surface training t=28648, loss=0.015109050087630749\n",
      "Surface training t=28649, loss=0.014599665999412537\n",
      "Surface training t=28650, loss=0.01361848646774888\n",
      "Surface training t=28651, loss=0.013580809812992811\n",
      "Surface training t=28652, loss=0.017909192480146885\n",
      "Surface training t=28653, loss=0.01977213053032756\n",
      "Surface training t=28654, loss=0.024286245461553335\n",
      "Surface training t=28655, loss=0.023285096511244774\n",
      "Surface training t=28656, loss=0.020694509148597717\n",
      "Surface training t=28657, loss=0.02116505056619644\n",
      "Surface training t=28658, loss=0.025761038064956665\n",
      "Surface training t=28659, loss=0.02242252044379711\n",
      "Surface training t=28660, loss=0.021082296036183834\n",
      "Surface training t=28661, loss=0.02616732381284237\n",
      "Surface training t=28662, loss=0.024460296146571636\n",
      "Surface training t=28663, loss=0.02734267618507147\n",
      "Surface training t=28664, loss=0.020858388859778643\n",
      "Surface training t=28665, loss=0.031089761294424534\n",
      "Surface training t=28666, loss=0.024675600230693817\n",
      "Surface training t=28667, loss=0.019197425805032253\n",
      "Surface training t=28668, loss=0.018150226678699255\n",
      "Surface training t=28669, loss=0.020152038894593716\n",
      "Surface training t=28670, loss=0.02518480271100998\n",
      "Surface training t=28671, loss=0.023429205641150475\n",
      "Surface training t=28672, loss=0.029023428447544575\n",
      "Surface training t=28673, loss=0.025753919035196304\n",
      "Surface training t=28674, loss=0.023783556185662746\n",
      "Surface training t=28675, loss=0.02415228821337223\n",
      "Surface training t=28676, loss=0.02555968053638935\n",
      "Surface training t=28677, loss=0.020637065172195435\n",
      "Surface training t=28678, loss=0.020074542611837387\n",
      "Surface training t=28679, loss=0.02148884069174528\n",
      "Surface training t=28680, loss=0.0153060806915164\n",
      "Surface training t=28681, loss=0.017812975216656923\n",
      "Surface training t=28682, loss=0.015518495347350836\n",
      "Surface training t=28683, loss=0.015922699123620987\n",
      "Surface training t=28684, loss=0.017007476650178432\n",
      "Surface training t=28685, loss=0.022372011095285416\n",
      "Surface training t=28686, loss=0.015178636647760868\n",
      "Surface training t=28687, loss=0.01934704091399908\n",
      "Surface training t=28688, loss=0.01802152954041958\n",
      "Surface training t=28689, loss=0.03424214757978916\n",
      "Surface training t=28690, loss=0.027960054576396942\n",
      "Surface training t=28691, loss=0.040240734815597534\n",
      "Surface training t=28692, loss=0.024458741769194603\n",
      "Surface training t=28693, loss=0.023664986714720726\n",
      "Surface training t=28694, loss=0.015480916947126389\n",
      "Surface training t=28695, loss=0.017183910124003887\n",
      "Surface training t=28696, loss=0.015729650389403105\n",
      "Surface training t=28697, loss=0.015736148227006197\n",
      "Surface training t=28698, loss=0.011628500651568174\n",
      "Surface training t=28699, loss=0.020013801753520966\n",
      "Surface training t=28700, loss=0.01597942877560854\n",
      "Surface training t=28701, loss=0.016469918191432953\n",
      "Surface training t=28702, loss=0.015170364174991846\n",
      "Surface training t=28703, loss=0.016385957598686218\n",
      "Surface training t=28704, loss=0.022333993576467037\n",
      "Surface training t=28705, loss=0.019905644468963146\n",
      "Surface training t=28706, loss=0.01618623360991478\n",
      "Surface training t=28707, loss=0.023643736727535725\n",
      "Surface training t=28708, loss=0.04159179888665676\n",
      "Surface training t=28709, loss=0.029481614008545876\n",
      "Surface training t=28710, loss=0.027924579568207264\n",
      "Surface training t=28711, loss=0.02523474022746086\n",
      "Surface training t=28712, loss=0.030725307762622833\n",
      "Surface training t=28713, loss=0.029041406698524952\n",
      "Surface training t=28714, loss=0.02619022596627474\n",
      "Surface training t=28715, loss=0.027008600998669863\n",
      "Surface training t=28716, loss=0.041377509012818336\n",
      "Surface training t=28717, loss=0.028252776712179184\n",
      "Surface training t=28718, loss=0.032028255984187126\n",
      "Surface training t=28719, loss=0.030292819254100323\n",
      "Surface training t=28720, loss=0.03742188960313797\n",
      "Surface training t=28721, loss=0.0301689263433218\n",
      "Surface training t=28722, loss=0.036402798257768154\n",
      "Surface training t=28723, loss=0.03345274366438389\n",
      "Surface training t=28724, loss=0.025235358625650406\n",
      "Surface training t=28725, loss=0.02824537828564644\n",
      "Surface training t=28726, loss=0.023236708715558052\n",
      "Surface training t=28727, loss=0.02742371056228876\n",
      "Surface training t=28728, loss=0.029165362007915974\n",
      "Surface training t=28729, loss=0.026815312914550304\n",
      "Surface training t=28730, loss=0.026194454170763493\n",
      "Surface training t=28731, loss=0.023146926425397396\n",
      "Surface training t=28732, loss=0.01941586285829544\n",
      "Surface training t=28733, loss=0.017556891310960054\n",
      "Surface training t=28734, loss=0.016780005767941475\n",
      "Surface training t=28735, loss=0.021989237517118454\n",
      "Surface training t=28736, loss=0.020858964882791042\n",
      "Surface training t=28737, loss=0.019291997887194157\n",
      "Surface training t=28738, loss=0.016718525905162096\n",
      "Surface training t=28739, loss=0.01714777061715722\n",
      "Surface training t=28740, loss=0.01849451381713152\n",
      "Surface training t=28741, loss=0.016878126189112663\n",
      "Surface training t=28742, loss=0.02039286307990551\n",
      "Surface training t=28743, loss=0.02072046510875225\n",
      "Surface training t=28744, loss=0.020211869850754738\n",
      "Surface training t=28745, loss=0.01689011324197054\n",
      "Surface training t=28746, loss=0.014835386537015438\n",
      "Surface training t=28747, loss=0.02064063586294651\n",
      "Surface training t=28748, loss=0.01583193289116025\n",
      "Surface training t=28749, loss=0.0154852494597435\n",
      "Surface training t=28750, loss=0.01480346405878663\n",
      "Surface training t=28751, loss=0.020569725893437862\n",
      "Surface training t=28752, loss=0.022198928520083427\n",
      "Surface training t=28753, loss=0.03179384209215641\n",
      "Surface training t=28754, loss=0.0274944631382823\n",
      "Surface training t=28755, loss=0.02343053836375475\n",
      "Surface training t=28756, loss=0.027734791859984398\n",
      "Surface training t=28757, loss=0.028732172213494778\n",
      "Surface training t=28758, loss=0.03046286478638649\n",
      "Surface training t=28759, loss=0.03361563477665186\n",
      "Surface training t=28760, loss=0.0410242173820734\n",
      "Surface training t=28761, loss=0.02831463050097227\n",
      "Surface training t=28762, loss=0.02834676019847393\n",
      "Surface training t=28763, loss=0.02926216647028923\n",
      "Surface training t=28764, loss=0.03174104169011116\n",
      "Surface training t=28765, loss=0.024060195311903954\n",
      "Surface training t=28766, loss=0.02313469909131527\n",
      "Surface training t=28767, loss=0.025773138739168644\n",
      "Surface training t=28768, loss=0.02443073969334364\n",
      "Surface training t=28769, loss=0.019431869499385357\n",
      "Surface training t=28770, loss=0.02854607254266739\n",
      "Surface training t=28771, loss=0.023742498829960823\n",
      "Surface training t=28772, loss=0.02326162066310644\n",
      "Surface training t=28773, loss=0.01744471862912178\n",
      "Surface training t=28774, loss=0.020019972696900368\n",
      "Surface training t=28775, loss=0.020115974359214306\n",
      "Surface training t=28776, loss=0.02164110727608204\n",
      "Surface training t=28777, loss=0.017308940645307302\n",
      "Surface training t=28778, loss=0.021974864415824413\n",
      "Surface training t=28779, loss=0.016820174641907215\n",
      "Surface training t=28780, loss=0.019977744668722153\n",
      "Surface training t=28781, loss=0.024983711540699005\n",
      "Surface training t=28782, loss=0.026373582892119884\n",
      "Surface training t=28783, loss=0.02009469550102949\n",
      "Surface training t=28784, loss=0.02372752409428358\n",
      "Surface training t=28785, loss=0.022310370579361916\n",
      "Surface training t=28786, loss=0.019300821237266064\n",
      "Surface training t=28787, loss=0.018485432490706444\n",
      "Surface training t=28788, loss=0.019559200387448072\n",
      "Surface training t=28789, loss=0.01860159169882536\n",
      "Surface training t=28790, loss=0.019489172846078873\n",
      "Surface training t=28791, loss=0.028230033814907074\n",
      "Surface training t=28792, loss=0.021514147520065308\n",
      "Surface training t=28793, loss=0.029587802477180958\n",
      "Surface training t=28794, loss=0.02923878002911806\n",
      "Surface training t=28795, loss=0.024199373088777065\n",
      "Surface training t=28796, loss=0.02316349372267723\n",
      "Surface training t=28797, loss=0.02184141520410776\n",
      "Surface training t=28798, loss=0.0217899177223444\n",
      "Surface training t=28799, loss=0.021828167140483856\n",
      "Surface training t=28800, loss=0.02061367593705654\n",
      "Surface training t=28801, loss=0.02899410855025053\n",
      "Surface training t=28802, loss=0.03189657162874937\n",
      "Surface training t=28803, loss=0.02810358814895153\n",
      "Surface training t=28804, loss=0.02425932139158249\n",
      "Surface training t=28805, loss=0.025179386138916016\n",
      "Surface training t=28806, loss=0.024387115612626076\n",
      "Surface training t=28807, loss=0.024596462957561016\n",
      "Surface training t=28808, loss=0.03267845697700977\n",
      "Surface training t=28809, loss=0.025857675820589066\n",
      "Surface training t=28810, loss=0.02798400167375803\n",
      "Surface training t=28811, loss=0.024421782232820988\n",
      "Surface training t=28812, loss=0.01823372906073928\n",
      "Surface training t=28813, loss=0.029410372488200665\n",
      "Surface training t=28814, loss=0.02673538215458393\n",
      "Surface training t=28815, loss=0.02424589730799198\n",
      "Surface training t=28816, loss=0.01981656439602375\n",
      "Surface training t=28817, loss=0.029344402253627777\n",
      "Surface training t=28818, loss=0.023557888343930244\n",
      "Surface training t=28819, loss=0.029715897515416145\n",
      "Surface training t=28820, loss=0.028316041454672813\n",
      "Surface training t=28821, loss=0.018699723295867443\n",
      "Surface training t=28822, loss=0.017502466216683388\n",
      "Surface training t=28823, loss=0.02068777848035097\n",
      "Surface training t=28824, loss=0.02141114417463541\n",
      "Surface training t=28825, loss=0.016785511281341314\n",
      "Surface training t=28826, loss=0.0204333933070302\n",
      "Surface training t=28827, loss=0.018437087535858154\n",
      "Surface training t=28828, loss=0.014777667354792356\n",
      "Surface training t=28829, loss=0.01700802706182003\n",
      "Surface training t=28830, loss=0.017881841398775578\n",
      "Surface training t=28831, loss=0.02023445814847946\n",
      "Surface training t=28832, loss=0.017659086268395185\n",
      "Surface training t=28833, loss=0.020959454588592052\n",
      "Surface training t=28834, loss=0.01440106425434351\n",
      "Surface training t=28835, loss=0.019528976641595364\n",
      "Surface training t=28836, loss=0.01632807496935129\n",
      "Surface training t=28837, loss=0.020274344831705093\n",
      "Surface training t=28838, loss=0.026271960698068142\n",
      "Surface training t=28839, loss=0.02126418985426426\n",
      "Surface training t=28840, loss=0.02788092289119959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=28841, loss=0.02883092127740383\n",
      "Surface training t=28842, loss=0.02198103442788124\n",
      "Surface training t=28843, loss=0.02390776202082634\n",
      "Surface training t=28844, loss=0.03056465182453394\n",
      "Surface training t=28845, loss=0.01875076349824667\n",
      "Surface training t=28846, loss=0.021418257616460323\n",
      "Surface training t=28847, loss=0.02454329188913107\n",
      "Surface training t=28848, loss=0.019630645401775837\n",
      "Surface training t=28849, loss=0.019853732082992792\n",
      "Surface training t=28850, loss=0.018399087712168694\n",
      "Surface training t=28851, loss=0.017840920016169548\n",
      "Surface training t=28852, loss=0.02012085821479559\n",
      "Surface training t=28853, loss=0.02268061973154545\n",
      "Surface training t=28854, loss=0.019845749251544476\n",
      "Surface training t=28855, loss=0.01911738980561495\n",
      "Surface training t=28856, loss=0.018829562701284885\n",
      "Surface training t=28857, loss=0.018369009718298912\n",
      "Surface training t=28858, loss=0.017531585413962603\n",
      "Surface training t=28859, loss=0.018977747298777103\n",
      "Surface training t=28860, loss=0.019316932186484337\n",
      "Surface training t=28861, loss=0.018335613422095776\n",
      "Surface training t=28862, loss=0.013581760227680206\n",
      "Surface training t=28863, loss=0.015225476119667292\n",
      "Surface training t=28864, loss=0.013333756942301989\n",
      "Surface training t=28865, loss=0.019730282947421074\n",
      "Surface training t=28866, loss=0.018989264965057373\n",
      "Surface training t=28867, loss=0.020569021813571453\n",
      "Surface training t=28868, loss=0.021701590158045292\n",
      "Surface training t=28869, loss=0.023320446722209454\n",
      "Surface training t=28870, loss=0.022225501015782356\n",
      "Surface training t=28871, loss=0.017574114724993706\n",
      "Surface training t=28872, loss=0.020734231919050217\n",
      "Surface training t=28873, loss=0.021418355405330658\n",
      "Surface training t=28874, loss=0.02087731473147869\n",
      "Surface training t=28875, loss=0.01700648618862033\n",
      "Surface training t=28876, loss=0.0178216565400362\n",
      "Surface training t=28877, loss=0.019608101807534695\n",
      "Surface training t=28878, loss=0.018233872018754482\n",
      "Surface training t=28879, loss=0.022477074526250362\n",
      "Surface training t=28880, loss=0.024627002887427807\n",
      "Surface training t=28881, loss=0.02549657318741083\n",
      "Surface training t=28882, loss=0.0265802089124918\n",
      "Surface training t=28883, loss=0.02933629509061575\n",
      "Surface training t=28884, loss=0.024458744563162327\n",
      "Surface training t=28885, loss=0.032593647949397564\n",
      "Surface training t=28886, loss=0.026801890693604946\n",
      "Surface training t=28887, loss=0.024499036371707916\n",
      "Surface training t=28888, loss=0.025163549929857254\n",
      "Surface training t=28889, loss=0.02502670604735613\n",
      "Surface training t=28890, loss=0.0219503752887249\n",
      "Surface training t=28891, loss=0.01700450386852026\n",
      "Surface training t=28892, loss=0.021062754094600677\n",
      "Surface training t=28893, loss=0.014598532114177942\n",
      "Surface training t=28894, loss=0.017580529674887657\n",
      "Surface training t=28895, loss=0.01952362898737192\n",
      "Surface training t=28896, loss=0.015656289644539356\n",
      "Surface training t=28897, loss=0.020536082796752453\n",
      "Surface training t=28898, loss=0.02243290189653635\n",
      "Surface training t=28899, loss=0.02904733270406723\n",
      "Surface training t=28900, loss=0.022120478563010693\n",
      "Surface training t=28901, loss=0.019015426747500896\n",
      "Surface training t=28902, loss=0.018364201299846172\n",
      "Surface training t=28903, loss=0.017638548277318478\n",
      "Surface training t=28904, loss=0.022710010409355164\n",
      "Surface training t=28905, loss=0.022225075401365757\n",
      "Surface training t=28906, loss=0.01970438566058874\n",
      "Surface training t=28907, loss=0.018076550215482712\n",
      "Surface training t=28908, loss=0.017382608260959387\n",
      "Surface training t=28909, loss=0.025586330331861973\n",
      "Surface training t=28910, loss=0.023189784958958626\n",
      "Surface training t=28911, loss=0.02021990856155753\n",
      "Surface training t=28912, loss=0.018600427079945803\n",
      "Surface training t=28913, loss=0.021168794482946396\n",
      "Surface training t=28914, loss=0.01643923856317997\n",
      "Surface training t=28915, loss=0.017932779155671597\n",
      "Surface training t=28916, loss=0.024776059202849865\n",
      "Surface training t=28917, loss=0.02629140019416809\n",
      "Surface training t=28918, loss=0.021150107495486736\n",
      "Surface training t=28919, loss=0.02497540321201086\n",
      "Surface training t=28920, loss=0.026752776466310024\n",
      "Surface training t=28921, loss=0.02602919563651085\n",
      "Surface training t=28922, loss=0.029367961920797825\n",
      "Surface training t=28923, loss=0.026333915069699287\n",
      "Surface training t=28924, loss=0.022028453648090363\n",
      "Surface training t=28925, loss=0.024364996701478958\n",
      "Surface training t=28926, loss=0.02450737915933132\n",
      "Surface training t=28927, loss=0.03398304991424084\n",
      "Surface training t=28928, loss=0.03133765794336796\n",
      "Surface training t=28929, loss=0.029041379690170288\n",
      "Surface training t=28930, loss=0.023653510957956314\n",
      "Surface training t=28931, loss=0.02519781980663538\n",
      "Surface training t=28932, loss=0.0234657796099782\n",
      "Surface training t=28933, loss=0.021364656277000904\n",
      "Surface training t=28934, loss=0.029622036963701248\n",
      "Surface training t=28935, loss=0.0205538934096694\n",
      "Surface training t=28936, loss=0.024086695164442062\n",
      "Surface training t=28937, loss=0.04869600757956505\n",
      "Surface training t=28938, loss=0.031812150962650776\n",
      "Surface training t=28939, loss=0.04332895949482918\n",
      "Surface training t=28940, loss=0.038320668041706085\n",
      "Surface training t=28941, loss=0.026205839589238167\n",
      "Surface training t=28942, loss=0.028117189183831215\n",
      "Surface training t=28943, loss=0.02854194026440382\n",
      "Surface training t=28944, loss=0.045369165018200874\n",
      "Surface training t=28945, loss=0.02915758080780506\n",
      "Surface training t=28946, loss=0.03483576141297817\n",
      "Surface training t=28947, loss=0.02446893509477377\n",
      "Surface training t=28948, loss=0.016818768810480833\n",
      "Surface training t=28949, loss=0.02050791308283806\n",
      "Surface training t=28950, loss=0.018188880756497383\n",
      "Surface training t=28951, loss=0.01732589164748788\n",
      "Surface training t=28952, loss=0.01742196176201105\n",
      "Surface training t=28953, loss=0.014187021180987358\n",
      "Surface training t=28954, loss=0.013462575618177652\n",
      "Surface training t=28955, loss=0.01559570711106062\n",
      "Surface training t=28956, loss=0.01366403279826045\n",
      "Surface training t=28957, loss=0.014441362116485834\n",
      "Surface training t=28958, loss=0.01651781238615513\n",
      "Surface training t=28959, loss=0.015161244198679924\n",
      "Surface training t=28960, loss=0.018048147205263376\n",
      "Surface training t=28961, loss=0.016576132737100124\n",
      "Surface training t=28962, loss=0.02018397720530629\n",
      "Surface training t=28963, loss=0.0205221064388752\n",
      "Surface training t=28964, loss=0.024473993107676506\n",
      "Surface training t=28965, loss=0.021146065555512905\n",
      "Surface training t=28966, loss=0.02319229021668434\n",
      "Surface training t=28967, loss=0.02831050008535385\n",
      "Surface training t=28968, loss=0.021363914478570223\n",
      "Surface training t=28969, loss=0.03179134614765644\n",
      "Surface training t=28970, loss=0.028797121718525887\n",
      "Surface training t=28971, loss=0.026906350627541542\n",
      "Surface training t=28972, loss=0.02887977473437786\n",
      "Surface training t=28973, loss=0.023250059224665165\n",
      "Surface training t=28974, loss=0.03166938293725252\n",
      "Surface training t=28975, loss=0.028340721502900124\n",
      "Surface training t=28976, loss=0.028005415573716164\n",
      "Surface training t=28977, loss=0.025534235406666994\n",
      "Surface training t=28978, loss=0.029161717742681503\n",
      "Surface training t=28979, loss=0.042498474940657616\n",
      "Surface training t=28980, loss=0.028495926409959793\n",
      "Surface training t=28981, loss=0.034326525405049324\n",
      "Surface training t=28982, loss=0.03026292659342289\n",
      "Surface training t=28983, loss=0.03169596567749977\n",
      "Surface training t=28984, loss=0.02642642054706812\n",
      "Surface training t=28985, loss=0.02460915595293045\n",
      "Surface training t=28986, loss=0.023667446337640285\n",
      "Surface training t=28987, loss=0.018627166748046875\n",
      "Surface training t=28988, loss=0.016595509834587574\n",
      "Surface training t=28989, loss=0.020105899311602116\n",
      "Surface training t=28990, loss=0.020076075568795204\n",
      "Surface training t=28991, loss=0.01841938216239214\n",
      "Surface training t=28992, loss=0.01878813561052084\n",
      "Surface training t=28993, loss=0.016288040205836296\n",
      "Surface training t=28994, loss=0.019725517369806767\n",
      "Surface training t=28995, loss=0.02965123299509287\n",
      "Surface training t=28996, loss=0.024747767485678196\n",
      "Surface training t=28997, loss=0.021781429648399353\n",
      "Surface training t=28998, loss=0.022513565607368946\n",
      "Surface training t=28999, loss=0.027518387883901596\n",
      "Surface training t=29000, loss=0.02601041179150343\n",
      "Surface training t=29001, loss=0.021231855265796185\n",
      "Surface training t=29002, loss=0.021035823971033096\n",
      "Surface training t=29003, loss=0.023322278633713722\n",
      "Surface training t=29004, loss=0.021750114858150482\n",
      "Surface training t=29005, loss=0.020877010188996792\n",
      "Surface training t=29006, loss=0.021619701758027077\n",
      "Surface training t=29007, loss=0.023700899444520473\n",
      "Surface training t=29008, loss=0.022101880982518196\n",
      "Surface training t=29009, loss=0.02399952244013548\n",
      "Surface training t=29010, loss=0.018945744261145592\n",
      "Surface training t=29011, loss=0.025493111461400986\n",
      "Surface training t=29012, loss=0.021821946837008\n",
      "Surface training t=29013, loss=0.025528921745717525\n",
      "Surface training t=29014, loss=0.022250334732234478\n",
      "Surface training t=29015, loss=0.020716868340969086\n",
      "Surface training t=29016, loss=0.016701930202543736\n",
      "Surface training t=29017, loss=0.017323646694421768\n",
      "Surface training t=29018, loss=0.019664826802909374\n",
      "Surface training t=29019, loss=0.014055895619094372\n",
      "Surface training t=29020, loss=0.019447033759206533\n",
      "Surface training t=29021, loss=0.013848271686583757\n",
      "Surface training t=29022, loss=0.018778609111905098\n",
      "Surface training t=29023, loss=0.01968843024224043\n",
      "Surface training t=29024, loss=0.020904745906591415\n",
      "Surface training t=29025, loss=0.019562757574021816\n",
      "Surface training t=29026, loss=0.018358048051595688\n",
      "Surface training t=29027, loss=0.018602230586111546\n",
      "Surface training t=29028, loss=0.02153825107961893\n",
      "Surface training t=29029, loss=0.021220793947577477\n",
      "Surface training t=29030, loss=0.022448874078691006\n",
      "Surface training t=29031, loss=0.02965468540787697\n",
      "Surface training t=29032, loss=0.02786991186439991\n",
      "Surface training t=29033, loss=0.02151462994515896\n",
      "Surface training t=29034, loss=0.022925357334315777\n",
      "Surface training t=29035, loss=0.023618855513632298\n",
      "Surface training t=29036, loss=0.02438303269445896\n",
      "Surface training t=29037, loss=0.02121406514197588\n",
      "Surface training t=29038, loss=0.02392054721713066\n",
      "Surface training t=29039, loss=0.02198473922908306\n",
      "Surface training t=29040, loss=0.019114630296826363\n",
      "Surface training t=29041, loss=0.019874799996614456\n",
      "Surface training t=29042, loss=0.01971979159861803\n",
      "Surface training t=29043, loss=0.024318194948136806\n",
      "Surface training t=29044, loss=0.02373883780092001\n",
      "Surface training t=29045, loss=0.022397511638700962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=29046, loss=0.02365478314459324\n",
      "Surface training t=29047, loss=0.019000920467078686\n",
      "Surface training t=29048, loss=0.016217736061662436\n",
      "Surface training t=29049, loss=0.01839001104235649\n",
      "Surface training t=29050, loss=0.016377100721001625\n",
      "Surface training t=29051, loss=0.014949609525501728\n",
      "Surface training t=29052, loss=0.014391777571290731\n",
      "Surface training t=29053, loss=0.01698257215321064\n",
      "Surface training t=29054, loss=0.01973069179803133\n",
      "Surface training t=29055, loss=0.019637588411569595\n",
      "Surface training t=29056, loss=0.020920664072036743\n",
      "Surface training t=29057, loss=0.018585271202027798\n",
      "Surface training t=29058, loss=0.015574693214148283\n",
      "Surface training t=29059, loss=0.01574795925989747\n",
      "Surface training t=29060, loss=0.017517096363008022\n",
      "Surface training t=29061, loss=0.027605675160884857\n",
      "Surface training t=29062, loss=0.041939616203308105\n",
      "Surface training t=29063, loss=0.030446985736489296\n",
      "Surface training t=29064, loss=0.0289367763325572\n",
      "Surface training t=29065, loss=0.030250038020312786\n",
      "Surface training t=29066, loss=0.027966447174549103\n",
      "Surface training t=29067, loss=0.03712089918553829\n",
      "Surface training t=29068, loss=0.02530212700366974\n",
      "Surface training t=29069, loss=0.031778546050190926\n",
      "Surface training t=29070, loss=0.028310182504355907\n",
      "Surface training t=29071, loss=0.03136526979506016\n",
      "Surface training t=29072, loss=0.02518876362591982\n",
      "Surface training t=29073, loss=0.025934044271707535\n",
      "Surface training t=29074, loss=0.025129356421530247\n",
      "Surface training t=29075, loss=0.024759688414633274\n",
      "Surface training t=29076, loss=0.023445624858140945\n",
      "Surface training t=29077, loss=0.025275157764554024\n",
      "Surface training t=29078, loss=0.030595588497817516\n",
      "Surface training t=29079, loss=0.018973620608448982\n",
      "Surface training t=29080, loss=0.02272775210440159\n",
      "Surface training t=29081, loss=0.01890822034329176\n",
      "Surface training t=29082, loss=0.018763805739581585\n",
      "Surface training t=29083, loss=0.0198887400329113\n",
      "Surface training t=29084, loss=0.013873269781470299\n",
      "Surface training t=29085, loss=0.016722016036510468\n",
      "Surface training t=29086, loss=0.016068466007709503\n",
      "Surface training t=29087, loss=0.018215368036180735\n",
      "Surface training t=29088, loss=0.02239479124546051\n",
      "Surface training t=29089, loss=0.02291128132492304\n",
      "Surface training t=29090, loss=0.030494517646729946\n",
      "Surface training t=29091, loss=0.02865348570048809\n",
      "Surface training t=29092, loss=0.040721019729971886\n",
      "Surface training t=29093, loss=0.0373959019780159\n",
      "Surface training t=29094, loss=0.031535626389086246\n",
      "Surface training t=29095, loss=0.023177453316748142\n",
      "Surface training t=29096, loss=0.025034231133759022\n",
      "Surface training t=29097, loss=0.038260262459516525\n",
      "Surface training t=29098, loss=0.02607919368892908\n",
      "Surface training t=29099, loss=0.026672638952732086\n",
      "Surface training t=29100, loss=0.026893381029367447\n",
      "Surface training t=29101, loss=0.0267958901822567\n",
      "Surface training t=29102, loss=0.031211095862090588\n",
      "Surface training t=29103, loss=0.023375158198177814\n",
      "Surface training t=29104, loss=0.01526568178087473\n",
      "Surface training t=29105, loss=0.020774400793015957\n",
      "Surface training t=29106, loss=0.0155120138078928\n",
      "Surface training t=29107, loss=0.013624104205518961\n",
      "Surface training t=29108, loss=0.017780032940208912\n",
      "Surface training t=29109, loss=0.01646027248352766\n",
      "Surface training t=29110, loss=0.020263610407710075\n",
      "Surface training t=29111, loss=0.019472203217446804\n",
      "Surface training t=29112, loss=0.014038304798305035\n",
      "Surface training t=29113, loss=0.023983566090464592\n",
      "Surface training t=29114, loss=0.024921216070652008\n",
      "Surface training t=29115, loss=0.02679633069783449\n",
      "Surface training t=29116, loss=0.029677349142730236\n",
      "Surface training t=29117, loss=0.027380717918276787\n",
      "Surface training t=29118, loss=0.03638037107884884\n",
      "Surface training t=29119, loss=0.025472650304436684\n",
      "Surface training t=29120, loss=0.022736193612217903\n",
      "Surface training t=29121, loss=0.02303934656083584\n",
      "Surface training t=29122, loss=0.02412707917392254\n",
      "Surface training t=29123, loss=0.0209156246855855\n",
      "Surface training t=29124, loss=0.015140050556510687\n",
      "Surface training t=29125, loss=0.020905494689941406\n",
      "Surface training t=29126, loss=0.017991550732403994\n",
      "Surface training t=29127, loss=0.01971312239766121\n",
      "Surface training t=29128, loss=0.027680118568241596\n",
      "Surface training t=29129, loss=0.024545645341277122\n",
      "Surface training t=29130, loss=0.024562422186136246\n",
      "Surface training t=29131, loss=0.019870370626449585\n",
      "Surface training t=29132, loss=0.02242872677743435\n",
      "Surface training t=29133, loss=0.017490342259407043\n",
      "Surface training t=29134, loss=0.018191833049058914\n",
      "Surface training t=29135, loss=0.015123885124921799\n",
      "Surface training t=29136, loss=0.01972009427845478\n",
      "Surface training t=29137, loss=0.018441757187247276\n",
      "Surface training t=29138, loss=0.023781546391546726\n",
      "Surface training t=29139, loss=0.02267547557130456\n",
      "Surface training t=29140, loss=0.04036393202841282\n",
      "Surface training t=29141, loss=0.025511451065540314\n",
      "Surface training t=29142, loss=0.02549172844737768\n",
      "Surface training t=29143, loss=0.02379641029983759\n",
      "Surface training t=29144, loss=0.020553006790578365\n",
      "Surface training t=29145, loss=0.03043218143284321\n",
      "Surface training t=29146, loss=0.029002659022808075\n",
      "Surface training t=29147, loss=0.031513637863099575\n",
      "Surface training t=29148, loss=0.020550472661852837\n",
      "Surface training t=29149, loss=0.02111596893519163\n",
      "Surface training t=29150, loss=0.020428799092769623\n",
      "Surface training t=29151, loss=0.016455935779958963\n",
      "Surface training t=29152, loss=0.01994434092193842\n",
      "Surface training t=29153, loss=0.01939000701531768\n",
      "Surface training t=29154, loss=0.030014317482709885\n",
      "Surface training t=29155, loss=0.018613822758197784\n",
      "Surface training t=29156, loss=0.03286219108849764\n",
      "Surface training t=29157, loss=0.02469373494386673\n",
      "Surface training t=29158, loss=0.030068861320614815\n",
      "Surface training t=29159, loss=0.031307765282690525\n",
      "Surface training t=29160, loss=0.02456492930650711\n",
      "Surface training t=29161, loss=0.023273021914064884\n",
      "Surface training t=29162, loss=0.01840691128745675\n",
      "Surface training t=29163, loss=0.01964744133874774\n",
      "Surface training t=29164, loss=0.017116663977503777\n",
      "Surface training t=29165, loss=0.019438568502664566\n",
      "Surface training t=29166, loss=0.01942500565201044\n",
      "Surface training t=29167, loss=0.026316150091588497\n",
      "Surface training t=29168, loss=0.021960845217108727\n",
      "Surface training t=29169, loss=0.037206023931503296\n",
      "Surface training t=29170, loss=0.023588016629219055\n",
      "Surface training t=29171, loss=0.02397868037223816\n",
      "Surface training t=29172, loss=0.018405282869935036\n",
      "Surface training t=29173, loss=0.01898869313299656\n",
      "Surface training t=29174, loss=0.017798809334635735\n",
      "Surface training t=29175, loss=0.01959795318543911\n",
      "Surface training t=29176, loss=0.019226194359362125\n",
      "Surface training t=29177, loss=0.01778280781581998\n",
      "Surface training t=29178, loss=0.022001356817781925\n",
      "Surface training t=29179, loss=0.021079701371490955\n",
      "Surface training t=29180, loss=0.022535481490194798\n",
      "Surface training t=29181, loss=0.023041159845888615\n",
      "Surface training t=29182, loss=0.017192560248076916\n",
      "Surface training t=29183, loss=0.020600805059075356\n",
      "Surface training t=29184, loss=0.020657140761613846\n",
      "Surface training t=29185, loss=0.018676089122891426\n",
      "Surface training t=29186, loss=0.02421855367720127\n",
      "Surface training t=29187, loss=0.019537806510925293\n",
      "Surface training t=29188, loss=0.022753284312784672\n",
      "Surface training t=29189, loss=0.02269265428185463\n",
      "Surface training t=29190, loss=0.02129492722451687\n",
      "Surface training t=29191, loss=0.023215893656015396\n",
      "Surface training t=29192, loss=0.019843578338623047\n",
      "Surface training t=29193, loss=0.018458771519362926\n",
      "Surface training t=29194, loss=0.020084106363356113\n",
      "Surface training t=29195, loss=0.018168329261243343\n",
      "Surface training t=29196, loss=0.01645067147910595\n",
      "Surface training t=29197, loss=0.014021266251802444\n",
      "Surface training t=29198, loss=0.018999028019607067\n",
      "Surface training t=29199, loss=0.014657091349363327\n",
      "Surface training t=29200, loss=0.020985567942261696\n",
      "Surface training t=29201, loss=0.019296150654554367\n",
      "Surface training t=29202, loss=0.015884382650256157\n",
      "Surface training t=29203, loss=0.020510658621788025\n",
      "Surface training t=29204, loss=0.022551078349351883\n",
      "Surface training t=29205, loss=0.027913900092244148\n",
      "Surface training t=29206, loss=0.022670513950288296\n",
      "Surface training t=29207, loss=0.022257410921156406\n",
      "Surface training t=29208, loss=0.01989260222762823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=29209, loss=0.02028017397969961\n",
      "Surface training t=29210, loss=0.020237069576978683\n",
      "Surface training t=29211, loss=0.018841424956917763\n",
      "Surface training t=29212, loss=0.01623879000544548\n",
      "Surface training t=29213, loss=0.013659941963851452\n",
      "Surface training t=29214, loss=0.01849756482988596\n",
      "Surface training t=29215, loss=0.021340693347156048\n",
      "Surface training t=29216, loss=0.014382253400981426\n",
      "Surface training t=29217, loss=0.020169914700090885\n",
      "Surface training t=29218, loss=0.01980801671743393\n",
      "Surface training t=29219, loss=0.02505636401474476\n",
      "Surface training t=29220, loss=0.023336791433393955\n",
      "Surface training t=29221, loss=0.026109951548278332\n",
      "Surface training t=29222, loss=0.023057502694427967\n",
      "Surface training t=29223, loss=0.021965505555272102\n",
      "Surface training t=29224, loss=0.02689413633197546\n",
      "Surface training t=29225, loss=0.018129597418010235\n",
      "Surface training t=29226, loss=0.028236418962478638\n",
      "Surface training t=29227, loss=0.021503658033907413\n",
      "Surface training t=29228, loss=0.019109779968857765\n",
      "Surface training t=29229, loss=0.016900899820029736\n",
      "Surface training t=29230, loss=0.01902852300554514\n",
      "Surface training t=29231, loss=0.018580819480121136\n",
      "Surface training t=29232, loss=0.01950472593307495\n",
      "Surface training t=29233, loss=0.017330854199826717\n",
      "Surface training t=29234, loss=0.02372224908322096\n",
      "Surface training t=29235, loss=0.031132232397794724\n",
      "Surface training t=29236, loss=0.022282726131379604\n",
      "Surface training t=29237, loss=0.025475237518548965\n",
      "Surface training t=29238, loss=0.02886307705193758\n",
      "Surface training t=29239, loss=0.023722671903669834\n",
      "Surface training t=29240, loss=0.024788102135062218\n",
      "Surface training t=29241, loss=0.02905537374317646\n",
      "Surface training t=29242, loss=0.022905477322638035\n",
      "Surface training t=29243, loss=0.024247776716947556\n",
      "Surface training t=29244, loss=0.01874797698110342\n",
      "Surface training t=29245, loss=0.019943115301430225\n",
      "Surface training t=29246, loss=0.020948332734405994\n",
      "Surface training t=29247, loss=0.021441608667373657\n",
      "Surface training t=29248, loss=0.025065426714718342\n",
      "Surface training t=29249, loss=0.028221272863447666\n",
      "Surface training t=29250, loss=0.02639345731586218\n",
      "Surface training t=29251, loss=0.03278641030192375\n",
      "Surface training t=29252, loss=0.02747097983956337\n",
      "Surface training t=29253, loss=0.024299509823322296\n",
      "Surface training t=29254, loss=0.017981605604290962\n",
      "Surface training t=29255, loss=0.013690900057554245\n",
      "Surface training t=29256, loss=0.01294645806774497\n",
      "Surface training t=29257, loss=0.017528342083096504\n",
      "Surface training t=29258, loss=0.017040971666574478\n",
      "Surface training t=29259, loss=0.015492101199924946\n",
      "Surface training t=29260, loss=0.021295467391610146\n",
      "Surface training t=29261, loss=0.019248931668698788\n",
      "Surface training t=29262, loss=0.017660814337432384\n",
      "Surface training t=29263, loss=0.020385310985147953\n",
      "Surface training t=29264, loss=0.025747723877429962\n",
      "Surface training t=29265, loss=0.045113204047083855\n",
      "Surface training t=29266, loss=0.03633182682096958\n",
      "Surface training t=29267, loss=0.03886093385517597\n",
      "Surface training t=29268, loss=0.05038223974406719\n",
      "Surface training t=29269, loss=0.03262940235435963\n",
      "Surface training t=29270, loss=0.02717206533998251\n",
      "Surface training t=29271, loss=0.03855464793741703\n",
      "Surface training t=29272, loss=0.02835122775286436\n",
      "Surface training t=29273, loss=0.02457977831363678\n",
      "Surface training t=29274, loss=0.0404912494122982\n",
      "Surface training t=29275, loss=0.02791808918118477\n",
      "Surface training t=29276, loss=0.02052509319037199\n",
      "Surface training t=29277, loss=0.02311832085251808\n",
      "Surface training t=29278, loss=0.018979868851602077\n",
      "Surface training t=29279, loss=0.024195742793381214\n",
      "Surface training t=29280, loss=0.02151592541486025\n",
      "Surface training t=29281, loss=0.018146363086998463\n",
      "Surface training t=29282, loss=0.020632341504096985\n",
      "Surface training t=29283, loss=0.018075432628393173\n",
      "Surface training t=29284, loss=0.02723690029233694\n",
      "Surface training t=29285, loss=0.0210111141204834\n",
      "Surface training t=29286, loss=0.02050198893994093\n",
      "Surface training t=29287, loss=0.022049177438020706\n",
      "Surface training t=29288, loss=0.024195008911192417\n",
      "Surface training t=29289, loss=0.031491803005337715\n",
      "Surface training t=29290, loss=0.02928187232464552\n",
      "Surface training t=29291, loss=0.022766555659472942\n",
      "Surface training t=29292, loss=0.024096111766994\n",
      "Surface training t=29293, loss=0.022281189914792776\n",
      "Surface training t=29294, loss=0.018541046418249607\n",
      "Surface training t=29295, loss=0.015765012241899967\n",
      "Surface training t=29296, loss=0.01497985003516078\n",
      "Surface training t=29297, loss=0.023968957364559174\n",
      "Surface training t=29298, loss=0.018437738064676523\n",
      "Surface training t=29299, loss=0.017441716976463795\n",
      "Surface training t=29300, loss=0.017258302308619022\n",
      "Surface training t=29301, loss=0.021580509841442108\n",
      "Surface training t=29302, loss=0.022159661166369915\n",
      "Surface training t=29303, loss=0.023492394015192986\n",
      "Surface training t=29304, loss=0.020599202252924442\n",
      "Surface training t=29305, loss=0.02138794120401144\n",
      "Surface training t=29306, loss=0.018887692131102085\n",
      "Surface training t=29307, loss=0.020644099451601505\n",
      "Surface training t=29308, loss=0.02087893709540367\n",
      "Surface training t=29309, loss=0.02240639179944992\n",
      "Surface training t=29310, loss=0.022672610357403755\n",
      "Surface training t=29311, loss=0.02765767090022564\n",
      "Surface training t=29312, loss=0.02190059283748269\n",
      "Surface training t=29313, loss=0.015846506226807833\n",
      "Surface training t=29314, loss=0.02502893004566431\n",
      "Surface training t=29315, loss=0.022942733950912952\n",
      "Surface training t=29316, loss=0.030611030757427216\n",
      "Surface training t=29317, loss=0.03171465918421745\n",
      "Surface training t=29318, loss=0.02372313104569912\n",
      "Surface training t=29319, loss=0.026660412549972534\n",
      "Surface training t=29320, loss=0.03181570954620838\n",
      "Surface training t=29321, loss=0.029494822025299072\n",
      "Surface training t=29322, loss=0.026431843638420105\n",
      "Surface training t=29323, loss=0.028689892031252384\n",
      "Surface training t=29324, loss=0.03692781552672386\n",
      "Surface training t=29325, loss=0.0292420806363225\n",
      "Surface training t=29326, loss=0.026006408035755157\n",
      "Surface training t=29327, loss=0.02711578831076622\n",
      "Surface training t=29328, loss=0.021251468919217587\n",
      "Surface training t=29329, loss=0.017630507238209248\n",
      "Surface training t=29330, loss=0.019691821187734604\n",
      "Surface training t=29331, loss=0.014388163108378649\n",
      "Surface training t=29332, loss=0.015401257667690516\n",
      "Surface training t=29333, loss=0.017178313340991735\n",
      "Surface training t=29334, loss=0.020154439844191074\n",
      "Surface training t=29335, loss=0.01654163096100092\n",
      "Surface training t=29336, loss=0.01931009255349636\n",
      "Surface training t=29337, loss=0.021685520187020302\n",
      "Surface training t=29338, loss=0.022712526842951775\n",
      "Surface training t=29339, loss=0.02255096472799778\n",
      "Surface training t=29340, loss=0.02056992705911398\n",
      "Surface training t=29341, loss=0.02637281734496355\n",
      "Surface training t=29342, loss=0.021924849599599838\n",
      "Surface training t=29343, loss=0.01893447618931532\n",
      "Surface training t=29344, loss=0.01799632143229246\n",
      "Surface training t=29345, loss=0.02570034097880125\n",
      "Surface training t=29346, loss=0.026208776980638504\n",
      "Surface training t=29347, loss=0.016478739213198423\n",
      "Surface training t=29348, loss=0.019847498275339603\n",
      "Surface training t=29349, loss=0.021129991859197617\n",
      "Surface training t=29350, loss=0.020673324819654226\n",
      "Surface training t=29351, loss=0.02670413814485073\n",
      "Surface training t=29352, loss=0.02008654922246933\n",
      "Surface training t=29353, loss=0.01706905709579587\n",
      "Surface training t=29354, loss=0.014661320019513369\n",
      "Surface training t=29355, loss=0.01674505230039358\n",
      "Surface training t=29356, loss=0.01796823600307107\n",
      "Surface training t=29357, loss=0.018027547746896744\n",
      "Surface training t=29358, loss=0.019483771175146103\n",
      "Surface training t=29359, loss=0.02215210348367691\n",
      "Surface training t=29360, loss=0.021011849865317345\n",
      "Surface training t=29361, loss=0.021571194753050804\n",
      "Surface training t=29362, loss=0.02106405608355999\n",
      "Surface training t=29363, loss=0.02752532623708248\n",
      "Surface training t=29364, loss=0.024394833482801914\n",
      "Surface training t=29365, loss=0.023987092077732086\n",
      "Surface training t=29366, loss=0.020378471352159977\n",
      "Surface training t=29367, loss=0.01730310544371605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=29368, loss=0.019583331421017647\n",
      "Surface training t=29369, loss=0.019394539762288332\n",
      "Surface training t=29370, loss=0.0166319552809\n",
      "Surface training t=29371, loss=0.02518338803201914\n",
      "Surface training t=29372, loss=0.022376442328095436\n",
      "Surface training t=29373, loss=0.031769792549312115\n",
      "Surface training t=29374, loss=0.026970344595611095\n",
      "Surface training t=29375, loss=0.02402542717754841\n",
      "Surface training t=29376, loss=0.031439694575965405\n",
      "Surface training t=29377, loss=0.02417639084160328\n",
      "Surface training t=29378, loss=0.02395413164049387\n",
      "Surface training t=29379, loss=0.028180010616779327\n",
      "Surface training t=29380, loss=0.021694854833185673\n",
      "Surface training t=29381, loss=0.018867562524974346\n",
      "Surface training t=29382, loss=0.017978125251829624\n",
      "Surface training t=29383, loss=0.018406430259346962\n",
      "Surface training t=29384, loss=0.013503322377800941\n",
      "Surface training t=29385, loss=0.015190440230071545\n",
      "Surface training t=29386, loss=0.014592408668249846\n",
      "Surface training t=29387, loss=0.018536150455474854\n",
      "Surface training t=29388, loss=0.013827044051140547\n",
      "Surface training t=29389, loss=0.019632547162473202\n",
      "Surface training t=29390, loss=0.020400664769113064\n",
      "Surface training t=29391, loss=0.019324022345244884\n",
      "Surface training t=29392, loss=0.021567024290561676\n",
      "Surface training t=29393, loss=0.02208765083923936\n",
      "Surface training t=29394, loss=0.023997562937438488\n",
      "Surface training t=29395, loss=0.026177686639130116\n",
      "Surface training t=29396, loss=0.021909545175731182\n",
      "Surface training t=29397, loss=0.02401138376444578\n",
      "Surface training t=29398, loss=0.0404724795371294\n",
      "Surface training t=29399, loss=0.030462765134871006\n",
      "Surface training t=29400, loss=0.027183149941265583\n",
      "Surface training t=29401, loss=0.02951975166797638\n",
      "Surface training t=29402, loss=0.03385128080844879\n",
      "Surface training t=29403, loss=0.03221687488257885\n",
      "Surface training t=29404, loss=0.028475367464125156\n",
      "Surface training t=29405, loss=0.02739800699055195\n",
      "Surface training t=29406, loss=0.03414727374911308\n",
      "Surface training t=29407, loss=0.027074034325778484\n",
      "Surface training t=29408, loss=0.035030593164265156\n",
      "Surface training t=29409, loss=0.02430464792996645\n",
      "Surface training t=29410, loss=0.019341975916177034\n",
      "Surface training t=29411, loss=0.015602043364197016\n",
      "Surface training t=29412, loss=0.014371404889971018\n",
      "Surface training t=29413, loss=0.01681789569556713\n",
      "Surface training t=29414, loss=0.012915321625769138\n",
      "Surface training t=29415, loss=0.014116611797362566\n",
      "Surface training t=29416, loss=0.015919675584882498\n",
      "Surface training t=29417, loss=0.01725784782320261\n",
      "Surface training t=29418, loss=0.019672885537147522\n",
      "Surface training t=29419, loss=0.0241195410490036\n",
      "Surface training t=29420, loss=0.03107471950352192\n",
      "Surface training t=29421, loss=0.03113140445202589\n",
      "Surface training t=29422, loss=0.032946085557341576\n",
      "Surface training t=29423, loss=0.03527084458619356\n",
      "Surface training t=29424, loss=0.048583053052425385\n",
      "Surface training t=29425, loss=0.031590577214956284\n",
      "Surface training t=29426, loss=0.03631907142698765\n",
      "Surface training t=29427, loss=0.03344922885298729\n",
      "Surface training t=29428, loss=0.022930738516151905\n",
      "Surface training t=29429, loss=0.017076756805181503\n",
      "Surface training t=29430, loss=0.021931233815848827\n",
      "Surface training t=29431, loss=0.015717410948127508\n",
      "Surface training t=29432, loss=0.018428323790431023\n",
      "Surface training t=29433, loss=0.0193688552826643\n",
      "Surface training t=29434, loss=0.018753331154584885\n",
      "Surface training t=29435, loss=0.016884385608136654\n",
      "Surface training t=29436, loss=0.023841767571866512\n",
      "Surface training t=29437, loss=0.0367765836417675\n",
      "Surface training t=29438, loss=0.031195112504065037\n",
      "Surface training t=29439, loss=0.032971800304949284\n",
      "Surface training t=29440, loss=0.04029292240738869\n",
      "Surface training t=29441, loss=0.02532463427633047\n",
      "Surface training t=29442, loss=0.03003362100571394\n",
      "Surface training t=29443, loss=0.03138005826622248\n",
      "Surface training t=29444, loss=0.04467909410595894\n",
      "Surface training t=29445, loss=0.031953610479831696\n",
      "Surface training t=29446, loss=0.03888222575187683\n",
      "Surface training t=29447, loss=0.02919499482959509\n",
      "Surface training t=29448, loss=0.031166321597993374\n",
      "Surface training t=29449, loss=0.025690799579024315\n",
      "Surface training t=29450, loss=0.02947564795613289\n",
      "Surface training t=29451, loss=0.030030897818505764\n",
      "Surface training t=29452, loss=0.031788713298738\n",
      "Surface training t=29453, loss=0.02934448979794979\n",
      "Surface training t=29454, loss=0.023346341215074062\n",
      "Surface training t=29455, loss=0.02769977878779173\n",
      "Surface training t=29456, loss=0.02111941948533058\n",
      "Surface training t=29457, loss=0.02243757527321577\n",
      "Surface training t=29458, loss=0.02939139399677515\n",
      "Surface training t=29459, loss=0.018665705807507038\n",
      "Surface training t=29460, loss=0.01933712512254715\n",
      "Surface training t=29461, loss=0.01860786648467183\n",
      "Surface training t=29462, loss=0.019889009185135365\n",
      "Surface training t=29463, loss=0.03182004485279322\n",
      "Surface training t=29464, loss=0.0224622106179595\n",
      "Surface training t=29465, loss=0.024221650324761868\n",
      "Surface training t=29466, loss=0.022662472911179066\n",
      "Surface training t=29467, loss=0.022655491717159748\n",
      "Surface training t=29468, loss=0.024885278195142746\n",
      "Surface training t=29469, loss=0.02688170224428177\n",
      "Surface training t=29470, loss=0.032831630669534206\n",
      "Surface training t=29471, loss=0.031546613201498985\n",
      "Surface training t=29472, loss=0.02988307923078537\n",
      "Surface training t=29473, loss=0.02587086707353592\n",
      "Surface training t=29474, loss=0.02764324191957712\n",
      "Surface training t=29475, loss=0.028215081430971622\n",
      "Surface training t=29476, loss=0.03327199071645737\n",
      "Surface training t=29477, loss=0.029089517891407013\n",
      "Surface training t=29478, loss=0.03980428911745548\n",
      "Surface training t=29479, loss=0.02608107589185238\n",
      "Surface training t=29480, loss=0.02490478102117777\n",
      "Surface training t=29481, loss=0.028838111087679863\n",
      "Surface training t=29482, loss=0.03492502309381962\n",
      "Surface training t=29483, loss=0.027312958613038063\n",
      "Surface training t=29484, loss=0.021275797858834267\n",
      "Surface training t=29485, loss=0.01988876983523369\n",
      "Surface training t=29486, loss=0.017097957897931337\n",
      "Surface training t=29487, loss=0.020779625978320837\n",
      "Surface training t=29488, loss=0.014191565103828907\n",
      "Surface training t=29489, loss=0.020086741540580988\n",
      "Surface training t=29490, loss=0.013589482754468918\n",
      "Surface training t=29491, loss=0.017669321969151497\n",
      "Surface training t=29492, loss=0.01603561220690608\n",
      "Surface training t=29493, loss=0.012580476235598326\n",
      "Surface training t=29494, loss=0.017303123138844967\n",
      "Surface training t=29495, loss=0.016309037804603577\n",
      "Surface training t=29496, loss=0.02181675285100937\n",
      "Surface training t=29497, loss=0.020296700298786163\n",
      "Surface training t=29498, loss=0.018416711129248142\n",
      "Surface training t=29499, loss=0.016903511248528957\n",
      "Surface training t=29500, loss=0.020620360039174557\n",
      "Surface training t=29501, loss=0.023600898683071136\n",
      "Surface training t=29502, loss=0.021705593913793564\n",
      "Surface training t=29503, loss=0.020471067167818546\n",
      "Surface training t=29504, loss=0.02166762202978134\n",
      "Surface training t=29505, loss=0.017352741211652756\n",
      "Surface training t=29506, loss=0.021798133850097656\n",
      "Surface training t=29507, loss=0.021737227216362953\n",
      "Surface training t=29508, loss=0.016715078614652157\n",
      "Surface training t=29509, loss=0.019836957566440105\n",
      "Surface training t=29510, loss=0.02274981141090393\n",
      "Surface training t=29511, loss=0.019335986115038395\n",
      "Surface training t=29512, loss=0.0175213273614645\n",
      "Surface training t=29513, loss=0.018229457084089518\n",
      "Surface training t=29514, loss=0.019324075430631638\n",
      "Surface training t=29515, loss=0.01951152551919222\n",
      "Surface training t=29516, loss=0.025622384622693062\n",
      "Surface training t=29517, loss=0.017422419972717762\n",
      "Surface training t=29518, loss=0.0197078175842762\n",
      "Surface training t=29519, loss=0.020047455094754696\n",
      "Surface training t=29520, loss=0.020529597997665405\n",
      "Surface training t=29521, loss=0.017994807101786137\n",
      "Surface training t=29522, loss=0.01986873708665371\n",
      "Surface training t=29523, loss=0.02335538249462843\n",
      "Surface training t=29524, loss=0.022425963543355465\n",
      "Surface training t=29525, loss=0.02968546375632286\n",
      "Surface training t=29526, loss=0.020053458400070667\n",
      "Surface training t=29527, loss=0.02992229349911213\n",
      "Surface training t=29528, loss=0.0223309388384223\n",
      "Surface training t=29529, loss=0.027400633320212364\n",
      "Surface training t=29530, loss=0.024457491002976894\n",
      "Surface training t=29531, loss=0.017720289062708616\n",
      "Surface training t=29532, loss=0.017845946364104748\n",
      "Surface training t=29533, loss=0.03504388779401779\n",
      "Surface training t=29534, loss=0.02644712943583727\n",
      "Surface training t=29535, loss=0.025794202461838722\n",
      "Surface training t=29536, loss=0.032259054481983185\n",
      "Surface training t=29537, loss=0.023616457358002663\n",
      "Surface training t=29538, loss=0.033657921478152275\n",
      "Surface training t=29539, loss=0.024008145555853844\n",
      "Surface training t=29540, loss=0.026354470290243626\n",
      "Surface training t=29541, loss=0.023582992143929005\n",
      "Surface training t=29542, loss=0.028335070237517357\n",
      "Surface training t=29543, loss=0.021920649334788322\n",
      "Surface training t=29544, loss=0.025348499417304993\n",
      "Surface training t=29545, loss=0.03366624005138874\n",
      "Surface training t=29546, loss=0.02339800726622343\n",
      "Surface training t=29547, loss=0.03580856695771217\n",
      "Surface training t=29548, loss=0.02399979904294014\n",
      "Surface training t=29549, loss=0.030684998258948326\n",
      "Surface training t=29550, loss=0.028315476141870022\n",
      "Surface training t=29551, loss=0.02336817979812622\n",
      "Surface training t=29552, loss=0.022250869311392307\n",
      "Surface training t=29553, loss=0.024952294304966927\n",
      "Surface training t=29554, loss=0.02589479275047779\n",
      "Surface training t=29555, loss=0.022311614826321602\n",
      "Surface training t=29556, loss=0.03107792790979147\n",
      "Surface training t=29557, loss=0.028414483182132244\n",
      "Surface training t=29558, loss=0.027178998105227947\n",
      "Surface training t=29559, loss=0.026786159723997116\n",
      "Surface training t=29560, loss=0.024786902591586113\n",
      "Surface training t=29561, loss=0.02555557992309332\n",
      "Surface training t=29562, loss=0.035740869119763374\n",
      "Surface training t=29563, loss=0.022794298827648163\n",
      "Surface training t=29564, loss=0.037112802267074585\n",
      "Surface training t=29565, loss=0.02799899782985449\n",
      "Surface training t=29566, loss=0.03569687530398369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=29567, loss=0.02475880552083254\n",
      "Surface training t=29568, loss=0.025749681517481804\n",
      "Surface training t=29569, loss=0.027271827682852745\n",
      "Surface training t=29570, loss=0.02011022437363863\n",
      "Surface training t=29571, loss=0.021458139643073082\n",
      "Surface training t=29572, loss=0.02654960099607706\n",
      "Surface training t=29573, loss=0.024014399386942387\n",
      "Surface training t=29574, loss=0.024801072664558887\n",
      "Surface training t=29575, loss=0.029582575894892216\n",
      "Surface training t=29576, loss=0.02415164466947317\n",
      "Surface training t=29577, loss=0.026303276419639587\n",
      "Surface training t=29578, loss=0.03741687163710594\n",
      "Surface training t=29579, loss=0.022091861814260483\n",
      "Surface training t=29580, loss=0.026933244429528713\n",
      "Surface training t=29581, loss=0.02378106117248535\n",
      "Surface training t=29582, loss=0.03515072353184223\n",
      "Surface training t=29583, loss=0.02922656200826168\n",
      "Surface training t=29584, loss=0.02729581482708454\n",
      "Surface training t=29585, loss=0.022264248691499233\n",
      "Surface training t=29586, loss=0.02256679628044367\n",
      "Surface training t=29587, loss=0.021777364425361156\n",
      "Surface training t=29588, loss=0.023205159232020378\n",
      "Surface training t=29589, loss=0.021864193491637707\n",
      "Surface training t=29590, loss=0.022179963067173958\n",
      "Surface training t=29591, loss=0.01924283057451248\n",
      "Surface training t=29592, loss=0.026082657277584076\n",
      "Surface training t=29593, loss=0.022666187956929207\n",
      "Surface training t=29594, loss=0.029782886616885662\n",
      "Surface training t=29595, loss=0.022903243079781532\n",
      "Surface training t=29596, loss=0.02119898423552513\n",
      "Surface training t=29597, loss=0.021346301771700382\n",
      "Surface training t=29598, loss=0.01853455137461424\n",
      "Surface training t=29599, loss=0.026257683522999287\n",
      "Surface training t=29600, loss=0.018598022405058146\n",
      "Surface training t=29601, loss=0.025964653119444847\n",
      "Surface training t=29602, loss=0.022004151716828346\n",
      "Surface training t=29603, loss=0.021862090565264225\n",
      "Surface training t=29604, loss=0.023362922482192516\n",
      "Surface training t=29605, loss=0.016468938440084457\n",
      "Surface training t=29606, loss=0.017783548682928085\n",
      "Surface training t=29607, loss=0.01411726651713252\n",
      "Surface training t=29608, loss=0.014458620455116034\n",
      "Surface training t=29609, loss=0.017261475324630737\n",
      "Surface training t=29610, loss=0.02419565198943019\n",
      "Surface training t=29611, loss=0.022871244698762894\n",
      "Surface training t=29612, loss=0.030119390226900578\n",
      "Surface training t=29613, loss=0.02209140546619892\n",
      "Surface training t=29614, loss=0.018272737972438335\n",
      "Surface training t=29615, loss=0.017883997410535812\n",
      "Surface training t=29616, loss=0.021214899607002735\n",
      "Surface training t=29617, loss=0.02050775568932295\n",
      "Surface training t=29618, loss=0.023993327282369137\n",
      "Surface training t=29619, loss=0.028156431391835213\n",
      "Surface training t=29620, loss=0.027623753994703293\n",
      "Surface training t=29621, loss=0.04202774539589882\n",
      "Surface training t=29622, loss=0.029473312199115753\n",
      "Surface training t=29623, loss=0.030000525526702404\n",
      "Surface training t=29624, loss=0.02211825642734766\n",
      "Surface training t=29625, loss=0.015525564085692167\n",
      "Surface training t=29626, loss=0.014226252678781748\n",
      "Surface training t=29627, loss=0.019230348989367485\n",
      "Surface training t=29628, loss=0.029354729689657688\n",
      "Surface training t=29629, loss=0.024400577880442142\n",
      "Surface training t=29630, loss=0.025558216497302055\n",
      "Surface training t=29631, loss=0.01861862652003765\n",
      "Surface training t=29632, loss=0.024203951470553875\n",
      "Surface training t=29633, loss=0.022617959417402744\n",
      "Surface training t=29634, loss=0.017292147036641836\n",
      "Surface training t=29635, loss=0.018560541793704033\n",
      "Surface training t=29636, loss=0.016839567571878433\n",
      "Surface training t=29637, loss=0.019571200013160706\n",
      "Surface training t=29638, loss=0.01864648424088955\n",
      "Surface training t=29639, loss=0.016224182210862637\n",
      "Surface training t=29640, loss=0.022502263076603413\n",
      "Surface training t=29641, loss=0.02109378855675459\n",
      "Surface training t=29642, loss=0.021711258217692375\n",
      "Surface training t=29643, loss=0.022792129777371883\n",
      "Surface training t=29644, loss=0.018778773956000805\n",
      "Surface training t=29645, loss=0.015395778696984053\n",
      "Surface training t=29646, loss=0.01983144972473383\n",
      "Surface training t=29647, loss=0.023535221815109253\n",
      "Surface training t=29648, loss=0.01988779567182064\n",
      "Surface training t=29649, loss=0.018785260152071714\n",
      "Surface training t=29650, loss=0.025851329788565636\n",
      "Surface training t=29651, loss=0.02036323957145214\n",
      "Surface training t=29652, loss=0.018074178136885166\n",
      "Surface training t=29653, loss=0.0163605697453022\n",
      "Surface training t=29654, loss=0.018782184924930334\n",
      "Surface training t=29655, loss=0.023432280868291855\n",
      "Surface training t=29656, loss=0.020849241875112057\n",
      "Surface training t=29657, loss=0.017664218321442604\n",
      "Surface training t=29658, loss=0.020799074321985245\n",
      "Surface training t=29659, loss=0.01663651457056403\n",
      "Surface training t=29660, loss=0.016496209893375635\n",
      "Surface training t=29661, loss=0.01702385675162077\n",
      "Surface training t=29662, loss=0.01453926507383585\n",
      "Surface training t=29663, loss=0.012836601585149765\n",
      "Surface training t=29664, loss=0.01861500460654497\n",
      "Surface training t=29665, loss=0.014850514940917492\n",
      "Surface training t=29666, loss=0.01733216503635049\n",
      "Surface training t=29667, loss=0.01959361881017685\n",
      "Surface training t=29668, loss=0.015564522705972195\n",
      "Surface training t=29669, loss=0.016947218216955662\n",
      "Surface training t=29670, loss=0.017223251052200794\n",
      "Surface training t=29671, loss=0.01917840540409088\n",
      "Surface training t=29672, loss=0.019630596041679382\n",
      "Surface training t=29673, loss=0.018777587451040745\n",
      "Surface training t=29674, loss=0.020675363019108772\n",
      "Surface training t=29675, loss=0.01967785693705082\n",
      "Surface training t=29676, loss=0.017175479792058468\n",
      "Surface training t=29677, loss=0.016369094140827656\n",
      "Surface training t=29678, loss=0.02133307233452797\n",
      "Surface training t=29679, loss=0.030416259542107582\n",
      "Surface training t=29680, loss=0.019442042335867882\n",
      "Surface training t=29681, loss=0.019787516444921494\n",
      "Surface training t=29682, loss=0.019965705927461386\n",
      "Surface training t=29683, loss=0.014823487028479576\n",
      "Surface training t=29684, loss=0.01634002709761262\n",
      "Surface training t=29685, loss=0.01482740556821227\n",
      "Surface training t=29686, loss=0.017588055226951838\n",
      "Surface training t=29687, loss=0.016701769083738327\n",
      "Surface training t=29688, loss=0.01692149043083191\n",
      "Surface training t=29689, loss=0.013840577099472284\n",
      "Surface training t=29690, loss=0.017711053602397442\n",
      "Surface training t=29691, loss=0.015047397464513779\n",
      "Surface training t=29692, loss=0.016697216778993607\n",
      "Surface training t=29693, loss=0.014408404473215342\n",
      "Surface training t=29694, loss=0.01996171986684203\n",
      "Surface training t=29695, loss=0.016629245597869158\n",
      "Surface training t=29696, loss=0.01642533764243126\n",
      "Surface training t=29697, loss=0.014021879527717829\n",
      "Surface training t=29698, loss=0.02031697239726782\n",
      "Surface training t=29699, loss=0.023931751027703285\n",
      "Surface training t=29700, loss=0.02210702421143651\n",
      "Surface training t=29701, loss=0.01987493224442005\n",
      "Surface training t=29702, loss=0.015795886516571045\n",
      "Surface training t=29703, loss=0.016110908705741167\n",
      "Surface training t=29704, loss=0.01890998287126422\n",
      "Surface training t=29705, loss=0.015659977681934834\n",
      "Surface training t=29706, loss=0.016956143081188202\n",
      "Surface training t=29707, loss=0.018176656682044268\n",
      "Surface training t=29708, loss=0.017635583877563477\n",
      "Surface training t=29709, loss=0.014587538782507181\n",
      "Surface training t=29710, loss=0.02498567197471857\n",
      "Surface training t=29711, loss=0.020480629056692123\n",
      "Surface training t=29712, loss=0.018000999931246042\n",
      "Surface training t=29713, loss=0.024383334442973137\n",
      "Surface training t=29714, loss=0.023536997847259045\n",
      "Surface training t=29715, loss=0.024314158596098423\n",
      "Surface training t=29716, loss=0.018718082457780838\n",
      "Surface training t=29717, loss=0.02956498321145773\n",
      "Surface training t=29718, loss=0.023268534801900387\n",
      "Surface training t=29719, loss=0.021544702351093292\n",
      "Surface training t=29720, loss=0.021416292525827885\n",
      "Surface training t=29721, loss=0.018578249029815197\n",
      "Surface training t=29722, loss=0.0205730851739645\n",
      "Surface training t=29723, loss=0.02019891981035471\n",
      "Surface training t=29724, loss=0.027872641570866108\n",
      "Surface training t=29725, loss=0.021472956985235214\n",
      "Surface training t=29726, loss=0.019467683508992195\n",
      "Surface training t=29727, loss=0.019790716469287872\n",
      "Surface training t=29728, loss=0.021712946705520153\n",
      "Surface training t=29729, loss=0.019040679559111595\n",
      "Surface training t=29730, loss=0.017476605251431465\n",
      "Surface training t=29731, loss=0.01804474927484989\n",
      "Surface training t=29732, loss=0.021810476668179035\n",
      "Surface training t=29733, loss=0.015769347082823515\n",
      "Surface training t=29734, loss=0.018455184530466795\n",
      "Surface training t=29735, loss=0.01941262185573578\n",
      "Surface training t=29736, loss=0.030244327150285244\n",
      "Surface training t=29737, loss=0.02086898684501648\n",
      "Surface training t=29738, loss=0.023434383794665337\n",
      "Surface training t=29739, loss=0.020617444068193436\n",
      "Surface training t=29740, loss=0.021015114150941372\n",
      "Surface training t=29741, loss=0.021029938012361526\n",
      "Surface training t=29742, loss=0.014487390406429768\n",
      "Surface training t=29743, loss=0.016313265077769756\n",
      "Surface training t=29744, loss=0.017555433325469494\n",
      "Surface training t=29745, loss=0.017005237750709057\n",
      "Surface training t=29746, loss=0.01943307090550661\n",
      "Surface training t=29747, loss=0.01830923091620207\n",
      "Surface training t=29748, loss=0.017674684524536133\n",
      "Surface training t=29749, loss=0.02199212647974491\n",
      "Surface training t=29750, loss=0.01697978051379323\n",
      "Surface training t=29751, loss=0.017465852200984955\n",
      "Surface training t=29752, loss=0.02186691015958786\n",
      "Surface training t=29753, loss=0.02639701310545206\n",
      "Surface training t=29754, loss=0.027212275192141533\n",
      "Surface training t=29755, loss=0.02876330353319645\n",
      "Surface training t=29756, loss=0.0258684316650033\n",
      "Surface training t=29757, loss=0.0299200639128685\n",
      "Surface training t=29758, loss=0.02748439460992813\n",
      "Surface training t=29759, loss=0.025767656974494457\n",
      "Surface training t=29760, loss=0.031119368970394135\n",
      "Surface training t=29761, loss=0.026126042939722538\n",
      "Surface training t=29762, loss=0.022317731752991676\n",
      "Surface training t=29763, loss=0.029162914492189884\n",
      "Surface training t=29764, loss=0.023656032979488373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=29765, loss=0.016956055536866188\n",
      "Surface training t=29766, loss=0.019558067433536053\n",
      "Surface training t=29767, loss=0.023637499660253525\n",
      "Surface training t=29768, loss=0.0249646520242095\n",
      "Surface training t=29769, loss=0.022520417347550392\n",
      "Surface training t=29770, loss=0.02576547395437956\n",
      "Surface training t=29771, loss=0.02561144344508648\n",
      "Surface training t=29772, loss=0.01382001442834735\n",
      "Surface training t=29773, loss=0.018536929972469807\n",
      "Surface training t=29774, loss=0.034269120544195175\n",
      "Surface training t=29775, loss=0.03775027208030224\n",
      "Surface training t=29776, loss=0.030297388322651386\n",
      "Surface training t=29777, loss=0.04409445635974407\n",
      "Surface training t=29778, loss=0.04274570196866989\n",
      "Surface training t=29779, loss=0.03362511470913887\n",
      "Surface training t=29780, loss=0.03916196525096893\n",
      "Surface training t=29781, loss=0.041422126814723015\n",
      "Surface training t=29782, loss=0.031338369473814964\n",
      "Surface training t=29783, loss=0.03141450043767691\n",
      "Surface training t=29784, loss=0.0323192048817873\n",
      "Surface training t=29785, loss=0.03344509843736887\n",
      "Surface training t=29786, loss=0.026646245270967484\n",
      "Surface training t=29787, loss=0.022895346395671368\n",
      "Surface training t=29788, loss=0.025487881153821945\n",
      "Surface training t=29789, loss=0.024377775378525257\n",
      "Surface training t=29790, loss=0.022395480424165726\n",
      "Surface training t=29791, loss=0.026486877351999283\n",
      "Surface training t=29792, loss=0.02231714129447937\n",
      "Surface training t=29793, loss=0.026445355266332626\n",
      "Surface training t=29794, loss=0.03576459735631943\n",
      "Surface training t=29795, loss=0.026310067623853683\n",
      "Surface training t=29796, loss=0.030548982322216034\n",
      "Surface training t=29797, loss=0.02735634706914425\n",
      "Surface training t=29798, loss=0.021867597475647926\n",
      "Surface training t=29799, loss=0.029501114040613174\n",
      "Surface training t=29800, loss=0.022474393248558044\n",
      "Surface training t=29801, loss=0.021917909383773804\n",
      "Surface training t=29802, loss=0.0236304746940732\n",
      "Surface training t=29803, loss=0.027166773565113544\n",
      "Surface training t=29804, loss=0.019848919473588467\n",
      "Surface training t=29805, loss=0.01635623862966895\n",
      "Surface training t=29806, loss=0.017867091111838818\n",
      "Surface training t=29807, loss=0.014093933627009392\n",
      "Surface training t=29808, loss=0.017371566966176033\n",
      "Surface training t=29809, loss=0.020894217304885387\n",
      "Surface training t=29810, loss=0.020786705426871777\n",
      "Surface training t=29811, loss=0.01723946537822485\n",
      "Surface training t=29812, loss=0.014667208306491375\n",
      "Surface training t=29813, loss=0.021070796065032482\n",
      "Surface training t=29814, loss=0.016498122364282608\n",
      "Surface training t=29815, loss=0.019323824904859066\n",
      "Surface training t=29816, loss=0.023630964569747448\n",
      "Surface training t=29817, loss=0.019100741017609835\n",
      "Surface training t=29818, loss=0.0201626168563962\n",
      "Surface training t=29819, loss=0.018714705482125282\n",
      "Surface training t=29820, loss=0.028260022401809692\n",
      "Surface training t=29821, loss=0.024133740924298763\n",
      "Surface training t=29822, loss=0.03949212096631527\n",
      "Surface training t=29823, loss=0.030551202595233917\n",
      "Surface training t=29824, loss=0.028837747871875763\n",
      "Surface training t=29825, loss=0.043964486569166183\n",
      "Surface training t=29826, loss=0.0315208500251174\n",
      "Surface training t=29827, loss=0.043217411264777184\n",
      "Surface training t=29828, loss=0.040820635855197906\n",
      "Surface training t=29829, loss=0.027639145962893963\n",
      "Surface training t=29830, loss=0.03685053251683712\n",
      "Surface training t=29831, loss=0.053267670795321465\n",
      "Surface training t=29832, loss=0.03338435757905245\n",
      "Surface training t=29833, loss=0.034456295892596245\n",
      "Surface training t=29834, loss=0.03831023536622524\n",
      "Surface training t=29835, loss=0.03215789096429944\n",
      "Surface training t=29836, loss=0.03216804005205631\n",
      "Surface training t=29837, loss=0.03775830194354057\n",
      "Surface training t=29838, loss=0.0289964834228158\n",
      "Surface training t=29839, loss=0.026733197271823883\n",
      "Surface training t=29840, loss=0.021111072041094303\n",
      "Surface training t=29841, loss=0.022943045012652874\n",
      "Surface training t=29842, loss=0.022401430644094944\n",
      "Surface training t=29843, loss=0.021067174151539803\n",
      "Surface training t=29844, loss=0.021948888897895813\n",
      "Surface training t=29845, loss=0.023562116548419\n",
      "Surface training t=29846, loss=0.017937767319381237\n",
      "Surface training t=29847, loss=0.022302063181996346\n",
      "Surface training t=29848, loss=0.026492957025766373\n",
      "Surface training t=29849, loss=0.025242981500923634\n",
      "Surface training t=29850, loss=0.018926067277789116\n",
      "Surface training t=29851, loss=0.019880699925124645\n",
      "Surface training t=29852, loss=0.02215729746967554\n",
      "Surface training t=29853, loss=0.02385109756141901\n",
      "Surface training t=29854, loss=0.025536350905895233\n",
      "Surface training t=29855, loss=0.018466725014150143\n",
      "Surface training t=29856, loss=0.018444428220391273\n",
      "Surface training t=29857, loss=0.021265394985675812\n",
      "Surface training t=29858, loss=0.022503361105918884\n",
      "Surface training t=29859, loss=0.027162892743945122\n",
      "Surface training t=29860, loss=0.018499884754419327\n",
      "Surface training t=29861, loss=0.01788622047752142\n",
      "Surface training t=29862, loss=0.026463842950761318\n",
      "Surface training t=29863, loss=0.023549973033368587\n",
      "Surface training t=29864, loss=0.025976010598242283\n",
      "Surface training t=29865, loss=0.03111969493329525\n",
      "Surface training t=29866, loss=0.021951322443783283\n",
      "Surface training t=29867, loss=0.01594352535903454\n",
      "Surface training t=29868, loss=0.025850324891507626\n",
      "Surface training t=29869, loss=0.024150501936674118\n",
      "Surface training t=29870, loss=0.021279857493937016\n",
      "Surface training t=29871, loss=0.027887058444321156\n",
      "Surface training t=29872, loss=0.02519084233790636\n",
      "Surface training t=29873, loss=0.023912601172924042\n",
      "Surface training t=29874, loss=0.02248184010386467\n",
      "Surface training t=29875, loss=0.02645520307123661\n",
      "Surface training t=29876, loss=0.022230667993426323\n",
      "Surface training t=29877, loss=0.020519452169537544\n",
      "Surface training t=29878, loss=0.023555954918265343\n",
      "Surface training t=29879, loss=0.023764507845044136\n",
      "Surface training t=29880, loss=0.019906549714505672\n",
      "Surface training t=29881, loss=0.017668385058641434\n",
      "Surface training t=29882, loss=0.016709087416529655\n",
      "Surface training t=29883, loss=0.01885075494647026\n",
      "Surface training t=29884, loss=0.016649367287755013\n",
      "Surface training t=29885, loss=0.018492022063583136\n",
      "Surface training t=29886, loss=0.02134653925895691\n",
      "Surface training t=29887, loss=0.022268468514084816\n",
      "Surface training t=29888, loss=0.01828672271221876\n",
      "Surface training t=29889, loss=0.021973959635943174\n",
      "Surface training t=29890, loss=0.020839079283177853\n",
      "Surface training t=29891, loss=0.023682722821831703\n",
      "Surface training t=29892, loss=0.0172679228708148\n",
      "Surface training t=29893, loss=0.02072834689170122\n",
      "Surface training t=29894, loss=0.021198797039687634\n",
      "Surface training t=29895, loss=0.02180352807044983\n",
      "Surface training t=29896, loss=0.02964165899902582\n",
      "Surface training t=29897, loss=0.021031071431934834\n",
      "Surface training t=29898, loss=0.0275711826980114\n",
      "Surface training t=29899, loss=0.02875956241041422\n",
      "Surface training t=29900, loss=0.03277090843766928\n",
      "Surface training t=29901, loss=0.0358774047344923\n",
      "Surface training t=29902, loss=0.024120579473674297\n",
      "Surface training t=29903, loss=0.022778033278882504\n",
      "Surface training t=29904, loss=0.019836565479636192\n",
      "Surface training t=29905, loss=0.019876624457538128\n",
      "Surface training t=29906, loss=0.025227314792573452\n",
      "Surface training t=29907, loss=0.024410754907876253\n",
      "Surface training t=29908, loss=0.026927822269499302\n",
      "Surface training t=29909, loss=0.025020082481205463\n",
      "Surface training t=29910, loss=0.0245969220995903\n",
      "Surface training t=29911, loss=0.02024265006184578\n",
      "Surface training t=29912, loss=0.015593790914863348\n",
      "Surface training t=29913, loss=0.027205895632505417\n",
      "Surface training t=29914, loss=0.026862865313887596\n",
      "Surface training t=29915, loss=0.020365812815725803\n",
      "Surface training t=29916, loss=0.030282177962362766\n",
      "Surface training t=29917, loss=0.022527108900249004\n",
      "Surface training t=29918, loss=0.023563306778669357\n",
      "Surface training t=29919, loss=0.03521520085632801\n",
      "Surface training t=29920, loss=0.025697171688079834\n",
      "Surface training t=29921, loss=0.03385773580521345\n",
      "Surface training t=29922, loss=0.03205635026097298\n",
      "Surface training t=29923, loss=0.028877452947199345\n",
      "Surface training t=29924, loss=0.03841453418135643\n",
      "Surface training t=29925, loss=0.028819821774959564\n",
      "Surface training t=29926, loss=0.03132994379848242\n",
      "Surface training t=29927, loss=0.029990973882377148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=29928, loss=0.02642448339611292\n",
      "Surface training t=29929, loss=0.027686800807714462\n",
      "Surface training t=29930, loss=0.021089820191264153\n",
      "Surface training t=29931, loss=0.02075739111751318\n",
      "Surface training t=29932, loss=0.020472056232392788\n",
      "Surface training t=29933, loss=0.01676527038216591\n",
      "Surface training t=29934, loss=0.02317589521408081\n",
      "Surface training t=29935, loss=0.018600044306367636\n",
      "Surface training t=29936, loss=0.01698196865618229\n",
      "Surface training t=29937, loss=0.020858106203377247\n",
      "Surface training t=29938, loss=0.019633425399661064\n",
      "Surface training t=29939, loss=0.019619879312813282\n",
      "Surface training t=29940, loss=0.017031174153089523\n",
      "Surface training t=29941, loss=0.022278064861893654\n",
      "Surface training t=29942, loss=0.018939172849059105\n",
      "Surface training t=29943, loss=0.020570162683725357\n",
      "Surface training t=29944, loss=0.023606237024068832\n",
      "Surface training t=29945, loss=0.029701542109251022\n",
      "Surface training t=29946, loss=0.026683267205953598\n",
      "Surface training t=29947, loss=0.03314616531133652\n",
      "Surface training t=29948, loss=0.029278100468218327\n",
      "Surface training t=29949, loss=0.032357266172766685\n",
      "Surface training t=29950, loss=0.03977397456765175\n",
      "Surface training t=29951, loss=0.02930482104420662\n",
      "Surface training t=29952, loss=0.026708455756306648\n",
      "Surface training t=29953, loss=0.02707634773105383\n",
      "Surface training t=29954, loss=0.01695746462792158\n",
      "Surface training t=29955, loss=0.020759696140885353\n",
      "Surface training t=29956, loss=0.023641916923224926\n",
      "Surface training t=29957, loss=0.028189082629978657\n",
      "Surface training t=29958, loss=0.031408810056746006\n",
      "Surface training t=29959, loss=0.024554569274187088\n",
      "Surface training t=29960, loss=0.025314408354461193\n",
      "Surface training t=29961, loss=0.022815164178609848\n",
      "Surface training t=29962, loss=0.019645512104034424\n",
      "Surface training t=29963, loss=0.016298016533255577\n",
      "Surface training t=29964, loss=0.01823502592742443\n",
      "Surface training t=29965, loss=0.020264480262994766\n",
      "Surface training t=29966, loss=0.020106705836951733\n",
      "Surface training t=29967, loss=0.02497521135956049\n",
      "Surface training t=29968, loss=0.026623583398759365\n",
      "Surface training t=29969, loss=0.022527338936924934\n",
      "Surface training t=29970, loss=0.0234150318428874\n",
      "Surface training t=29971, loss=0.023072976619005203\n",
      "Surface training t=29972, loss=0.027752848342061043\n",
      "Surface training t=29973, loss=0.02444053627550602\n",
      "Surface training t=29974, loss=0.031344374641776085\n",
      "Surface training t=29975, loss=0.02630443312227726\n",
      "Surface training t=29976, loss=0.028761442750692368\n",
      "Surface training t=29977, loss=0.02367036510258913\n",
      "Surface training t=29978, loss=0.022562340833246708\n",
      "Surface training t=29979, loss=0.027838035486638546\n",
      "Surface training t=29980, loss=0.03453532326966524\n",
      "Surface training t=29981, loss=0.029410461895167828\n",
      "Surface training t=29982, loss=0.029580150730907917\n",
      "Surface training t=29983, loss=0.03222330566495657\n",
      "Surface training t=29984, loss=0.020469761453568935\n",
      "Surface training t=29985, loss=0.025193878449499607\n",
      "Surface training t=29986, loss=0.026290534995496273\n",
      "Surface training t=29987, loss=0.024595025926828384\n",
      "Surface training t=29988, loss=0.024653633125126362\n",
      "Surface training t=29989, loss=0.02577211055904627\n",
      "Surface training t=29990, loss=0.021138569340109825\n",
      "Surface training t=29991, loss=0.021417255513370037\n",
      "Surface training t=29992, loss=0.02271947544068098\n",
      "Surface training t=29993, loss=0.03226565849035978\n",
      "Surface training t=29994, loss=0.02346069924533367\n",
      "Surface training t=29995, loss=0.029104454442858696\n",
      "Surface training t=29996, loss=0.033703284338116646\n",
      "Surface training t=29997, loss=0.028827685862779617\n",
      "Surface training t=29998, loss=0.02709855232387781\n",
      "Surface training t=29999, loss=0.02614935953170061\n",
      "Surface training t=30000, loss=0.02121205534785986\n",
      "Surface training t=30001, loss=0.019366323947906494\n",
      "Surface training t=30002, loss=0.02009957656264305\n",
      "Surface training t=30003, loss=0.02429382223635912\n",
      "Surface training t=30004, loss=0.026982540264725685\n",
      "Surface training t=30005, loss=0.019755369052290916\n",
      "Surface training t=30006, loss=0.023983304388821125\n",
      "Surface training t=30007, loss=0.025794003158807755\n",
      "Surface training t=30008, loss=0.020284302532672882\n",
      "Surface training t=30009, loss=0.018048047553747892\n",
      "Surface training t=30010, loss=0.028002538718283176\n",
      "Surface training t=30011, loss=0.01623123837634921\n",
      "Surface training t=30012, loss=0.01995561085641384\n",
      "Surface training t=30013, loss=0.018577595241367817\n",
      "Surface training t=30014, loss=0.01499567599967122\n",
      "Surface training t=30015, loss=0.020400434732437134\n",
      "Surface training t=30016, loss=0.017057958990335464\n",
      "Surface training t=30017, loss=0.01994256256148219\n",
      "Surface training t=30018, loss=0.017478258349001408\n",
      "Surface training t=30019, loss=0.022696410305798054\n",
      "Surface training t=30020, loss=0.02762853540480137\n",
      "Surface training t=30021, loss=0.021894045174121857\n",
      "Surface training t=30022, loss=0.021228255704045296\n",
      "Surface training t=30023, loss=0.03134319186210632\n",
      "Surface training t=30024, loss=0.022667404264211655\n",
      "Surface training t=30025, loss=0.022603895515203476\n",
      "Surface training t=30026, loss=0.029594890773296356\n",
      "Surface training t=30027, loss=0.031882429495453835\n",
      "Surface training t=30028, loss=0.027673528529703617\n",
      "Surface training t=30029, loss=0.02681001555174589\n",
      "Surface training t=30030, loss=0.020423143170773983\n",
      "Surface training t=30031, loss=0.024343044497072697\n",
      "Surface training t=30032, loss=0.02300516702234745\n",
      "Surface training t=30033, loss=0.021536018699407578\n",
      "Surface training t=30034, loss=0.024779162369668484\n",
      "Surface training t=30035, loss=0.02337107341736555\n",
      "Surface training t=30036, loss=0.02754343394190073\n",
      "Surface training t=30037, loss=0.022670172154903412\n",
      "Surface training t=30038, loss=0.03342890739440918\n",
      "Surface training t=30039, loss=0.02117978036403656\n",
      "Surface training t=30040, loss=0.026369054801762104\n",
      "Surface training t=30041, loss=0.02700144611299038\n",
      "Surface training t=30042, loss=0.01765490137040615\n",
      "Surface training t=30043, loss=0.018366679549217224\n",
      "Surface training t=30044, loss=0.016619027592241764\n",
      "Surface training t=30045, loss=0.018888727761805058\n",
      "Surface training t=30046, loss=0.014766551554203033\n",
      "Surface training t=30047, loss=0.017029359005391598\n",
      "Surface training t=30048, loss=0.0177621990442276\n",
      "Surface training t=30049, loss=0.021549150347709656\n",
      "Surface training t=30050, loss=0.019239983521401882\n",
      "Surface training t=30051, loss=0.01978157740086317\n",
      "Surface training t=30052, loss=0.020651943050324917\n",
      "Surface training t=30053, loss=0.024527840316295624\n",
      "Surface training t=30054, loss=0.026113157160580158\n",
      "Surface training t=30055, loss=0.02252940833568573\n",
      "Surface training t=30056, loss=0.020874837413430214\n",
      "Surface training t=30057, loss=0.020957719534635544\n",
      "Surface training t=30058, loss=0.02230705600231886\n",
      "Surface training t=30059, loss=0.021434967406094074\n",
      "Surface training t=30060, loss=0.019883456639945507\n",
      "Surface training t=30061, loss=0.019384542480111122\n",
      "Surface training t=30062, loss=0.020793313160538673\n",
      "Surface training t=30063, loss=0.01930155698210001\n",
      "Surface training t=30064, loss=0.024890273809432983\n",
      "Surface training t=30065, loss=0.028030998073518276\n",
      "Surface training t=30066, loss=0.022131385281682014\n",
      "Surface training t=30067, loss=0.022545292042195797\n",
      "Surface training t=30068, loss=0.017846661619842052\n",
      "Surface training t=30069, loss=0.021931500174105167\n",
      "Surface training t=30070, loss=0.023292768746614456\n",
      "Surface training t=30071, loss=0.017087172716856003\n",
      "Surface training t=30072, loss=0.022082599811255932\n",
      "Surface training t=30073, loss=0.02352540660649538\n",
      "Surface training t=30074, loss=0.01837153546512127\n",
      "Surface training t=30075, loss=0.019731877371668816\n",
      "Surface training t=30076, loss=0.017526560463011265\n",
      "Surface training t=30077, loss=0.023539508692920208\n",
      "Surface training t=30078, loss=0.022596430964767933\n",
      "Surface training t=30079, loss=0.02023919578641653\n",
      "Surface training t=30080, loss=0.02203489188104868\n",
      "Surface training t=30081, loss=0.028204704634845257\n",
      "Surface training t=30082, loss=0.020013445988297462\n",
      "Surface training t=30083, loss=0.018154735676944256\n",
      "Surface training t=30084, loss=0.019858418963849545\n",
      "Surface training t=30085, loss=0.014326940756291151\n",
      "Surface training t=30086, loss=0.012438159435987473\n",
      "Surface training t=30087, loss=0.01474677212536335\n",
      "Surface training t=30088, loss=0.022937225177884102\n",
      "Surface training t=30089, loss=0.01909984601661563\n",
      "Surface training t=30090, loss=0.02017873339354992\n",
      "Surface training t=30091, loss=0.019638894125819206\n",
      "Surface training t=30092, loss=0.02052687481045723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=30093, loss=0.026641232892870903\n",
      "Surface training t=30094, loss=0.021247711032629013\n",
      "Surface training t=30095, loss=0.023207041434943676\n",
      "Surface training t=30096, loss=0.021659404039382935\n",
      "Surface training t=30097, loss=0.021060915663838387\n",
      "Surface training t=30098, loss=0.021518023684620857\n",
      "Surface training t=30099, loss=0.019540305249392986\n",
      "Surface training t=30100, loss=0.019942390732467175\n",
      "Surface training t=30101, loss=0.01891844905912876\n",
      "Surface training t=30102, loss=0.019467510282993317\n",
      "Surface training t=30103, loss=0.029407584108412266\n",
      "Surface training t=30104, loss=0.022293605841696262\n",
      "Surface training t=30105, loss=0.016435297206044197\n",
      "Surface training t=30106, loss=0.018737328238785267\n",
      "Surface training t=30107, loss=0.015193986240774393\n",
      "Surface training t=30108, loss=0.01343499543145299\n",
      "Surface training t=30109, loss=0.015497022774070501\n",
      "Surface training t=30110, loss=0.014747264795005322\n",
      "Surface training t=30111, loss=0.016089362557977438\n",
      "Surface training t=30112, loss=0.01763517502695322\n",
      "Surface training t=30113, loss=0.021029677242040634\n",
      "Surface training t=30114, loss=0.022969553247094154\n",
      "Surface training t=30115, loss=0.020270648412406445\n",
      "Surface training t=30116, loss=0.021282700821757317\n",
      "Surface training t=30117, loss=0.015602228697389364\n",
      "Surface training t=30118, loss=0.014922526199370623\n",
      "Surface training t=30119, loss=0.01924442732706666\n",
      "Surface training t=30120, loss=0.02450655773282051\n",
      "Surface training t=30121, loss=0.026616438291966915\n",
      "Surface training t=30122, loss=0.025301342830061913\n",
      "Surface training t=30123, loss=0.019549040123820305\n",
      "Surface training t=30124, loss=0.03179658018052578\n",
      "Surface training t=30125, loss=0.02592285443097353\n",
      "Surface training t=30126, loss=0.0286439536139369\n",
      "Surface training t=30127, loss=0.026456313207745552\n",
      "Surface training t=30128, loss=0.03530466463416815\n",
      "Surface training t=30129, loss=0.02657717652618885\n",
      "Surface training t=30130, loss=0.027264916338026524\n",
      "Surface training t=30131, loss=0.023518324363976717\n",
      "Surface training t=30132, loss=0.03125813603401184\n",
      "Surface training t=30133, loss=0.024959313683211803\n",
      "Surface training t=30134, loss=0.01820072252303362\n",
      "Surface training t=30135, loss=0.01923261024057865\n",
      "Surface training t=30136, loss=0.018911337479948997\n",
      "Surface training t=30137, loss=0.023085668683052063\n",
      "Surface training t=30138, loss=0.023622374050319195\n",
      "Surface training t=30139, loss=0.018195015378296375\n",
      "Surface training t=30140, loss=0.02072493825107813\n",
      "Surface training t=30141, loss=0.01564959902316332\n",
      "Surface training t=30142, loss=0.014064391609281301\n",
      "Surface training t=30143, loss=0.02372051775455475\n",
      "Surface training t=30144, loss=0.026914574205875397\n",
      "Surface training t=30145, loss=0.029591906815767288\n",
      "Surface training t=30146, loss=0.02413240633904934\n",
      "Surface training t=30147, loss=0.02207991061732173\n",
      "Surface training t=30148, loss=0.029191850684583187\n",
      "Surface training t=30149, loss=0.028419803828001022\n",
      "Surface training t=30150, loss=0.02498050034046173\n",
      "Surface training t=30151, loss=0.036567214876413345\n",
      "Surface training t=30152, loss=0.034224728122353554\n",
      "Surface training t=30153, loss=0.025589806959033012\n",
      "Surface training t=30154, loss=0.023875524289906025\n",
      "Surface training t=30155, loss=0.0284960875287652\n",
      "Surface training t=30156, loss=0.027679838240146637\n",
      "Surface training t=30157, loss=0.02502734586596489\n",
      "Surface training t=30158, loss=0.023351416923105717\n",
      "Surface training t=30159, loss=0.023581373505294323\n",
      "Surface training t=30160, loss=0.022862049750983715\n",
      "Surface training t=30161, loss=0.021836928091943264\n",
      "Surface training t=30162, loss=0.022787057794630527\n",
      "Surface training t=30163, loss=0.01712504867464304\n",
      "Surface training t=30164, loss=0.016679388470947742\n",
      "Surface training t=30165, loss=0.015289675910025835\n",
      "Surface training t=30166, loss=0.02296159416437149\n",
      "Surface training t=30167, loss=0.020962289534509182\n",
      "Surface training t=30168, loss=0.02163452235981822\n",
      "Surface training t=30169, loss=0.021404827013611794\n",
      "Surface training t=30170, loss=0.01801363192498684\n",
      "Surface training t=30171, loss=0.01767650432884693\n",
      "Surface training t=30172, loss=0.01439825538545847\n",
      "Surface training t=30173, loss=0.019715573638677597\n",
      "Surface training t=30174, loss=0.01549709215760231\n",
      "Surface training t=30175, loss=0.018298951908946037\n",
      "Surface training t=30176, loss=0.022534270770847797\n",
      "Surface training t=30177, loss=0.019427484832704067\n",
      "Surface training t=30178, loss=0.017375607043504715\n",
      "Surface training t=30179, loss=0.01697184145450592\n",
      "Surface training t=30180, loss=0.015129862818866968\n",
      "Surface training t=30181, loss=0.017470458522439003\n",
      "Surface training t=30182, loss=0.018843566067516804\n",
      "Surface training t=30183, loss=0.019897734746336937\n",
      "Surface training t=30184, loss=0.02016375120729208\n",
      "Surface training t=30185, loss=0.021061338484287262\n",
      "Surface training t=30186, loss=0.02243449818342924\n",
      "Surface training t=30187, loss=0.02328451629728079\n",
      "Surface training t=30188, loss=0.02659463882446289\n",
      "Surface training t=30189, loss=0.02070104330778122\n",
      "Surface training t=30190, loss=0.025356742553412914\n",
      "Surface training t=30191, loss=0.03020498715341091\n",
      "Surface training t=30192, loss=0.05169198848307133\n",
      "Surface training t=30193, loss=0.03051411546766758\n",
      "Surface training t=30194, loss=0.04847103916108608\n",
      "Surface training t=30195, loss=0.03129871655255556\n",
      "Surface training t=30196, loss=0.02587093599140644\n",
      "Surface training t=30197, loss=0.021591484546661377\n",
      "Surface training t=30198, loss=0.019869668409228325\n",
      "Surface training t=30199, loss=0.019766757264733315\n",
      "Surface training t=30200, loss=0.022085273638367653\n",
      "Surface training t=30201, loss=0.02049893606454134\n",
      "Surface training t=30202, loss=0.015168290119618177\n",
      "Surface training t=30203, loss=0.018302184529602528\n",
      "Surface training t=30204, loss=0.019269919022917747\n",
      "Surface training t=30205, loss=0.015817560255527496\n",
      "Surface training t=30206, loss=0.014598476234823465\n",
      "Surface training t=30207, loss=0.017432482447475195\n",
      "Surface training t=30208, loss=0.02118737529963255\n",
      "Surface training t=30209, loss=0.01604813151061535\n",
      "Surface training t=30210, loss=0.01781326998025179\n",
      "Surface training t=30211, loss=0.014138655737042427\n",
      "Surface training t=30212, loss=0.015071232803165913\n",
      "Surface training t=30213, loss=0.0181210208684206\n",
      "Surface training t=30214, loss=0.01804128661751747\n",
      "Surface training t=30215, loss=0.017196201719343662\n",
      "Surface training t=30216, loss=0.015662228222936392\n",
      "Surface training t=30217, loss=0.019243518821895123\n",
      "Surface training t=30218, loss=0.02025447692722082\n",
      "Surface training t=30219, loss=0.015877728816121817\n",
      "Surface training t=30220, loss=0.016834781505167484\n",
      "Surface training t=30221, loss=0.020500591956079006\n",
      "Surface training t=30222, loss=0.01679364126175642\n",
      "Surface training t=30223, loss=0.014803662896156311\n",
      "Surface training t=30224, loss=0.013795065693557262\n",
      "Surface training t=30225, loss=0.01784153562039137\n",
      "Surface training t=30226, loss=0.020948628429323435\n",
      "Surface training t=30227, loss=0.018319767899811268\n",
      "Surface training t=30228, loss=0.026581693440675735\n",
      "Surface training t=30229, loss=0.018649731762707233\n",
      "Surface training t=30230, loss=0.019411936402320862\n",
      "Surface training t=30231, loss=0.01519564026966691\n",
      "Surface training t=30232, loss=0.023018358275294304\n",
      "Surface training t=30233, loss=0.01790808606892824\n",
      "Surface training t=30234, loss=0.018248924985527992\n",
      "Surface training t=30235, loss=0.01858637109398842\n",
      "Surface training t=30236, loss=0.022444315254688263\n",
      "Surface training t=30237, loss=0.022405878640711308\n",
      "Surface training t=30238, loss=0.018499448895454407\n",
      "Surface training t=30239, loss=0.018348570447415113\n",
      "Surface training t=30240, loss=0.024542628787457943\n",
      "Surface training t=30241, loss=0.034406282007694244\n",
      "Surface training t=30242, loss=0.031788005493581295\n",
      "Surface training t=30243, loss=0.028325458988547325\n",
      "Surface training t=30244, loss=0.034543175250291824\n",
      "Surface training t=30245, loss=0.03342744708061218\n",
      "Surface training t=30246, loss=0.02738950587809086\n",
      "Surface training t=30247, loss=0.02821602113544941\n",
      "Surface training t=30248, loss=0.029483179561793804\n",
      "Surface training t=30249, loss=0.02845163643360138\n",
      "Surface training t=30250, loss=0.02345473598688841\n",
      "Surface training t=30251, loss=0.022569100372493267\n",
      "Surface training t=30252, loss=0.0276763578876853\n",
      "Surface training t=30253, loss=0.025665512308478355\n",
      "Surface training t=30254, loss=0.02726823091506958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=30255, loss=0.024767662398517132\n",
      "Surface training t=30256, loss=0.020433559082448483\n",
      "Surface training t=30257, loss=0.01760731264948845\n",
      "Surface training t=30258, loss=0.017924470361322165\n",
      "Surface training t=30259, loss=0.015080548357218504\n",
      "Surface training t=30260, loss=0.016705811955034733\n",
      "Surface training t=30261, loss=0.015264589805155993\n",
      "Surface training t=30262, loss=0.016447209287434816\n",
      "Surface training t=30263, loss=0.014977718237787485\n",
      "Surface training t=30264, loss=0.015434719622135162\n",
      "Surface training t=30265, loss=0.01987127587199211\n",
      "Surface training t=30266, loss=0.016954248305410147\n",
      "Surface training t=30267, loss=0.02245166338980198\n",
      "Surface training t=30268, loss=0.03115004301071167\n",
      "Surface training t=30269, loss=0.020067655481398106\n",
      "Surface training t=30270, loss=0.02890156302601099\n",
      "Surface training t=30271, loss=0.019489459227770567\n",
      "Surface training t=30272, loss=0.025641875341534615\n",
      "Surface training t=30273, loss=0.02837575227022171\n",
      "Surface training t=30274, loss=0.0322549669072032\n",
      "Surface training t=30275, loss=0.03275369945913553\n",
      "Surface training t=30276, loss=0.026061322540044785\n",
      "Surface training t=30277, loss=0.02343699149787426\n",
      "Surface training t=30278, loss=0.022319745272397995\n",
      "Surface training t=30279, loss=0.021894452162086964\n",
      "Surface training t=30280, loss=0.021009432151913643\n",
      "Surface training t=30281, loss=0.020227530039846897\n",
      "Surface training t=30282, loss=0.023190980777144432\n",
      "Surface training t=30283, loss=0.018586162012070417\n",
      "Surface training t=30284, loss=0.025667348876595497\n",
      "Surface training t=30285, loss=0.018072500824928284\n",
      "Surface training t=30286, loss=0.027356602251529694\n",
      "Surface training t=30287, loss=0.019473527558147907\n",
      "Surface training t=30288, loss=0.024035231210291386\n",
      "Surface training t=30289, loss=0.027668505907058716\n",
      "Surface training t=30290, loss=0.0236518457531929\n",
      "Surface training t=30291, loss=0.0244386475533247\n",
      "Surface training t=30292, loss=0.04424732364714146\n",
      "Surface training t=30293, loss=0.02741921693086624\n",
      "Surface training t=30294, loss=0.027135031297802925\n",
      "Surface training t=30295, loss=0.02970596682280302\n",
      "Surface training t=30296, loss=0.025326097384095192\n",
      "Surface training t=30297, loss=0.023243378847837448\n",
      "Surface training t=30298, loss=0.016997373662889004\n",
      "Surface training t=30299, loss=0.018193554133176804\n",
      "Surface training t=30300, loss=0.02084594313055277\n",
      "Surface training t=30301, loss=0.028769508004188538\n",
      "Surface training t=30302, loss=0.023036371916532516\n",
      "Surface training t=30303, loss=0.027847088873386383\n",
      "Surface training t=30304, loss=0.023002672009170055\n",
      "Surface training t=30305, loss=0.02203697105869651\n",
      "Surface training t=30306, loss=0.027588012628257275\n",
      "Surface training t=30307, loss=0.016985359601676464\n",
      "Surface training t=30308, loss=0.024464777670800686\n",
      "Surface training t=30309, loss=0.02113099955022335\n",
      "Surface training t=30310, loss=0.018752921372652054\n",
      "Surface training t=30311, loss=0.021460979245603085\n",
      "Surface training t=30312, loss=0.019586590118706226\n",
      "Surface training t=30313, loss=0.020058623515069485\n",
      "Surface training t=30314, loss=0.021525805816054344\n",
      "Surface training t=30315, loss=0.01811946928501129\n",
      "Surface training t=30316, loss=0.01913685631006956\n",
      "Surface training t=30317, loss=0.02180527988821268\n",
      "Surface training t=30318, loss=0.024725121445953846\n",
      "Surface training t=30319, loss=0.02053782856091857\n",
      "Surface training t=30320, loss=0.021076440811157227\n",
      "Surface training t=30321, loss=0.022037615068256855\n",
      "Surface training t=30322, loss=0.01979187037795782\n",
      "Surface training t=30323, loss=0.019685188308358192\n",
      "Surface training t=30324, loss=0.03214937634766102\n",
      "Surface training t=30325, loss=0.023386275395751\n",
      "Surface training t=30326, loss=0.019882965832948685\n",
      "Surface training t=30327, loss=0.020132120233029127\n",
      "Surface training t=30328, loss=0.027854792773723602\n",
      "Surface training t=30329, loss=0.02432952355593443\n",
      "Surface training t=30330, loss=0.02100077737122774\n",
      "Surface training t=30331, loss=0.02939605712890625\n",
      "Surface training t=30332, loss=0.02311095781624317\n",
      "Surface training t=30333, loss=0.02284516766667366\n",
      "Surface training t=30334, loss=0.01786146964877844\n",
      "Surface training t=30335, loss=0.016299735754728317\n",
      "Surface training t=30336, loss=0.011743294540792704\n",
      "Surface training t=30337, loss=0.017099441029131413\n",
      "Surface training t=30338, loss=0.016122776549309492\n",
      "Surface training t=30339, loss=0.015606751665472984\n",
      "Surface training t=30340, loss=0.021370931528508663\n",
      "Surface training t=30341, loss=0.018419820349663496\n",
      "Surface training t=30342, loss=0.017135869711637497\n",
      "Surface training t=30343, loss=0.020294656045734882\n",
      "Surface training t=30344, loss=0.017176250461488962\n",
      "Surface training t=30345, loss=0.020970270037651062\n",
      "Surface training t=30346, loss=0.016350108198821545\n",
      "Surface training t=30347, loss=0.014395551290363073\n",
      "Surface training t=30348, loss=0.018276960588991642\n",
      "Surface training t=30349, loss=0.017672039102762938\n",
      "Surface training t=30350, loss=0.019752216525375843\n",
      "Surface training t=30351, loss=0.02042219042778015\n",
      "Surface training t=30352, loss=0.016011111438274384\n",
      "Surface training t=30353, loss=0.014998744707554579\n",
      "Surface training t=30354, loss=0.01917335484176874\n",
      "Surface training t=30355, loss=0.01764701446518302\n",
      "Surface training t=30356, loss=0.015045455656945705\n",
      "Surface training t=30357, loss=0.029754954390227795\n",
      "Surface training t=30358, loss=0.021887335926294327\n",
      "Surface training t=30359, loss=0.015616110526025295\n",
      "Surface training t=30360, loss=0.016711031086742878\n",
      "Surface training t=30361, loss=0.01583361579105258\n",
      "Surface training t=30362, loss=0.021908404305577278\n",
      "Surface training t=30363, loss=0.022882329300045967\n",
      "Surface training t=30364, loss=0.02294286247342825\n",
      "Surface training t=30365, loss=0.021125219296664\n",
      "Surface training t=30366, loss=0.02768889721482992\n",
      "Surface training t=30367, loss=0.01879495568573475\n",
      "Surface training t=30368, loss=0.01865407219156623\n",
      "Surface training t=30369, loss=0.0237520607188344\n",
      "Surface training t=30370, loss=0.029049724340438843\n",
      "Surface training t=30371, loss=0.03305353131145239\n",
      "Surface training t=30372, loss=0.029393106698989868\n",
      "Surface training t=30373, loss=0.019447889178991318\n",
      "Surface training t=30374, loss=0.023014944046735764\n",
      "Surface training t=30375, loss=0.02863044012337923\n",
      "Surface training t=30376, loss=0.03723658435046673\n",
      "Surface training t=30377, loss=0.027772845700383186\n",
      "Surface training t=30378, loss=0.033472731709480286\n",
      "Surface training t=30379, loss=0.03179283067584038\n",
      "Surface training t=30380, loss=0.03808809444308281\n",
      "Surface training t=30381, loss=0.0325018335133791\n",
      "Surface training t=30382, loss=0.041165195405483246\n",
      "Surface training t=30383, loss=0.039441149681806564\n",
      "Surface training t=30384, loss=0.03405898064374924\n",
      "Surface training t=30385, loss=0.045501573011279106\n",
      "Surface training t=30386, loss=0.035266779363155365\n",
      "Surface training t=30387, loss=0.028466753661632538\n",
      "Surface training t=30388, loss=0.02740198839455843\n",
      "Surface training t=30389, loss=0.033092984929680824\n",
      "Surface training t=30390, loss=0.027453687973320484\n",
      "Surface training t=30391, loss=0.035893005318939686\n",
      "Surface training t=30392, loss=0.03485035616904497\n",
      "Surface training t=30393, loss=0.044424306601285934\n",
      "Surface training t=30394, loss=0.03392850048840046\n",
      "Surface training t=30395, loss=0.02373263891786337\n",
      "Surface training t=30396, loss=0.022137273102998734\n",
      "Surface training t=30397, loss=0.022272082045674324\n",
      "Surface training t=30398, loss=0.022729823365807533\n",
      "Surface training t=30399, loss=0.019451047759503126\n",
      "Surface training t=30400, loss=0.026139666326344013\n",
      "Surface training t=30401, loss=0.02455348428338766\n",
      "Surface training t=30402, loss=0.020513633266091347\n",
      "Surface training t=30403, loss=0.014471313916146755\n",
      "Surface training t=30404, loss=0.018001343123614788\n",
      "Surface training t=30405, loss=0.018512009643018246\n",
      "Surface training t=30406, loss=0.025820263661444187\n",
      "Surface training t=30407, loss=0.020157049410045147\n",
      "Surface training t=30408, loss=0.02314098086208105\n",
      "Surface training t=30409, loss=0.021796721033751965\n",
      "Surface training t=30410, loss=0.02163080219179392\n",
      "Surface training t=30411, loss=0.01678782980889082\n",
      "Surface training t=30412, loss=0.018573741894215345\n",
      "Surface training t=30413, loss=0.019267013296484947\n",
      "Surface training t=30414, loss=0.024992997758090496\n",
      "Surface training t=30415, loss=0.021609937772154808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=30416, loss=0.024737635627388954\n",
      "Surface training t=30417, loss=0.020386621356010437\n",
      "Surface training t=30418, loss=0.019847537390887737\n",
      "Surface training t=30419, loss=0.017960297875106335\n",
      "Surface training t=30420, loss=0.021155452355742455\n",
      "Surface training t=30421, loss=0.01833919622004032\n",
      "Surface training t=30422, loss=0.020925402641296387\n",
      "Surface training t=30423, loss=0.021316079422831535\n",
      "Surface training t=30424, loss=0.01604806585237384\n",
      "Surface training t=30425, loss=0.017381984740495682\n",
      "Surface training t=30426, loss=0.018530248198658228\n",
      "Surface training t=30427, loss=0.023220956325531006\n",
      "Surface training t=30428, loss=0.028316419571638107\n",
      "Surface training t=30429, loss=0.02392439730465412\n",
      "Surface training t=30430, loss=0.01780786830931902\n",
      "Surface training t=30431, loss=0.02622645813971758\n",
      "Surface training t=30432, loss=0.020184789784252644\n",
      "Surface training t=30433, loss=0.02595561183989048\n",
      "Surface training t=30434, loss=0.019215531647205353\n",
      "Surface training t=30435, loss=0.01961960457265377\n",
      "Surface training t=30436, loss=0.020292650908231735\n",
      "Surface training t=30437, loss=0.023040710017085075\n",
      "Surface training t=30438, loss=0.025698029436171055\n",
      "Surface training t=30439, loss=0.02763746865093708\n",
      "Surface training t=30440, loss=0.023235472850501537\n",
      "Surface training t=30441, loss=0.04087135195732117\n",
      "Surface training t=30442, loss=0.02989078313112259\n",
      "Surface training t=30443, loss=0.033881233073771\n",
      "Surface training t=30444, loss=0.04510360024869442\n",
      "Surface training t=30445, loss=0.03763153217732906\n",
      "Surface training t=30446, loss=0.035235695540905\n",
      "Surface training t=30447, loss=0.03435867093503475\n",
      "Surface training t=30448, loss=0.033244469203054905\n",
      "Surface training t=30449, loss=0.03811822086572647\n",
      "Surface training t=30450, loss=0.045479342341423035\n",
      "Surface training t=30451, loss=0.03599133342504501\n",
      "Surface training t=30452, loss=0.04466931335628033\n",
      "Surface training t=30453, loss=0.03523289691656828\n",
      "Surface training t=30454, loss=0.034524308517575264\n",
      "Surface training t=30455, loss=0.03486512787640095\n",
      "Surface training t=30456, loss=0.029109743423759937\n",
      "Surface training t=30457, loss=0.023273350670933723\n",
      "Surface training t=30458, loss=0.027932888828217983\n",
      "Surface training t=30459, loss=0.027298112399876118\n",
      "Surface training t=30460, loss=0.020396835170686245\n",
      "Surface training t=30461, loss=0.02070951648056507\n",
      "Surface training t=30462, loss=0.019007153809070587\n",
      "Surface training t=30463, loss=0.019498310983181\n",
      "Surface training t=30464, loss=0.016328041441738605\n",
      "Surface training t=30465, loss=0.017097472213208675\n",
      "Surface training t=30466, loss=0.020037801936268806\n",
      "Surface training t=30467, loss=0.023553671315312386\n",
      "Surface training t=30468, loss=0.016765085980296135\n",
      "Surface training t=30469, loss=0.015388489235192537\n",
      "Surface training t=30470, loss=0.01615159399807453\n",
      "Surface training t=30471, loss=0.018153498880565166\n",
      "Surface training t=30472, loss=0.023022697307169437\n",
      "Surface training t=30473, loss=0.020226852037012577\n",
      "Surface training t=30474, loss=0.019535979256033897\n",
      "Surface training t=30475, loss=0.025623860768973827\n",
      "Surface training t=30476, loss=0.01731927413493395\n",
      "Surface training t=30477, loss=0.02644845936447382\n",
      "Surface training t=30478, loss=0.02764751110225916\n",
      "Surface training t=30479, loss=0.026048356667160988\n",
      "Surface training t=30480, loss=0.023519201204180717\n",
      "Surface training t=30481, loss=0.023733170703053474\n",
      "Surface training t=30482, loss=0.016077525448054075\n",
      "Surface training t=30483, loss=0.018965084105730057\n",
      "Surface training t=30484, loss=0.016808882355690002\n",
      "Surface training t=30485, loss=0.021113798022270203\n",
      "Surface training t=30486, loss=0.02338599320501089\n",
      "Surface training t=30487, loss=0.015455417335033417\n",
      "Surface training t=30488, loss=0.019334642216563225\n",
      "Surface training t=30489, loss=0.019305717200040817\n",
      "Surface training t=30490, loss=0.021050759591162205\n",
      "Surface training t=30491, loss=0.021417281590402126\n",
      "Surface training t=30492, loss=0.02548833843320608\n",
      "Surface training t=30493, loss=0.02299786638468504\n",
      "Surface training t=30494, loss=0.01673975819721818\n",
      "Surface training t=30495, loss=0.01769954664632678\n",
      "Surface training t=30496, loss=0.019478258676826954\n",
      "Surface training t=30497, loss=0.017933111172169447\n",
      "Surface training t=30498, loss=0.020964650437235832\n",
      "Surface training t=30499, loss=0.012581111397594213\n",
      "Surface training t=30500, loss=0.018675796687602997\n",
      "Surface training t=30501, loss=0.026184489019215107\n",
      "Surface training t=30502, loss=0.02420988492667675\n",
      "Surface training t=30503, loss=0.01963402796536684\n",
      "Surface training t=30504, loss=0.018589258193969727\n",
      "Surface training t=30505, loss=0.02121660578995943\n",
      "Surface training t=30506, loss=0.017309337854385376\n",
      "Surface training t=30507, loss=0.013991652056574821\n",
      "Surface training t=30508, loss=0.02030839305371046\n",
      "Surface training t=30509, loss=0.026818901300430298\n",
      "Surface training t=30510, loss=0.024728833697736263\n",
      "Surface training t=30511, loss=0.029563486576080322\n",
      "Surface training t=30512, loss=0.02868972159922123\n",
      "Surface training t=30513, loss=0.02749725431203842\n",
      "Surface training t=30514, loss=0.02431714814156294\n",
      "Surface training t=30515, loss=0.02669634111225605\n",
      "Surface training t=30516, loss=0.01613633893430233\n",
      "Surface training t=30517, loss=0.014170636422932148\n",
      "Surface training t=30518, loss=0.018834155052900314\n",
      "Surface training t=30519, loss=0.022858685813844204\n",
      "Surface training t=30520, loss=0.018346437718719244\n",
      "Surface training t=30521, loss=0.026759743690490723\n",
      "Surface training t=30522, loss=0.03292881418019533\n",
      "Surface training t=30523, loss=0.024595823138952255\n",
      "Surface training t=30524, loss=0.0324977757409215\n",
      "Surface training t=30525, loss=0.03232346195727587\n",
      "Surface training t=30526, loss=0.02275900077074766\n",
      "Surface training t=30527, loss=0.02008875086903572\n",
      "Surface training t=30528, loss=0.02594356145709753\n",
      "Surface training t=30529, loss=0.021043360233306885\n",
      "Surface training t=30530, loss=0.020611178129911423\n",
      "Surface training t=30531, loss=0.02410210855305195\n",
      "Surface training t=30532, loss=0.021246613934636116\n",
      "Surface training t=30533, loss=0.025120475329458714\n",
      "Surface training t=30534, loss=0.01797364093363285\n",
      "Surface training t=30535, loss=0.02280845958739519\n",
      "Surface training t=30536, loss=0.022264896892011166\n",
      "Surface training t=30537, loss=0.02167173381894827\n",
      "Surface training t=30538, loss=0.023530724458396435\n",
      "Surface training t=30539, loss=0.025690333917737007\n",
      "Surface training t=30540, loss=0.01688190270215273\n",
      "Surface training t=30541, loss=0.017317157238721848\n",
      "Surface training t=30542, loss=0.014639560598880053\n",
      "Surface training t=30543, loss=0.01774778589606285\n",
      "Surface training t=30544, loss=0.01964077167212963\n",
      "Surface training t=30545, loss=0.017807023599743843\n",
      "Surface training t=30546, loss=0.017487955279648304\n",
      "Surface training t=30547, loss=0.015027317218482494\n",
      "Surface training t=30548, loss=0.018069779500365257\n",
      "Surface training t=30549, loss=0.01476193219423294\n",
      "Surface training t=30550, loss=0.022547096945345402\n",
      "Surface training t=30551, loss=0.019233337603509426\n",
      "Surface training t=30552, loss=0.014497108291834593\n",
      "Surface training t=30553, loss=0.01833906304091215\n",
      "Surface training t=30554, loss=0.014423145912587643\n",
      "Surface training t=30555, loss=0.017537535168230534\n",
      "Surface training t=30556, loss=0.012981365900486708\n",
      "Surface training t=30557, loss=0.018926560878753662\n",
      "Surface training t=30558, loss=0.01916605420410633\n",
      "Surface training t=30559, loss=0.017635755240917206\n",
      "Surface training t=30560, loss=0.02197412308305502\n",
      "Surface training t=30561, loss=0.022463388741016388\n",
      "Surface training t=30562, loss=0.016457892023026943\n",
      "Surface training t=30563, loss=0.024166353046894073\n",
      "Surface training t=30564, loss=0.023484227247536182\n",
      "Surface training t=30565, loss=0.021873525343835354\n",
      "Surface training t=30566, loss=0.020223813131451607\n",
      "Surface training t=30567, loss=0.01646760944277048\n",
      "Surface training t=30568, loss=0.01997016929090023\n",
      "Surface training t=30569, loss=0.023825600743293762\n",
      "Surface training t=30570, loss=0.04392109252512455\n",
      "Surface training t=30571, loss=0.031810885295271873\n",
      "Surface training t=30572, loss=0.035784751176834106\n",
      "Surface training t=30573, loss=0.04953017644584179\n",
      "Surface training t=30574, loss=0.03280220925807953\n",
      "Surface training t=30575, loss=0.047600919380784035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=30576, loss=0.04137565474957228\n",
      "Surface training t=30577, loss=0.028518376871943474\n",
      "Surface training t=30578, loss=0.04212718829512596\n",
      "Surface training t=30579, loss=0.034916749224066734\n",
      "Surface training t=30580, loss=0.03310711123049259\n",
      "Surface training t=30581, loss=0.03320298343896866\n",
      "Surface training t=30582, loss=0.03506818413734436\n",
      "Surface training t=30583, loss=0.028503963723778725\n",
      "Surface training t=30584, loss=0.025666335597634315\n",
      "Surface training t=30585, loss=0.026028256863355637\n",
      "Surface training t=30586, loss=0.031375402584671974\n",
      "Surface training t=30587, loss=0.029314936138689518\n",
      "Surface training t=30588, loss=0.02390549797564745\n",
      "Surface training t=30589, loss=0.028352237306535244\n",
      "Surface training t=30590, loss=0.027437439188361168\n",
      "Surface training t=30591, loss=0.029373208060860634\n",
      "Surface training t=30592, loss=0.018964664079248905\n",
      "Surface training t=30593, loss=0.029299275018274784\n",
      "Surface training t=30594, loss=0.026617701165378094\n",
      "Surface training t=30595, loss=0.023650435730814934\n",
      "Surface training t=30596, loss=0.0378093458712101\n",
      "Surface training t=30597, loss=0.02142405789345503\n",
      "Surface training t=30598, loss=0.037339530885219574\n",
      "Surface training t=30599, loss=0.022802975960075855\n",
      "Surface training t=30600, loss=0.02749539166688919\n",
      "Surface training t=30601, loss=0.028043298050761223\n",
      "Surface training t=30602, loss=0.020216525997966528\n",
      "Surface training t=30603, loss=0.022359076887369156\n",
      "Surface training t=30604, loss=0.02400452457368374\n",
      "Surface training t=30605, loss=0.01690005511045456\n",
      "Surface training t=30606, loss=0.020345482509583235\n",
      "Surface training t=30607, loss=0.019705379381775856\n",
      "Surface training t=30608, loss=0.026350123807787895\n",
      "Surface training t=30609, loss=0.025834323838353157\n",
      "Surface training t=30610, loss=0.019386257976293564\n",
      "Surface training t=30611, loss=0.033303163945674896\n",
      "Surface training t=30612, loss=0.02399855013936758\n",
      "Surface training t=30613, loss=0.022622719407081604\n",
      "Surface training t=30614, loss=0.023023373447358608\n",
      "Surface training t=30615, loss=0.018259103409945965\n",
      "Surface training t=30616, loss=0.023933332413434982\n",
      "Surface training t=30617, loss=0.023835711181163788\n",
      "Surface training t=30618, loss=0.017450283747166395\n",
      "Surface training t=30619, loss=0.023859770968556404\n",
      "Surface training t=30620, loss=0.028325723484158516\n",
      "Surface training t=30621, loss=0.020053766667842865\n",
      "Surface training t=30622, loss=0.019750433042645454\n",
      "Surface training t=30623, loss=0.01810718048363924\n",
      "Surface training t=30624, loss=0.01783764362335205\n",
      "Surface training t=30625, loss=0.02057238481938839\n",
      "Surface training t=30626, loss=0.018188506364822388\n",
      "Surface training t=30627, loss=0.015942785888910294\n",
      "Surface training t=30628, loss=0.015775613486766815\n",
      "Surface training t=30629, loss=0.013515650294721127\n",
      "Surface training t=30630, loss=0.018944703973829746\n",
      "Surface training t=30631, loss=0.01778400456532836\n",
      "Surface training t=30632, loss=0.02133302576839924\n",
      "Surface training t=30633, loss=0.01687951758503914\n",
      "Surface training t=30634, loss=0.018380057066679\n",
      "Surface training t=30635, loss=0.01770534086972475\n",
      "Surface training t=30636, loss=0.02100516203790903\n",
      "Surface training t=30637, loss=0.023858544416725636\n",
      "Surface training t=30638, loss=0.021896008402109146\n",
      "Surface training t=30639, loss=0.02179577760398388\n",
      "Surface training t=30640, loss=0.023882689885795116\n",
      "Surface training t=30641, loss=0.030803125351667404\n",
      "Surface training t=30642, loss=0.03315687458962202\n",
      "Surface training t=30643, loss=0.028795775026082993\n",
      "Surface training t=30644, loss=0.024631555192172527\n",
      "Surface training t=30645, loss=0.024311221204698086\n",
      "Surface training t=30646, loss=0.027559563517570496\n",
      "Surface training t=30647, loss=0.02143586240708828\n",
      "Surface training t=30648, loss=0.024630757048726082\n",
      "Surface training t=30649, loss=0.022555391304194927\n",
      "Surface training t=30650, loss=0.02157822996377945\n",
      "Surface training t=30651, loss=0.02822139672935009\n",
      "Surface training t=30652, loss=0.02969115786254406\n",
      "Surface training t=30653, loss=0.024477345868945122\n",
      "Surface training t=30654, loss=0.023532788269221783\n",
      "Surface training t=30655, loss=0.021311523392796516\n",
      "Surface training t=30656, loss=0.028848969377577305\n",
      "Surface training t=30657, loss=0.021409199573099613\n",
      "Surface training t=30658, loss=0.018951947800815105\n",
      "Surface training t=30659, loss=0.02254068572074175\n",
      "Surface training t=30660, loss=0.01643639337271452\n",
      "Surface training t=30661, loss=0.015871526673436165\n",
      "Surface training t=30662, loss=0.018341180868446827\n",
      "Surface training t=30663, loss=0.015469701960682869\n",
      "Surface training t=30664, loss=0.017045216634869576\n",
      "Surface training t=30665, loss=0.014416616410017014\n",
      "Surface training t=30666, loss=0.02078006137162447\n",
      "Surface training t=30667, loss=0.01777272205799818\n",
      "Surface training t=30668, loss=0.017429118044674397\n",
      "Surface training t=30669, loss=0.014409711118787527\n",
      "Surface training t=30670, loss=0.0168314753100276\n",
      "Surface training t=30671, loss=0.015039064455777407\n",
      "Surface training t=30672, loss=0.01605619490146637\n",
      "Surface training t=30673, loss=0.01954176276922226\n",
      "Surface training t=30674, loss=0.019101842306554317\n",
      "Surface training t=30675, loss=0.016898096073418856\n",
      "Surface training t=30676, loss=0.020283610559999943\n",
      "Surface training t=30677, loss=0.013591774739325047\n",
      "Surface training t=30678, loss=0.013689756393432617\n",
      "Surface training t=30679, loss=0.016232993453741074\n",
      "Surface training t=30680, loss=0.015376986935734749\n",
      "Surface training t=30681, loss=0.018204539082944393\n",
      "Surface training t=30682, loss=0.01672386797145009\n",
      "Surface training t=30683, loss=0.01559029845520854\n",
      "Surface training t=30684, loss=0.01629507727921009\n",
      "Surface training t=30685, loss=0.02120337914675474\n",
      "Surface training t=30686, loss=0.023559930734336376\n",
      "Surface training t=30687, loss=0.02187881339341402\n",
      "Surface training t=30688, loss=0.020329533610492945\n",
      "Surface training t=30689, loss=0.018855771981179714\n",
      "Surface training t=30690, loss=0.018687570467591286\n",
      "Surface training t=30691, loss=0.01667479146271944\n",
      "Surface training t=30692, loss=0.01931939832866192\n",
      "Surface training t=30693, loss=0.019668705761432648\n",
      "Surface training t=30694, loss=0.02146190172061324\n",
      "Surface training t=30695, loss=0.01914101280272007\n",
      "Surface training t=30696, loss=0.015028176363557577\n",
      "Surface training t=30697, loss=0.014962790999561548\n",
      "Surface training t=30698, loss=0.015066292136907578\n",
      "Surface training t=30699, loss=0.020940730813890696\n",
      "Surface training t=30700, loss=0.018807812593877316\n",
      "Surface training t=30701, loss=0.015472308732569218\n",
      "Surface training t=30702, loss=0.022288269363343716\n",
      "Surface training t=30703, loss=0.019436330534517765\n",
      "Surface training t=30704, loss=0.016380468383431435\n",
      "Surface training t=30705, loss=0.01696903631091118\n",
      "Surface training t=30706, loss=0.017413124442100525\n",
      "Surface training t=30707, loss=0.02087066601961851\n",
      "Surface training t=30708, loss=0.018038753420114517\n",
      "Surface training t=30709, loss=0.01592828892171383\n",
      "Surface training t=30710, loss=0.018449645955115557\n",
      "Surface training t=30711, loss=0.015108661726117134\n",
      "Surface training t=30712, loss=0.020707485266029835\n",
      "Surface training t=30713, loss=0.020454774610698223\n",
      "Surface training t=30714, loss=0.019247029908001423\n",
      "Surface training t=30715, loss=0.023490614257752895\n",
      "Surface training t=30716, loss=0.023550412617623806\n",
      "Surface training t=30717, loss=0.02110708551481366\n",
      "Surface training t=30718, loss=0.02418868988752365\n",
      "Surface training t=30719, loss=0.023113254457712173\n",
      "Surface training t=30720, loss=0.019302714616060257\n",
      "Surface training t=30721, loss=0.020672340877354145\n",
      "Surface training t=30722, loss=0.022239629179239273\n",
      "Surface training t=30723, loss=0.01864449493587017\n",
      "Surface training t=30724, loss=0.022415971383452415\n",
      "Surface training t=30725, loss=0.020731224678456783\n",
      "Surface training t=30726, loss=0.01700976211577654\n",
      "Surface training t=30727, loss=0.020064614713191986\n",
      "Surface training t=30728, loss=0.01832054927945137\n",
      "Surface training t=30729, loss=0.018034445121884346\n",
      "Surface training t=30730, loss=0.015852182172238827\n",
      "Surface training t=30731, loss=0.026176284067332745\n",
      "Surface training t=30732, loss=0.019539727829396725\n",
      "Surface training t=30733, loss=0.018138673156499863\n",
      "Surface training t=30734, loss=0.021168198436498642\n",
      "Surface training t=30735, loss=0.017996075563132763\n",
      "Surface training t=30736, loss=0.01673252461478114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=30737, loss=0.013125296216458082\n",
      "Surface training t=30738, loss=0.01781069952994585\n",
      "Surface training t=30739, loss=0.01965885329991579\n",
      "Surface training t=30740, loss=0.02197952289134264\n",
      "Surface training t=30741, loss=0.020777767524123192\n",
      "Surface training t=30742, loss=0.02307339757680893\n",
      "Surface training t=30743, loss=0.020579921081662178\n",
      "Surface training t=30744, loss=0.015565100125968456\n",
      "Surface training t=30745, loss=0.014686009380966425\n",
      "Surface training t=30746, loss=0.017404763959348202\n",
      "Surface training t=30747, loss=0.018704101908951998\n",
      "Surface training t=30748, loss=0.019524057395756245\n",
      "Surface training t=30749, loss=0.01487384457141161\n",
      "Surface training t=30750, loss=0.016180651262402534\n",
      "Surface training t=30751, loss=0.013479314278811216\n",
      "Surface training t=30752, loss=0.01997518725693226\n",
      "Surface training t=30753, loss=0.018605953082442284\n",
      "Surface training t=30754, loss=0.025010778568685055\n",
      "Surface training t=30755, loss=0.024744135327637196\n",
      "Surface training t=30756, loss=0.02880672924220562\n",
      "Surface training t=30757, loss=0.02906864508986473\n",
      "Surface training t=30758, loss=0.032422468066215515\n",
      "Surface training t=30759, loss=0.020230038091540337\n",
      "Surface training t=30760, loss=0.02950431313365698\n",
      "Surface training t=30761, loss=0.021607653237879276\n",
      "Surface training t=30762, loss=0.01747121149674058\n",
      "Surface training t=30763, loss=0.02573563065379858\n",
      "Surface training t=30764, loss=0.022110690362751484\n",
      "Surface training t=30765, loss=0.01861858880147338\n",
      "Surface training t=30766, loss=0.022342287003993988\n",
      "Surface training t=30767, loss=0.025539526715874672\n",
      "Surface training t=30768, loss=0.022268389351665974\n",
      "Surface training t=30769, loss=0.024488944560289383\n",
      "Surface training t=30770, loss=0.021054468117654324\n",
      "Surface training t=30771, loss=0.023669585585594177\n",
      "Surface training t=30772, loss=0.02162361890077591\n",
      "Surface training t=30773, loss=0.021504126489162445\n",
      "Surface training t=30774, loss=0.021165631711483\n",
      "Surface training t=30775, loss=0.013458941597491503\n",
      "Surface training t=30776, loss=0.02191029954701662\n",
      "Surface training t=30777, loss=0.01899200864136219\n",
      "Surface training t=30778, loss=0.02665200922638178\n",
      "Surface training t=30779, loss=0.022206529043614864\n",
      "Surface training t=30780, loss=0.017873436212539673\n",
      "Surface training t=30781, loss=0.02577979303896427\n",
      "Surface training t=30782, loss=0.018717357888817787\n",
      "Surface training t=30783, loss=0.028974590823054314\n",
      "Surface training t=30784, loss=0.02667070832103491\n",
      "Surface training t=30785, loss=0.023196294903755188\n",
      "Surface training t=30786, loss=0.021854261867702007\n",
      "Surface training t=30787, loss=0.025726059451699257\n",
      "Surface training t=30788, loss=0.02738122083246708\n",
      "Surface training t=30789, loss=0.034002939239144325\n",
      "Surface training t=30790, loss=0.027227938175201416\n",
      "Surface training t=30791, loss=0.023363211192190647\n",
      "Surface training t=30792, loss=0.026771003380417824\n",
      "Surface training t=30793, loss=0.029815979301929474\n",
      "Surface training t=30794, loss=0.035365333780646324\n",
      "Surface training t=30795, loss=0.026186014525592327\n",
      "Surface training t=30796, loss=0.035637445747852325\n",
      "Surface training t=30797, loss=0.03240780532360077\n",
      "Surface training t=30798, loss=0.0410111416131258\n",
      "Surface training t=30799, loss=0.029243584722280502\n",
      "Surface training t=30800, loss=0.02623093221336603\n",
      "Surface training t=30801, loss=0.03360269032418728\n",
      "Surface training t=30802, loss=0.025829712860286236\n",
      "Surface training t=30803, loss=0.023407457396388054\n",
      "Surface training t=30804, loss=0.02338547632098198\n",
      "Surface training t=30805, loss=0.024569100700318813\n",
      "Surface training t=30806, loss=0.024327071849256754\n",
      "Surface training t=30807, loss=0.02983145322650671\n",
      "Surface training t=30808, loss=0.024181769229471684\n",
      "Surface training t=30809, loss=0.02227955963462591\n",
      "Surface training t=30810, loss=0.019163536839187145\n",
      "Surface training t=30811, loss=0.015098688192665577\n",
      "Surface training t=30812, loss=0.016502326354384422\n",
      "Surface training t=30813, loss=0.015512467361986637\n",
      "Surface training t=30814, loss=0.014463773928582668\n",
      "Surface training t=30815, loss=0.013880625832825899\n",
      "Surface training t=30816, loss=0.016512103378772736\n",
      "Surface training t=30817, loss=0.018687967211008072\n",
      "Surface training t=30818, loss=0.019375205971300602\n",
      "Surface training t=30819, loss=0.02227914985269308\n",
      "Surface training t=30820, loss=0.015592919662594795\n",
      "Surface training t=30821, loss=0.01374881248921156\n",
      "Surface training t=30822, loss=0.019704723730683327\n",
      "Surface training t=30823, loss=0.02387682255357504\n",
      "Surface training t=30824, loss=0.018586951307952404\n",
      "Surface training t=30825, loss=0.027766231447458267\n",
      "Surface training t=30826, loss=0.02731917053461075\n",
      "Surface training t=30827, loss=0.0220790421590209\n",
      "Surface training t=30828, loss=0.018243389204144478\n",
      "Surface training t=30829, loss=0.01949572190642357\n",
      "Surface training t=30830, loss=0.01761636510491371\n",
      "Surface training t=30831, loss=0.02119380608201027\n",
      "Surface training t=30832, loss=0.022258838638663292\n",
      "Surface training t=30833, loss=0.018783987499773502\n",
      "Surface training t=30834, loss=0.020214281976222992\n",
      "Surface training t=30835, loss=0.01803181041032076\n",
      "Surface training t=30836, loss=0.026370616629719734\n",
      "Surface training t=30837, loss=0.027431846596300602\n",
      "Surface training t=30838, loss=0.01898652408272028\n",
      "Surface training t=30839, loss=0.02226497232913971\n",
      "Surface training t=30840, loss=0.02229951787739992\n",
      "Surface training t=30841, loss=0.021350139752030373\n",
      "Surface training t=30842, loss=0.031020919792354107\n",
      "Surface training t=30843, loss=0.0315221231430769\n",
      "Surface training t=30844, loss=0.02932106051594019\n",
      "Surface training t=30845, loss=0.023981391452252865\n",
      "Surface training t=30846, loss=0.03161788359284401\n",
      "Surface training t=30847, loss=0.023105645552277565\n",
      "Surface training t=30848, loss=0.028568396344780922\n",
      "Surface training t=30849, loss=0.026794892735779285\n",
      "Surface training t=30850, loss=0.022686856798827648\n",
      "Surface training t=30851, loss=0.019068048801273108\n",
      "Surface training t=30852, loss=0.01817223895341158\n",
      "Surface training t=30853, loss=0.02188391052186489\n",
      "Surface training t=30854, loss=0.024184051901102066\n",
      "Surface training t=30855, loss=0.024561375379562378\n",
      "Surface training t=30856, loss=0.0243116095662117\n",
      "Surface training t=30857, loss=0.02541803102940321\n",
      "Surface training t=30858, loss=0.027682330459356308\n",
      "Surface training t=30859, loss=0.0227094367146492\n",
      "Surface training t=30860, loss=0.029706399887800217\n",
      "Surface training t=30861, loss=0.022352258674800396\n",
      "Surface training t=30862, loss=0.02352918405085802\n",
      "Surface training t=30863, loss=0.029915506951510906\n",
      "Surface training t=30864, loss=0.02646341361105442\n",
      "Surface training t=30865, loss=0.019958623684942722\n",
      "Surface training t=30866, loss=0.019989054650068283\n",
      "Surface training t=30867, loss=0.019956338219344616\n",
      "Surface training t=30868, loss=0.018857352435588837\n",
      "Surface training t=30869, loss=0.01528285164386034\n",
      "Surface training t=30870, loss=0.01860093232244253\n",
      "Surface training t=30871, loss=0.02080896496772766\n",
      "Surface training t=30872, loss=0.017420226708054543\n",
      "Surface training t=30873, loss=0.018046105280518532\n",
      "Surface training t=30874, loss=0.019015257246792316\n",
      "Surface training t=30875, loss=0.01877467054873705\n",
      "Surface training t=30876, loss=0.02130650170147419\n",
      "Surface training t=30877, loss=0.022657714784145355\n",
      "Surface training t=30878, loss=0.023233805783092976\n",
      "Surface training t=30879, loss=0.030612265691161156\n",
      "Surface training t=30880, loss=0.02090730145573616\n",
      "Surface training t=30881, loss=0.02911736350506544\n",
      "Surface training t=30882, loss=0.019671724177896976\n",
      "Surface training t=30883, loss=0.024170946329832077\n",
      "Surface training t=30884, loss=0.016475358977913857\n",
      "Surface training t=30885, loss=0.01905378047376871\n",
      "Surface training t=30886, loss=0.020240182988345623\n",
      "Surface training t=30887, loss=0.019570395350456238\n",
      "Surface training t=30888, loss=0.01601585652679205\n",
      "Surface training t=30889, loss=0.015471785329282284\n",
      "Surface training t=30890, loss=0.01609754841774702\n",
      "Surface training t=30891, loss=0.014005512464791536\n",
      "Surface training t=30892, loss=0.012359629850834608\n",
      "Surface training t=30893, loss=0.015824039466679096\n",
      "Surface training t=30894, loss=0.01726577151566744\n",
      "Surface training t=30895, loss=0.021512148901820183\n",
      "Surface training t=30896, loss=0.02467342745512724\n",
      "Surface training t=30897, loss=0.025814545340836048\n",
      "Surface training t=30898, loss=0.02878912352025509\n",
      "Surface training t=30899, loss=0.0266472976654768\n",
      "Surface training t=30900, loss=0.03776436299085617\n",
      "Surface training t=30901, loss=0.029542719945311546\n",
      "Surface training t=30902, loss=0.04443454183638096\n",
      "Surface training t=30903, loss=0.02453674655407667\n",
      "Surface training t=30904, loss=0.029457329772412777\n",
      "Surface training t=30905, loss=0.02815253846347332\n",
      "Surface training t=30906, loss=0.02623734436929226\n",
      "Surface training t=30907, loss=0.034443242475390434\n",
      "Surface training t=30908, loss=0.028003198094666004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=30909, loss=0.028077365830540657\n",
      "Surface training t=30910, loss=0.024272669106721878\n",
      "Surface training t=30911, loss=0.03179974853992462\n",
      "Surface training t=30912, loss=0.02376769669353962\n",
      "Surface training t=30913, loss=0.01769090536981821\n",
      "Surface training t=30914, loss=0.018193818628787994\n",
      "Surface training t=30915, loss=0.02025389578193426\n",
      "Surface training t=30916, loss=0.02130327746272087\n",
      "Surface training t=30917, loss=0.022393220104277134\n",
      "Surface training t=30918, loss=0.017398124560713768\n",
      "Surface training t=30919, loss=0.019123036414384842\n",
      "Surface training t=30920, loss=0.0172822424210608\n",
      "Surface training t=30921, loss=0.021231520920991898\n",
      "Surface training t=30922, loss=0.019781234674155712\n",
      "Surface training t=30923, loss=0.020351174287497997\n",
      "Surface training t=30924, loss=0.014669748954474926\n",
      "Surface training t=30925, loss=0.016189510002732277\n",
      "Surface training t=30926, loss=0.017223238945007324\n",
      "Surface training t=30927, loss=0.014447185210883617\n",
      "Surface training t=30928, loss=0.016017980873584747\n",
      "Surface training t=30929, loss=0.019062248058617115\n",
      "Surface training t=30930, loss=0.01746402308344841\n",
      "Surface training t=30931, loss=0.017800084315240383\n",
      "Surface training t=30932, loss=0.015481628943234682\n",
      "Surface training t=30933, loss=0.018137947656214237\n",
      "Surface training t=30934, loss=0.024591870605945587\n",
      "Surface training t=30935, loss=0.01768628228455782\n",
      "Surface training t=30936, loss=0.020778309553861618\n",
      "Surface training t=30937, loss=0.027078957296907902\n",
      "Surface training t=30938, loss=0.025300106033682823\n",
      "Surface training t=30939, loss=0.020607872866094112\n",
      "Surface training t=30940, loss=0.03694678656756878\n",
      "Surface training t=30941, loss=0.024529782123863697\n",
      "Surface training t=30942, loss=0.03494008257985115\n",
      "Surface training t=30943, loss=0.023868978023529053\n",
      "Surface training t=30944, loss=0.026098966132849455\n",
      "Surface training t=30945, loss=0.048745639622211456\n",
      "Surface training t=30946, loss=0.03630760312080383\n",
      "Surface training t=30947, loss=0.027730916626751423\n",
      "Surface training t=30948, loss=0.05387418158352375\n",
      "Surface training t=30949, loss=0.03213398531079292\n",
      "Surface training t=30950, loss=0.037733133882284164\n",
      "Surface training t=30951, loss=0.03565196879208088\n",
      "Surface training t=30952, loss=0.029742212034761906\n",
      "Surface training t=30953, loss=0.03660757839679718\n",
      "Surface training t=30954, loss=0.03049459680914879\n",
      "Surface training t=30955, loss=0.030100581236183643\n",
      "Surface training t=30956, loss=0.026863140054047108\n",
      "Surface training t=30957, loss=0.026667636819183826\n",
      "Surface training t=30958, loss=0.027028869837522507\n",
      "Surface training t=30959, loss=0.02784813567996025\n",
      "Surface training t=30960, loss=0.02644863072782755\n",
      "Surface training t=30961, loss=0.021351893432438374\n",
      "Surface training t=30962, loss=0.019245279021561146\n",
      "Surface training t=30963, loss=0.016852005384862423\n",
      "Surface training t=30964, loss=0.01761044468730688\n",
      "Surface training t=30965, loss=0.01798113528639078\n",
      "Surface training t=30966, loss=0.0230823066085577\n",
      "Surface training t=30967, loss=0.025647557340562344\n",
      "Surface training t=30968, loss=0.02329183742403984\n",
      "Surface training t=30969, loss=0.02689053677022457\n",
      "Surface training t=30970, loss=0.029048506170511246\n",
      "Surface training t=30971, loss=0.02520553022623062\n",
      "Surface training t=30972, loss=0.025722422637045383\n",
      "Surface training t=30973, loss=0.026408463716506958\n",
      "Surface training t=30974, loss=0.03558485209941864\n",
      "Surface training t=30975, loss=0.03089703619480133\n",
      "Surface training t=30976, loss=0.028457660228013992\n",
      "Surface training t=30977, loss=0.03301105089485645\n",
      "Surface training t=30978, loss=0.02523920312523842\n",
      "Surface training t=30979, loss=0.0236098850145936\n",
      "Surface training t=30980, loss=0.02485487051308155\n",
      "Surface training t=30981, loss=0.02250255737453699\n",
      "Surface training t=30982, loss=0.019029458053410053\n",
      "Surface training t=30983, loss=0.022178327664732933\n",
      "Surface training t=30984, loss=0.017912553623318672\n",
      "Surface training t=30985, loss=0.021222950890660286\n",
      "Surface training t=30986, loss=0.02610298153012991\n",
      "Surface training t=30987, loss=0.021767484489828348\n",
      "Surface training t=30988, loss=0.03321005217730999\n",
      "Surface training t=30989, loss=0.022938740439713\n",
      "Surface training t=30990, loss=0.0240258714184165\n",
      "Surface training t=30991, loss=0.0289052901789546\n",
      "Surface training t=30992, loss=0.018610458821058273\n",
      "Surface training t=30993, loss=0.021977203898131847\n",
      "Surface training t=30994, loss=0.020711303688585758\n",
      "Surface training t=30995, loss=0.022785566747188568\n",
      "Surface training t=30996, loss=0.018467310816049576\n",
      "Surface training t=30997, loss=0.017401807941496372\n",
      "Surface training t=30998, loss=0.019616620615124702\n",
      "Surface training t=30999, loss=0.019204518757760525\n",
      "Surface training t=31000, loss=0.019811619073152542\n",
      "Surface training t=31001, loss=0.02556951530277729\n",
      "Surface training t=31002, loss=0.020460618659853935\n",
      "Surface training t=31003, loss=0.022358646616339684\n",
      "Surface training t=31004, loss=0.028320465236902237\n",
      "Surface training t=31005, loss=0.02071969583630562\n",
      "Surface training t=31006, loss=0.022763632237911224\n",
      "Surface training t=31007, loss=0.020372424274683\n",
      "Surface training t=31008, loss=0.025409973226487637\n",
      "Surface training t=31009, loss=0.023668045178055763\n",
      "Surface training t=31010, loss=0.03170101251453161\n",
      "Surface training t=31011, loss=0.022899296134710312\n",
      "Surface training t=31012, loss=0.025456794537603855\n",
      "Surface training t=31013, loss=0.025671658106148243\n",
      "Surface training t=31014, loss=0.020453290082514286\n",
      "Surface training t=31015, loss=0.02065649814903736\n",
      "Surface training t=31016, loss=0.016090385615825653\n",
      "Surface training t=31017, loss=0.020154946483671665\n",
      "Surface training t=31018, loss=0.015864025335758924\n",
      "Surface training t=31019, loss=0.01771001610904932\n",
      "Surface training t=31020, loss=0.021515926346182823\n",
      "Surface training t=31021, loss=0.023560301400721073\n",
      "Surface training t=31022, loss=0.01964735798537731\n",
      "Surface training t=31023, loss=0.02589493151754141\n",
      "Surface training t=31024, loss=0.02175731025636196\n",
      "Surface training t=31025, loss=0.022722672671079636\n",
      "Surface training t=31026, loss=0.022190591786056757\n",
      "Surface training t=31027, loss=0.021762527525424957\n",
      "Surface training t=31028, loss=0.02824784442782402\n",
      "Surface training t=31029, loss=0.0262697571888566\n",
      "Surface training t=31030, loss=0.022571607492864132\n",
      "Surface training t=31031, loss=0.019349422305822372\n",
      "Surface training t=31032, loss=0.02234881930053234\n",
      "Surface training t=31033, loss=0.019784159027040005\n",
      "Surface training t=31034, loss=0.019587356597185135\n",
      "Surface training t=31035, loss=0.020676455460488796\n",
      "Surface training t=31036, loss=0.02028217539191246\n",
      "Surface training t=31037, loss=0.020596111193299294\n",
      "Surface training t=31038, loss=0.020807961001992226\n",
      "Surface training t=31039, loss=0.024005405604839325\n",
      "Surface training t=31040, loss=0.023144403472542763\n",
      "Surface training t=31041, loss=0.023539898917078972\n",
      "Surface training t=31042, loss=0.017985929735004902\n",
      "Surface training t=31043, loss=0.022717738524079323\n",
      "Surface training t=31044, loss=0.02929131593555212\n",
      "Surface training t=31045, loss=0.031141839921474457\n",
      "Surface training t=31046, loss=0.02761219535022974\n",
      "Surface training t=31047, loss=0.029102216474711895\n",
      "Surface training t=31048, loss=0.04869444668292999\n",
      "Surface training t=31049, loss=0.027459953911602497\n",
      "Surface training t=31050, loss=0.029733208008110523\n",
      "Surface training t=31051, loss=0.05715685337781906\n",
      "Surface training t=31052, loss=0.034520335495471954\n",
      "Surface training t=31053, loss=0.03778074588626623\n",
      "Surface training t=31054, loss=0.02507773693650961\n",
      "Surface training t=31055, loss=0.023427413776516914\n",
      "Surface training t=31056, loss=0.02497377060353756\n",
      "Surface training t=31057, loss=0.02869915682822466\n",
      "Surface training t=31058, loss=0.027871381491422653\n",
      "Surface training t=31059, loss=0.037260839715600014\n",
      "Surface training t=31060, loss=0.025497371330857277\n",
      "Surface training t=31061, loss=0.022888343781232834\n",
      "Surface training t=31062, loss=0.02046285942196846\n",
      "Surface training t=31063, loss=0.018293297849595547\n",
      "Surface training t=31064, loss=0.024722625501453876\n",
      "Surface training t=31065, loss=0.025639524683356285\n",
      "Surface training t=31066, loss=0.026434236206114292\n",
      "Surface training t=31067, loss=0.023108912631869316\n",
      "Surface training t=31068, loss=0.023828454315662384\n",
      "Surface training t=31069, loss=0.028068164363503456\n",
      "Surface training t=31070, loss=0.030044580809772015\n",
      "Surface training t=31071, loss=0.04427112266421318\n",
      "Surface training t=31072, loss=0.031697848811745644\n",
      "Surface training t=31073, loss=0.04473084211349487\n",
      "Surface training t=31074, loss=0.047311995178461075\n",
      "Surface training t=31075, loss=0.03307529725134373\n",
      "Surface training t=31076, loss=0.02815055660903454\n",
      "Surface training t=31077, loss=0.02512978669255972\n",
      "Surface training t=31078, loss=0.0176415485329926\n",
      "Surface training t=31079, loss=0.02315274439752102\n",
      "Surface training t=31080, loss=0.02824767306447029\n",
      "Surface training t=31081, loss=0.02392004895955324\n",
      "Surface training t=31082, loss=0.023432815447449684\n",
      "Surface training t=31083, loss=0.025105107575654984\n",
      "Surface training t=31084, loss=0.025284831412136555\n",
      "Surface training t=31085, loss=0.03308192174881697\n",
      "Surface training t=31086, loss=0.025357069447636604\n",
      "Surface training t=31087, loss=0.031169716268777847\n",
      "Surface training t=31088, loss=0.03378382883965969\n",
      "Surface training t=31089, loss=0.02773091569542885\n",
      "Surface training t=31090, loss=0.029849437065422535\n",
      "Surface training t=31091, loss=0.022697472013533115\n",
      "Surface training t=31092, loss=0.035506078973412514\n",
      "Surface training t=31093, loss=0.026789920404553413\n",
      "Surface training t=31094, loss=0.03538598492741585\n",
      "Surface training t=31095, loss=0.023504395969212055\n",
      "Surface training t=31096, loss=0.022009778767824173\n",
      "Surface training t=31097, loss=0.017276298254728317\n",
      "Surface training t=31098, loss=0.021733173169195652\n",
      "Surface training t=31099, loss=0.016176776960492134\n",
      "Surface training t=31100, loss=0.022259457036852837\n",
      "Surface training t=31101, loss=0.02731277421116829\n",
      "Surface training t=31102, loss=0.021559261716902256\n",
      "Surface training t=31103, loss=0.022247301414608955\n",
      "Surface training t=31104, loss=0.023073342628777027\n",
      "Surface training t=31105, loss=0.0176779143512249\n",
      "Surface training t=31106, loss=0.0179916275665164\n",
      "Surface training t=31107, loss=0.018177254125475883\n",
      "Surface training t=31108, loss=0.018801226280629635\n",
      "Surface training t=31109, loss=0.022633477114140987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=31110, loss=0.018861854448914528\n",
      "Surface training t=31111, loss=0.017792578786611557\n",
      "Surface training t=31112, loss=0.01645312737673521\n",
      "Surface training t=31113, loss=0.014172923751175404\n",
      "Surface training t=31114, loss=0.017144797835499048\n",
      "Surface training t=31115, loss=0.014572849497199059\n",
      "Surface training t=31116, loss=0.017840477637946606\n",
      "Surface training t=31117, loss=0.022377949208021164\n",
      "Surface training t=31118, loss=0.01587688084691763\n",
      "Surface training t=31119, loss=0.022743981331586838\n",
      "Surface training t=31120, loss=0.018923147581517696\n",
      "Surface training t=31121, loss=0.01717520784586668\n",
      "Surface training t=31122, loss=0.024499655701220036\n",
      "Surface training t=31123, loss=0.016796457581222057\n",
      "Surface training t=31124, loss=0.017950231209397316\n",
      "Surface training t=31125, loss=0.018842319957911968\n",
      "Surface training t=31126, loss=0.01484040729701519\n",
      "Surface training t=31127, loss=0.018870347179472446\n",
      "Surface training t=31128, loss=0.020044599194079638\n",
      "Surface training t=31129, loss=0.01759258285164833\n",
      "Surface training t=31130, loss=0.02439814619719982\n",
      "Surface training t=31131, loss=0.022560017183423042\n",
      "Surface training t=31132, loss=0.021354402415454388\n",
      "Surface training t=31133, loss=0.021472240798175335\n",
      "Surface training t=31134, loss=0.016401457600295544\n",
      "Surface training t=31135, loss=0.017074682284146547\n",
      "Surface training t=31136, loss=0.015429545659571886\n",
      "Surface training t=31137, loss=0.014599511865526438\n",
      "Surface training t=31138, loss=0.01808188110589981\n",
      "Surface training t=31139, loss=0.012429197318851948\n",
      "Surface training t=31140, loss=0.016935926862061024\n",
      "Surface training t=31141, loss=0.013910643756389618\n",
      "Surface training t=31142, loss=0.016864206176251173\n",
      "Surface training t=31143, loss=0.019263855181634426\n",
      "Surface training t=31144, loss=0.021192224696278572\n",
      "Surface training t=31145, loss=0.017702936194837093\n",
      "Surface training t=31146, loss=0.021343142725527287\n",
      "Surface training t=31147, loss=0.02573332842439413\n",
      "Surface training t=31148, loss=0.022030198946595192\n",
      "Surface training t=31149, loss=0.023689416237175465\n",
      "Surface training t=31150, loss=0.02303020842373371\n",
      "Surface training t=31151, loss=0.018329672515392303\n",
      "Surface training t=31152, loss=0.020318481139838696\n",
      "Surface training t=31153, loss=0.025073405355215073\n",
      "Surface training t=31154, loss=0.023520108312368393\n",
      "Surface training t=31155, loss=0.024246327579021454\n",
      "Surface training t=31156, loss=0.01666281186044216\n",
      "Surface training t=31157, loss=0.013397786766290665\n",
      "Surface training t=31158, loss=0.015098975971341133\n",
      "Surface training t=31159, loss=0.014989637769758701\n",
      "Surface training t=31160, loss=0.01771657168865204\n",
      "Surface training t=31161, loss=0.014833731111139059\n",
      "Surface training t=31162, loss=0.012682870496064425\n",
      "Surface training t=31163, loss=0.016027767211198807\n",
      "Surface training t=31164, loss=0.02117341011762619\n",
      "Surface training t=31165, loss=0.01837847102433443\n",
      "Surface training t=31166, loss=0.014765241649001837\n",
      "Surface training t=31167, loss=0.02025699056684971\n",
      "Surface training t=31168, loss=0.019737769849598408\n",
      "Surface training t=31169, loss=0.01788717694580555\n",
      "Surface training t=31170, loss=0.01447942666709423\n",
      "Surface training t=31171, loss=0.017826471477746964\n",
      "Surface training t=31172, loss=0.01728300005197525\n",
      "Surface training t=31173, loss=0.01890139142051339\n",
      "Surface training t=31174, loss=0.019110572524368763\n",
      "Surface training t=31175, loss=0.018729595001786947\n",
      "Surface training t=31176, loss=0.017666511237621307\n",
      "Surface training t=31177, loss=0.02170017547905445\n",
      "Surface training t=31178, loss=0.0187787851318717\n",
      "Surface training t=31179, loss=0.020093818195164204\n",
      "Surface training t=31180, loss=0.02149307820945978\n",
      "Surface training t=31181, loss=0.02326325699687004\n",
      "Surface training t=31182, loss=0.022861012257635593\n",
      "Surface training t=31183, loss=0.023158648051321507\n",
      "Surface training t=31184, loss=0.027772240340709686\n",
      "Surface training t=31185, loss=0.026342935860157013\n",
      "Surface training t=31186, loss=0.029307441785931587\n",
      "Surface training t=31187, loss=0.03166802879422903\n",
      "Surface training t=31188, loss=0.02894817292690277\n",
      "Surface training t=31189, loss=0.030622167512774467\n",
      "Surface training t=31190, loss=0.03945746272802353\n",
      "Surface training t=31191, loss=0.02895102370530367\n",
      "Surface training t=31192, loss=0.029045546427369118\n",
      "Surface training t=31193, loss=0.03021124005317688\n",
      "Surface training t=31194, loss=0.026771039701998234\n",
      "Surface training t=31195, loss=0.021996885538101196\n",
      "Surface training t=31196, loss=0.02343838009983301\n",
      "Surface training t=31197, loss=0.023152335546910763\n",
      "Surface training t=31198, loss=0.02368579152971506\n",
      "Surface training t=31199, loss=0.030070949345827103\n",
      "Surface training t=31200, loss=0.028669243678450584\n",
      "Surface training t=31201, loss=0.02226813230663538\n",
      "Surface training t=31202, loss=0.021684548817574978\n",
      "Surface training t=31203, loss=0.03091593272984028\n",
      "Surface training t=31204, loss=0.029637746512889862\n",
      "Surface training t=31205, loss=0.02118044439703226\n",
      "Surface training t=31206, loss=0.024375861510634422\n",
      "Surface training t=31207, loss=0.025451596826314926\n",
      "Surface training t=31208, loss=0.02716411091387272\n",
      "Surface training t=31209, loss=0.02765549346804619\n",
      "Surface training t=31210, loss=0.02577340416610241\n",
      "Surface training t=31211, loss=0.020329607650637627\n",
      "Surface training t=31212, loss=0.02320107351988554\n",
      "Surface training t=31213, loss=0.02194057498127222\n",
      "Surface training t=31214, loss=0.02108998131006956\n",
      "Surface training t=31215, loss=0.02778080478310585\n",
      "Surface training t=31216, loss=0.021425454877316952\n",
      "Surface training t=31217, loss=0.02332341531291604\n",
      "Surface training t=31218, loss=0.02817175444215536\n",
      "Surface training t=31219, loss=0.028014250099658966\n",
      "Surface training t=31220, loss=0.036898763850331306\n",
      "Surface training t=31221, loss=0.029959427192807198\n",
      "Surface training t=31222, loss=0.021934594959020615\n",
      "Surface training t=31223, loss=0.021265167742967606\n",
      "Surface training t=31224, loss=0.0201428453437984\n",
      "Surface training t=31225, loss=0.020740190520882607\n",
      "Surface training t=31226, loss=0.028476744890213013\n",
      "Surface training t=31227, loss=0.025937138125300407\n",
      "Surface training t=31228, loss=0.022631828673183918\n",
      "Surface training t=31229, loss=0.025232222862541676\n",
      "Surface training t=31230, loss=0.02835180051624775\n",
      "Surface training t=31231, loss=0.029576119035482407\n",
      "Surface training t=31232, loss=0.031588852405548096\n",
      "Surface training t=31233, loss=0.026169544085860252\n",
      "Surface training t=31234, loss=0.03297797217965126\n",
      "Surface training t=31235, loss=0.02318707201629877\n",
      "Surface training t=31236, loss=0.024345552548766136\n",
      "Surface training t=31237, loss=0.021815288811922073\n",
      "Surface training t=31238, loss=0.020580248907208443\n",
      "Surface training t=31239, loss=0.017218823544681072\n",
      "Surface training t=31240, loss=0.02134643029421568\n",
      "Surface training t=31241, loss=0.020808217115700245\n",
      "Surface training t=31242, loss=0.020731857046484947\n",
      "Surface training t=31243, loss=0.021197820082306862\n",
      "Surface training t=31244, loss=0.02206450328230858\n",
      "Surface training t=31245, loss=0.02406348194926977\n",
      "Surface training t=31246, loss=0.02285655215382576\n",
      "Surface training t=31247, loss=0.02513351570814848\n",
      "Surface training t=31248, loss=0.029294834472239017\n",
      "Surface training t=31249, loss=0.02267823275178671\n",
      "Surface training t=31250, loss=0.021331846714019775\n",
      "Surface training t=31251, loss=0.02701607346534729\n",
      "Surface training t=31252, loss=0.021445728838443756\n",
      "Surface training t=31253, loss=0.023537684231996536\n",
      "Surface training t=31254, loss=0.022607010789215565\n",
      "Surface training t=31255, loss=0.020573781803250313\n",
      "Surface training t=31256, loss=0.02333930879831314\n",
      "Surface training t=31257, loss=0.017860862892121077\n",
      "Surface training t=31258, loss=0.020002197474241257\n",
      "Surface training t=31259, loss=0.01853177696466446\n",
      "Surface training t=31260, loss=0.017543353606015444\n",
      "Surface training t=31261, loss=0.015084166545420885\n",
      "Surface training t=31262, loss=0.02070473972707987\n",
      "Surface training t=31263, loss=0.017445386853069067\n",
      "Surface training t=31264, loss=0.01962167676538229\n",
      "Surface training t=31265, loss=0.017385901883244514\n",
      "Surface training t=31266, loss=0.02342900261282921\n",
      "Surface training t=31267, loss=0.026018520817160606\n",
      "Surface training t=31268, loss=0.02204983774572611\n",
      "Surface training t=31269, loss=0.025212572887539864\n",
      "Surface training t=31270, loss=0.018358993344008923\n",
      "Surface training t=31271, loss=0.017742310650646687\n",
      "Surface training t=31272, loss=0.022545387037098408\n",
      "Surface training t=31273, loss=0.020275787450373173\n",
      "Surface training t=31274, loss=0.015618319623172283\n",
      "Surface training t=31275, loss=0.012581860180944204\n",
      "Surface training t=31276, loss=0.016806933097541332\n",
      "Surface training t=31277, loss=0.01499428367242217\n",
      "Surface training t=31278, loss=0.01727949921041727\n",
      "Surface training t=31279, loss=0.012858332134783268\n",
      "Surface training t=31280, loss=0.01751085091382265\n",
      "Surface training t=31281, loss=0.020019681192934513\n",
      "Surface training t=31282, loss=0.01857017818838358\n",
      "Surface training t=31283, loss=0.013997839763760567\n",
      "Surface training t=31284, loss=0.02085446286946535\n",
      "Surface training t=31285, loss=0.01742195524275303\n",
      "Surface training t=31286, loss=0.014043821953237057\n",
      "Surface training t=31287, loss=0.01375392870977521\n",
      "Surface training t=31288, loss=0.014616210479289293\n",
      "Surface training t=31289, loss=0.015882056206464767\n",
      "Surface training t=31290, loss=0.014082333538681269\n",
      "Surface training t=31291, loss=0.01703295623883605\n",
      "Surface training t=31292, loss=0.014341877773404121\n",
      "Surface training t=31293, loss=0.01637892471626401\n",
      "Surface training t=31294, loss=0.023103890009224415\n",
      "Surface training t=31295, loss=0.023236796259880066\n",
      "Surface training t=31296, loss=0.024074113927781582\n",
      "Surface training t=31297, loss=0.028402808122336864\n",
      "Surface training t=31298, loss=0.024726026691496372\n",
      "Surface training t=31299, loss=0.026950877159833908\n",
      "Surface training t=31300, loss=0.030244845896959305\n",
      "Surface training t=31301, loss=0.02953343093395233\n",
      "Surface training t=31302, loss=0.03109780792146921\n",
      "Surface training t=31303, loss=0.027003244496881962\n",
      "Surface training t=31304, loss=0.02254507690668106\n",
      "Surface training t=31305, loss=0.032811244018375874\n",
      "Surface training t=31306, loss=0.03034823201596737\n",
      "Surface training t=31307, loss=0.03495967946946621\n",
      "Surface training t=31308, loss=0.02738560177385807\n",
      "Surface training t=31309, loss=0.02576202806085348\n",
      "Surface training t=31310, loss=0.02934043575078249\n",
      "Surface training t=31311, loss=0.020622425246983767\n",
      "Surface training t=31312, loss=0.02233834657818079\n",
      "Surface training t=31313, loss=0.020415770821273327\n",
      "Surface training t=31314, loss=0.013159302063286304\n",
      "Surface training t=31315, loss=0.011383767239749432\n",
      "Surface training t=31316, loss=0.01777977216988802\n",
      "Surface training t=31317, loss=0.01826987136155367\n",
      "Surface training t=31318, loss=0.024924395605921745\n",
      "Surface training t=31319, loss=0.015076599549502134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=31320, loss=0.014566889498382807\n",
      "Surface training t=31321, loss=0.013668870087713003\n",
      "Surface training t=31322, loss=0.014749841764569283\n",
      "Surface training t=31323, loss=0.017920562997460365\n",
      "Surface training t=31324, loss=0.017024674452841282\n",
      "Surface training t=31325, loss=0.030920916236937046\n",
      "Surface training t=31326, loss=0.022411136887967587\n",
      "Surface training t=31327, loss=0.02212773635983467\n",
      "Surface training t=31328, loss=0.025568109937012196\n",
      "Surface training t=31329, loss=0.019127259962260723\n",
      "Surface training t=31330, loss=0.021244648844003677\n",
      "Surface training t=31331, loss=0.01739572174847126\n",
      "Surface training t=31332, loss=0.015532426536083221\n",
      "Surface training t=31333, loss=0.015270924661308527\n",
      "Surface training t=31334, loss=0.023110268637537956\n",
      "Surface training t=31335, loss=0.020040569826960564\n",
      "Surface training t=31336, loss=0.020372127182781696\n",
      "Surface training t=31337, loss=0.017576446756720543\n",
      "Surface training t=31338, loss=0.01888156868517399\n",
      "Surface training t=31339, loss=0.015846344642341137\n",
      "Surface training t=31340, loss=0.018043773248791695\n",
      "Surface training t=31341, loss=0.014933302532881498\n",
      "Surface training t=31342, loss=0.014851722400635481\n",
      "Surface training t=31343, loss=0.017724746372550726\n",
      "Surface training t=31344, loss=0.015861572697758675\n",
      "Surface training t=31345, loss=0.016991546843200922\n",
      "Surface training t=31346, loss=0.015080531127750874\n",
      "Surface training t=31347, loss=0.021210570819675922\n",
      "Surface training t=31348, loss=0.017777697183191776\n",
      "Surface training t=31349, loss=0.019745752215385437\n",
      "Surface training t=31350, loss=0.018176719546318054\n",
      "Surface training t=31351, loss=0.021820809692144394\n",
      "Surface training t=31352, loss=0.02488772664219141\n",
      "Surface training t=31353, loss=0.031823476776480675\n",
      "Surface training t=31354, loss=0.021492594853043556\n",
      "Surface training t=31355, loss=0.023809523321688175\n",
      "Surface training t=31356, loss=0.03030870109796524\n",
      "Surface training t=31357, loss=0.02262212708592415\n",
      "Surface training t=31358, loss=0.025444994680583477\n",
      "Surface training t=31359, loss=0.03483946807682514\n",
      "Surface training t=31360, loss=0.027057903818786144\n",
      "Surface training t=31361, loss=0.030609088949859142\n",
      "Surface training t=31362, loss=0.02150397840887308\n",
      "Surface training t=31363, loss=0.023825164884328842\n",
      "Surface training t=31364, loss=0.022786506451666355\n",
      "Surface training t=31365, loss=0.016988455783575773\n",
      "Surface training t=31366, loss=0.014332524966448545\n",
      "Surface training t=31367, loss=0.013658751267939806\n",
      "Surface training t=31368, loss=0.018549318425357342\n",
      "Surface training t=31369, loss=0.014662113040685654\n",
      "Surface training t=31370, loss=0.014820091426372528\n",
      "Surface training t=31371, loss=0.01444930024445057\n",
      "Surface training t=31372, loss=0.014484417624771595\n",
      "Surface training t=31373, loss=0.012089679017663002\n",
      "Surface training t=31374, loss=0.01877328660339117\n",
      "Surface training t=31375, loss=0.019480491988360882\n",
      "Surface training t=31376, loss=0.01918713329359889\n",
      "Surface training t=31377, loss=0.018763063475489616\n",
      "Surface training t=31378, loss=0.019069834612309933\n",
      "Surface training t=31379, loss=0.01729932613670826\n",
      "Surface training t=31380, loss=0.014968317933380604\n",
      "Surface training t=31381, loss=0.017823030706495047\n",
      "Surface training t=31382, loss=0.01841587759554386\n",
      "Surface training t=31383, loss=0.01621126988902688\n",
      "Surface training t=31384, loss=0.016770226415246725\n",
      "Surface training t=31385, loss=0.018190770410001278\n",
      "Surface training t=31386, loss=0.015844188164919615\n",
      "Surface training t=31387, loss=0.018277236726135015\n",
      "Surface training t=31388, loss=0.01751608122140169\n",
      "Surface training t=31389, loss=0.020454647950828075\n",
      "Surface training t=31390, loss=0.027139524929225445\n",
      "Surface training t=31391, loss=0.015581587329506874\n",
      "Surface training t=31392, loss=0.016138351056724787\n",
      "Surface training t=31393, loss=0.014462967403233051\n",
      "Surface training t=31394, loss=0.02169802412390709\n",
      "Surface training t=31395, loss=0.027771499007940292\n",
      "Surface training t=31396, loss=0.013778834138065577\n",
      "Surface training t=31397, loss=0.01890389248728752\n",
      "Surface training t=31398, loss=0.01297124708071351\n",
      "Surface training t=31399, loss=0.0162758594378829\n",
      "Surface training t=31400, loss=0.016856740228831768\n",
      "Surface training t=31401, loss=0.016410741489380598\n",
      "Surface training t=31402, loss=0.016846491023898125\n",
      "Surface training t=31403, loss=0.02001732587814331\n",
      "Surface training t=31404, loss=0.018806008622050285\n",
      "Surface training t=31405, loss=0.01751884911209345\n",
      "Surface training t=31406, loss=0.02642058953642845\n",
      "Surface training t=31407, loss=0.029362106695771217\n",
      "Surface training t=31408, loss=0.018860424868762493\n",
      "Surface training t=31409, loss=0.021018455736339092\n",
      "Surface training t=31410, loss=0.019402318634092808\n",
      "Surface training t=31411, loss=0.020334629341959953\n",
      "Surface training t=31412, loss=0.026226747781038284\n",
      "Surface training t=31413, loss=0.02654543798416853\n",
      "Surface training t=31414, loss=0.03079352341592312\n",
      "Surface training t=31415, loss=0.02860096376389265\n",
      "Surface training t=31416, loss=0.023068535141646862\n",
      "Surface training t=31417, loss=0.03945389948785305\n",
      "Surface training t=31418, loss=0.030740367248654366\n",
      "Surface training t=31419, loss=0.04199008457362652\n",
      "Surface training t=31420, loss=0.042596084997057915\n",
      "Surface training t=31421, loss=0.024976913817226887\n",
      "Surface training t=31422, loss=0.025651666335761547\n",
      "Surface training t=31423, loss=0.023851764388382435\n",
      "Surface training t=31424, loss=0.024917359463870525\n",
      "Surface training t=31425, loss=0.020484191831201315\n",
      "Surface training t=31426, loss=0.023077955469489098\n",
      "Surface training t=31427, loss=0.02962053008377552\n",
      "Surface training t=31428, loss=0.01984311593696475\n",
      "Surface training t=31429, loss=0.018334840890020132\n",
      "Surface training t=31430, loss=0.015771168284118176\n",
      "Surface training t=31431, loss=0.01859208196401596\n",
      "Surface training t=31432, loss=0.016121381893754005\n",
      "Surface training t=31433, loss=0.01663816813379526\n",
      "Surface training t=31434, loss=0.015997829381376505\n",
      "Surface training t=31435, loss=0.0174195384606719\n",
      "Surface training t=31436, loss=0.019129004329442978\n",
      "Surface training t=31437, loss=0.020442655310034752\n",
      "Surface training t=31438, loss=0.021663901396095753\n",
      "Surface training t=31439, loss=0.023658533580601215\n",
      "Surface training t=31440, loss=0.025907916016876698\n",
      "Surface training t=31441, loss=0.021450551226735115\n",
      "Surface training t=31442, loss=0.02550707757472992\n",
      "Surface training t=31443, loss=0.02617199346423149\n",
      "Surface training t=31444, loss=0.021295741200447083\n",
      "Surface training t=31445, loss=0.029819704592227936\n",
      "Surface training t=31446, loss=0.020934070460498333\n",
      "Surface training t=31447, loss=0.02449663355946541\n",
      "Surface training t=31448, loss=0.03478994220495224\n",
      "Surface training t=31449, loss=0.020741766318678856\n",
      "Surface training t=31450, loss=0.024327293038368225\n",
      "Surface training t=31451, loss=0.024377258494496346\n",
      "Surface training t=31452, loss=0.02049280796200037\n",
      "Surface training t=31453, loss=0.02520717866718769\n",
      "Surface training t=31454, loss=0.03185020387172699\n",
      "Surface training t=31455, loss=0.02655896171927452\n",
      "Surface training t=31456, loss=0.025310425087809563\n",
      "Surface training t=31457, loss=0.02772080898284912\n",
      "Surface training t=31458, loss=0.021805405616760254\n",
      "Surface training t=31459, loss=0.01989186927676201\n",
      "Surface training t=31460, loss=0.02458650153130293\n",
      "Surface training t=31461, loss=0.02756587602198124\n",
      "Surface training t=31462, loss=0.02573216985911131\n",
      "Surface training t=31463, loss=0.03909456543624401\n",
      "Surface training t=31464, loss=0.02851983532309532\n",
      "Surface training t=31465, loss=0.038733597844839096\n",
      "Surface training t=31466, loss=0.026969797909259796\n",
      "Surface training t=31467, loss=0.03194193169474602\n",
      "Surface training t=31468, loss=0.025412906892597675\n",
      "Surface training t=31469, loss=0.02078048139810562\n",
      "Surface training t=31470, loss=0.0295419804751873\n",
      "Surface training t=31471, loss=0.018217979930341244\n",
      "Surface training t=31472, loss=0.019404776394367218\n",
      "Surface training t=31473, loss=0.02030657511204481\n",
      "Surface training t=31474, loss=0.02429046295583248\n",
      "Surface training t=31475, loss=0.018252789042890072\n",
      "Surface training t=31476, loss=0.019092770293354988\n",
      "Surface training t=31477, loss=0.016896547749638557\n",
      "Surface training t=31478, loss=0.020611307583749294\n",
      "Surface training t=31479, loss=0.02640987280756235\n",
      "Surface training t=31480, loss=0.02030285820364952\n",
      "Surface training t=31481, loss=0.017101735342293978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=31482, loss=0.020773540250957012\n",
      "Surface training t=31483, loss=0.01934247836470604\n",
      "Surface training t=31484, loss=0.015454719308763742\n",
      "Surface training t=31485, loss=0.01703680958598852\n",
      "Surface training t=31486, loss=0.015698226168751717\n",
      "Surface training t=31487, loss=0.015424089040607214\n",
      "Surface training t=31488, loss=0.01883736066520214\n",
      "Surface training t=31489, loss=0.0178697993978858\n",
      "Surface training t=31490, loss=0.015125604346394539\n",
      "Surface training t=31491, loss=0.022175723686814308\n",
      "Surface training t=31492, loss=0.023303932510316372\n",
      "Surface training t=31493, loss=0.021931841038167477\n",
      "Surface training t=31494, loss=0.019261018373072147\n",
      "Surface training t=31495, loss=0.01602647313848138\n",
      "Surface training t=31496, loss=0.019522791728377342\n",
      "Surface training t=31497, loss=0.01901439717039466\n",
      "Surface training t=31498, loss=0.01877547614276409\n",
      "Surface training t=31499, loss=0.01828390872105956\n",
      "Surface training t=31500, loss=0.01840351428836584\n",
      "Surface training t=31501, loss=0.016873972490429878\n",
      "Surface training t=31502, loss=0.02078296197578311\n",
      "Surface training t=31503, loss=0.017225646879523993\n",
      "Surface training t=31504, loss=0.028245627880096436\n",
      "Surface training t=31505, loss=0.023587489034980536\n",
      "Surface training t=31506, loss=0.024284370243549347\n",
      "Surface training t=31507, loss=0.019770989194512367\n",
      "Surface training t=31508, loss=0.01993311196565628\n",
      "Surface training t=31509, loss=0.02267392724752426\n",
      "Surface training t=31510, loss=0.02231808751821518\n",
      "Surface training t=31511, loss=0.028942031785845757\n",
      "Surface training t=31512, loss=0.02571684867143631\n",
      "Surface training t=31513, loss=0.03159850835800171\n",
      "Surface training t=31514, loss=0.023438196629285812\n",
      "Surface training t=31515, loss=0.04143303260207176\n",
      "Surface training t=31516, loss=0.022424462717026472\n",
      "Surface training t=31517, loss=0.02326946146786213\n",
      "Surface training t=31518, loss=0.02817562874406576\n",
      "Surface training t=31519, loss=0.023452979512512684\n",
      "Surface training t=31520, loss=0.024603654630482197\n",
      "Surface training t=31521, loss=0.020470992662012577\n",
      "Surface training t=31522, loss=0.0195217477157712\n",
      "Surface training t=31523, loss=0.015858526341617107\n",
      "Surface training t=31524, loss=0.017085328698158264\n",
      "Surface training t=31525, loss=0.020630222745239735\n",
      "Surface training t=31526, loss=0.02047007903456688\n",
      "Surface training t=31527, loss=0.020494802854955196\n",
      "Surface training t=31528, loss=0.022076519206166267\n",
      "Surface training t=31529, loss=0.021759798750281334\n",
      "Surface training t=31530, loss=0.022743982262909412\n",
      "Surface training t=31531, loss=0.021438458003103733\n",
      "Surface training t=31532, loss=0.019486350007355213\n",
      "Surface training t=31533, loss=0.017589805647730827\n",
      "Surface training t=31534, loss=0.017537434585392475\n",
      "Surface training t=31535, loss=0.018440968357026577\n",
      "Surface training t=31536, loss=0.017599374987185\n",
      "Surface training t=31537, loss=0.018488743342459202\n",
      "Surface training t=31538, loss=0.018321858253329992\n",
      "Surface training t=31539, loss=0.022004205733537674\n",
      "Surface training t=31540, loss=0.016606703866273165\n",
      "Surface training t=31541, loss=0.022002228535711765\n",
      "Surface training t=31542, loss=0.020216984674334526\n",
      "Surface training t=31543, loss=0.02022161241620779\n",
      "Surface training t=31544, loss=0.019003278575837612\n",
      "Surface training t=31545, loss=0.018740321043878794\n",
      "Surface training t=31546, loss=0.013548875926062465\n",
      "Surface training t=31547, loss=0.015488734934478998\n",
      "Surface training t=31548, loss=0.01454890239983797\n",
      "Surface training t=31549, loss=0.016977777238935232\n",
      "Surface training t=31550, loss=0.014134691096842289\n",
      "Surface training t=31551, loss=0.014772471971809864\n",
      "Surface training t=31552, loss=0.017374878749251366\n",
      "Surface training t=31553, loss=0.015216054394841194\n",
      "Surface training t=31554, loss=0.01250514481216669\n",
      "Surface training t=31555, loss=0.014916206244379282\n",
      "Surface training t=31556, loss=0.019141544587910175\n",
      "Surface training t=31557, loss=0.01589520974084735\n",
      "Surface training t=31558, loss=0.0214325450360775\n",
      "Surface training t=31559, loss=0.02147108130156994\n",
      "Surface training t=31560, loss=0.017810934223234653\n",
      "Surface training t=31561, loss=0.02038162387907505\n",
      "Surface training t=31562, loss=0.01879693940281868\n",
      "Surface training t=31563, loss=0.01187288947403431\n",
      "Surface training t=31564, loss=0.02097765076905489\n",
      "Surface training t=31565, loss=0.0225864015519619\n",
      "Surface training t=31566, loss=0.022594663314521313\n",
      "Surface training t=31567, loss=0.020515681244432926\n",
      "Surface training t=31568, loss=0.016222139354795218\n",
      "Surface training t=31569, loss=0.02122800424695015\n",
      "Surface training t=31570, loss=0.017772743478417397\n",
      "Surface training t=31571, loss=0.02312683779746294\n",
      "Surface training t=31572, loss=0.01843951642513275\n",
      "Surface training t=31573, loss=0.019143485464155674\n",
      "Surface training t=31574, loss=0.0187136922031641\n",
      "Surface training t=31575, loss=0.020812413655221462\n",
      "Surface training t=31576, loss=0.019233556929975748\n",
      "Surface training t=31577, loss=0.02216147817671299\n",
      "Surface training t=31578, loss=0.021800247952342033\n",
      "Surface training t=31579, loss=0.01733270101249218\n",
      "Surface training t=31580, loss=0.016036442015320063\n",
      "Surface training t=31581, loss=0.01872256165370345\n",
      "Surface training t=31582, loss=0.021886066533625126\n",
      "Surface training t=31583, loss=0.018651796504855156\n",
      "Surface training t=31584, loss=0.027019179426133633\n",
      "Surface training t=31585, loss=0.0237515801563859\n",
      "Surface training t=31586, loss=0.02932501584291458\n",
      "Surface training t=31587, loss=0.02517731674015522\n",
      "Surface training t=31588, loss=0.023690635338425636\n",
      "Surface training t=31589, loss=0.0218449579551816\n",
      "Surface training t=31590, loss=0.02610913384705782\n",
      "Surface training t=31591, loss=0.019483164884150028\n",
      "Surface training t=31592, loss=0.031393518671393394\n",
      "Surface training t=31593, loss=0.022950095124542713\n",
      "Surface training t=31594, loss=0.021509915590286255\n",
      "Surface training t=31595, loss=0.020459930412471294\n",
      "Surface training t=31596, loss=0.02172758150845766\n",
      "Surface training t=31597, loss=0.021586737595498562\n",
      "Surface training t=31598, loss=0.01823993120342493\n",
      "Surface training t=31599, loss=0.021478568203747272\n",
      "Surface training t=31600, loss=0.016524906270205975\n",
      "Surface training t=31601, loss=0.021365657448768616\n",
      "Surface training t=31602, loss=0.01703025307506323\n",
      "Surface training t=31603, loss=0.022389203310012817\n",
      "Surface training t=31604, loss=0.01716245850548148\n",
      "Surface training t=31605, loss=0.020873033441603184\n",
      "Surface training t=31606, loss=0.029212414287030697\n",
      "Surface training t=31607, loss=0.02488118316978216\n",
      "Surface training t=31608, loss=0.026236635632812977\n",
      "Surface training t=31609, loss=0.02839543204754591\n",
      "Surface training t=31610, loss=0.018460062332451344\n",
      "Surface training t=31611, loss=0.029536203481256962\n",
      "Surface training t=31612, loss=0.028469271026551723\n",
      "Surface training t=31613, loss=0.025525981560349464\n",
      "Surface training t=31614, loss=0.025240185670554638\n",
      "Surface training t=31615, loss=0.023143121041357517\n",
      "Surface training t=31616, loss=0.02328728511929512\n",
      "Surface training t=31617, loss=0.03892316110432148\n",
      "Surface training t=31618, loss=0.034234113059937954\n",
      "Surface training t=31619, loss=0.037115637212991714\n",
      "Surface training t=31620, loss=0.03375200368463993\n",
      "Surface training t=31621, loss=0.033706012181937695\n",
      "Surface training t=31622, loss=0.020069553516805172\n",
      "Surface training t=31623, loss=0.02563518099486828\n",
      "Surface training t=31624, loss=0.032949321903288364\n",
      "Surface training t=31625, loss=0.026188377290964127\n",
      "Surface training t=31626, loss=0.02309979498386383\n",
      "Surface training t=31627, loss=0.022624272853136063\n",
      "Surface training t=31628, loss=0.021442892961204052\n",
      "Surface training t=31629, loss=0.023314359597861767\n",
      "Surface training t=31630, loss=0.02614258974790573\n",
      "Surface training t=31631, loss=0.021561297588050365\n",
      "Surface training t=31632, loss=0.01692625554278493\n",
      "Surface training t=31633, loss=0.020083949901163578\n",
      "Surface training t=31634, loss=0.019704554229974747\n",
      "Surface training t=31635, loss=0.016888338141143322\n",
      "Surface training t=31636, loss=0.015338391531258821\n",
      "Surface training t=31637, loss=0.0178654370829463\n",
      "Surface training t=31638, loss=0.017109195701777935\n",
      "Surface training t=31639, loss=0.022074181586503983\n",
      "Surface training t=31640, loss=0.02213677018880844\n",
      "Surface training t=31641, loss=0.015590520109981298\n",
      "Surface training t=31642, loss=0.022724890150129795\n",
      "Surface training t=31643, loss=0.02794571779668331\n",
      "Surface training t=31644, loss=0.022460637614130974\n",
      "Surface training t=31645, loss=0.02304420806467533\n",
      "Surface training t=31646, loss=0.026943267323076725\n",
      "Surface training t=31647, loss=0.021516839042305946\n",
      "Surface training t=31648, loss=0.028263344429433346\n",
      "Surface training t=31649, loss=0.031796859577298164\n",
      "Surface training t=31650, loss=0.021500715985894203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=31651, loss=0.0266485707834363\n",
      "Surface training t=31652, loss=0.017611790914088488\n",
      "Surface training t=31653, loss=0.017531944438815117\n",
      "Surface training t=31654, loss=0.01811231765896082\n",
      "Surface training t=31655, loss=0.020903379656374454\n",
      "Surface training t=31656, loss=0.020687540993094444\n",
      "Surface training t=31657, loss=0.0237714946269989\n",
      "Surface training t=31658, loss=0.023460181429982185\n",
      "Surface training t=31659, loss=0.02771819196641445\n",
      "Surface training t=31660, loss=0.020637279376387596\n",
      "Surface training t=31661, loss=0.024966386146843433\n",
      "Surface training t=31662, loss=0.02890391182154417\n",
      "Surface training t=31663, loss=0.020380811765789986\n",
      "Surface training t=31664, loss=0.02393200993537903\n",
      "Surface training t=31665, loss=0.023006852716207504\n",
      "Surface training t=31666, loss=0.032477072440087795\n",
      "Surface training t=31667, loss=0.02512252051383257\n",
      "Surface training t=31668, loss=0.026010616682469845\n",
      "Surface training t=31669, loss=0.026309234090149403\n",
      "Surface training t=31670, loss=0.02543742209672928\n",
      "Surface training t=31671, loss=0.031589386984705925\n",
      "Surface training t=31672, loss=0.02273723389953375\n",
      "Surface training t=31673, loss=0.027001271955668926\n",
      "Surface training t=31674, loss=0.02981269173324108\n",
      "Surface training t=31675, loss=0.025980323553085327\n",
      "Surface training t=31676, loss=0.026833162643015385\n",
      "Surface training t=31677, loss=0.02878436353057623\n",
      "Surface training t=31678, loss=0.029328991658985615\n",
      "Surface training t=31679, loss=0.01881712581962347\n",
      "Surface training t=31680, loss=0.027451329864561558\n",
      "Surface training t=31681, loss=0.024492471478879452\n",
      "Surface training t=31682, loss=0.02103446703404188\n",
      "Surface training t=31683, loss=0.028981134295463562\n",
      "Surface training t=31684, loss=0.029919108375906944\n",
      "Surface training t=31685, loss=0.025292891077697277\n",
      "Surface training t=31686, loss=0.022634731605648994\n",
      "Surface training t=31687, loss=0.02227292489260435\n",
      "Surface training t=31688, loss=0.021951855160295963\n",
      "Surface training t=31689, loss=0.019914877600967884\n",
      "Surface training t=31690, loss=0.0233811903744936\n",
      "Surface training t=31691, loss=0.033341056667268276\n",
      "Surface training t=31692, loss=0.025933729484677315\n",
      "Surface training t=31693, loss=0.022089187987148762\n",
      "Surface training t=31694, loss=0.01933689322322607\n",
      "Surface training t=31695, loss=0.024419442750513554\n",
      "Surface training t=31696, loss=0.02144687483087182\n",
      "Surface training t=31697, loss=0.02776057180017233\n",
      "Surface training t=31698, loss=0.0328294700011611\n",
      "Surface training t=31699, loss=0.024158881045877934\n",
      "Surface training t=31700, loss=0.028284059837460518\n",
      "Surface training t=31701, loss=0.02283464465290308\n",
      "Surface training t=31702, loss=0.02239036839455366\n",
      "Surface training t=31703, loss=0.017245493829250336\n",
      "Surface training t=31704, loss=0.018350102938711643\n",
      "Surface training t=31705, loss=0.014715946279466152\n",
      "Surface training t=31706, loss=0.016301451716572046\n",
      "Surface training t=31707, loss=0.02783918846398592\n",
      "Surface training t=31708, loss=0.022931700572371483\n",
      "Surface training t=31709, loss=0.022013562731444836\n",
      "Surface training t=31710, loss=0.02250766009092331\n",
      "Surface training t=31711, loss=0.01696218829602003\n",
      "Surface training t=31712, loss=0.02060869801789522\n",
      "Surface training t=31713, loss=0.016850578598678112\n",
      "Surface training t=31714, loss=0.02503543347120285\n",
      "Surface training t=31715, loss=0.0212246086448431\n",
      "Surface training t=31716, loss=0.01800009235739708\n",
      "Surface training t=31717, loss=0.01946706371381879\n",
      "Surface training t=31718, loss=0.022578797303140163\n",
      "Surface training t=31719, loss=0.024907857179641724\n",
      "Surface training t=31720, loss=0.026775093749165535\n",
      "Surface training t=31721, loss=0.022009387612342834\n",
      "Surface training t=31722, loss=0.02264727745205164\n",
      "Surface training t=31723, loss=0.01846588496118784\n",
      "Surface training t=31724, loss=0.01471830578520894\n",
      "Surface training t=31725, loss=0.018612063489854336\n",
      "Surface training t=31726, loss=0.015338202472776175\n",
      "Surface training t=31727, loss=0.01642636489123106\n",
      "Surface training t=31728, loss=0.015326833818107843\n",
      "Surface training t=31729, loss=0.01621103100478649\n",
      "Surface training t=31730, loss=0.012008856050670147\n",
      "Surface training t=31731, loss=0.014462142251431942\n",
      "Surface training t=31732, loss=0.01436497364193201\n",
      "Surface training t=31733, loss=0.018834597431123257\n",
      "Surface training t=31734, loss=0.02098620543256402\n",
      "Surface training t=31735, loss=0.018927183467894793\n",
      "Surface training t=31736, loss=0.021587176248431206\n",
      "Surface training t=31737, loss=0.020583702251315117\n",
      "Surface training t=31738, loss=0.022177569568157196\n",
      "Surface training t=31739, loss=0.027163781225681305\n",
      "Surface training t=31740, loss=0.02981044352054596\n",
      "Surface training t=31741, loss=0.023409302346408367\n",
      "Surface training t=31742, loss=0.021648630499839783\n",
      "Surface training t=31743, loss=0.02226703055202961\n",
      "Surface training t=31744, loss=0.020689726807177067\n",
      "Surface training t=31745, loss=0.028286920860409737\n",
      "Surface training t=31746, loss=0.02022380381822586\n",
      "Surface training t=31747, loss=0.020489174406975508\n",
      "Surface training t=31748, loss=0.024107552133500576\n",
      "Surface training t=31749, loss=0.019826477393507957\n",
      "Surface training t=31750, loss=0.017493192106485367\n",
      "Surface training t=31751, loss=0.017016681842505932\n",
      "Surface training t=31752, loss=0.017279379535466433\n",
      "Surface training t=31753, loss=0.021308953873813152\n",
      "Surface training t=31754, loss=0.01611924171447754\n",
      "Surface training t=31755, loss=0.016762330196797848\n",
      "Surface training t=31756, loss=0.017221621237695217\n",
      "Surface training t=31757, loss=0.016303192358464003\n",
      "Surface training t=31758, loss=0.016058363020420074\n",
      "Surface training t=31759, loss=0.023664092645049095\n",
      "Surface training t=31760, loss=0.01870372984558344\n",
      "Surface training t=31761, loss=0.018116469494998455\n",
      "Surface training t=31762, loss=0.017413201741874218\n",
      "Surface training t=31763, loss=0.016572038643062115\n",
      "Surface training t=31764, loss=0.016720180865377188\n",
      "Surface training t=31765, loss=0.01942266244441271\n",
      "Surface training t=31766, loss=0.018645627424120903\n",
      "Surface training t=31767, loss=0.019204318523406982\n",
      "Surface training t=31768, loss=0.021826637908816338\n",
      "Surface training t=31769, loss=0.016722965054214\n",
      "Surface training t=31770, loss=0.0164561215788126\n",
      "Surface training t=31771, loss=0.015414470341056585\n",
      "Surface training t=31772, loss=0.014285088516771793\n",
      "Surface training t=31773, loss=0.013648869004100561\n",
      "Surface training t=31774, loss=0.013413375243544579\n",
      "Surface training t=31775, loss=0.01375039154663682\n",
      "Surface training t=31776, loss=0.012153130024671555\n",
      "Surface training t=31777, loss=0.015250655822455883\n",
      "Surface training t=31778, loss=0.018823889084160328\n",
      "Surface training t=31779, loss=0.02098730392754078\n",
      "Surface training t=31780, loss=0.02383441850543022\n",
      "Surface training t=31781, loss=0.02415100857615471\n",
      "Surface training t=31782, loss=0.024158373475074768\n",
      "Surface training t=31783, loss=0.024735563434660435\n",
      "Surface training t=31784, loss=0.025087126530706882\n",
      "Surface training t=31785, loss=0.0198984332382679\n",
      "Surface training t=31786, loss=0.021569199860095978\n",
      "Surface training t=31787, loss=0.019632717594504356\n",
      "Surface training t=31788, loss=0.024515245109796524\n",
      "Surface training t=31789, loss=0.01768570765852928\n",
      "Surface training t=31790, loss=0.022332348860800266\n",
      "Surface training t=31791, loss=0.01715732179582119\n",
      "Surface training t=31792, loss=0.020023642107844353\n",
      "Surface training t=31793, loss=0.02976500429213047\n",
      "Surface training t=31794, loss=0.03755866549909115\n",
      "Surface training t=31795, loss=0.025379382073879242\n",
      "Surface training t=31796, loss=0.025220601819455624\n",
      "Surface training t=31797, loss=0.029866471886634827\n",
      "Surface training t=31798, loss=0.023152834735810757\n",
      "Surface training t=31799, loss=0.0196651853621006\n",
      "Surface training t=31800, loss=0.024986895732581615\n",
      "Surface training t=31801, loss=0.01959201879799366\n",
      "Surface training t=31802, loss=0.018239546567201614\n",
      "Surface training t=31803, loss=0.01866935659199953\n",
      "Surface training t=31804, loss=0.017856163904070854\n",
      "Surface training t=31805, loss=0.01685479376465082\n",
      "Surface training t=31806, loss=0.017743587493896484\n",
      "Surface training t=31807, loss=0.014143044129014015\n",
      "Surface training t=31808, loss=0.018932384438812733\n",
      "Surface training t=31809, loss=0.01869814097881317\n",
      "Surface training t=31810, loss=0.020578149240463972\n",
      "Surface training t=31811, loss=0.02348928712308407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=31812, loss=0.01807880587875843\n",
      "Surface training t=31813, loss=0.019237026572227478\n",
      "Surface training t=31814, loss=0.023182361386716366\n",
      "Surface training t=31815, loss=0.02120669186115265\n",
      "Surface training t=31816, loss=0.017645090352743864\n",
      "Surface training t=31817, loss=0.023964369669556618\n",
      "Surface training t=31818, loss=0.019304496236145496\n",
      "Surface training t=31819, loss=0.020603908225893974\n",
      "Surface training t=31820, loss=0.020956620573997498\n",
      "Surface training t=31821, loss=0.0149376867339015\n",
      "Surface training t=31822, loss=0.019458732567727566\n",
      "Surface training t=31823, loss=0.022034022957086563\n",
      "Surface training t=31824, loss=0.029232236556708813\n",
      "Surface training t=31825, loss=0.03885439224541187\n",
      "Surface training t=31826, loss=0.03419564012438059\n",
      "Surface training t=31827, loss=0.03261382505297661\n",
      "Surface training t=31828, loss=0.03439190238714218\n",
      "Surface training t=31829, loss=0.0318774227052927\n",
      "Surface training t=31830, loss=0.02771791350096464\n",
      "Surface training t=31831, loss=0.033346982672810555\n",
      "Surface training t=31832, loss=0.032054075971245766\n",
      "Surface training t=31833, loss=0.030646996572613716\n",
      "Surface training t=31834, loss=0.02416826318949461\n",
      "Surface training t=31835, loss=0.02118463721126318\n",
      "Surface training t=31836, loss=0.01851469185203314\n",
      "Surface training t=31837, loss=0.02356844488531351\n",
      "Surface training t=31838, loss=0.02480935398489237\n",
      "Surface training t=31839, loss=0.029345953837037086\n",
      "Surface training t=31840, loss=0.03002075105905533\n",
      "Surface training t=31841, loss=0.03305264562368393\n",
      "Surface training t=31842, loss=0.03197542019188404\n",
      "Surface training t=31843, loss=0.041719451546669006\n",
      "Surface training t=31844, loss=0.027194502763450146\n",
      "Surface training t=31845, loss=0.03506763745099306\n",
      "Surface training t=31846, loss=0.035982364788651466\n",
      "Surface training t=31847, loss=0.027532867155969143\n",
      "Surface training t=31848, loss=0.026990871876478195\n",
      "Surface training t=31849, loss=0.03200885560363531\n",
      "Surface training t=31850, loss=0.027010077610611916\n",
      "Surface training t=31851, loss=0.020258440636098385\n",
      "Surface training t=31852, loss=0.01793870236724615\n",
      "Surface training t=31853, loss=0.01512637734413147\n",
      "Surface training t=31854, loss=0.014465317130088806\n",
      "Surface training t=31855, loss=0.027840206399559975\n",
      "Surface training t=31856, loss=0.033416641876101494\n",
      "Surface training t=31857, loss=0.028578493744134903\n",
      "Surface training t=31858, loss=0.026541794650256634\n",
      "Surface training t=31859, loss=0.030349519103765488\n",
      "Surface training t=31860, loss=0.039059484377503395\n",
      "Surface training t=31861, loss=0.029813772067427635\n",
      "Surface training t=31862, loss=0.0274886442348361\n",
      "Surface training t=31863, loss=0.03328533098101616\n",
      "Surface training t=31864, loss=0.027151904068887234\n",
      "Surface training t=31865, loss=0.03335694130510092\n",
      "Surface training t=31866, loss=0.03924708254635334\n",
      "Surface training t=31867, loss=0.03021279815584421\n",
      "Surface training t=31868, loss=0.03182341158390045\n",
      "Surface training t=31869, loss=0.03144690487533808\n",
      "Surface training t=31870, loss=0.021245873533189297\n",
      "Surface training t=31871, loss=0.02705573197454214\n",
      "Surface training t=31872, loss=0.022461102344095707\n",
      "Surface training t=31873, loss=0.02670020330697298\n",
      "Surface training t=31874, loss=0.02247734647244215\n",
      "Surface training t=31875, loss=0.02039347868412733\n",
      "Surface training t=31876, loss=0.02167273685336113\n",
      "Surface training t=31877, loss=0.02358545921742916\n",
      "Surface training t=31878, loss=0.02487613633275032\n",
      "Surface training t=31879, loss=0.023856227286159992\n",
      "Surface training t=31880, loss=0.02334771677851677\n",
      "Surface training t=31881, loss=0.01822221651673317\n",
      "Surface training t=31882, loss=0.017911951057612896\n",
      "Surface training t=31883, loss=0.02504351083189249\n",
      "Surface training t=31884, loss=0.028783895075321198\n",
      "Surface training t=31885, loss=0.028127682395279408\n",
      "Surface training t=31886, loss=0.021867679432034492\n",
      "Surface training t=31887, loss=0.03344609774649143\n",
      "Surface training t=31888, loss=0.030733361840248108\n",
      "Surface training t=31889, loss=0.02693911176174879\n",
      "Surface training t=31890, loss=0.029915332794189453\n",
      "Surface training t=31891, loss=0.04246007651090622\n",
      "Surface training t=31892, loss=0.03011052869260311\n",
      "Surface training t=31893, loss=0.032845936715602875\n",
      "Surface training t=31894, loss=0.03884581848978996\n",
      "Surface training t=31895, loss=0.027451064437627792\n",
      "Surface training t=31896, loss=0.025101874954998493\n",
      "Surface training t=31897, loss=0.03135473467409611\n",
      "Surface training t=31898, loss=0.026297442615032196\n",
      "Surface training t=31899, loss=0.030808190815150738\n",
      "Surface training t=31900, loss=0.024411695078015327\n",
      "Surface training t=31901, loss=0.02518498059362173\n",
      "Surface training t=31902, loss=0.027201100252568722\n",
      "Surface training t=31903, loss=0.02398465108126402\n",
      "Surface training t=31904, loss=0.02328253909945488\n",
      "Surface training t=31905, loss=0.026685772463679314\n",
      "Surface training t=31906, loss=0.019380255602300167\n",
      "Surface training t=31907, loss=0.020348680671304464\n",
      "Surface training t=31908, loss=0.018777384888380766\n",
      "Surface training t=31909, loss=0.02477563265711069\n",
      "Surface training t=31910, loss=0.019548303447663784\n",
      "Surface training t=31911, loss=0.02284923940896988\n",
      "Surface training t=31912, loss=0.020172858610749245\n",
      "Surface training t=31913, loss=0.017458231188356876\n",
      "Surface training t=31914, loss=0.02155964355915785\n",
      "Surface training t=31915, loss=0.01912716683000326\n",
      "Surface training t=31916, loss=0.017593235708773136\n",
      "Surface training t=31917, loss=0.013001868966966867\n",
      "Surface training t=31918, loss=0.014506903011351824\n",
      "Surface training t=31919, loss=0.01913201715797186\n",
      "Surface training t=31920, loss=0.015314494259655476\n",
      "Surface training t=31921, loss=0.013900575693696737\n",
      "Surface training t=31922, loss=0.017660008743405342\n",
      "Surface training t=31923, loss=0.013043311890214682\n",
      "Surface training t=31924, loss=0.013793320395052433\n",
      "Surface training t=31925, loss=0.014824292622506618\n",
      "Surface training t=31926, loss=0.01483335392549634\n",
      "Surface training t=31927, loss=0.02091361116617918\n",
      "Surface training t=31928, loss=0.019282586872577667\n",
      "Surface training t=31929, loss=0.01849161833524704\n",
      "Surface training t=31930, loss=0.017267661169171333\n",
      "Surface training t=31931, loss=0.016148705035448074\n",
      "Surface training t=31932, loss=0.017386948689818382\n",
      "Surface training t=31933, loss=0.01984660793095827\n",
      "Surface training t=31934, loss=0.022434919141232967\n",
      "Surface training t=31935, loss=0.02625484485179186\n",
      "Surface training t=31936, loss=0.022779476828873158\n",
      "Surface training t=31937, loss=0.02020704187452793\n",
      "Surface training t=31938, loss=0.021434679627418518\n",
      "Surface training t=31939, loss=0.0224551884457469\n",
      "Surface training t=31940, loss=0.018486064858734608\n",
      "Surface training t=31941, loss=0.02419462241232395\n",
      "Surface training t=31942, loss=0.02151441853493452\n",
      "Surface training t=31943, loss=0.024075797758996487\n",
      "Surface training t=31944, loss=0.021379261277616024\n",
      "Surface training t=31945, loss=0.019430256448686123\n",
      "Surface training t=31946, loss=0.02167033776640892\n",
      "Surface training t=31947, loss=0.025315916165709496\n",
      "Surface training t=31948, loss=0.01693993527442217\n",
      "Surface training t=31949, loss=0.02354517951607704\n",
      "Surface training t=31950, loss=0.028678245842456818\n",
      "Surface training t=31951, loss=0.022790505550801754\n",
      "Surface training t=31952, loss=0.02006593346595764\n",
      "Surface training t=31953, loss=0.023453914560377598\n",
      "Surface training t=31954, loss=0.022708688862621784\n",
      "Surface training t=31955, loss=0.014493880793452263\n",
      "Surface training t=31956, loss=0.020037523470818996\n",
      "Surface training t=31957, loss=0.019162897020578384\n",
      "Surface training t=31958, loss=0.026270883157849312\n",
      "Surface training t=31959, loss=0.019845396280288696\n",
      "Surface training t=31960, loss=0.019973022863268852\n",
      "Surface training t=31961, loss=0.017050166614353657\n",
      "Surface training t=31962, loss=0.020500555634498596\n",
      "Surface training t=31963, loss=0.012706402689218521\n",
      "Surface training t=31964, loss=0.009979096706956625\n",
      "Surface training t=31965, loss=0.015956182964146137\n",
      "Surface training t=31966, loss=0.01584840239956975\n",
      "Surface training t=31967, loss=0.016891983337700367\n",
      "Surface training t=31968, loss=0.013613796792924404\n",
      "Surface training t=31969, loss=0.01348243746906519\n",
      "Surface training t=31970, loss=0.016423813998699188\n",
      "Surface training t=31971, loss=0.014876525849103928\n",
      "Surface training t=31972, loss=0.019666610285639763\n",
      "Surface training t=31973, loss=0.02458795625716448\n",
      "Surface training t=31974, loss=0.020709446631371975\n",
      "Surface training t=31975, loss=0.027807668782770634\n",
      "Surface training t=31976, loss=0.02767165284603834\n",
      "Surface training t=31977, loss=0.032400213181972504\n",
      "Surface training t=31978, loss=0.024999210610985756\n",
      "Surface training t=31979, loss=0.03246892988681793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=31980, loss=0.025067650713026524\n",
      "Surface training t=31981, loss=0.033358894288539886\n",
      "Surface training t=31982, loss=0.0228875745087862\n",
      "Surface training t=31983, loss=0.023090598173439503\n",
      "Surface training t=31984, loss=0.021593699231743813\n",
      "Surface training t=31985, loss=0.01837494969367981\n",
      "Surface training t=31986, loss=0.022617454640567303\n",
      "Surface training t=31987, loss=0.020213716197758913\n",
      "Surface training t=31988, loss=0.024015923961997032\n",
      "Surface training t=31989, loss=0.02455836534500122\n",
      "Surface training t=31990, loss=0.020400156266987324\n",
      "Surface training t=31991, loss=0.025608133524656296\n",
      "Surface training t=31992, loss=0.031387895345687866\n",
      "Surface training t=31993, loss=0.02527075633406639\n",
      "Surface training t=31994, loss=0.026943921111524105\n",
      "Surface training t=31995, loss=0.02511338982731104\n",
      "Surface training t=31996, loss=0.01971265859901905\n",
      "Surface training t=31997, loss=0.017695109359920025\n",
      "Surface training t=31998, loss=0.022239554673433304\n",
      "Surface training t=31999, loss=0.02556985942646861\n",
      "Surface training t=32000, loss=0.031327356584370136\n",
      "Surface training t=32001, loss=0.03710460104048252\n",
      "Surface training t=32002, loss=0.030051100999116898\n",
      "Surface training t=32003, loss=0.02924490161240101\n",
      "Surface training t=32004, loss=0.03283398877829313\n",
      "Surface training t=32005, loss=0.027797465212643147\n",
      "Surface training t=32006, loss=0.02537248283624649\n",
      "Surface training t=32007, loss=0.025898395106196404\n",
      "Surface training t=32008, loss=0.028007177636027336\n",
      "Surface training t=32009, loss=0.022541391663253307\n",
      "Surface training t=32010, loss=0.020624669268727303\n",
      "Surface training t=32011, loss=0.014757452066987753\n",
      "Surface training t=32012, loss=0.016444582492113113\n",
      "Surface training t=32013, loss=0.016958783380687237\n",
      "Surface training t=32014, loss=0.019478037022054195\n",
      "Surface training t=32015, loss=0.020751510746777058\n",
      "Surface training t=32016, loss=0.017917749471962452\n",
      "Surface training t=32017, loss=0.020010861568152905\n",
      "Surface training t=32018, loss=0.017271529417485\n",
      "Surface training t=32019, loss=0.016943960450589657\n",
      "Surface training t=32020, loss=0.019191091880202293\n",
      "Surface training t=32021, loss=0.019367026165127754\n",
      "Surface training t=32022, loss=0.02018184121698141\n",
      "Surface training t=32023, loss=0.025191962718963623\n",
      "Surface training t=32024, loss=0.01898617297410965\n",
      "Surface training t=32025, loss=0.022951471619307995\n",
      "Surface training t=32026, loss=0.031107627786695957\n",
      "Surface training t=32027, loss=0.024595793336629868\n",
      "Surface training t=32028, loss=0.02731336187571287\n",
      "Surface training t=32029, loss=0.02537485957145691\n",
      "Surface training t=32030, loss=0.0228828527033329\n",
      "Surface training t=32031, loss=0.01869822572916746\n",
      "Surface training t=32032, loss=0.026963232085108757\n",
      "Surface training t=32033, loss=0.018679513595998287\n",
      "Surface training t=32034, loss=0.019453839398920536\n",
      "Surface training t=32035, loss=0.01998717710375786\n",
      "Surface training t=32036, loss=0.018905099481344223\n",
      "Surface training t=32037, loss=0.018274585716426373\n",
      "Surface training t=32038, loss=0.017412858549505472\n",
      "Surface training t=32039, loss=0.015409841202199459\n",
      "Surface training t=32040, loss=0.023442331701517105\n",
      "Surface training t=32041, loss=0.01993097923696041\n",
      "Surface training t=32042, loss=0.017893034033477306\n",
      "Surface training t=32043, loss=0.023647794499993324\n",
      "Surface training t=32044, loss=0.023243446834385395\n",
      "Surface training t=32045, loss=0.020560607314109802\n",
      "Surface training t=32046, loss=0.026200314983725548\n",
      "Surface training t=32047, loss=0.027232928201556206\n",
      "Surface training t=32048, loss=0.0209196163341403\n",
      "Surface training t=32049, loss=0.020208347588777542\n",
      "Surface training t=32050, loss=0.023886163718998432\n",
      "Surface training t=32051, loss=0.019304286688566208\n",
      "Surface training t=32052, loss=0.01857459358870983\n",
      "Surface training t=32053, loss=0.01579015702009201\n",
      "Surface training t=32054, loss=0.015389932319521904\n",
      "Surface training t=32055, loss=0.017194773070514202\n",
      "Surface training t=32056, loss=0.01214371295645833\n",
      "Surface training t=32057, loss=0.01674136146903038\n",
      "Surface training t=32058, loss=0.01295064715668559\n",
      "Surface training t=32059, loss=0.016826478764414787\n",
      "Surface training t=32060, loss=0.01546224532648921\n",
      "Surface training t=32061, loss=0.016594101674854755\n",
      "Surface training t=32062, loss=0.018745748326182365\n",
      "Surface training t=32063, loss=0.016194021794945\n",
      "Surface training t=32064, loss=0.017830348573625088\n",
      "Surface training t=32065, loss=0.020084908232092857\n",
      "Surface training t=32066, loss=0.02081977389752865\n",
      "Surface training t=32067, loss=0.0195384593680501\n",
      "Surface training t=32068, loss=0.01915445365011692\n",
      "Surface training t=32069, loss=0.01754652103409171\n",
      "Surface training t=32070, loss=0.016262613236904144\n",
      "Surface training t=32071, loss=0.02280164696276188\n",
      "Surface training t=32072, loss=0.01977479550987482\n",
      "Surface training t=32073, loss=0.01730621512979269\n",
      "Surface training t=32074, loss=0.019011356867849827\n",
      "Surface training t=32075, loss=0.02438170462846756\n",
      "Surface training t=32076, loss=0.020421992987394333\n",
      "Surface training t=32077, loss=0.02171665895730257\n",
      "Surface training t=32078, loss=0.027805417776107788\n",
      "Surface training t=32079, loss=0.0270155631005764\n",
      "Surface training t=32080, loss=0.022201349958777428\n",
      "Surface training t=32081, loss=0.027890028432011604\n",
      "Surface training t=32082, loss=0.026020245626568794\n",
      "Surface training t=32083, loss=0.018727115355432034\n",
      "Surface training t=32084, loss=0.02603452280163765\n",
      "Surface training t=32085, loss=0.025020591914653778\n",
      "Surface training t=32086, loss=0.018090403638780117\n",
      "Surface training t=32087, loss=0.019884747453033924\n",
      "Surface training t=32088, loss=0.015392806846648455\n",
      "Surface training t=32089, loss=0.015257641673088074\n",
      "Surface training t=32090, loss=0.018104853574186563\n",
      "Surface training t=32091, loss=0.01547855930402875\n",
      "Surface training t=32092, loss=0.009995732922106981\n",
      "Surface training t=32093, loss=0.013483834452927113\n",
      "Surface training t=32094, loss=0.016463851556181908\n",
      "Surface training t=32095, loss=0.02574711386114359\n",
      "Surface training t=32096, loss=0.026732501573860645\n",
      "Surface training t=32097, loss=0.028830664232373238\n",
      "Surface training t=32098, loss=0.022380510345101357\n",
      "Surface training t=32099, loss=0.023544128984212875\n",
      "Surface training t=32100, loss=0.02339724637567997\n",
      "Surface training t=32101, loss=0.018174683675169945\n",
      "Surface training t=32102, loss=0.019622799940407276\n",
      "Surface training t=32103, loss=0.02163707185536623\n",
      "Surface training t=32104, loss=0.02458583004772663\n",
      "Surface training t=32105, loss=0.022236371412873268\n",
      "Surface training t=32106, loss=0.020128188654780388\n",
      "Surface training t=32107, loss=0.014677934348583221\n",
      "Surface training t=32108, loss=0.015695279464125633\n",
      "Surface training t=32109, loss=0.01617457391694188\n",
      "Surface training t=32110, loss=0.01537492498755455\n",
      "Surface training t=32111, loss=0.014894146006554365\n",
      "Surface training t=32112, loss=0.015122407581657171\n",
      "Surface training t=32113, loss=0.017239860258996487\n",
      "Surface training t=32114, loss=0.019135019276291132\n",
      "Surface training t=32115, loss=0.02017619926482439\n",
      "Surface training t=32116, loss=0.028135123662650585\n",
      "Surface training t=32117, loss=0.032669758424162865\n",
      "Surface training t=32118, loss=0.026456782594323158\n",
      "Surface training t=32119, loss=0.022024571895599365\n",
      "Surface training t=32120, loss=0.022214755415916443\n",
      "Surface training t=32121, loss=0.018809927627444267\n",
      "Surface training t=32122, loss=0.02263381192460656\n",
      "Surface training t=32123, loss=0.022848238237202168\n",
      "Surface training t=32124, loss=0.02167176641523838\n",
      "Surface training t=32125, loss=0.02346639707684517\n",
      "Surface training t=32126, loss=0.025608745403587818\n",
      "Surface training t=32127, loss=0.025537991896271706\n",
      "Surface training t=32128, loss=0.02360715437680483\n",
      "Surface training t=32129, loss=0.01643746066838503\n",
      "Surface training t=32130, loss=0.015130533371120691\n",
      "Surface training t=32131, loss=0.015239039901643991\n",
      "Surface training t=32132, loss=0.014163593761622906\n",
      "Surface training t=32133, loss=0.011934702284634113\n",
      "Surface training t=32134, loss=0.01837730873376131\n",
      "Surface training t=32135, loss=0.021037074737250805\n",
      "Surface training t=32136, loss=0.01733420928940177\n",
      "Surface training t=32137, loss=0.014492700807750225\n",
      "Surface training t=32138, loss=0.01662476733326912\n",
      "Surface training t=32139, loss=0.029486405663192272\n",
      "Surface training t=32140, loss=0.022235766053199768\n",
      "Surface training t=32141, loss=0.02178407646715641\n",
      "Surface training t=32142, loss=0.023924673907458782\n",
      "Surface training t=32143, loss=0.0150491907261312\n",
      "Surface training t=32144, loss=0.022608143277466297\n",
      "Surface training t=32145, loss=0.02044328674674034\n",
      "Surface training t=32146, loss=0.021295790560543537\n",
      "Surface training t=32147, loss=0.02337261103093624\n",
      "Surface training t=32148, loss=0.020274107344448566\n",
      "Surface training t=32149, loss=0.020557045470923185\n",
      "Surface training t=32150, loss=0.015354265458881855\n",
      "Surface training t=32151, loss=0.019479372538626194\n",
      "Surface training t=32152, loss=0.025275501422584057\n",
      "Surface training t=32153, loss=0.02415355760604143\n",
      "Surface training t=32154, loss=0.02320605143904686\n",
      "Surface training t=32155, loss=0.02814778033643961\n",
      "Surface training t=32156, loss=0.017508994787931442\n",
      "Surface training t=32157, loss=0.025257865898311138\n",
      "Surface training t=32158, loss=0.01971391076222062\n",
      "Surface training t=32159, loss=0.018729494884610176\n",
      "Surface training t=32160, loss=0.019330947659909725\n",
      "Surface training t=32161, loss=0.01776161789894104\n",
      "Surface training t=32162, loss=0.014773022383451462\n",
      "Surface training t=32163, loss=0.017312703654170036\n",
      "Surface training t=32164, loss=0.01585868326947093\n",
      "Surface training t=32165, loss=0.019283868372440338\n",
      "Surface training t=32166, loss=0.018822981510311365\n",
      "Surface training t=32167, loss=0.021810833364725113\n",
      "Surface training t=32168, loss=0.024683864787220955\n",
      "Surface training t=32169, loss=0.02120845764875412\n",
      "Surface training t=32170, loss=0.016112460754811764\n",
      "Surface training t=32171, loss=0.020065071992576122\n",
      "Surface training t=32172, loss=0.023231370374560356\n",
      "Surface training t=32173, loss=0.01862217579036951\n",
      "Surface training t=32174, loss=0.016297268215566874\n",
      "Surface training t=32175, loss=0.028510307893157005\n",
      "Surface training t=32176, loss=0.02119491621851921\n",
      "Surface training t=32177, loss=0.02243934292346239\n",
      "Surface training t=32178, loss=0.022343595512211323\n",
      "Surface training t=32179, loss=0.016772584058344364\n",
      "Surface training t=32180, loss=0.017388688400387764\n",
      "Surface training t=32181, loss=0.014080271124839783\n",
      "Surface training t=32182, loss=0.022102169692516327\n",
      "Surface training t=32183, loss=0.023185953497886658\n",
      "Surface training t=32184, loss=0.01700607454404235\n",
      "Surface training t=32185, loss=0.02127816155552864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=32186, loss=0.023045984096825123\n",
      "Surface training t=32187, loss=0.022331788204610348\n",
      "Surface training t=32188, loss=0.018960861954838037\n",
      "Surface training t=32189, loss=0.02576448582112789\n",
      "Surface training t=32190, loss=0.02071464527398348\n",
      "Surface training t=32191, loss=0.017220077104866505\n",
      "Surface training t=32192, loss=0.01740844640880823\n",
      "Surface training t=32193, loss=0.017378696240484715\n",
      "Surface training t=32194, loss=0.021583636291325092\n",
      "Surface training t=32195, loss=0.02058332785964012\n",
      "Surface training t=32196, loss=0.01751123322173953\n",
      "Surface training t=32197, loss=0.0195280434563756\n",
      "Surface training t=32198, loss=0.021471203304827213\n",
      "Surface training t=32199, loss=0.025829647667706013\n",
      "Surface training t=32200, loss=0.020398260094225407\n",
      "Surface training t=32201, loss=0.02462160587310791\n",
      "Surface training t=32202, loss=0.022505709901452065\n",
      "Surface training t=32203, loss=0.01999987754970789\n",
      "Surface training t=32204, loss=0.027185522951185703\n",
      "Surface training t=32205, loss=0.021745694801211357\n",
      "Surface training t=32206, loss=0.023843123577535152\n",
      "Surface training t=32207, loss=0.03245346248149872\n",
      "Surface training t=32208, loss=0.020891391672194004\n",
      "Surface training t=32209, loss=0.02737096231430769\n",
      "Surface training t=32210, loss=0.01605390477925539\n",
      "Surface training t=32211, loss=0.01758326217532158\n",
      "Surface training t=32212, loss=0.015680096112191677\n",
      "Surface training t=32213, loss=0.01701846532523632\n",
      "Surface training t=32214, loss=0.019428598694503307\n",
      "Surface training t=32215, loss=0.014216359239071608\n",
      "Surface training t=32216, loss=0.01638654712587595\n",
      "Surface training t=32217, loss=0.018709152936935425\n",
      "Surface training t=32218, loss=0.016409809701144695\n",
      "Surface training t=32219, loss=0.026558964513242245\n",
      "Surface training t=32220, loss=0.019908016547560692\n",
      "Surface training t=32221, loss=0.025746939703822136\n",
      "Surface training t=32222, loss=0.01820227410644293\n",
      "Surface training t=32223, loss=0.02037122007459402\n",
      "Surface training t=32224, loss=0.0206237668171525\n",
      "Surface training t=32225, loss=0.023047535680234432\n",
      "Surface training t=32226, loss=0.02609902899712324\n",
      "Surface training t=32227, loss=0.016526897437870502\n",
      "Surface training t=32228, loss=0.018346368335187435\n",
      "Surface training t=32229, loss=0.01775941113010049\n",
      "Surface training t=32230, loss=0.01913338713347912\n",
      "Surface training t=32231, loss=0.01728896051645279\n",
      "Surface training t=32232, loss=0.01822343049570918\n",
      "Surface training t=32233, loss=0.020623348653316498\n",
      "Surface training t=32234, loss=0.013409883715212345\n",
      "Surface training t=32235, loss=0.022714709863066673\n",
      "Surface training t=32236, loss=0.021416252478957176\n",
      "Surface training t=32237, loss=0.01687949988991022\n",
      "Surface training t=32238, loss=0.014614731539040804\n",
      "Surface training t=32239, loss=0.013150118757039309\n",
      "Surface training t=32240, loss=0.01303816121071577\n",
      "Surface training t=32241, loss=0.019120252691209316\n",
      "Surface training t=32242, loss=0.017058245837688446\n",
      "Surface training t=32243, loss=0.02163013443350792\n",
      "Surface training t=32244, loss=0.02567678689956665\n",
      "Surface training t=32245, loss=0.022518531419336796\n",
      "Surface training t=32246, loss=0.021674432791769505\n",
      "Surface training t=32247, loss=0.025510680861771107\n",
      "Surface training t=32248, loss=0.02884878497570753\n",
      "Surface training t=32249, loss=0.02299946267157793\n",
      "Surface training t=32250, loss=0.026584860868752003\n",
      "Surface training t=32251, loss=0.027210148982703686\n",
      "Surface training t=32252, loss=0.021405960898846388\n",
      "Surface training t=32253, loss=0.017191470600664616\n",
      "Surface training t=32254, loss=0.019691425375640392\n",
      "Surface training t=32255, loss=0.02181934006512165\n",
      "Surface training t=32256, loss=0.023372307419776917\n",
      "Surface training t=32257, loss=0.01786023285239935\n",
      "Surface training t=32258, loss=0.01675008237361908\n",
      "Surface training t=32259, loss=0.016234072856605053\n",
      "Surface training t=32260, loss=0.02167435083538294\n",
      "Surface training t=32261, loss=0.02196356002241373\n",
      "Surface training t=32262, loss=0.03675636649131775\n",
      "Surface training t=32263, loss=0.02786421962082386\n",
      "Surface training t=32264, loss=0.03856154344975948\n",
      "Surface training t=32265, loss=0.037813663482666016\n",
      "Surface training t=32266, loss=0.030575696378946304\n",
      "Surface training t=32267, loss=0.025530831888318062\n",
      "Surface training t=32268, loss=0.048249250277876854\n",
      "Surface training t=32269, loss=0.028662952594459057\n",
      "Surface training t=32270, loss=0.048906974494457245\n",
      "Surface training t=32271, loss=0.031779089011251926\n",
      "Surface training t=32272, loss=0.025868045166134834\n",
      "Surface training t=32273, loss=0.02578673232346773\n",
      "Surface training t=32274, loss=0.02831456810235977\n",
      "Surface training t=32275, loss=0.022365002427250147\n",
      "Surface training t=32276, loss=0.03552353195846081\n",
      "Surface training t=32277, loss=0.03372032381594181\n",
      "Surface training t=32278, loss=0.024331798776984215\n",
      "Surface training t=32279, loss=0.032382259145379066\n",
      "Surface training t=32280, loss=0.02619423158466816\n",
      "Surface training t=32281, loss=0.02564684860408306\n",
      "Surface training t=32282, loss=0.02451013121753931\n",
      "Surface training t=32283, loss=0.02114164549857378\n",
      "Surface training t=32284, loss=0.023199450224637985\n",
      "Surface training t=32285, loss=0.022015581838786602\n",
      "Surface training t=32286, loss=0.018954120576381683\n",
      "Surface training t=32287, loss=0.01787098776549101\n",
      "Surface training t=32288, loss=0.01523550320416689\n",
      "Surface training t=32289, loss=0.02690315991640091\n",
      "Surface training t=32290, loss=0.01975888293236494\n",
      "Surface training t=32291, loss=0.016113745514303446\n",
      "Surface training t=32292, loss=0.01930266711860895\n",
      "Surface training t=32293, loss=0.019500105641782284\n",
      "Surface training t=32294, loss=0.017773982603102922\n",
      "Surface training t=32295, loss=0.01848178170621395\n",
      "Surface training t=32296, loss=0.01954091154038906\n",
      "Surface training t=32297, loss=0.023090858943760395\n",
      "Surface training t=32298, loss=0.019233187660574913\n",
      "Surface training t=32299, loss=0.019538582302629948\n",
      "Surface training t=32300, loss=0.02390069793909788\n",
      "Surface training t=32301, loss=0.023673207499086857\n",
      "Surface training t=32302, loss=0.02999445702880621\n",
      "Surface training t=32303, loss=0.023493272718042135\n",
      "Surface training t=32304, loss=0.020254099741578102\n",
      "Surface training t=32305, loss=0.028819415718317032\n",
      "Surface training t=32306, loss=0.028541692532598972\n",
      "Surface training t=32307, loss=0.03316161409020424\n",
      "Surface training t=32308, loss=0.03003319539129734\n",
      "Surface training t=32309, loss=0.020092779770493507\n",
      "Surface training t=32310, loss=0.029273193329572678\n",
      "Surface training t=32311, loss=0.03075644001364708\n",
      "Surface training t=32312, loss=0.022306298837065697\n",
      "Surface training t=32313, loss=0.028980839997529984\n",
      "Surface training t=32314, loss=0.02535204589366913\n",
      "Surface training t=32315, loss=0.02472736593335867\n",
      "Surface training t=32316, loss=0.026909103617072105\n",
      "Surface training t=32317, loss=0.02333619724959135\n",
      "Surface training t=32318, loss=0.022971656173467636\n",
      "Surface training t=32319, loss=0.030626585707068443\n",
      "Surface training t=32320, loss=0.026098208501935005\n",
      "Surface training t=32321, loss=0.03081005346029997\n",
      "Surface training t=32322, loss=0.03302090801298618\n",
      "Surface training t=32323, loss=0.030059575103223324\n",
      "Surface training t=32324, loss=0.02442788053303957\n",
      "Surface training t=32325, loss=0.02645008359104395\n",
      "Surface training t=32326, loss=0.0327809127047658\n",
      "Surface training t=32327, loss=0.03705624956637621\n",
      "Surface training t=32328, loss=0.04220941849052906\n",
      "Surface training t=32329, loss=0.040290530771017075\n",
      "Surface training t=32330, loss=0.026467307470738888\n",
      "Surface training t=32331, loss=0.025935925543308258\n",
      "Surface training t=32332, loss=0.02439158782362938\n",
      "Surface training t=32333, loss=0.0273444764316082\n",
      "Surface training t=32334, loss=0.0260746655985713\n",
      "Surface training t=32335, loss=0.023789330385625362\n",
      "Surface training t=32336, loss=0.02862053830176592\n",
      "Surface training t=32337, loss=0.026677658781409264\n",
      "Surface training t=32338, loss=0.02398320473730564\n",
      "Surface training t=32339, loss=0.022172892466187477\n",
      "Surface training t=32340, loss=0.024334198795259\n",
      "Surface training t=32341, loss=0.030229585245251656\n",
      "Surface training t=32342, loss=0.031721118837594986\n",
      "Surface training t=32343, loss=0.02424686960875988\n",
      "Surface training t=32344, loss=0.023614694364368916\n",
      "Surface training t=32345, loss=0.020638979971408844\n",
      "Surface training t=32346, loss=0.023386464454233646\n",
      "Surface training t=32347, loss=0.022312330082058907\n",
      "Surface training t=32348, loss=0.017202730756253004\n",
      "Surface training t=32349, loss=0.02246508002281189\n",
      "Surface training t=32350, loss=0.02044464461505413\n",
      "Surface training t=32351, loss=0.015208699740469456\n",
      "Surface training t=32352, loss=0.01802571676671505\n",
      "Surface training t=32353, loss=0.017073050141334534\n",
      "Surface training t=32354, loss=0.016244901809841394\n",
      "Surface training t=32355, loss=0.021857883781194687\n",
      "Surface training t=32356, loss=0.019822814501821995\n",
      "Surface training t=32357, loss=0.023487890139222145\n",
      "Surface training t=32358, loss=0.017677444498986006\n",
      "Surface training t=32359, loss=0.013494379352778196\n",
      "Surface training t=32360, loss=0.02209279965609312\n",
      "Surface training t=32361, loss=0.026006357744336128\n",
      "Surface training t=32362, loss=0.02250723261386156\n",
      "Surface training t=32363, loss=0.01888480968773365\n",
      "Surface training t=32364, loss=0.01863790675997734\n",
      "Surface training t=32365, loss=0.025178035721182823\n",
      "Surface training t=32366, loss=0.02408025972545147\n",
      "Surface training t=32367, loss=0.017880600411444902\n",
      "Surface training t=32368, loss=0.028011941350996494\n",
      "Surface training t=32369, loss=0.021148445084691048\n",
      "Surface training t=32370, loss=0.01800904981791973\n",
      "Surface training t=32371, loss=0.023865479044616222\n",
      "Surface training t=32372, loss=0.01904117316007614\n",
      "Surface training t=32373, loss=0.01960413856431842\n",
      "Surface training t=32374, loss=0.014329458121210337\n",
      "Surface training t=32375, loss=0.013261392246931791\n",
      "Surface training t=32376, loss=0.021596801467239857\n",
      "Surface training t=32377, loss=0.015971310436725616\n",
      "Surface training t=32378, loss=0.014950146898627281\n",
      "Surface training t=32379, loss=0.012352143879979849\n",
      "Surface training t=32380, loss=0.017749857623130083\n",
      "Surface training t=32381, loss=0.01458056177943945\n",
      "Surface training t=32382, loss=0.015909319277852774\n",
      "Surface training t=32383, loss=0.024109622463583946\n",
      "Surface training t=32384, loss=0.021809258498251438\n",
      "Surface training t=32385, loss=0.01767433062195778\n",
      "Surface training t=32386, loss=0.015309962909668684\n",
      "Surface training t=32387, loss=0.012857989873737097\n",
      "Surface training t=32388, loss=0.015208656899631023\n",
      "Surface training t=32389, loss=0.014734196476638317\n",
      "Surface training t=32390, loss=0.012303427327424288\n",
      "Surface training t=32391, loss=0.017034471035003662\n",
      "Surface training t=32392, loss=0.022791617549955845\n",
      "Surface training t=32393, loss=0.02465160470455885\n",
      "Surface training t=32394, loss=0.019359944388270378\n",
      "Surface training t=32395, loss=0.02329425700008869\n",
      "Surface training t=32396, loss=0.020351099781692028\n",
      "Surface training t=32397, loss=0.01772024855017662\n",
      "Surface training t=32398, loss=0.018278279341757298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=32399, loss=0.016927525866776705\n",
      "Surface training t=32400, loss=0.018549637869000435\n",
      "Surface training t=32401, loss=0.02128921914845705\n",
      "Surface training t=32402, loss=0.016687991097569466\n",
      "Surface training t=32403, loss=0.02089668530970812\n",
      "Surface training t=32404, loss=0.024669061414897442\n",
      "Surface training t=32405, loss=0.021606692112982273\n",
      "Surface training t=32406, loss=0.0191554743796587\n",
      "Surface training t=32407, loss=0.0224901270121336\n",
      "Surface training t=32408, loss=0.017045811749994755\n",
      "Surface training t=32409, loss=0.017641705460846424\n",
      "Surface training t=32410, loss=0.01534306164830923\n",
      "Surface training t=32411, loss=0.015731381718069315\n",
      "Surface training t=32412, loss=0.01589849079027772\n",
      "Surface training t=32413, loss=0.013870427384972572\n",
      "Surface training t=32414, loss=0.016091421712189913\n",
      "Surface training t=32415, loss=0.014291329775005579\n",
      "Surface training t=32416, loss=0.012233120389282703\n",
      "Surface training t=32417, loss=0.017009103205055\n",
      "Surface training t=32418, loss=0.015313081443309784\n",
      "Surface training t=32419, loss=0.014235257171094418\n",
      "Surface training t=32420, loss=0.015807490330189466\n",
      "Surface training t=32421, loss=0.01372389355674386\n",
      "Surface training t=32422, loss=0.01481198612600565\n",
      "Surface training t=32423, loss=0.02118590474128723\n",
      "Surface training t=32424, loss=0.019233491737395525\n",
      "Surface training t=32425, loss=0.015770782716572285\n",
      "Surface training t=32426, loss=0.022444169968366623\n",
      "Surface training t=32427, loss=0.02274128422141075\n",
      "Surface training t=32428, loss=0.02059973683208227\n",
      "Surface training t=32429, loss=0.02615782804787159\n",
      "Surface training t=32430, loss=0.018012825399637222\n",
      "Surface training t=32431, loss=0.016777626238763332\n",
      "Surface training t=32432, loss=0.02222632337361574\n",
      "Surface training t=32433, loss=0.01785496436059475\n",
      "Surface training t=32434, loss=0.023212797939777374\n",
      "Surface training t=32435, loss=0.02195593435317278\n",
      "Surface training t=32436, loss=0.0200482327491045\n",
      "Surface training t=32437, loss=0.018484235275536776\n",
      "Surface training t=32438, loss=0.011770732700824738\n",
      "Surface training t=32439, loss=0.010587343480437994\n",
      "Surface training t=32440, loss=0.01002914970740676\n",
      "Surface training t=32441, loss=0.014191137626767159\n",
      "Surface training t=32442, loss=0.011050566099584103\n",
      "Surface training t=32443, loss=0.020018531009554863\n",
      "Surface training t=32444, loss=0.016868348699063063\n",
      "Surface training t=32445, loss=0.017297659069299698\n",
      "Surface training t=32446, loss=0.022061574272811413\n",
      "Surface training t=32447, loss=0.01930201333016157\n",
      "Surface training t=32448, loss=0.01839373353868723\n",
      "Surface training t=32449, loss=0.022880693897604942\n",
      "Surface training t=32450, loss=0.019022916443645954\n",
      "Surface training t=32451, loss=0.020660043694078922\n",
      "Surface training t=32452, loss=0.019267968833446503\n",
      "Surface training t=32453, loss=0.02286667888984084\n",
      "Surface training t=32454, loss=0.02041078731417656\n",
      "Surface training t=32455, loss=0.021022322587668896\n",
      "Surface training t=32456, loss=0.021068288013339043\n",
      "Surface training t=32457, loss=0.01965176872909069\n",
      "Surface training t=32458, loss=0.019443174824118614\n",
      "Surface training t=32459, loss=0.019541322253644466\n",
      "Surface training t=32460, loss=0.018131832592189312\n",
      "Surface training t=32461, loss=0.017748495563864708\n",
      "Surface training t=32462, loss=0.021522280760109425\n",
      "Surface training t=32463, loss=0.023361527360975742\n",
      "Surface training t=32464, loss=0.023907732218503952\n",
      "Surface training t=32465, loss=0.020039409399032593\n",
      "Surface training t=32466, loss=0.022333348635584116\n",
      "Surface training t=32467, loss=0.02935570292174816\n",
      "Surface training t=32468, loss=0.019302881322801113\n",
      "Surface training t=32469, loss=0.02087399736046791\n",
      "Surface training t=32470, loss=0.021685561165213585\n",
      "Surface training t=32471, loss=0.02121166419237852\n",
      "Surface training t=32472, loss=0.02298156078904867\n",
      "Surface training t=32473, loss=0.020377865992486477\n",
      "Surface training t=32474, loss=0.01826311368495226\n",
      "Surface training t=32475, loss=0.013084628153592348\n",
      "Surface training t=32476, loss=0.016806181985884905\n",
      "Surface training t=32477, loss=0.01466682180762291\n",
      "Surface training t=32478, loss=0.01622240897268057\n",
      "Surface training t=32479, loss=0.012688161339610815\n",
      "Surface training t=32480, loss=0.016585294622927904\n",
      "Surface training t=32481, loss=0.020596562884747982\n",
      "Surface training t=32482, loss=0.01754970569163561\n",
      "Surface training t=32483, loss=0.018712827004492283\n",
      "Surface training t=32484, loss=0.020004747435450554\n",
      "Surface training t=32485, loss=0.024308142252266407\n",
      "Surface training t=32486, loss=0.024808382615447044\n",
      "Surface training t=32487, loss=0.026829006150364876\n",
      "Surface training t=32488, loss=0.024053587578237057\n",
      "Surface training t=32489, loss=0.036717044189572334\n",
      "Surface training t=32490, loss=0.02531797345727682\n",
      "Surface training t=32491, loss=0.026620078831911087\n",
      "Surface training t=32492, loss=0.018756825476884842\n",
      "Surface training t=32493, loss=0.014651639387011528\n",
      "Surface training t=32494, loss=0.01731119677424431\n",
      "Surface training t=32495, loss=0.018162349238991737\n",
      "Surface training t=32496, loss=0.019515677355229855\n",
      "Surface training t=32497, loss=0.016344073228538036\n",
      "Surface training t=32498, loss=0.018446842674165964\n",
      "Surface training t=32499, loss=0.018719611689448357\n",
      "Surface training t=32500, loss=0.02789641823619604\n",
      "Surface training t=32501, loss=0.02295362576842308\n",
      "Surface training t=32502, loss=0.0251583531498909\n",
      "Surface training t=32503, loss=0.02143536787480116\n",
      "Surface training t=32504, loss=0.019528338685631752\n",
      "Surface training t=32505, loss=0.021965700201690197\n",
      "Surface training t=32506, loss=0.019817430526018143\n",
      "Surface training t=32507, loss=0.021760561503469944\n",
      "Surface training t=32508, loss=0.02372610569000244\n",
      "Surface training t=32509, loss=0.021467302925884724\n",
      "Surface training t=32510, loss=0.019677472300827503\n",
      "Surface training t=32511, loss=0.017521914560347795\n",
      "Surface training t=32512, loss=0.020115556195378304\n",
      "Surface training t=32513, loss=0.019797297194600105\n",
      "Surface training t=32514, loss=0.02403914090245962\n",
      "Surface training t=32515, loss=0.01871771551668644\n",
      "Surface training t=32516, loss=0.022632683627307415\n",
      "Surface training t=32517, loss=0.02018290013074875\n",
      "Surface training t=32518, loss=0.018852626904845238\n",
      "Surface training t=32519, loss=0.016482902225106955\n",
      "Surface training t=32520, loss=0.01758509036153555\n",
      "Surface training t=32521, loss=0.019264549016952515\n",
      "Surface training t=32522, loss=0.012966119218617678\n",
      "Surface training t=32523, loss=0.01905785035341978\n",
      "Surface training t=32524, loss=0.015727849677205086\n",
      "Surface training t=32525, loss=0.021537354215979576\n",
      "Surface training t=32526, loss=0.021801834926009178\n",
      "Surface training t=32527, loss=0.01971443183720112\n",
      "Surface training t=32528, loss=0.031364116817712784\n",
      "Surface training t=32529, loss=0.02784135192632675\n",
      "Surface training t=32530, loss=0.024825449101626873\n",
      "Surface training t=32531, loss=0.022380194626748562\n",
      "Surface training t=32532, loss=0.022151030600070953\n",
      "Surface training t=32533, loss=0.024462045170366764\n",
      "Surface training t=32534, loss=0.020454798825085163\n",
      "Surface training t=32535, loss=0.01574198715388775\n",
      "Surface training t=32536, loss=0.01931222714483738\n",
      "Surface training t=32537, loss=0.01835074182599783\n",
      "Surface training t=32538, loss=0.022116427309811115\n",
      "Surface training t=32539, loss=0.016732505057007074\n",
      "Surface training t=32540, loss=0.01882775966078043\n",
      "Surface training t=32541, loss=0.020902770571410656\n",
      "Surface training t=32542, loss=0.02173755969852209\n",
      "Surface training t=32543, loss=0.017372731119394302\n",
      "Surface training t=32544, loss=0.01624395838007331\n",
      "Surface training t=32545, loss=0.02121574431657791\n",
      "Surface training t=32546, loss=0.03687787801027298\n",
      "Surface training t=32547, loss=0.028347380459308624\n",
      "Surface training t=32548, loss=0.019764390774071217\n",
      "Surface training t=32549, loss=0.02287125028669834\n",
      "Surface training t=32550, loss=0.026084898971021175\n",
      "Surface training t=32551, loss=0.01998309977352619\n",
      "Surface training t=32552, loss=0.021974786184728146\n",
      "Surface training t=32553, loss=0.03353959135711193\n",
      "Surface training t=32554, loss=0.02088384795933962\n",
      "Surface training t=32555, loss=0.02183604333549738\n",
      "Surface training t=32556, loss=0.02504565566778183\n",
      "Surface training t=32557, loss=0.025274711661040783\n",
      "Surface training t=32558, loss=0.019565725699067116\n",
      "Surface training t=32559, loss=0.021364539861679077\n",
      "Surface training t=32560, loss=0.028858748264610767\n",
      "Surface training t=32561, loss=0.025892846286296844\n",
      "Surface training t=32562, loss=0.02083089016377926\n",
      "Surface training t=32563, loss=0.022447523660957813\n",
      "Surface training t=32564, loss=0.02266183029860258\n",
      "Surface training t=32565, loss=0.01627439446747303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=32566, loss=0.01926009263843298\n",
      "Surface training t=32567, loss=0.01686867792159319\n",
      "Surface training t=32568, loss=0.01636617397889495\n",
      "Surface training t=32569, loss=0.019487449899315834\n",
      "Surface training t=32570, loss=0.01609215885400772\n",
      "Surface training t=32571, loss=0.01699543697759509\n",
      "Surface training t=32572, loss=0.015024973079562187\n",
      "Surface training t=32573, loss=0.02022834960371256\n",
      "Surface training t=32574, loss=0.027171400375664234\n",
      "Surface training t=32575, loss=0.029363976791501045\n",
      "Surface training t=32576, loss=0.02125030942261219\n",
      "Surface training t=32577, loss=0.021673440001904964\n",
      "Surface training t=32578, loss=0.017912287265062332\n",
      "Surface training t=32579, loss=0.03204888477921486\n",
      "Surface training t=32580, loss=0.021646550856530666\n",
      "Surface training t=32581, loss=0.02517481241375208\n",
      "Surface training t=32582, loss=0.0311855711042881\n",
      "Surface training t=32583, loss=0.020427404902875423\n",
      "Surface training t=32584, loss=0.02650531381368637\n",
      "Surface training t=32585, loss=0.022065259516239166\n",
      "Surface training t=32586, loss=0.02198673039674759\n",
      "Surface training t=32587, loss=0.0221760431304574\n",
      "Surface training t=32588, loss=0.014655082020908594\n",
      "Surface training t=32589, loss=0.01404052134603262\n",
      "Surface training t=32590, loss=0.01727381069213152\n",
      "Surface training t=32591, loss=0.01521500525996089\n",
      "Surface training t=32592, loss=0.01700657233595848\n",
      "Surface training t=32593, loss=0.01727332640439272\n",
      "Surface training t=32594, loss=0.017828427255153656\n",
      "Surface training t=32595, loss=0.019830435514450073\n",
      "Surface training t=32596, loss=0.0200692443177104\n",
      "Surface training t=32597, loss=0.02218629326671362\n",
      "Surface training t=32598, loss=0.018924042582511902\n",
      "Surface training t=32599, loss=0.017359976656734943\n",
      "Surface training t=32600, loss=0.01687179133296013\n",
      "Surface training t=32601, loss=0.016765445936471224\n",
      "Surface training t=32602, loss=0.017197466921061277\n",
      "Surface training t=32603, loss=0.01744602434337139\n",
      "Surface training t=32604, loss=0.021449058316648006\n",
      "Surface training t=32605, loss=0.01789945736527443\n",
      "Surface training t=32606, loss=0.015066618099808693\n",
      "Surface training t=32607, loss=0.015357046388089657\n",
      "Surface training t=32608, loss=0.011975346598774195\n",
      "Surface training t=32609, loss=0.019062149338424206\n",
      "Surface training t=32610, loss=0.020428535528481007\n",
      "Surface training t=32611, loss=0.016259903088212013\n",
      "Surface training t=32612, loss=0.01606424432247877\n",
      "Surface training t=32613, loss=0.019834790378808975\n",
      "Surface training t=32614, loss=0.01585903251543641\n",
      "Surface training t=32615, loss=0.019678454846143723\n",
      "Surface training t=32616, loss=0.01785015780478716\n",
      "Surface training t=32617, loss=0.0194559246301651\n",
      "Surface training t=32618, loss=0.02114919200539589\n",
      "Surface training t=32619, loss=0.01991963293403387\n",
      "Surface training t=32620, loss=0.01939601730555296\n",
      "Surface training t=32621, loss=0.017994089052081108\n",
      "Surface training t=32622, loss=0.018253911286592484\n",
      "Surface training t=32623, loss=0.015386282466351986\n",
      "Surface training t=32624, loss=0.01895052846521139\n",
      "Surface training t=32625, loss=0.014783751219511032\n",
      "Surface training t=32626, loss=0.016116127837449312\n",
      "Surface training t=32627, loss=0.018690672237426043\n",
      "Surface training t=32628, loss=0.02176580624654889\n",
      "Surface training t=32629, loss=0.018336630892008543\n",
      "Surface training t=32630, loss=0.024292555637657642\n",
      "Surface training t=32631, loss=0.03526755329221487\n",
      "Surface training t=32632, loss=0.02493102476000786\n",
      "Surface training t=32633, loss=0.02394018042832613\n",
      "Surface training t=32634, loss=0.02253019716590643\n",
      "Surface training t=32635, loss=0.01981774065643549\n",
      "Surface training t=32636, loss=0.02422290202230215\n",
      "Surface training t=32637, loss=0.02620516438037157\n",
      "Surface training t=32638, loss=0.020991861820220947\n",
      "Surface training t=32639, loss=0.022247308399528265\n",
      "Surface training t=32640, loss=0.027272955514490604\n",
      "Surface training t=32641, loss=0.017123814672231674\n",
      "Surface training t=32642, loss=0.029417306184768677\n",
      "Surface training t=32643, loss=0.02740601822733879\n",
      "Surface training t=32644, loss=0.018497186712920666\n",
      "Surface training t=32645, loss=0.021284975111484528\n",
      "Surface training t=32646, loss=0.020198681391775608\n",
      "Surface training t=32647, loss=0.021454254165291786\n",
      "Surface training t=32648, loss=0.02411322435364127\n",
      "Surface training t=32649, loss=0.02415954601019621\n",
      "Surface training t=32650, loss=0.031995171681046486\n",
      "Surface training t=32651, loss=0.037650651298463345\n",
      "Surface training t=32652, loss=0.0235591484233737\n",
      "Surface training t=32653, loss=0.02731897309422493\n",
      "Surface training t=32654, loss=0.033199492841959\n",
      "Surface training t=32655, loss=0.0306604141369462\n",
      "Surface training t=32656, loss=0.027220268733799458\n",
      "Surface training t=32657, loss=0.0209308797493577\n",
      "Surface training t=32658, loss=0.02002089051529765\n",
      "Surface training t=32659, loss=0.0220239395275712\n",
      "Surface training t=32660, loss=0.022879095748066902\n",
      "Surface training t=32661, loss=0.022127380594611168\n",
      "Surface training t=32662, loss=0.022633563727140427\n",
      "Surface training t=32663, loss=0.019857344217598438\n",
      "Surface training t=32664, loss=0.02063494175672531\n",
      "Surface training t=32665, loss=0.018166874535381794\n",
      "Surface training t=32666, loss=0.020296738483011723\n",
      "Surface training t=32667, loss=0.01927434653043747\n",
      "Surface training t=32668, loss=0.014870548620820045\n",
      "Surface training t=32669, loss=0.01598910754546523\n",
      "Surface training t=32670, loss=0.020434238016605377\n",
      "Surface training t=32671, loss=0.01527920039370656\n",
      "Surface training t=32672, loss=0.014786939602345228\n",
      "Surface training t=32673, loss=0.018596449866890907\n",
      "Surface training t=32674, loss=0.03269138466566801\n",
      "Surface training t=32675, loss=0.025819960050284863\n",
      "Surface training t=32676, loss=0.02279931865632534\n",
      "Surface training t=32677, loss=0.02497909776866436\n",
      "Surface training t=32678, loss=0.02094187680631876\n",
      "Surface training t=32679, loss=0.0245633190497756\n",
      "Surface training t=32680, loss=0.019916346296668053\n",
      "Surface training t=32681, loss=0.019766093231737614\n",
      "Surface training t=32682, loss=0.015439332462847233\n",
      "Surface training t=32683, loss=0.015800952911376953\n",
      "Surface training t=32684, loss=0.01854559686034918\n",
      "Surface training t=32685, loss=0.01997549459338188\n",
      "Surface training t=32686, loss=0.019625038839876652\n",
      "Surface training t=32687, loss=0.016680866479873657\n",
      "Surface training t=32688, loss=0.018699503503739834\n",
      "Surface training t=32689, loss=0.015484289731830359\n",
      "Surface training t=32690, loss=0.018360912334173918\n",
      "Surface training t=32691, loss=0.011837402824312449\n",
      "Surface training t=32692, loss=0.013931077439337969\n",
      "Surface training t=32693, loss=0.015305935870856047\n",
      "Surface training t=32694, loss=0.014916946645826101\n",
      "Surface training t=32695, loss=0.020204706117510796\n",
      "Surface training t=32696, loss=0.019438154064118862\n",
      "Surface training t=32697, loss=0.021204554475843906\n",
      "Surface training t=32698, loss=0.0204209852963686\n",
      "Surface training t=32699, loss=0.016635197214782238\n",
      "Surface training t=32700, loss=0.02335718646645546\n",
      "Surface training t=32701, loss=0.02094079554080963\n",
      "Surface training t=32702, loss=0.019242013804614544\n",
      "Surface training t=32703, loss=0.013697800226509571\n",
      "Surface training t=32704, loss=0.01491232356056571\n",
      "Surface training t=32705, loss=0.01889181323349476\n",
      "Surface training t=32706, loss=0.01590202283114195\n",
      "Surface training t=32707, loss=0.016027656383812428\n",
      "Surface training t=32708, loss=0.016828753519803286\n",
      "Surface training t=32709, loss=0.016236462630331516\n",
      "Surface training t=32710, loss=0.013881601858884096\n",
      "Surface training t=32711, loss=0.01937265694141388\n",
      "Surface training t=32712, loss=0.016935831867158413\n",
      "Surface training t=32713, loss=0.013632831163704395\n",
      "Surface training t=32714, loss=0.016048388555645943\n",
      "Surface training t=32715, loss=0.013207674957811832\n",
      "Surface training t=32716, loss=0.015436096582561731\n",
      "Surface training t=32717, loss=0.014598243404179811\n",
      "Surface training t=32718, loss=0.014892938546836376\n",
      "Surface training t=32719, loss=0.014970772434026003\n",
      "Surface training t=32720, loss=0.017658750526607037\n",
      "Surface training t=32721, loss=0.014325880911201239\n",
      "Surface training t=32722, loss=0.013190797064453363\n",
      "Surface training t=32723, loss=0.0158822201192379\n",
      "Surface training t=32724, loss=0.013690924271941185\n",
      "Surface training t=32725, loss=0.013233205769211054\n",
      "Surface training t=32726, loss=0.01294622989371419\n",
      "Surface training t=32727, loss=0.015149144921451807\n",
      "Surface training t=32728, loss=0.013040728867053986\n",
      "Surface training t=32729, loss=0.021438656374812126\n",
      "Surface training t=32730, loss=0.016228361055254936\n",
      "Surface training t=32731, loss=0.020711434073746204\n",
      "Surface training t=32732, loss=0.020884661935269833\n",
      "Surface training t=32733, loss=0.020957290194928646\n",
      "Surface training t=32734, loss=0.02266003657132387\n",
      "Surface training t=32735, loss=0.03355822339653969\n",
      "Surface training t=32736, loss=0.0288256723433733\n",
      "Surface training t=32737, loss=0.030607868917286396\n",
      "Surface training t=32738, loss=0.029514603316783905\n",
      "Surface training t=32739, loss=0.028701824136078358\n",
      "Surface training t=32740, loss=0.03006286919116974\n",
      "Surface training t=32741, loss=0.019201181828975677\n",
      "Surface training t=32742, loss=0.018181881867349148\n",
      "Surface training t=32743, loss=0.015134533867239952\n",
      "Surface training t=32744, loss=0.022618823684751987\n",
      "Surface training t=32745, loss=0.017587565816938877\n",
      "Surface training t=32746, loss=0.021803184412419796\n",
      "Surface training t=32747, loss=0.026978074572980404\n",
      "Surface training t=32748, loss=0.018501194193959236\n",
      "Surface training t=32749, loss=0.02288957592099905\n",
      "Surface training t=32750, loss=0.03599017299711704\n",
      "Surface training t=32751, loss=0.01805374026298523\n",
      "Surface training t=32752, loss=0.020510174334049225\n",
      "Surface training t=32753, loss=0.0215360214933753\n",
      "Surface training t=32754, loss=0.020376057364046574\n",
      "Surface training t=32755, loss=0.022314843721687794\n",
      "Surface training t=32756, loss=0.02493907045572996\n",
      "Surface training t=32757, loss=0.02477350737899542\n",
      "Surface training t=32758, loss=0.037188272923231125\n",
      "Surface training t=32759, loss=0.029015754349529743\n",
      "Surface training t=32760, loss=0.023772825486958027\n",
      "Surface training t=32761, loss=0.02271357085555792\n",
      "Surface training t=32762, loss=0.023030459880828857\n",
      "Surface training t=32763, loss=0.020164051093161106\n",
      "Surface training t=32764, loss=0.018425357528030872\n",
      "Surface training t=32765, loss=0.018893171101808548\n",
      "Surface training t=32766, loss=0.020345399156212807\n",
      "Surface training t=32767, loss=0.0191332520917058\n",
      "Surface training t=32768, loss=0.021736920811235905\n",
      "Surface training t=32769, loss=0.02219028677791357\n",
      "Surface training t=32770, loss=0.02006977330893278\n",
      "Surface training t=32771, loss=0.02403072128072381\n",
      "Surface training t=32772, loss=0.022674499079585075\n",
      "Surface training t=32773, loss=0.02023510728031397\n",
      "Surface training t=32774, loss=0.021424136124551296\n",
      "Surface training t=32775, loss=0.022488804534077644\n",
      "Surface training t=32776, loss=0.020967703312635422\n",
      "Surface training t=32777, loss=0.025042440742254257\n",
      "Surface training t=32778, loss=0.020008156076073647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=32779, loss=0.02005286980420351\n",
      "Surface training t=32780, loss=0.018515589646995068\n",
      "Surface training t=32781, loss=0.023312545381486416\n",
      "Surface training t=32782, loss=0.022790311835706234\n",
      "Surface training t=32783, loss=0.016246437095105648\n",
      "Surface training t=32784, loss=0.021102366037666798\n",
      "Surface training t=32785, loss=0.013141804374754429\n",
      "Surface training t=32786, loss=0.015084845013916492\n",
      "Surface training t=32787, loss=0.01090680854395032\n",
      "Surface training t=32788, loss=0.015364705584943295\n",
      "Surface training t=32789, loss=0.012559607159346342\n",
      "Surface training t=32790, loss=0.01859392784535885\n",
      "Surface training t=32791, loss=0.01886165887117386\n",
      "Surface training t=32792, loss=0.017551546916365623\n",
      "Surface training t=32793, loss=0.012795133516192436\n",
      "Surface training t=32794, loss=0.020019925199449062\n",
      "Surface training t=32795, loss=0.01684232661500573\n",
      "Surface training t=32796, loss=0.01623823307454586\n",
      "Surface training t=32797, loss=0.015218676999211311\n",
      "Surface training t=32798, loss=0.015474861953407526\n",
      "Surface training t=32799, loss=0.013319446239620447\n",
      "Surface training t=32800, loss=0.013701543677598238\n",
      "Surface training t=32801, loss=0.014031900558620691\n",
      "Surface training t=32802, loss=0.013777378480881453\n",
      "Surface training t=32803, loss=0.01368878548964858\n",
      "Surface training t=32804, loss=0.014153629541397095\n",
      "Surface training t=32805, loss=0.014531305059790611\n",
      "Surface training t=32806, loss=0.01682233391329646\n",
      "Surface training t=32807, loss=0.014408629853278399\n",
      "Surface training t=32808, loss=0.013259312603622675\n",
      "Surface training t=32809, loss=0.016426309943199158\n",
      "Surface training t=32810, loss=0.019596122205257416\n",
      "Surface training t=32811, loss=0.01643369160592556\n",
      "Surface training t=32812, loss=0.016147498972713947\n",
      "Surface training t=32813, loss=0.019840294495224953\n",
      "Surface training t=32814, loss=0.01544644357636571\n",
      "Surface training t=32815, loss=0.015158360823988914\n",
      "Surface training t=32816, loss=0.015139584429562092\n",
      "Surface training t=32817, loss=0.022032818756997585\n",
      "Surface training t=32818, loss=0.015603568870574236\n",
      "Surface training t=32819, loss=0.01846904680132866\n",
      "Surface training t=32820, loss=0.017433973029255867\n",
      "Surface training t=32821, loss=0.018183155916631222\n",
      "Surface training t=32822, loss=0.015708522871136665\n",
      "Surface training t=32823, loss=0.017974390648305416\n",
      "Surface training t=32824, loss=0.019150770269334316\n",
      "Surface training t=32825, loss=0.015289624221622944\n",
      "Surface training t=32826, loss=0.017968108877539635\n",
      "Surface training t=32827, loss=0.011790810152888298\n",
      "Surface training t=32828, loss=0.018345551565289497\n",
      "Surface training t=32829, loss=0.018524502404034138\n",
      "Surface training t=32830, loss=0.017582839354872704\n",
      "Surface training t=32831, loss=0.017924832180142403\n",
      "Surface training t=32832, loss=0.013541263993829489\n",
      "Surface training t=32833, loss=0.017309994902461767\n",
      "Surface training t=32834, loss=0.017292417585849762\n",
      "Surface training t=32835, loss=0.024556712247431278\n",
      "Surface training t=32836, loss=0.01653006626293063\n",
      "Surface training t=32837, loss=0.015347465872764587\n",
      "Surface training t=32838, loss=0.016829223837703466\n",
      "Surface training t=32839, loss=0.018356594257056713\n",
      "Surface training t=32840, loss=0.021718810312449932\n",
      "Surface training t=32841, loss=0.023009326308965683\n",
      "Surface training t=32842, loss=0.02157040499150753\n",
      "Surface training t=32843, loss=0.02014291286468506\n",
      "Surface training t=32844, loss=0.01776363980025053\n",
      "Surface training t=32845, loss=0.018011704087257385\n",
      "Surface training t=32846, loss=0.0196042126044631\n",
      "Surface training t=32847, loss=0.01900898665189743\n",
      "Surface training t=32848, loss=0.0204067500308156\n",
      "Surface training t=32849, loss=0.021338234655559063\n",
      "Surface training t=32850, loss=0.027445195242762566\n",
      "Surface training t=32851, loss=0.022701513022184372\n",
      "Surface training t=32852, loss=0.021541522815823555\n",
      "Surface training t=32853, loss=0.02870226465165615\n",
      "Surface training t=32854, loss=0.025677974335849285\n",
      "Surface training t=32855, loss=0.026094917207956314\n",
      "Surface training t=32856, loss=0.025728324428200722\n",
      "Surface training t=32857, loss=0.018488485366106033\n",
      "Surface training t=32858, loss=0.01704376842826605\n",
      "Surface training t=32859, loss=0.028300335630774498\n",
      "Surface training t=32860, loss=0.014180027414113283\n",
      "Surface training t=32861, loss=0.020072096958756447\n",
      "Surface training t=32862, loss=0.017977941781282425\n",
      "Surface training t=32863, loss=0.020218861289322376\n",
      "Surface training t=32864, loss=0.018255590461194515\n",
      "Surface training t=32865, loss=0.01843391451984644\n",
      "Surface training t=32866, loss=0.015798773616552353\n",
      "Surface training t=32867, loss=0.023387434892356396\n",
      "Surface training t=32868, loss=0.026469601318240166\n",
      "Surface training t=32869, loss=0.030205927789211273\n",
      "Surface training t=32870, loss=0.02354816161096096\n",
      "Surface training t=32871, loss=0.02529712114483118\n",
      "Surface training t=32872, loss=0.027432389557361603\n",
      "Surface training t=32873, loss=0.02080217469483614\n",
      "Surface training t=32874, loss=0.018650400452315807\n",
      "Surface training t=32875, loss=0.020180012099444866\n",
      "Surface training t=32876, loss=0.015751118771731853\n",
      "Surface training t=32877, loss=0.021486626006662846\n",
      "Surface training t=32878, loss=0.01761234737932682\n",
      "Surface training t=32879, loss=0.015125802718102932\n",
      "Surface training t=32880, loss=0.02355827484279871\n",
      "Surface training t=32881, loss=0.02078984212130308\n",
      "Surface training t=32882, loss=0.021720866672694683\n",
      "Surface training t=32883, loss=0.021220573224127293\n",
      "Surface training t=32884, loss=0.01953467819839716\n",
      "Surface training t=32885, loss=0.019105969928205013\n",
      "Surface training t=32886, loss=0.01920204423367977\n",
      "Surface training t=32887, loss=0.02104510646313429\n",
      "Surface training t=32888, loss=0.022220169194042683\n",
      "Surface training t=32889, loss=0.02069553779438138\n",
      "Surface training t=32890, loss=0.02374010719358921\n",
      "Surface training t=32891, loss=0.025652894750237465\n",
      "Surface training t=32892, loss=0.020057372748851776\n",
      "Surface training t=32893, loss=0.01703010220080614\n",
      "Surface training t=32894, loss=0.022467021830379963\n",
      "Surface training t=32895, loss=0.017412032932043076\n",
      "Surface training t=32896, loss=0.022755936719477177\n",
      "Surface training t=32897, loss=0.03010251186788082\n",
      "Surface training t=32898, loss=0.027951004914939404\n",
      "Surface training t=32899, loss=0.03423931077122688\n",
      "Surface training t=32900, loss=0.02282391581684351\n",
      "Surface training t=32901, loss=0.024971485137939453\n",
      "Surface training t=32902, loss=0.023308920674026012\n",
      "Surface training t=32903, loss=0.01950515154749155\n",
      "Surface training t=32904, loss=0.02118520252406597\n",
      "Surface training t=32905, loss=0.018275336362421513\n",
      "Surface training t=32906, loss=0.02332463301718235\n",
      "Surface training t=32907, loss=0.016873056534677744\n",
      "Surface training t=32908, loss=0.01450301380828023\n",
      "Surface training t=32909, loss=0.01612609252333641\n",
      "Surface training t=32910, loss=0.015989332459867\n",
      "Surface training t=32911, loss=0.016577578149735928\n",
      "Surface training t=32912, loss=0.020195172168314457\n",
      "Surface training t=32913, loss=0.020238989032804966\n",
      "Surface training t=32914, loss=0.01943869423121214\n",
      "Surface training t=32915, loss=0.01878211833536625\n",
      "Surface training t=32916, loss=0.02318960428237915\n",
      "Surface training t=32917, loss=0.020635255612432957\n",
      "Surface training t=32918, loss=0.013559951446950436\n",
      "Surface training t=32919, loss=0.019614944234490395\n",
      "Surface training t=32920, loss=0.01588681060820818\n",
      "Surface training t=32921, loss=0.014990294352173805\n",
      "Surface training t=32922, loss=0.0129085429944098\n",
      "Surface training t=32923, loss=0.015298128128051758\n",
      "Surface training t=32924, loss=0.016173304058611393\n",
      "Surface training t=32925, loss=0.02039544377475977\n",
      "Surface training t=32926, loss=0.024211709387600422\n",
      "Surface training t=32927, loss=0.035119129344820976\n",
      "Surface training t=32928, loss=0.021689416840672493\n",
      "Surface training t=32929, loss=0.024159131571650505\n",
      "Surface training t=32930, loss=0.01947061438113451\n",
      "Surface training t=32931, loss=0.020339482463896275\n",
      "Surface training t=32932, loss=0.01804258394986391\n",
      "Surface training t=32933, loss=0.023926270194351673\n",
      "Surface training t=32934, loss=0.022425814531743526\n",
      "Surface training t=32935, loss=0.014381480403244495\n",
      "Surface training t=32936, loss=0.017752004321664572\n",
      "Surface training t=32937, loss=0.014646390918642282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=32938, loss=0.01290197717025876\n",
      "Surface training t=32939, loss=0.016126959584653378\n",
      "Surface training t=32940, loss=0.01777850277721882\n",
      "Surface training t=32941, loss=0.019049442373216152\n",
      "Surface training t=32942, loss=0.01705993991345167\n",
      "Surface training t=32943, loss=0.018974103964865208\n",
      "Surface training t=32944, loss=0.02432376053184271\n",
      "Surface training t=32945, loss=0.020029413513839245\n",
      "Surface training t=32946, loss=0.01705049816519022\n",
      "Surface training t=32947, loss=0.015928922221064568\n",
      "Surface training t=32948, loss=0.013730295933783054\n",
      "Surface training t=32949, loss=0.016438521444797516\n",
      "Surface training t=32950, loss=0.014782984275370836\n",
      "Surface training t=32951, loss=0.013299878686666489\n",
      "Surface training t=32952, loss=0.014745957218110561\n",
      "Surface training t=32953, loss=0.012403835542500019\n",
      "Surface training t=32954, loss=0.024904842488467693\n",
      "Surface training t=32955, loss=0.026816113851964474\n",
      "Surface training t=32956, loss=0.02199309691786766\n",
      "Surface training t=32957, loss=0.025862730108201504\n",
      "Surface training t=32958, loss=0.02103637531399727\n",
      "Surface training t=32959, loss=0.025438674725592136\n",
      "Surface training t=32960, loss=0.025233393535017967\n",
      "Surface training t=32961, loss=0.02301181945949793\n",
      "Surface training t=32962, loss=0.023612212389707565\n",
      "Surface training t=32963, loss=0.03599230386316776\n",
      "Surface training t=32964, loss=0.0329066002741456\n",
      "Surface training t=32965, loss=0.032930982299149036\n",
      "Surface training t=32966, loss=0.04523215629160404\n",
      "Surface training t=32967, loss=0.023759964853525162\n",
      "Surface training t=32968, loss=0.027599677443504333\n",
      "Surface training t=32969, loss=0.02663690410554409\n",
      "Surface training t=32970, loss=0.02125880029052496\n",
      "Surface training t=32971, loss=0.023247022181749344\n",
      "Surface training t=32972, loss=0.019517093896865845\n",
      "Surface training t=32973, loss=0.01910205651074648\n",
      "Surface training t=32974, loss=0.023685581050813198\n",
      "Surface training t=32975, loss=0.02473632525652647\n",
      "Surface training t=32976, loss=0.023700273595750332\n",
      "Surface training t=32977, loss=0.024257349781692028\n",
      "Surface training t=32978, loss=0.026220455765724182\n",
      "Surface training t=32979, loss=0.026979115791618824\n",
      "Surface training t=32980, loss=0.031728798523545265\n",
      "Surface training t=32981, loss=0.02603348158299923\n",
      "Surface training t=32982, loss=0.02536874171346426\n",
      "Surface training t=32983, loss=0.02404133975505829\n",
      "Surface training t=32984, loss=0.020851424895226955\n",
      "Surface training t=32985, loss=0.02575293555855751\n",
      "Surface training t=32986, loss=0.03453131578862667\n",
      "Surface training t=32987, loss=0.026349158957600594\n",
      "Surface training t=32988, loss=0.02889242861419916\n",
      "Surface training t=32989, loss=0.03730468824505806\n",
      "Surface training t=32990, loss=0.03506948612630367\n",
      "Surface training t=32991, loss=0.028432332910597324\n",
      "Surface training t=32992, loss=0.03429390862584114\n",
      "Surface training t=32993, loss=0.03827916458249092\n",
      "Surface training t=32994, loss=0.027629923075437546\n",
      "Surface training t=32995, loss=0.029177356511354446\n",
      "Surface training t=32996, loss=0.02911987714469433\n",
      "Surface training t=32997, loss=0.017361104488372803\n",
      "Surface training t=32998, loss=0.018086583353579044\n",
      "Surface training t=32999, loss=0.02371976338326931\n",
      "Surface training t=33000, loss=0.026080715470016003\n",
      "Surface training t=33001, loss=0.033968398347496986\n",
      "Surface training t=33002, loss=0.029693779535591602\n",
      "Surface training t=33003, loss=0.02940808515995741\n",
      "Surface training t=33004, loss=0.030790437012910843\n",
      "Surface training t=33005, loss=0.033974066376686096\n",
      "Surface training t=33006, loss=0.028589047491550446\n",
      "Surface training t=33007, loss=0.028650577180087566\n",
      "Surface training t=33008, loss=0.04172770492732525\n",
      "Surface training t=33009, loss=0.027946150861680508\n",
      "Surface training t=33010, loss=0.027464217506349087\n",
      "Surface training t=33011, loss=0.01980271190404892\n",
      "Surface training t=33012, loss=0.018982402980327606\n",
      "Surface training t=33013, loss=0.02041514217853546\n",
      "Surface training t=33014, loss=0.022964179515838623\n",
      "Surface training t=33015, loss=0.021853657439351082\n",
      "Surface training t=33016, loss=0.020445192698389292\n",
      "Surface training t=33017, loss=0.021136442199349403\n",
      "Surface training t=33018, loss=0.019356686621904373\n",
      "Surface training t=33019, loss=0.018734985031187534\n",
      "Surface training t=33020, loss=0.0221097432076931\n",
      "Surface training t=33021, loss=0.02008890174329281\n",
      "Surface training t=33022, loss=0.018742565996944904\n",
      "Surface training t=33023, loss=0.021508892066776752\n",
      "Surface training t=33024, loss=0.017852271907031536\n",
      "Surface training t=33025, loss=0.03104453720152378\n",
      "Surface training t=33026, loss=0.031749785877764225\n",
      "Surface training t=33027, loss=0.0214218869805336\n",
      "Surface training t=33028, loss=0.021073507145047188\n",
      "Surface training t=33029, loss=0.021495572291314602\n",
      "Surface training t=33030, loss=0.018226711079478264\n",
      "Surface training t=33031, loss=0.018273686058819294\n",
      "Surface training t=33032, loss=0.019655068404972553\n",
      "Surface training t=33033, loss=0.02119604405015707\n",
      "Surface training t=33034, loss=0.01498638279736042\n",
      "Surface training t=33035, loss=0.019310126081109047\n",
      "Surface training t=33036, loss=0.016258603893220425\n",
      "Surface training t=33037, loss=0.02131043467670679\n",
      "Surface training t=33038, loss=0.014415573328733444\n",
      "Surface training t=33039, loss=0.014799903146922588\n",
      "Surface training t=33040, loss=0.01641182415187359\n",
      "Surface training t=33041, loss=0.014238890260457993\n",
      "Surface training t=33042, loss=0.0199557701125741\n",
      "Surface training t=33043, loss=0.01873343251645565\n",
      "Surface training t=33044, loss=0.01835598796606064\n",
      "Surface training t=33045, loss=0.017007440328598022\n",
      "Surface training t=33046, loss=0.022055609151721\n",
      "Surface training t=33047, loss=0.017000076826661825\n",
      "Surface training t=33048, loss=0.019357162062078714\n",
      "Surface training t=33049, loss=0.018158715218305588\n",
      "Surface training t=33050, loss=0.023886202834546566\n",
      "Surface training t=33051, loss=0.020599269308149815\n",
      "Surface training t=33052, loss=0.0170269962400198\n",
      "Surface training t=33053, loss=0.018890121020376682\n",
      "Surface training t=33054, loss=0.01859051827341318\n",
      "Surface training t=33055, loss=0.021776563487946987\n",
      "Surface training t=33056, loss=0.021018117666244507\n",
      "Surface training t=33057, loss=0.027841471135616302\n",
      "Surface training t=33058, loss=0.02819297183305025\n",
      "Surface training t=33059, loss=0.021809490397572517\n",
      "Surface training t=33060, loss=0.01666687522083521\n",
      "Surface training t=33061, loss=0.024708562530577183\n",
      "Surface training t=33062, loss=0.020737530197948217\n",
      "Surface training t=33063, loss=0.017864480149000883\n",
      "Surface training t=33064, loss=0.02658180333673954\n",
      "Surface training t=33065, loss=0.021841609850525856\n",
      "Surface training t=33066, loss=0.024736884981393814\n",
      "Surface training t=33067, loss=0.040504876524209976\n",
      "Surface training t=33068, loss=0.0249236561357975\n",
      "Surface training t=33069, loss=0.03721056319773197\n",
      "Surface training t=33070, loss=0.025929341092705727\n",
      "Surface training t=33071, loss=0.037798844277858734\n",
      "Surface training t=33072, loss=0.026538193225860596\n",
      "Surface training t=33073, loss=0.04463760368525982\n",
      "Surface training t=33074, loss=0.027262147516012192\n",
      "Surface training t=33075, loss=0.02936583198606968\n",
      "Surface training t=33076, loss=0.0397974643856287\n",
      "Surface training t=33077, loss=0.027018863707780838\n",
      "Surface training t=33078, loss=0.027037888765335083\n",
      "Surface training t=33079, loss=0.027424180880188942\n",
      "Surface training t=33080, loss=0.027665198780596256\n",
      "Surface training t=33081, loss=0.02178390510380268\n",
      "Surface training t=33082, loss=0.018715565092861652\n",
      "Surface training t=33083, loss=0.026413097977638245\n",
      "Surface training t=33084, loss=0.02202023286372423\n",
      "Surface training t=33085, loss=0.023420636542141438\n",
      "Surface training t=33086, loss=0.02431851252913475\n",
      "Surface training t=33087, loss=0.018469913862645626\n",
      "Surface training t=33088, loss=0.02068949956446886\n",
      "Surface training t=33089, loss=0.018010065890848637\n",
      "Surface training t=33090, loss=0.019086091313511133\n",
      "Surface training t=33091, loss=0.013817619066685438\n",
      "Surface training t=33092, loss=0.015195684507489204\n",
      "Surface training t=33093, loss=0.01431306079030037\n",
      "Surface training t=33094, loss=0.019635888747870922\n",
      "Surface training t=33095, loss=0.02290683425962925\n",
      "Surface training t=33096, loss=0.020887925289571285\n",
      "Surface training t=33097, loss=0.018175081349909306\n",
      "Surface training t=33098, loss=0.025985374115407467\n",
      "Surface training t=33099, loss=0.018646291457116604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=33100, loss=0.018663191236555576\n",
      "Surface training t=33101, loss=0.022541466169059277\n",
      "Surface training t=33102, loss=0.02081814594566822\n",
      "Surface training t=33103, loss=0.021309715695679188\n",
      "Surface training t=33104, loss=0.020563552156090736\n",
      "Surface training t=33105, loss=0.017453315667808056\n",
      "Surface training t=33106, loss=0.02000079955905676\n",
      "Surface training t=33107, loss=0.01674237661063671\n",
      "Surface training t=33108, loss=0.01715884916484356\n",
      "Surface training t=33109, loss=0.012949917931109667\n",
      "Surface training t=33110, loss=0.017147202510386705\n",
      "Surface training t=33111, loss=0.013149387668818235\n",
      "Surface training t=33112, loss=0.01676900777965784\n",
      "Surface training t=33113, loss=0.01695389673113823\n",
      "Surface training t=33114, loss=0.018938139081001282\n",
      "Surface training t=33115, loss=0.01867459900677204\n",
      "Surface training t=33116, loss=0.027760298922657967\n",
      "Surface training t=33117, loss=0.02961482759565115\n",
      "Surface training t=33118, loss=0.029693054035305977\n",
      "Surface training t=33119, loss=0.024826234206557274\n",
      "Surface training t=33120, loss=0.022426416166126728\n",
      "Surface training t=33121, loss=0.030242029577493668\n",
      "Surface training t=33122, loss=0.03765479288995266\n",
      "Surface training t=33123, loss=0.02449618186801672\n",
      "Surface training t=33124, loss=0.02409871108829975\n",
      "Surface training t=33125, loss=0.02536135818809271\n",
      "Surface training t=33126, loss=0.037422480061650276\n",
      "Surface training t=33127, loss=0.03246868774294853\n",
      "Surface training t=33128, loss=0.026364021934568882\n",
      "Surface training t=33129, loss=0.02786312811076641\n",
      "Surface training t=33130, loss=0.03333117440342903\n",
      "Surface training t=33131, loss=0.027055750600993633\n",
      "Surface training t=33132, loss=0.022554270923137665\n",
      "Surface training t=33133, loss=0.028892124071717262\n",
      "Surface training t=33134, loss=0.019023770466446877\n",
      "Surface training t=33135, loss=0.038551392033696175\n",
      "Surface training t=33136, loss=0.031100855208933353\n",
      "Surface training t=33137, loss=0.030333999544382095\n",
      "Surface training t=33138, loss=0.03628198988735676\n",
      "Surface training t=33139, loss=0.03070523403584957\n",
      "Surface training t=33140, loss=0.0291574839502573\n",
      "Surface training t=33141, loss=0.028185158036649227\n",
      "Surface training t=33142, loss=0.02199279423803091\n",
      "Surface training t=33143, loss=0.018745571840554476\n",
      "Surface training t=33144, loss=0.020224949344992638\n",
      "Surface training t=33145, loss=0.017809417098760605\n",
      "Surface training t=33146, loss=0.021327592432498932\n",
      "Surface training t=33147, loss=0.028510550037026405\n",
      "Surface training t=33148, loss=0.02621632721275091\n",
      "Surface training t=33149, loss=0.023765522055327892\n",
      "Surface training t=33150, loss=0.023839113768190145\n",
      "Surface training t=33151, loss=0.04374055191874504\n",
      "Surface training t=33152, loss=0.031627426855266094\n",
      "Surface training t=33153, loss=0.028626474551856518\n",
      "Surface training t=33154, loss=0.030570238828659058\n",
      "Surface training t=33155, loss=0.022679291665554047\n",
      "Surface training t=33156, loss=0.0166400708258152\n",
      "Surface training t=33157, loss=0.017496160697191954\n",
      "Surface training t=33158, loss=0.02553630992770195\n",
      "Surface training t=33159, loss=0.021455987356603146\n",
      "Surface training t=33160, loss=0.02033207193017006\n",
      "Surface training t=33161, loss=0.02615693025290966\n",
      "Surface training t=33162, loss=0.019692382775247097\n",
      "Surface training t=33163, loss=0.026710767298936844\n",
      "Surface training t=33164, loss=0.02472232747823\n",
      "Surface training t=33165, loss=0.024904241785407066\n",
      "Surface training t=33166, loss=0.024562419392168522\n",
      "Surface training t=33167, loss=0.0216107415035367\n",
      "Surface training t=33168, loss=0.027650625444948673\n",
      "Surface training t=33169, loss=0.02006638702005148\n",
      "Surface training t=33170, loss=0.024150820448994637\n",
      "Surface training t=33171, loss=0.028263256885111332\n",
      "Surface training t=33172, loss=0.02227986603975296\n",
      "Surface training t=33173, loss=0.0226952712982893\n",
      "Surface training t=33174, loss=0.020374782383441925\n",
      "Surface training t=33175, loss=0.019555270671844482\n",
      "Surface training t=33176, loss=0.021382524631917477\n",
      "Surface training t=33177, loss=0.014248973689973354\n",
      "Surface training t=33178, loss=0.02903041895478964\n",
      "Surface training t=33179, loss=0.01754055079072714\n",
      "Surface training t=33180, loss=0.01671734731644392\n",
      "Surface training t=33181, loss=0.01636949647217989\n",
      "Surface training t=33182, loss=0.015602560713887215\n",
      "Surface training t=33183, loss=0.013648394029587507\n",
      "Surface training t=33184, loss=0.014996212907135487\n",
      "Surface training t=33185, loss=0.019042435102164745\n",
      "Surface training t=33186, loss=0.02711651474237442\n",
      "Surface training t=33187, loss=0.017513977829366922\n",
      "Surface training t=33188, loss=0.031074869446456432\n",
      "Surface training t=33189, loss=0.021722341887652874\n",
      "Surface training t=33190, loss=0.023210860788822174\n",
      "Surface training t=33191, loss=0.019363616593182087\n",
      "Surface training t=33192, loss=0.020927581004798412\n",
      "Surface training t=33193, loss=0.022828223183751106\n",
      "Surface training t=33194, loss=0.023769158869981766\n",
      "Surface training t=33195, loss=0.029168817214667797\n",
      "Surface training t=33196, loss=0.022809199057519436\n",
      "Surface training t=33197, loss=0.021747520193457603\n",
      "Surface training t=33198, loss=0.02577841281890869\n",
      "Surface training t=33199, loss=0.02399030327796936\n",
      "Surface training t=33200, loss=0.015692666172981262\n",
      "Surface training t=33201, loss=0.025110759772360325\n",
      "Surface training t=33202, loss=0.01777139399200678\n",
      "Surface training t=33203, loss=0.021558518521487713\n",
      "Surface training t=33204, loss=0.022480535320937634\n",
      "Surface training t=33205, loss=0.018553157802671194\n",
      "Surface training t=33206, loss=0.021775823086500168\n",
      "Surface training t=33207, loss=0.01881977915763855\n",
      "Surface training t=33208, loss=0.02350273635238409\n",
      "Surface training t=33209, loss=0.027670403942465782\n",
      "Surface training t=33210, loss=0.023582132533192635\n",
      "Surface training t=33211, loss=0.025594988837838173\n",
      "Surface training t=33212, loss=0.022353891283273697\n",
      "Surface training t=33213, loss=0.014533607754856348\n",
      "Surface training t=33214, loss=0.015196156688034534\n",
      "Surface training t=33215, loss=0.013744547963142395\n",
      "Surface training t=33216, loss=0.012025842908769846\n",
      "Surface training t=33217, loss=0.014303081668913364\n",
      "Surface training t=33218, loss=0.016450033523142338\n",
      "Surface training t=33219, loss=0.016843893565237522\n",
      "Surface training t=33220, loss=0.019895865581929684\n",
      "Surface training t=33221, loss=0.01693400228396058\n",
      "Surface training t=33222, loss=0.020236180163919926\n",
      "Surface training t=33223, loss=0.022149828262627125\n",
      "Surface training t=33224, loss=0.023040343075990677\n",
      "Surface training t=33225, loss=0.022032827138900757\n",
      "Surface training t=33226, loss=0.019284342881292105\n",
      "Surface training t=33227, loss=0.017392731737345457\n",
      "Surface training t=33228, loss=0.02577149774879217\n",
      "Surface training t=33229, loss=0.017498656176030636\n",
      "Surface training t=33230, loss=0.016632897313684225\n",
      "Surface training t=33231, loss=0.020326973870396614\n",
      "Surface training t=33232, loss=0.01931905746459961\n",
      "Surface training t=33233, loss=0.019870120100677013\n",
      "Surface training t=33234, loss=0.018753918819129467\n",
      "Surface training t=33235, loss=0.024907837621867657\n",
      "Surface training t=33236, loss=0.02407128270715475\n",
      "Surface training t=33237, loss=0.020844684913754463\n",
      "Surface training t=33238, loss=0.029978198930621147\n",
      "Surface training t=33239, loss=0.024521522223949432\n",
      "Surface training t=33240, loss=0.023182393983006477\n",
      "Surface training t=33241, loss=0.026155010797083378\n",
      "Surface training t=33242, loss=0.0173633499071002\n",
      "Surface training t=33243, loss=0.016775258351117373\n",
      "Surface training t=33244, loss=0.020977354142814875\n",
      "Surface training t=33245, loss=0.01764017529785633\n",
      "Surface training t=33246, loss=0.016494459472596645\n",
      "Surface training t=33247, loss=0.021034237928688526\n",
      "Surface training t=33248, loss=0.016759385354816914\n",
      "Surface training t=33249, loss=0.019017926417291164\n",
      "Surface training t=33250, loss=0.014062830246984959\n",
      "Surface training t=33251, loss=0.012339469976723194\n",
      "Surface training t=33252, loss=0.011867284774780273\n",
      "Surface training t=33253, loss=0.012162288185209036\n",
      "Surface training t=33254, loss=0.012892950791865587\n",
      "Surface training t=33255, loss=0.01233866112306714\n",
      "Surface training t=33256, loss=0.01704293303191662\n",
      "Surface training t=33257, loss=0.02110538398846984\n",
      "Surface training t=33258, loss=0.03779635764658451\n",
      "Surface training t=33259, loss=0.02531556924805045\n",
      "Surface training t=33260, loss=0.031043410301208496\n",
      "Surface training t=33261, loss=0.032045735977590084\n",
      "Surface training t=33262, loss=0.037659959867596626\n",
      "Surface training t=33263, loss=0.026834369637072086\n",
      "Surface training t=33264, loss=0.022768489085137844\n",
      "Surface training t=33265, loss=0.019309365190565586\n",
      "Surface training t=33266, loss=0.02048428636044264\n",
      "Surface training t=33267, loss=0.016563445329666138\n",
      "Surface training t=33268, loss=0.015257460996508598\n",
      "Surface training t=33269, loss=0.01369239017367363\n",
      "Surface training t=33270, loss=0.013238741084933281\n",
      "Surface training t=33271, loss=0.010744384489953518\n",
      "Surface training t=33272, loss=0.012319622095674276\n",
      "Surface training t=33273, loss=0.014059233944863081\n",
      "Surface training t=33274, loss=0.018247469328343868\n",
      "Surface training t=33275, loss=0.01691820379346609\n",
      "Surface training t=33276, loss=0.01833920180797577\n",
      "Surface training t=33277, loss=0.018637409433722496\n",
      "Surface training t=33278, loss=0.018202586099505424\n",
      "Surface training t=33279, loss=0.013956017326563597\n",
      "Surface training t=33280, loss=0.020000622607767582\n",
      "Surface training t=33281, loss=0.015699410811066628\n",
      "Surface training t=33282, loss=0.017984462901949883\n",
      "Surface training t=33283, loss=0.015176326036453247\n",
      "Surface training t=33284, loss=0.017859716899693012\n",
      "Surface training t=33285, loss=0.01600809395313263\n",
      "Surface training t=33286, loss=0.012069802731275558\n",
      "Surface training t=33287, loss=0.019679876044392586\n",
      "Surface training t=33288, loss=0.017893700394779444\n",
      "Surface training t=33289, loss=0.0157048930414021\n",
      "Surface training t=33290, loss=0.016534077003598213\n",
      "Surface training t=33291, loss=0.01655855868011713\n",
      "Surface training t=33292, loss=0.01808973215520382\n",
      "Surface training t=33293, loss=0.013477022759616375\n",
      "Surface training t=33294, loss=0.015891637187451124\n",
      "Surface training t=33295, loss=0.01724455412477255\n",
      "Surface training t=33296, loss=0.016909840516746044\n",
      "Surface training t=33297, loss=0.023392523638904095\n",
      "Surface training t=33298, loss=0.021325914189219475\n",
      "Surface training t=33299, loss=0.024809534661471844\n",
      "Surface training t=33300, loss=0.027924852445721626\n",
      "Surface training t=33301, loss=0.016222224570810795\n",
      "Surface training t=33302, loss=0.01620754785835743\n",
      "Surface training t=33303, loss=0.01880729477852583\n",
      "Surface training t=33304, loss=0.01793274749070406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=33305, loss=0.019654029980301857\n",
      "Surface training t=33306, loss=0.022394304629415274\n",
      "Surface training t=33307, loss=0.027414722368121147\n",
      "Surface training t=33308, loss=0.027997948229312897\n",
      "Surface training t=33309, loss=0.04248836450278759\n",
      "Surface training t=33310, loss=0.030530275776982307\n",
      "Surface training t=33311, loss=0.02299628686159849\n",
      "Surface training t=33312, loss=0.032207886688411236\n",
      "Surface training t=33313, loss=0.03278076834976673\n",
      "Surface training t=33314, loss=0.028141967952251434\n",
      "Surface training t=33315, loss=0.027867102064192295\n",
      "Surface training t=33316, loss=0.03365509584546089\n",
      "Surface training t=33317, loss=0.036979444324970245\n",
      "Surface training t=33318, loss=0.030827634036540985\n",
      "Surface training t=33319, loss=0.03488121647387743\n",
      "Surface training t=33320, loss=0.031218646094202995\n",
      "Surface training t=33321, loss=0.02688182145357132\n",
      "Surface training t=33322, loss=0.026368768885731697\n",
      "Surface training t=33323, loss=0.024712628684937954\n",
      "Surface training t=33324, loss=0.017806705087423325\n",
      "Surface training t=33325, loss=0.022527440451085567\n",
      "Surface training t=33326, loss=0.020852893590927124\n",
      "Surface training t=33327, loss=0.02096790401265025\n",
      "Surface training t=33328, loss=0.02202643221244216\n",
      "Surface training t=33329, loss=0.021851937286555767\n",
      "Surface training t=33330, loss=0.01506034191697836\n",
      "Surface training t=33331, loss=0.01340013463050127\n",
      "Surface training t=33332, loss=0.016250040847808123\n",
      "Surface training t=33333, loss=0.01832801941782236\n",
      "Surface training t=33334, loss=0.0193193219602108\n",
      "Surface training t=33335, loss=0.015624880325049162\n",
      "Surface training t=33336, loss=0.01687409169971943\n",
      "Surface training t=33337, loss=0.01872679591178894\n",
      "Surface training t=33338, loss=0.019642755389213562\n",
      "Surface training t=33339, loss=0.019882201217114925\n",
      "Surface training t=33340, loss=0.020270521752536297\n",
      "Surface training t=33341, loss=0.02187598729506135\n",
      "Surface training t=33342, loss=0.023636437952518463\n",
      "Surface training t=33343, loss=0.030961986631155014\n",
      "Surface training t=33344, loss=0.02092032227665186\n",
      "Surface training t=33345, loss=0.01915975846350193\n",
      "Surface training t=33346, loss=0.029351843520998955\n",
      "Surface training t=33347, loss=0.021700113080441952\n",
      "Surface training t=33348, loss=0.022935903165489435\n",
      "Surface training t=33349, loss=0.03902001492679119\n",
      "Surface training t=33350, loss=0.026570425368845463\n",
      "Surface training t=33351, loss=0.03432512283325195\n",
      "Surface training t=33352, loss=0.027573765255510807\n",
      "Surface training t=33353, loss=0.03174274228513241\n",
      "Surface training t=33354, loss=0.02597783412784338\n",
      "Surface training t=33355, loss=0.024303605780005455\n",
      "Surface training t=33356, loss=0.029454481787979603\n",
      "Surface training t=33357, loss=0.02373784501105547\n",
      "Surface training t=33358, loss=0.02454322762787342\n",
      "Surface training t=33359, loss=0.022403921000659466\n",
      "Surface training t=33360, loss=0.014879852999001741\n",
      "Surface training t=33361, loss=0.022006302140653133\n",
      "Surface training t=33362, loss=0.018344824202358723\n",
      "Surface training t=33363, loss=0.017082177102565765\n",
      "Surface training t=33364, loss=0.01945915911346674\n",
      "Surface training t=33365, loss=0.016355618834495544\n",
      "Surface training t=33366, loss=0.019136658869683743\n",
      "Surface training t=33367, loss=0.018225783482193947\n",
      "Surface training t=33368, loss=0.023810644634068012\n",
      "Surface training t=33369, loss=0.013943767175078392\n",
      "Surface training t=33370, loss=0.015935472212731838\n",
      "Surface training t=33371, loss=0.013430989347398281\n",
      "Surface training t=33372, loss=0.013793220277875662\n",
      "Surface training t=33373, loss=0.013613395392894745\n",
      "Surface training t=33374, loss=0.015602677129209042\n",
      "Surface training t=33375, loss=0.014600866939872503\n",
      "Surface training t=33376, loss=0.015323888510465622\n",
      "Surface training t=33377, loss=0.017212919890880585\n",
      "Surface training t=33378, loss=0.01579040288925171\n",
      "Surface training t=33379, loss=0.01928406860679388\n",
      "Surface training t=33380, loss=0.015545057132840157\n",
      "Surface training t=33381, loss=0.019791637547314167\n",
      "Surface training t=33382, loss=0.02159357350319624\n",
      "Surface training t=33383, loss=0.017669569235295057\n",
      "Surface training t=33384, loss=0.016663160175085068\n",
      "Surface training t=33385, loss=0.017729448154568672\n",
      "Surface training t=33386, loss=0.019511114805936813\n",
      "Surface training t=33387, loss=0.02081699762493372\n",
      "Surface training t=33388, loss=0.01696034101769328\n",
      "Surface training t=33389, loss=0.018994729034602642\n",
      "Surface training t=33390, loss=0.014865138567984104\n",
      "Surface training t=33391, loss=0.01563844084739685\n",
      "Surface training t=33392, loss=0.019197986461222172\n",
      "Surface training t=33393, loss=0.02089601755142212\n",
      "Surface training t=33394, loss=0.015769769437611103\n",
      "Surface training t=33395, loss=0.016616615001112223\n",
      "Surface training t=33396, loss=0.014699906576424837\n",
      "Surface training t=33397, loss=0.014931289479136467\n",
      "Surface training t=33398, loss=0.01606095489114523\n",
      "Surface training t=33399, loss=0.018379163462668657\n",
      "Surface training t=33400, loss=0.016850837506353855\n",
      "Surface training t=33401, loss=0.01803898811340332\n",
      "Surface training t=33402, loss=0.014362819027155638\n",
      "Surface training t=33403, loss=0.015117723494768143\n",
      "Surface training t=33404, loss=0.02115374244749546\n",
      "Surface training t=33405, loss=0.01963143702596426\n",
      "Surface training t=33406, loss=0.019778812304139137\n",
      "Surface training t=33407, loss=0.02639195416122675\n",
      "Surface training t=33408, loss=0.03268163278698921\n",
      "Surface training t=33409, loss=0.025603728368878365\n",
      "Surface training t=33410, loss=0.03633539378643036\n",
      "Surface training t=33411, loss=0.035910047590732574\n",
      "Surface training t=33412, loss=0.02230376284569502\n",
      "Surface training t=33413, loss=0.022747116163372993\n",
      "Surface training t=33414, loss=0.021410485729575157\n",
      "Surface training t=33415, loss=0.021244478411972523\n",
      "Surface training t=33416, loss=0.023831401951611042\n",
      "Surface training t=33417, loss=0.017571045085787773\n",
      "Surface training t=33418, loss=0.020108959171921015\n",
      "Surface training t=33419, loss=0.02952869702130556\n",
      "Surface training t=33420, loss=0.024142018519341946\n",
      "Surface training t=33421, loss=0.022092747036367655\n",
      "Surface training t=33422, loss=0.01995608303695917\n",
      "Surface training t=33423, loss=0.025024783797562122\n",
      "Surface training t=33424, loss=0.024670480750501156\n",
      "Surface training t=33425, loss=0.021484900265932083\n",
      "Surface training t=33426, loss=0.023819196969270706\n",
      "Surface training t=33427, loss=0.0224422300234437\n",
      "Surface training t=33428, loss=0.020945731550455093\n",
      "Surface training t=33429, loss=0.024035818874835968\n",
      "Surface training t=33430, loss=0.01737777702510357\n",
      "Surface training t=33431, loss=0.018736968748271465\n",
      "Surface training t=33432, loss=0.01867916714400053\n",
      "Surface training t=33433, loss=0.027074862271547318\n",
      "Surface training t=33434, loss=0.019463343545794487\n",
      "Surface training t=33435, loss=0.022577226161956787\n",
      "Surface training t=33436, loss=0.02359954919666052\n",
      "Surface training t=33437, loss=0.019742218777537346\n",
      "Surface training t=33438, loss=0.027341232635080814\n",
      "Surface training t=33439, loss=0.01910924818366766\n",
      "Surface training t=33440, loss=0.019903534092009068\n",
      "Surface training t=33441, loss=0.020655427128076553\n",
      "Surface training t=33442, loss=0.01843022182583809\n",
      "Surface training t=33443, loss=0.0201606173068285\n",
      "Surface training t=33444, loss=0.02174239791929722\n",
      "Surface training t=33445, loss=0.019243664108216763\n",
      "Surface training t=33446, loss=0.020045205019414425\n",
      "Surface training t=33447, loss=0.017505422234535217\n",
      "Surface training t=33448, loss=0.01866993587464094\n",
      "Surface training t=33449, loss=0.01410703081637621\n",
      "Surface training t=33450, loss=0.01594300242140889\n",
      "Surface training t=33451, loss=0.01573343388736248\n",
      "Surface training t=33452, loss=0.016060238238424063\n",
      "Surface training t=33453, loss=0.01588422991335392\n",
      "Surface training t=33454, loss=0.014151147101074457\n",
      "Surface training t=33455, loss=0.01859048893675208\n",
      "Surface training t=33456, loss=0.01216938765719533\n",
      "Surface training t=33457, loss=0.018785400316119194\n",
      "Surface training t=33458, loss=0.016827309504151344\n",
      "Surface training t=33459, loss=0.020646178163588047\n",
      "Surface training t=33460, loss=0.020358015783131123\n",
      "Surface training t=33461, loss=0.01753230206668377\n",
      "Surface training t=33462, loss=0.02499349322170019\n",
      "Surface training t=33463, loss=0.018510494381189346\n",
      "Surface training t=33464, loss=0.015691678039729595\n",
      "Surface training t=33465, loss=0.01677736919373274\n",
      "Surface training t=33466, loss=0.017138656228780746\n",
      "Surface training t=33467, loss=0.01899526361376047\n",
      "Surface training t=33468, loss=0.01671765884384513\n",
      "Surface training t=33469, loss=0.014939462766051292\n",
      "Surface training t=33470, loss=0.015691839158535004\n",
      "Surface training t=33471, loss=0.013462862465530634\n",
      "Surface training t=33472, loss=0.015729985665529966\n",
      "Surface training t=33473, loss=0.012477961368858814\n",
      "Surface training t=33474, loss=0.012245941441506147\n",
      "Surface training t=33475, loss=0.015489879064261913\n",
      "Surface training t=33476, loss=0.019823451526463032\n",
      "Surface training t=33477, loss=0.019444129895418882\n",
      "Surface training t=33478, loss=0.014788246247917414\n",
      "Surface training t=33479, loss=0.017681067809462547\n",
      "Surface training t=33480, loss=0.02077318634837866\n",
      "Surface training t=33481, loss=0.016298183239996433\n",
      "Surface training t=33482, loss=0.0174621376208961\n",
      "Surface training t=33483, loss=0.014662547037005424\n",
      "Surface training t=33484, loss=0.017524597235023975\n",
      "Surface training t=33485, loss=0.017750510945916176\n",
      "Surface training t=33486, loss=0.01889851875603199\n",
      "Surface training t=33487, loss=0.01785392127931118\n",
      "Surface training t=33488, loss=0.01726281549781561\n",
      "Surface training t=33489, loss=0.013527484610676765\n",
      "Surface training t=33490, loss=0.019892691634595394\n",
      "Surface training t=33491, loss=0.017282200045883656\n",
      "Surface training t=33492, loss=0.02103670872747898\n",
      "Surface training t=33493, loss=0.015775049105286598\n",
      "Surface training t=33494, loss=0.016168592032045126\n",
      "Surface training t=33495, loss=0.013227174989879131\n",
      "Surface training t=33496, loss=0.01657973136752844\n",
      "Surface training t=33497, loss=0.017795459367334843\n",
      "Surface training t=33498, loss=0.018408951349556446\n",
      "Surface training t=33499, loss=0.01679317746311426\n",
      "Surface training t=33500, loss=0.01563043799251318\n",
      "Surface training t=33501, loss=0.020949273370206356\n",
      "Surface training t=33502, loss=0.015558045357465744\n",
      "Surface training t=33503, loss=0.015388261061161757\n",
      "Surface training t=33504, loss=0.01837698183953762\n",
      "Surface training t=33505, loss=0.020190782845020294\n",
      "Surface training t=33506, loss=0.024065164849162102\n",
      "Surface training t=33507, loss=0.026036408729851246\n",
      "Surface training t=33508, loss=0.0212433198466897\n",
      "Surface training t=33509, loss=0.022776884958148003\n",
      "Surface training t=33510, loss=0.021899080835282803\n",
      "Surface training t=33511, loss=0.022495200857520103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=33512, loss=0.02472543716430664\n",
      "Surface training t=33513, loss=0.02333847526460886\n",
      "Surface training t=33514, loss=0.02220938727259636\n",
      "Surface training t=33515, loss=0.02302120253443718\n",
      "Surface training t=33516, loss=0.027150838635861874\n",
      "Surface training t=33517, loss=0.02901867963373661\n",
      "Surface training t=33518, loss=0.022964011877775192\n",
      "Surface training t=33519, loss=0.01643393188714981\n",
      "Surface training t=33520, loss=0.017773855477571487\n",
      "Surface training t=33521, loss=0.020327914506196976\n",
      "Surface training t=33522, loss=0.015128653030842543\n",
      "Surface training t=33523, loss=0.030352283269166946\n",
      "Surface training t=33524, loss=0.02367630321532488\n",
      "Surface training t=33525, loss=0.022231847047805786\n",
      "Surface training t=33526, loss=0.017808577977120876\n",
      "Surface training t=33527, loss=0.02481174934655428\n",
      "Surface training t=33528, loss=0.021489719860255718\n",
      "Surface training t=33529, loss=0.018476814031600952\n",
      "Surface training t=33530, loss=0.026056968607008457\n",
      "Surface training t=33531, loss=0.02061107289046049\n",
      "Surface training t=33532, loss=0.026152504608035088\n",
      "Surface training t=33533, loss=0.023375103250145912\n",
      "Surface training t=33534, loss=0.025974511168897152\n",
      "Surface training t=33535, loss=0.023769780062139034\n",
      "Surface training t=33536, loss=0.022232369519770145\n",
      "Surface training t=33537, loss=0.0208826819434762\n",
      "Surface training t=33538, loss=0.01964341662824154\n",
      "Surface training t=33539, loss=0.02195211872458458\n",
      "Surface training t=33540, loss=0.021665502339601517\n",
      "Surface training t=33541, loss=0.02300321962684393\n",
      "Surface training t=33542, loss=0.016915253829210997\n",
      "Surface training t=33543, loss=0.020618046633899212\n",
      "Surface training t=33544, loss=0.022132715210318565\n",
      "Surface training t=33545, loss=0.016593400854617357\n",
      "Surface training t=33546, loss=0.023533287458121777\n",
      "Surface training t=33547, loss=0.024561353027820587\n",
      "Surface training t=33548, loss=0.026568072848021984\n",
      "Surface training t=33549, loss=0.022264020517468452\n",
      "Surface training t=33550, loss=0.02050386182963848\n",
      "Surface training t=33551, loss=0.02440367266535759\n",
      "Surface training t=33552, loss=0.02146059460937977\n",
      "Surface training t=33553, loss=0.01503372797742486\n",
      "Surface training t=33554, loss=0.02200896106660366\n",
      "Surface training t=33555, loss=0.025032655335962772\n",
      "Surface training t=33556, loss=0.020543815568089485\n",
      "Surface training t=33557, loss=0.024465246126055717\n",
      "Surface training t=33558, loss=0.02230995148420334\n",
      "Surface training t=33559, loss=0.018745900131762028\n",
      "Surface training t=33560, loss=0.018491188995540142\n",
      "Surface training t=33561, loss=0.022256758995354176\n",
      "Surface training t=33562, loss=0.017067939043045044\n",
      "Surface training t=33563, loss=0.017240645363926888\n",
      "Surface training t=33564, loss=0.015372362919151783\n",
      "Surface training t=33565, loss=0.017826570197939873\n",
      "Surface training t=33566, loss=0.016029586549848318\n",
      "Surface training t=33567, loss=0.019511211663484573\n",
      "Surface training t=33568, loss=0.01691809669137001\n",
      "Surface training t=33569, loss=0.020640505477786064\n",
      "Surface training t=33570, loss=0.020355941727757454\n",
      "Surface training t=33571, loss=0.02341928333044052\n",
      "Surface training t=33572, loss=0.023462802171707153\n",
      "Surface training t=33573, loss=0.022580732591450214\n",
      "Surface training t=33574, loss=0.023485593497753143\n",
      "Surface training t=33575, loss=0.02315775863826275\n",
      "Surface training t=33576, loss=0.02558052632957697\n",
      "Surface training t=33577, loss=0.02985704131424427\n",
      "Surface training t=33578, loss=0.029349863529205322\n",
      "Surface training t=33579, loss=0.02820601873099804\n",
      "Surface training t=33580, loss=0.02220605267211795\n",
      "Surface training t=33581, loss=0.01874749455600977\n",
      "Surface training t=33582, loss=0.021882777102291584\n",
      "Surface training t=33583, loss=0.02260914444923401\n",
      "Surface training t=33584, loss=0.03078649565577507\n",
      "Surface training t=33585, loss=0.034963203594088554\n",
      "Surface training t=33586, loss=0.03289512265473604\n",
      "Surface training t=33587, loss=0.030460676178336143\n",
      "Surface training t=33588, loss=0.04168976843357086\n",
      "Surface training t=33589, loss=0.028576667420566082\n",
      "Surface training t=33590, loss=0.031398250721395016\n",
      "Surface training t=33591, loss=0.02750989329069853\n",
      "Surface training t=33592, loss=0.028532547876238823\n",
      "Surface training t=33593, loss=0.028057977557182312\n",
      "Surface training t=33594, loss=0.02243850566446781\n",
      "Surface training t=33595, loss=0.02802858967334032\n",
      "Surface training t=33596, loss=0.025055008940398693\n",
      "Surface training t=33597, loss=0.01899910718202591\n",
      "Surface training t=33598, loss=0.017039808444678783\n",
      "Surface training t=33599, loss=0.02298235148191452\n",
      "Surface training t=33600, loss=0.020199733786284924\n",
      "Surface training t=33601, loss=0.01890137605369091\n",
      "Surface training t=33602, loss=0.017900302074849606\n",
      "Surface training t=33603, loss=0.019868900999426842\n",
      "Surface training t=33604, loss=0.019865553826093674\n",
      "Surface training t=33605, loss=0.018630788661539555\n",
      "Surface training t=33606, loss=0.01862445566803217\n",
      "Surface training t=33607, loss=0.013472025282680988\n",
      "Surface training t=33608, loss=0.016311755403876305\n",
      "Surface training t=33609, loss=0.013086590450257063\n",
      "Surface training t=33610, loss=0.018585247918963432\n",
      "Surface training t=33611, loss=0.012995489407330751\n",
      "Surface training t=33612, loss=0.01621480006724596\n",
      "Surface training t=33613, loss=0.015187184792011976\n",
      "Surface training t=33614, loss=0.013168246950954199\n",
      "Surface training t=33615, loss=0.015681983437389135\n",
      "Surface training t=33616, loss=0.01829115767031908\n",
      "Surface training t=33617, loss=0.020742039196193218\n",
      "Surface training t=33618, loss=0.01793621899560094\n",
      "Surface training t=33619, loss=0.014978352934122086\n",
      "Surface training t=33620, loss=0.014707009308040142\n",
      "Surface training t=33621, loss=0.014669615775346756\n",
      "Surface training t=33622, loss=0.018822651356458664\n",
      "Surface training t=33623, loss=0.019863881170749664\n",
      "Surface training t=33624, loss=0.01833872962743044\n",
      "Surface training t=33625, loss=0.02104614395648241\n",
      "Surface training t=33626, loss=0.015162514988332987\n",
      "Surface training t=33627, loss=0.014263244811445475\n",
      "Surface training t=33628, loss=0.015383835881948471\n",
      "Surface training t=33629, loss=0.021389644593000412\n",
      "Surface training t=33630, loss=0.021053611300885677\n",
      "Surface training t=33631, loss=0.021863173693418503\n",
      "Surface training t=33632, loss=0.014757076278328896\n",
      "Surface training t=33633, loss=0.01874316670000553\n",
      "Surface training t=33634, loss=0.016925452277064323\n",
      "Surface training t=33635, loss=0.016588258557021618\n",
      "Surface training t=33636, loss=0.015958426520228386\n",
      "Surface training t=33637, loss=0.01144347433000803\n",
      "Surface training t=33638, loss=0.015301431529223919\n",
      "Surface training t=33639, loss=0.0169024970382452\n",
      "Surface training t=33640, loss=0.028487269766628742\n",
      "Surface training t=33641, loss=0.022198754362761974\n",
      "Surface training t=33642, loss=0.020724477246403694\n",
      "Surface training t=33643, loss=0.021431440487504005\n",
      "Surface training t=33644, loss=0.018583512865006924\n",
      "Surface training t=33645, loss=0.01568632200360298\n",
      "Surface training t=33646, loss=0.019637273624539375\n",
      "Surface training t=33647, loss=0.022755103185772896\n",
      "Surface training t=33648, loss=0.019892282783985138\n",
      "Surface training t=33649, loss=0.019108968321233988\n",
      "Surface training t=33650, loss=0.021777884103357792\n",
      "Surface training t=33651, loss=0.02292633429169655\n",
      "Surface training t=33652, loss=0.021261822432279587\n",
      "Surface training t=33653, loss=0.024344262667000294\n",
      "Surface training t=33654, loss=0.018995496444404125\n",
      "Surface training t=33655, loss=0.017454579006880522\n",
      "Surface training t=33656, loss=0.022791183553636074\n",
      "Surface training t=33657, loss=0.021308427676558495\n",
      "Surface training t=33658, loss=0.021171100437641144\n",
      "Surface training t=33659, loss=0.020681865513324738\n",
      "Surface training t=33660, loss=0.014687629882246256\n",
      "Surface training t=33661, loss=0.017340533435344696\n",
      "Surface training t=33662, loss=0.015395582653582096\n",
      "Surface training t=33663, loss=0.015941903926432133\n",
      "Surface training t=33664, loss=0.015591452363878489\n",
      "Surface training t=33665, loss=0.015865579713135958\n",
      "Surface training t=33666, loss=0.0141586409881711\n",
      "Surface training t=33667, loss=0.013378440868109465\n",
      "Surface training t=33668, loss=0.016525051556527615\n",
      "Surface training t=33669, loss=0.016989526338875294\n",
      "Surface training t=33670, loss=0.01623118156567216\n",
      "Surface training t=33671, loss=0.019135344307869673\n",
      "Surface training t=33672, loss=0.019147463142871857\n",
      "Surface training t=33673, loss=0.019933540374040604\n",
      "Surface training t=33674, loss=0.02196918986737728\n",
      "Surface training t=33675, loss=0.02093452587723732\n",
      "Surface training t=33676, loss=0.0191757557913661\n",
      "Surface training t=33677, loss=0.016533592715859413\n",
      "Surface training t=33678, loss=0.01710367575287819\n",
      "Surface training t=33679, loss=0.01619343552738428\n",
      "Surface training t=33680, loss=0.020065651275217533\n",
      "Surface training t=33681, loss=0.01634122058749199\n",
      "Surface training t=33682, loss=0.02185138501226902\n",
      "Surface training t=33683, loss=0.019379764795303345\n",
      "Surface training t=33684, loss=0.018689045682549477\n",
      "Surface training t=33685, loss=0.017597077414393425\n",
      "Surface training t=33686, loss=0.018307351507246494\n",
      "Surface training t=33687, loss=0.017476443201303482\n",
      "Surface training t=33688, loss=0.01688039069995284\n",
      "Surface training t=33689, loss=0.01627338072285056\n",
      "Surface training t=33690, loss=0.015899081714451313\n",
      "Surface training t=33691, loss=0.014712198171764612\n",
      "Surface training t=33692, loss=0.016699479892849922\n",
      "Surface training t=33693, loss=0.014718869235366583\n",
      "Surface training t=33694, loss=0.017883055843412876\n",
      "Surface training t=33695, loss=0.013743238523602486\n",
      "Surface training t=33696, loss=0.012928293086588383\n",
      "Surface training t=33697, loss=0.019563181325793266\n",
      "Surface training t=33698, loss=0.028577986173331738\n",
      "Surface training t=33699, loss=0.022641570307314396\n",
      "Surface training t=33700, loss=0.022160095162689686\n",
      "Surface training t=33701, loss=0.018915350548923016\n",
      "Surface training t=33702, loss=0.02346455119550228\n",
      "Surface training t=33703, loss=0.01743116369470954\n",
      "Surface training t=33704, loss=0.021071933209896088\n",
      "Surface training t=33705, loss=0.03593256138265133\n",
      "Surface training t=33706, loss=0.02166316658258438\n",
      "Surface training t=33707, loss=0.025544450618326664\n",
      "Surface training t=33708, loss=0.021788734011352062\n",
      "Surface training t=33709, loss=0.022280822042375803\n",
      "Surface training t=33710, loss=0.0215455605648458\n",
      "Surface training t=33711, loss=0.033974927850067616\n",
      "Surface training t=33712, loss=0.021719452925026417\n",
      "Surface training t=33713, loss=0.023016364313662052\n",
      "Surface training t=33714, loss=0.026202148757874966\n",
      "Surface training t=33715, loss=0.024465481750667095\n",
      "Surface training t=33716, loss=0.023940250277519226\n",
      "Surface training t=33717, loss=0.02036196179687977\n",
      "Surface training t=33718, loss=0.015398729126900434\n",
      "Surface training t=33719, loss=0.014385655988007784\n",
      "Surface training t=33720, loss=0.01804307848215103\n",
      "Surface training t=33721, loss=0.015548900235444307\n",
      "Surface training t=33722, loss=0.021772028878331184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=33723, loss=0.027281480841338634\n",
      "Surface training t=33724, loss=0.019305242225527763\n",
      "Surface training t=33725, loss=0.0192714910954237\n",
      "Surface training t=33726, loss=0.019660172052681446\n",
      "Surface training t=33727, loss=0.02255992591381073\n",
      "Surface training t=33728, loss=0.02094045653939247\n",
      "Surface training t=33729, loss=0.023638557642698288\n",
      "Surface training t=33730, loss=0.021218023262917995\n",
      "Surface training t=33731, loss=0.01762948650866747\n",
      "Surface training t=33732, loss=0.01973615027964115\n",
      "Surface training t=33733, loss=0.016444501467049122\n",
      "Surface training t=33734, loss=0.01833073701709509\n",
      "Surface training t=33735, loss=0.01771390065550804\n",
      "Surface training t=33736, loss=0.023680337704718113\n",
      "Surface training t=33737, loss=0.020179640501737595\n",
      "Surface training t=33738, loss=0.015571830794215202\n",
      "Surface training t=33739, loss=0.014078206848353148\n",
      "Surface training t=33740, loss=0.012154394295066595\n",
      "Surface training t=33741, loss=0.011049816384911537\n",
      "Surface training t=33742, loss=0.017833810299634933\n",
      "Surface training t=33743, loss=0.012398631311953068\n",
      "Surface training t=33744, loss=0.01448741415515542\n",
      "Surface training t=33745, loss=0.01493513397872448\n",
      "Surface training t=33746, loss=0.01790173351764679\n",
      "Surface training t=33747, loss=0.012356085702776909\n",
      "Surface training t=33748, loss=0.022203121334314346\n",
      "Surface training t=33749, loss=0.016222922131419182\n",
      "Surface training t=33750, loss=0.026665550656616688\n",
      "Surface training t=33751, loss=0.021885008551180363\n",
      "Surface training t=33752, loss=0.01722982246428728\n",
      "Surface training t=33753, loss=0.020530092529952526\n",
      "Surface training t=33754, loss=0.032646819949150085\n",
      "Surface training t=33755, loss=0.026041449047625065\n",
      "Surface training t=33756, loss=0.026750660501420498\n",
      "Surface training t=33757, loss=0.020780005492269993\n",
      "Surface training t=33758, loss=0.025206785649061203\n",
      "Surface training t=33759, loss=0.024928918108344078\n",
      "Surface training t=33760, loss=0.022024639882147312\n",
      "Surface training t=33761, loss=0.020179052837193012\n",
      "Surface training t=33762, loss=0.018939275294542313\n",
      "Surface training t=33763, loss=0.022513370029628277\n",
      "Surface training t=33764, loss=0.02347113098949194\n",
      "Surface training t=33765, loss=0.024223029613494873\n",
      "Surface training t=33766, loss=0.026006699539721012\n",
      "Surface training t=33767, loss=0.020880154334008694\n",
      "Surface training t=33768, loss=0.024516788311302662\n",
      "Surface training t=33769, loss=0.0290997507981956\n",
      "Surface training t=33770, loss=0.04228440672159195\n",
      "Surface training t=33771, loss=0.03493486903607845\n",
      "Surface training t=33772, loss=0.03828258439898491\n",
      "Surface training t=33773, loss=0.025216447189450264\n",
      "Surface training t=33774, loss=0.034072283655405045\n",
      "Surface training t=33775, loss=0.027518936432898045\n",
      "Surface training t=33776, loss=0.05117533355951309\n",
      "Surface training t=33777, loss=0.02753487415611744\n",
      "Surface training t=33778, loss=0.045360738411545753\n",
      "Surface training t=33779, loss=0.03184041008353233\n",
      "Surface training t=33780, loss=0.04049849137663841\n",
      "Surface training t=33781, loss=0.03146104793995619\n",
      "Surface training t=33782, loss=0.03985575772821903\n",
      "Surface training t=33783, loss=0.02441577333956957\n",
      "Surface training t=33784, loss=0.025810109451413155\n",
      "Surface training t=33785, loss=0.037348467856645584\n",
      "Surface training t=33786, loss=0.037032959051430225\n",
      "Surface training t=33787, loss=0.03719217889010906\n",
      "Surface training t=33788, loss=0.038728371262550354\n",
      "Surface training t=33789, loss=0.03512363135814667\n",
      "Surface training t=33790, loss=0.02483054529875517\n",
      "Surface training t=33791, loss=0.03274299576878548\n",
      "Surface training t=33792, loss=0.030205057933926582\n",
      "Surface training t=33793, loss=0.02475210838019848\n",
      "Surface training t=33794, loss=0.02303749043494463\n",
      "Surface training t=33795, loss=0.020825975108891726\n",
      "Surface training t=33796, loss=0.019531571306288242\n",
      "Surface training t=33797, loss=0.019229838624596596\n",
      "Surface training t=33798, loss=0.015851494390517473\n",
      "Surface training t=33799, loss=0.0173803330399096\n",
      "Surface training t=33800, loss=0.016921683680266142\n",
      "Surface training t=33801, loss=0.014483008533716202\n",
      "Surface training t=33802, loss=0.01387192727997899\n",
      "Surface training t=33803, loss=0.014677670784294605\n",
      "Surface training t=33804, loss=0.013642291072756052\n",
      "Surface training t=33805, loss=0.013563556596636772\n",
      "Surface training t=33806, loss=0.015037906356155872\n",
      "Surface training t=33807, loss=0.014071444515138865\n",
      "Surface training t=33808, loss=0.01684750523418188\n",
      "Surface training t=33809, loss=0.018002250231802464\n",
      "Surface training t=33810, loss=0.015251229517161846\n",
      "Surface training t=33811, loss=0.01951136253774166\n",
      "Surface training t=33812, loss=0.022093535400927067\n",
      "Surface training t=33813, loss=0.022792239673435688\n",
      "Surface training t=33814, loss=0.02282619383186102\n",
      "Surface training t=33815, loss=0.023367066867649555\n",
      "Surface training t=33816, loss=0.027256385423243046\n",
      "Surface training t=33817, loss=0.02677702158689499\n",
      "Surface training t=33818, loss=0.0156288156285882\n",
      "Surface training t=33819, loss=0.01398157561197877\n",
      "Surface training t=33820, loss=0.01829170435667038\n",
      "Surface training t=33821, loss=0.016151635441929102\n",
      "Surface training t=33822, loss=0.019608041271567345\n",
      "Surface training t=33823, loss=0.020228521898388863\n",
      "Surface training t=33824, loss=0.015000661835074425\n",
      "Surface training t=33825, loss=0.015101377386599779\n",
      "Surface training t=33826, loss=0.013201527297496796\n",
      "Surface training t=33827, loss=0.016426921356469393\n",
      "Surface training t=33828, loss=0.017017492093145847\n",
      "Surface training t=33829, loss=0.016133676283061504\n",
      "Surface training t=33830, loss=0.01347911637276411\n",
      "Surface training t=33831, loss=0.017362154088914394\n",
      "Surface training t=33832, loss=0.022942964918911457\n",
      "Surface training t=33833, loss=0.019082977436482906\n",
      "Surface training t=33834, loss=0.022568058222532272\n",
      "Surface training t=33835, loss=0.023341641761362553\n",
      "Surface training t=33836, loss=0.029202907346189022\n",
      "Surface training t=33837, loss=0.023986726999282837\n",
      "Surface training t=33838, loss=0.021979878656566143\n",
      "Surface training t=33839, loss=0.021487019024789333\n",
      "Surface training t=33840, loss=0.02338003134354949\n",
      "Surface training t=33841, loss=0.024057677946984768\n",
      "Surface training t=33842, loss=0.021155223716050386\n",
      "Surface training t=33843, loss=0.018459773622453213\n",
      "Surface training t=33844, loss=0.020916045643389225\n",
      "Surface training t=33845, loss=0.019572071731090546\n",
      "Surface training t=33846, loss=0.0170839661732316\n",
      "Surface training t=33847, loss=0.0148237906396389\n",
      "Surface training t=33848, loss=0.015887211076915264\n",
      "Surface training t=33849, loss=0.014221817720681429\n",
      "Surface training t=33850, loss=0.019113317131996155\n",
      "Surface training t=33851, loss=0.01610091933980584\n",
      "Surface training t=33852, loss=0.016968553885817528\n",
      "Surface training t=33853, loss=0.021021436899900436\n",
      "Surface training t=33854, loss=0.01719280332326889\n",
      "Surface training t=33855, loss=0.016884435899555683\n",
      "Surface training t=33856, loss=0.017751798033714294\n",
      "Surface training t=33857, loss=0.025638815015554428\n",
      "Surface training t=33858, loss=0.03051650896668434\n",
      "Surface training t=33859, loss=0.02753121592104435\n",
      "Surface training t=33860, loss=0.022177789360284805\n",
      "Surface training t=33861, loss=0.019323029555380344\n",
      "Surface training t=33862, loss=0.014123786240816116\n",
      "Surface training t=33863, loss=0.014875768683850765\n",
      "Surface training t=33864, loss=0.021801949478685856\n",
      "Surface training t=33865, loss=0.033926110714673996\n",
      "Surface training t=33866, loss=0.02869432605803013\n",
      "Surface training t=33867, loss=0.024846111424267292\n",
      "Surface training t=33868, loss=0.028253107331693172\n",
      "Surface training t=33869, loss=0.03862711787223816\n",
      "Surface training t=33870, loss=0.033794110640883446\n",
      "Surface training t=33871, loss=0.033513396978378296\n",
      "Surface training t=33872, loss=0.04419167339801788\n",
      "Surface training t=33873, loss=0.030012885108590126\n",
      "Surface training t=33874, loss=0.03718572296202183\n",
      "Surface training t=33875, loss=0.04066864028573036\n",
      "Surface training t=33876, loss=0.03069249726831913\n",
      "Surface training t=33877, loss=0.03631831891834736\n",
      "Surface training t=33878, loss=0.03826039843261242\n",
      "Surface training t=33879, loss=0.026829211972653866\n",
      "Surface training t=33880, loss=0.030396681278944016\n",
      "Surface training t=33881, loss=0.056126032024621964\n",
      "Surface training t=33882, loss=0.0359613336622715\n",
      "Surface training t=33883, loss=0.05105219781398773\n",
      "Surface training t=33884, loss=0.03855429496616125\n",
      "Surface training t=33885, loss=0.05441332422196865\n",
      "Surface training t=33886, loss=0.03717868961393833\n",
      "Surface training t=33887, loss=0.03371203690767288\n",
      "Surface training t=33888, loss=0.04501292295753956\n",
      "Surface training t=33889, loss=0.026528935879468918\n",
      "Surface training t=33890, loss=0.027772740460932255\n",
      "Surface training t=33891, loss=0.03531987592577934\n",
      "Surface training t=33892, loss=0.02788070309907198\n",
      "Surface training t=33893, loss=0.022745024412870407\n",
      "Surface training t=33894, loss=0.03382510878145695\n",
      "Surface training t=33895, loss=0.025926725007593632\n",
      "Surface training t=33896, loss=0.02149264421314001\n",
      "Surface training t=33897, loss=0.026620343327522278\n",
      "Surface training t=33898, loss=0.018931737169623375\n",
      "Surface training t=33899, loss=0.022456221282482147\n",
      "Surface training t=33900, loss=0.014412558637559414\n",
      "Surface training t=33901, loss=0.03093619178980589\n",
      "Surface training t=33902, loss=0.025854209437966347\n",
      "Surface training t=33903, loss=0.03160783275961876\n",
      "Surface training t=33904, loss=0.022188570350408554\n",
      "Surface training t=33905, loss=0.023076306097209454\n",
      "Surface training t=33906, loss=0.02782412339001894\n",
      "Surface training t=33907, loss=0.028246548026800156\n",
      "Surface training t=33908, loss=0.025979149155318737\n",
      "Surface training t=33909, loss=0.022760416381061077\n",
      "Surface training t=33910, loss=0.02529869880527258\n",
      "Surface training t=33911, loss=0.02489305566996336\n",
      "Surface training t=33912, loss=0.023364567197859287\n",
      "Surface training t=33913, loss=0.020507839508354664\n",
      "Surface training t=33914, loss=0.017083369195461273\n",
      "Surface training t=33915, loss=0.01417296752333641\n",
      "Surface training t=33916, loss=0.012761679943650961\n",
      "Surface training t=33917, loss=0.014986429363489151\n",
      "Surface training t=33918, loss=0.018051044084131718\n",
      "Surface training t=33919, loss=0.015666844323277473\n",
      "Surface training t=33920, loss=0.019341924227774143\n",
      "Surface training t=33921, loss=0.014908676035702229\n",
      "Surface training t=33922, loss=0.015665696933865547\n",
      "Surface training t=33923, loss=0.015951858833432198\n",
      "Surface training t=33924, loss=0.013413131702691317\n",
      "Surface training t=33925, loss=0.014828399755060673\n",
      "Surface training t=33926, loss=0.013253703713417053\n",
      "Surface training t=33927, loss=0.012845577206462622\n",
      "Surface training t=33928, loss=0.018445960246026516\n",
      "Surface training t=33929, loss=0.020678927190601826\n",
      "Surface training t=33930, loss=0.018093544524163008\n",
      "Surface training t=33931, loss=0.020845533348619938\n",
      "Surface training t=33932, loss=0.022813603281974792\n",
      "Surface training t=33933, loss=0.023889169096946716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=33934, loss=0.028574582189321518\n",
      "Surface training t=33935, loss=0.029191438108682632\n",
      "Surface training t=33936, loss=0.029144052416086197\n",
      "Surface training t=33937, loss=0.03181111998856068\n",
      "Surface training t=33938, loss=0.035624854266643524\n",
      "Surface training t=33939, loss=0.02635019924491644\n",
      "Surface training t=33940, loss=0.021478070877492428\n",
      "Surface training t=33941, loss=0.018608521670103073\n",
      "Surface training t=33942, loss=0.021564585156738758\n",
      "Surface training t=33943, loss=0.0163088021799922\n",
      "Surface training t=33944, loss=0.020436754450201988\n",
      "Surface training t=33945, loss=0.01411985233426094\n",
      "Surface training t=33946, loss=0.022027255967259407\n",
      "Surface training t=33947, loss=0.020080205984413624\n",
      "Surface training t=33948, loss=0.014420014806091785\n",
      "Surface training t=33949, loss=0.022015743888914585\n",
      "Surface training t=33950, loss=0.025585684925317764\n",
      "Surface training t=33951, loss=0.022159071639180183\n",
      "Surface training t=33952, loss=0.025485030375421047\n",
      "Surface training t=33953, loss=0.01894219033420086\n",
      "Surface training t=33954, loss=0.023087643086910248\n",
      "Surface training t=33955, loss=0.019232451915740967\n",
      "Surface training t=33956, loss=0.02108642691746354\n",
      "Surface training t=33957, loss=0.0231382017955184\n",
      "Surface training t=33958, loss=0.028116953559219837\n",
      "Surface training t=33959, loss=0.03225370869040489\n",
      "Surface training t=33960, loss=0.02706540748476982\n",
      "Surface training t=33961, loss=0.02373612765222788\n",
      "Surface training t=33962, loss=0.017419520765542984\n",
      "Surface training t=33963, loss=0.01920716743916273\n",
      "Surface training t=33964, loss=0.016802608966827393\n",
      "Surface training t=33965, loss=0.01900060474872589\n",
      "Surface training t=33966, loss=0.018854660913348198\n",
      "Surface training t=33967, loss=0.024557714350521564\n",
      "Surface training t=33968, loss=0.018842794932425022\n",
      "Surface training t=33969, loss=0.02340883854776621\n",
      "Surface training t=33970, loss=0.03715079836547375\n",
      "Surface training t=33971, loss=0.023033705540001392\n",
      "Surface training t=33972, loss=0.031387509778141975\n",
      "Surface training t=33973, loss=0.023328270763158798\n",
      "Surface training t=33974, loss=0.03480634465813637\n",
      "Surface training t=33975, loss=0.023613005876541138\n",
      "Surface training t=33976, loss=0.035176824778318405\n",
      "Surface training t=33977, loss=0.027617373503744602\n",
      "Surface training t=33978, loss=0.022214490920305252\n",
      "Surface training t=33979, loss=0.022616793867200613\n",
      "Surface training t=33980, loss=0.0206623375415802\n",
      "Surface training t=33981, loss=0.019564176443964243\n",
      "Surface training t=33982, loss=0.02025881316512823\n",
      "Surface training t=33983, loss=0.02432449907064438\n",
      "Surface training t=33984, loss=0.018061934038996696\n",
      "Surface training t=33985, loss=0.021285369992256165\n",
      "Surface training t=33986, loss=0.023222423158586025\n",
      "Surface training t=33987, loss=0.0163209424354136\n",
      "Surface training t=33988, loss=0.015825494192540646\n",
      "Surface training t=33989, loss=0.02004337217658758\n",
      "Surface training t=33990, loss=0.016914380714297295\n",
      "Surface training t=33991, loss=0.02206967119127512\n",
      "Surface training t=33992, loss=0.02342076040804386\n",
      "Surface training t=33993, loss=0.014150692149996758\n",
      "Surface training t=33994, loss=0.014409274328500032\n",
      "Surface training t=33995, loss=0.013915926683694124\n",
      "Surface training t=33996, loss=0.01413344545289874\n",
      "Surface training t=33997, loss=0.018708852119743824\n",
      "Surface training t=33998, loss=0.012869412079453468\n",
      "Surface training t=33999, loss=0.017343923449516296\n",
      "Surface training t=34000, loss=0.014543908182531595\n",
      "Surface training t=34001, loss=0.0144088389351964\n",
      "Surface training t=34002, loss=0.012219434604048729\n",
      "Surface training t=34003, loss=0.018455215264111757\n",
      "Surface training t=34004, loss=0.01796062383800745\n",
      "Surface training t=34005, loss=0.02048521488904953\n",
      "Surface training t=34006, loss=0.027843414805829525\n",
      "Surface training t=34007, loss=0.024668514262884855\n",
      "Surface training t=34008, loss=0.024309996515512466\n",
      "Surface training t=34009, loss=0.03816434554755688\n",
      "Surface training t=34010, loss=0.026081237010657787\n",
      "Surface training t=34011, loss=0.021654164418578148\n",
      "Surface training t=34012, loss=0.02053095493465662\n",
      "Surface training t=34013, loss=0.023222492076456547\n",
      "Surface training t=34014, loss=0.026980715803802013\n",
      "Surface training t=34015, loss=0.026290282607078552\n",
      "Surface training t=34016, loss=0.028081425465643406\n",
      "Surface training t=34017, loss=0.022644509561359882\n",
      "Surface training t=34018, loss=0.02436179853975773\n",
      "Surface training t=34019, loss=0.018416376784443855\n",
      "Surface training t=34020, loss=0.022978701628744602\n",
      "Surface training t=34021, loss=0.02788449451327324\n",
      "Surface training t=34022, loss=0.02101986575871706\n",
      "Surface training t=34023, loss=0.027118884958326817\n",
      "Surface training t=34024, loss=0.02598874643445015\n",
      "Surface training t=34025, loss=0.022175566293299198\n",
      "Surface training t=34026, loss=0.01937676128000021\n",
      "Surface training t=34027, loss=0.02088179811835289\n",
      "Surface training t=34028, loss=0.018929303623735905\n",
      "Surface training t=34029, loss=0.019248124212026596\n",
      "Surface training t=34030, loss=0.02745771687477827\n",
      "Surface training t=34031, loss=0.022358743473887444\n",
      "Surface training t=34032, loss=0.024291448295116425\n",
      "Surface training t=34033, loss=0.03301822394132614\n",
      "Surface training t=34034, loss=0.02517922967672348\n",
      "Surface training t=34035, loss=0.028079520910978317\n",
      "Surface training t=34036, loss=0.019294033758342266\n",
      "Surface training t=34037, loss=0.020725887268781662\n",
      "Surface training t=34038, loss=0.02050910796970129\n",
      "Surface training t=34039, loss=0.018785963766276836\n",
      "Surface training t=34040, loss=0.022375483065843582\n",
      "Surface training t=34041, loss=0.019879654981195927\n",
      "Surface training t=34042, loss=0.021438447758555412\n",
      "Surface training t=34043, loss=0.021102207712829113\n",
      "Surface training t=34044, loss=0.022007305175065994\n",
      "Surface training t=34045, loss=0.01814754121005535\n",
      "Surface training t=34046, loss=0.015374467708170414\n",
      "Surface training t=34047, loss=0.015630212146788836\n",
      "Surface training t=34048, loss=0.01753324829041958\n",
      "Surface training t=34049, loss=0.013214761391282082\n",
      "Surface training t=34050, loss=0.016271752305328846\n",
      "Surface training t=34051, loss=0.018850057385861874\n",
      "Surface training t=34052, loss=0.01638636039569974\n",
      "Surface training t=34053, loss=0.015869231894612312\n",
      "Surface training t=34054, loss=0.02293569501489401\n",
      "Surface training t=34055, loss=0.02339023444801569\n",
      "Surface training t=34056, loss=0.01778717339038849\n",
      "Surface training t=34057, loss=0.021146144717931747\n",
      "Surface training t=34058, loss=0.017371793277561665\n",
      "Surface training t=34059, loss=0.02034430205821991\n",
      "Surface training t=34060, loss=0.020041348412632942\n",
      "Surface training t=34061, loss=0.016692119650542736\n",
      "Surface training t=34062, loss=0.017019188031554222\n",
      "Surface training t=34063, loss=0.017468381207436323\n",
      "Surface training t=34064, loss=0.01759370695799589\n",
      "Surface training t=34065, loss=0.019874441903084517\n",
      "Surface training t=34066, loss=0.014811832457780838\n",
      "Surface training t=34067, loss=0.014709239825606346\n",
      "Surface training t=34068, loss=0.01449342630803585\n",
      "Surface training t=34069, loss=0.012309378944337368\n",
      "Surface training t=34070, loss=0.013797132298350334\n",
      "Surface training t=34071, loss=0.017980066128075123\n",
      "Surface training t=34072, loss=0.013822628650814295\n",
      "Surface training t=34073, loss=0.022936823777854443\n",
      "Surface training t=34074, loss=0.01847975142300129\n",
      "Surface training t=34075, loss=0.0178291373886168\n",
      "Surface training t=34076, loss=0.017377257347106934\n",
      "Surface training t=34077, loss=0.016769714653491974\n",
      "Surface training t=34078, loss=0.020848562940955162\n",
      "Surface training t=34079, loss=0.02149428427219391\n",
      "Surface training t=34080, loss=0.01994318701326847\n",
      "Surface training t=34081, loss=0.02003931626677513\n",
      "Surface training t=34082, loss=0.015594801865518093\n",
      "Surface training t=34083, loss=0.021931364201009274\n",
      "Surface training t=34084, loss=0.019631904549896717\n",
      "Surface training t=34085, loss=0.013791936449706554\n",
      "Surface training t=34086, loss=0.017724160104990005\n",
      "Surface training t=34087, loss=0.013804524205625057\n",
      "Surface training t=34088, loss=0.01837556902319193\n",
      "Surface training t=34089, loss=0.016372245736420155\n",
      "Surface training t=34090, loss=0.02085212990641594\n",
      "Surface training t=34091, loss=0.0204553185030818\n",
      "Surface training t=34092, loss=0.01587836630642414\n",
      "Surface training t=34093, loss=0.02019025757908821\n",
      "Surface training t=34094, loss=0.018012553453445435\n",
      "Surface training t=34095, loss=0.015388823579996824\n",
      "Surface training t=34096, loss=0.014180789701640606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=34097, loss=0.015232022386044264\n",
      "Surface training t=34098, loss=0.017841648310422897\n",
      "Surface training t=34099, loss=0.017715461552143097\n",
      "Surface training t=34100, loss=0.018659065011888742\n",
      "Surface training t=34101, loss=0.028388535603880882\n",
      "Surface training t=34102, loss=0.01737029105424881\n",
      "Surface training t=34103, loss=0.02866998501121998\n",
      "Surface training t=34104, loss=0.026737626641988754\n",
      "Surface training t=34105, loss=0.02247596811503172\n",
      "Surface training t=34106, loss=0.01894241012632847\n",
      "Surface training t=34107, loss=0.017893623560667038\n",
      "Surface training t=34108, loss=0.015787402167916298\n",
      "Surface training t=34109, loss=0.019313955679535866\n",
      "Surface training t=34110, loss=0.019937978126108646\n",
      "Surface training t=34111, loss=0.015240060165524483\n",
      "Surface training t=34112, loss=0.020747078582644463\n",
      "Surface training t=34113, loss=0.016422223765403032\n",
      "Surface training t=34114, loss=0.014541250187903643\n",
      "Surface training t=34115, loss=0.013432700652629137\n",
      "Surface training t=34116, loss=0.01648680306971073\n",
      "Surface training t=34117, loss=0.013172085396945477\n",
      "Surface training t=34118, loss=0.015470641665160656\n",
      "Surface training t=34119, loss=0.01285389345139265\n",
      "Surface training t=34120, loss=0.013679401949048042\n",
      "Surface training t=34121, loss=0.013025203254073858\n",
      "Surface training t=34122, loss=0.013218894600868225\n",
      "Surface training t=34123, loss=0.011555423028767109\n",
      "Surface training t=34124, loss=0.015486455522477627\n",
      "Surface training t=34125, loss=0.01461184537038207\n",
      "Surface training t=34126, loss=0.021402185782790184\n",
      "Surface training t=34127, loss=0.016427586786448956\n",
      "Surface training t=34128, loss=0.01969288196414709\n",
      "Surface training t=34129, loss=0.020327751524746418\n",
      "Surface training t=34130, loss=0.014363684691488743\n",
      "Surface training t=34131, loss=0.017553243786096573\n",
      "Surface training t=34132, loss=0.016565189696848392\n",
      "Surface training t=34133, loss=0.024892592802643776\n",
      "Surface training t=34134, loss=0.024601757526397705\n",
      "Surface training t=34135, loss=0.020659269765019417\n",
      "Surface training t=34136, loss=0.024444599635899067\n",
      "Surface training t=34137, loss=0.02241372410207987\n",
      "Surface training t=34138, loss=0.025886457413434982\n",
      "Surface training t=34139, loss=0.019787127152085304\n",
      "Surface training t=34140, loss=0.020588891580700874\n",
      "Surface training t=34141, loss=0.02036351803690195\n",
      "Surface training t=34142, loss=0.016401643864810467\n",
      "Surface training t=34143, loss=0.01929608080536127\n",
      "Surface training t=34144, loss=0.016794837079942226\n",
      "Surface training t=34145, loss=0.018226285930722952\n",
      "Surface training t=34146, loss=0.01583843119442463\n",
      "Surface training t=34147, loss=0.01877110544592142\n",
      "Surface training t=34148, loss=0.015467802062630653\n",
      "Surface training t=34149, loss=0.015227546449750662\n",
      "Surface training t=34150, loss=0.016804849728941917\n",
      "Surface training t=34151, loss=0.014624260365962982\n",
      "Surface training t=34152, loss=0.019875943660736084\n",
      "Surface training t=34153, loss=0.021765248849987984\n",
      "Surface training t=34154, loss=0.020309866406023502\n",
      "Surface training t=34155, loss=0.0221082316711545\n",
      "Surface training t=34156, loss=0.02811717428267002\n",
      "Surface training t=34157, loss=0.024711497128009796\n",
      "Surface training t=34158, loss=0.025918112136423588\n",
      "Surface training t=34159, loss=0.025430040434002876\n",
      "Surface training t=34160, loss=0.03500865399837494\n",
      "Surface training t=34161, loss=0.02520050387829542\n",
      "Surface training t=34162, loss=0.02740646433085203\n",
      "Surface training t=34163, loss=0.02184177841991186\n",
      "Surface training t=34164, loss=0.02957436442375183\n",
      "Surface training t=34165, loss=0.01915216725319624\n",
      "Surface training t=34166, loss=0.0198441743850708\n",
      "Surface training t=34167, loss=0.025496304966509342\n",
      "Surface training t=34168, loss=0.025572072714567184\n",
      "Surface training t=34169, loss=0.021354565396904945\n",
      "Surface training t=34170, loss=0.02347063645720482\n",
      "Surface training t=34171, loss=0.021246508695185184\n",
      "Surface training t=34172, loss=0.019236942753195763\n",
      "Surface training t=34173, loss=0.022310344502329826\n",
      "Surface training t=34174, loss=0.016380439046770334\n",
      "Surface training t=34175, loss=0.018334814347326756\n",
      "Surface training t=34176, loss=0.017659218981862068\n",
      "Surface training t=34177, loss=0.014251899905502796\n",
      "Surface training t=34178, loss=0.015594687312841415\n",
      "Surface training t=34179, loss=0.018162779975682497\n",
      "Surface training t=34180, loss=0.01647692173719406\n",
      "Surface training t=34181, loss=0.01457703672349453\n",
      "Surface training t=34182, loss=0.01689992006868124\n",
      "Surface training t=34183, loss=0.018512162379920483\n",
      "Surface training t=34184, loss=0.018225140869617462\n",
      "Surface training t=34185, loss=0.013676142320036888\n",
      "Surface training t=34186, loss=0.016387395560741425\n",
      "Surface training t=34187, loss=0.011829674243927002\n",
      "Surface training t=34188, loss=0.019544238224625587\n",
      "Surface training t=34189, loss=0.019351696595549583\n",
      "Surface training t=34190, loss=0.022752287797629833\n",
      "Surface training t=34191, loss=0.02647669054567814\n",
      "Surface training t=34192, loss=0.02060478925704956\n",
      "Surface training t=34193, loss=0.01862423773854971\n",
      "Surface training t=34194, loss=0.021940546110272408\n",
      "Surface training t=34195, loss=0.022183937951922417\n",
      "Surface training t=34196, loss=0.01534073194488883\n",
      "Surface training t=34197, loss=0.016650325618684292\n",
      "Surface training t=34198, loss=0.013811897486448288\n",
      "Surface training t=34199, loss=0.01670765271410346\n",
      "Surface training t=34200, loss=0.015614157542586327\n",
      "Surface training t=34201, loss=0.01542760245501995\n",
      "Surface training t=34202, loss=0.022223515436053276\n",
      "Surface training t=34203, loss=0.021307590417563915\n",
      "Surface training t=34204, loss=0.017042684368789196\n",
      "Surface training t=34205, loss=0.02132421638816595\n",
      "Surface training t=34206, loss=0.015713076572865248\n",
      "Surface training t=34207, loss=0.01641685562208295\n",
      "Surface training t=34208, loss=0.01588854333385825\n",
      "Surface training t=34209, loss=0.014185766689479351\n",
      "Surface training t=34210, loss=0.018194924108684063\n",
      "Surface training t=34211, loss=0.014387782663106918\n",
      "Surface training t=34212, loss=0.018365921452641487\n",
      "Surface training t=34213, loss=0.015302211977541447\n",
      "Surface training t=34214, loss=0.018455583602190018\n",
      "Surface training t=34215, loss=0.020680255256593227\n",
      "Surface training t=34216, loss=0.01999380998313427\n",
      "Surface training t=34217, loss=0.01803413638845086\n",
      "Surface training t=34218, loss=0.02724958211183548\n",
      "Surface training t=34219, loss=0.02859248500317335\n",
      "Surface training t=34220, loss=0.028526916168630123\n",
      "Surface training t=34221, loss=0.02323049120604992\n",
      "Surface training t=34222, loss=0.020644129253923893\n",
      "Surface training t=34223, loss=0.018691600300371647\n",
      "Surface training t=34224, loss=0.014571749605238438\n",
      "Surface training t=34225, loss=0.019613695330917835\n",
      "Surface training t=34226, loss=0.019389387220144272\n",
      "Surface training t=34227, loss=0.01604278478771448\n",
      "Surface training t=34228, loss=0.025825630873441696\n",
      "Surface training t=34229, loss=0.018369211815297604\n",
      "Surface training t=34230, loss=0.016562649980187416\n",
      "Surface training t=34231, loss=0.016070563811808825\n",
      "Surface training t=34232, loss=0.015583297703415155\n",
      "Surface training t=34233, loss=0.015217258129268885\n",
      "Surface training t=34234, loss=0.017288542352616787\n",
      "Surface training t=34235, loss=0.014199194498360157\n",
      "Surface training t=34236, loss=0.01176306139677763\n",
      "Surface training t=34237, loss=0.014074536506086588\n",
      "Surface training t=34238, loss=0.014157744124531746\n",
      "Surface training t=34239, loss=0.012630724348127842\n",
      "Surface training t=34240, loss=0.017108125612139702\n",
      "Surface training t=34241, loss=0.015431283507496119\n",
      "Surface training t=34242, loss=0.018857039511203766\n",
      "Surface training t=34243, loss=0.018925098702311516\n",
      "Surface training t=34244, loss=0.017940371297299862\n",
      "Surface training t=34245, loss=0.016246123239398003\n",
      "Surface training t=34246, loss=0.015775995794683695\n",
      "Surface training t=34247, loss=0.022376169450581074\n",
      "Surface training t=34248, loss=0.015017140656709671\n",
      "Surface training t=34249, loss=0.0150606669485569\n",
      "Surface training t=34250, loss=0.018197016790509224\n",
      "Surface training t=34251, loss=0.019614812918007374\n",
      "Surface training t=34252, loss=0.026169179007411003\n",
      "Surface training t=34253, loss=0.023946503177285194\n",
      "Surface training t=34254, loss=0.023133717477321625\n",
      "Surface training t=34255, loss=0.020102031528949738\n",
      "Surface training t=34256, loss=0.023796672001481056\n",
      "Surface training t=34257, loss=0.016210408881306648\n",
      "Surface training t=34258, loss=0.0189182311296463\n",
      "Surface training t=34259, loss=0.018052438739687204\n",
      "Surface training t=34260, loss=0.020058651454746723\n",
      "Surface training t=34261, loss=0.021989055909216404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=34262, loss=0.017505016177892685\n",
      "Surface training t=34263, loss=0.017804048024117947\n",
      "Surface training t=34264, loss=0.018916345201432705\n",
      "Surface training t=34265, loss=0.021994550712406635\n",
      "Surface training t=34266, loss=0.020437130704522133\n",
      "Surface training t=34267, loss=0.0226828558370471\n",
      "Surface training t=34268, loss=0.019050780683755875\n",
      "Surface training t=34269, loss=0.02762970421463251\n",
      "Surface training t=34270, loss=0.020742314867675304\n",
      "Surface training t=34271, loss=0.017441234551370144\n",
      "Surface training t=34272, loss=0.022512227296829224\n",
      "Surface training t=34273, loss=0.01674721809104085\n",
      "Surface training t=34274, loss=0.022592450492084026\n",
      "Surface training t=34275, loss=0.016205021180212498\n",
      "Surface training t=34276, loss=0.016641576774418354\n",
      "Surface training t=34277, loss=0.02339806966483593\n",
      "Surface training t=34278, loss=0.01693771965801716\n",
      "Surface training t=34279, loss=0.02201105933636427\n",
      "Surface training t=34280, loss=0.0185519652441144\n",
      "Surface training t=34281, loss=0.020432429388165474\n",
      "Surface training t=34282, loss=0.026331444270908833\n",
      "Surface training t=34283, loss=0.02194926142692566\n",
      "Surface training t=34284, loss=0.02333253249526024\n",
      "Surface training t=34285, loss=0.019378235563635826\n",
      "Surface training t=34286, loss=0.018407193012535572\n",
      "Surface training t=34287, loss=0.02095351368188858\n",
      "Surface training t=34288, loss=0.02395317144691944\n",
      "Surface training t=34289, loss=0.02715054713189602\n",
      "Surface training t=34290, loss=0.01663672085851431\n",
      "Surface training t=34291, loss=0.020758316852152348\n",
      "Surface training t=34292, loss=0.01527906209230423\n",
      "Surface training t=34293, loss=0.013663283549249172\n",
      "Surface training t=34294, loss=0.019578583538532257\n",
      "Surface training t=34295, loss=0.02064657863229513\n",
      "Surface training t=34296, loss=0.021268938668072224\n",
      "Surface training t=34297, loss=0.016005510464310646\n",
      "Surface training t=34298, loss=0.01554601127281785\n",
      "Surface training t=34299, loss=0.015575894620269537\n",
      "Surface training t=34300, loss=0.01352080749347806\n",
      "Surface training t=34301, loss=0.01312793605029583\n",
      "Surface training t=34302, loss=0.017773413099348545\n",
      "Surface training t=34303, loss=0.01941156294196844\n",
      "Surface training t=34304, loss=0.01741348672658205\n",
      "Surface training t=34305, loss=0.02300538495182991\n",
      "Surface training t=34306, loss=0.025708877481520176\n",
      "Surface training t=34307, loss=0.019879368133842945\n",
      "Surface training t=34308, loss=0.025573656894266605\n",
      "Surface training t=34309, loss=0.02809378318488598\n",
      "Surface training t=34310, loss=0.02262065652757883\n",
      "Surface training t=34311, loss=0.02279053069651127\n",
      "Surface training t=34312, loss=0.024313845671713352\n",
      "Surface training t=34313, loss=0.017500280402600765\n",
      "Surface training t=34314, loss=0.020920886658132076\n",
      "Surface training t=34315, loss=0.02217748947441578\n",
      "Surface training t=34316, loss=0.02544842381030321\n",
      "Surface training t=34317, loss=0.026054739952087402\n",
      "Surface training t=34318, loss=0.019735392183065414\n",
      "Surface training t=34319, loss=0.018711412325501442\n",
      "Surface training t=34320, loss=0.022338684648275375\n",
      "Surface training t=34321, loss=0.018547206185758114\n",
      "Surface training t=34322, loss=0.02316938154399395\n",
      "Surface training t=34323, loss=0.02191110420972109\n",
      "Surface training t=34324, loss=0.019915428943932056\n",
      "Surface training t=34325, loss=0.016841151751577854\n",
      "Surface training t=34326, loss=0.01343993004411459\n",
      "Surface training t=34327, loss=0.01932049309834838\n",
      "Surface training t=34328, loss=0.020959319546818733\n",
      "Surface training t=34329, loss=0.01431957446038723\n",
      "Surface training t=34330, loss=0.018441383726894855\n",
      "Surface training t=34331, loss=0.01684614736586809\n",
      "Surface training t=34332, loss=0.012992052361369133\n",
      "Surface training t=34333, loss=0.021382136270403862\n",
      "Surface training t=34334, loss=0.015863155480474234\n",
      "Surface training t=34335, loss=0.028578348457813263\n",
      "Surface training t=34336, loss=0.01985004171729088\n",
      "Surface training t=34337, loss=0.017499711364507675\n",
      "Surface training t=34338, loss=0.021847414784133434\n",
      "Surface training t=34339, loss=0.017613699659705162\n",
      "Surface training t=34340, loss=0.015160765498876572\n",
      "Surface training t=34341, loss=0.013096556533128023\n",
      "Surface training t=34342, loss=0.014938736334443092\n",
      "Surface training t=34343, loss=0.01993278693407774\n",
      "Surface training t=34344, loss=0.017388037405908108\n",
      "Surface training t=34345, loss=0.015990047715604305\n",
      "Surface training t=34346, loss=0.01634345483034849\n",
      "Surface training t=34347, loss=0.021023515611886978\n",
      "Surface training t=34348, loss=0.023374658077955246\n",
      "Surface training t=34349, loss=0.01949666952714324\n",
      "Surface training t=34350, loss=0.01894544856622815\n",
      "Surface training t=34351, loss=0.03243215195834637\n",
      "Surface training t=34352, loss=0.020512381568551064\n",
      "Surface training t=34353, loss=0.018626480363309383\n",
      "Surface training t=34354, loss=0.02216539904475212\n",
      "Surface training t=34355, loss=0.023570900782942772\n",
      "Surface training t=34356, loss=0.02049544919282198\n",
      "Surface training t=34357, loss=0.025039962492883205\n",
      "Surface training t=34358, loss=0.023535236716270447\n",
      "Surface training t=34359, loss=0.01754997903481126\n",
      "Surface training t=34360, loss=0.020720165688544512\n",
      "Surface training t=34361, loss=0.031915306113660336\n",
      "Surface training t=34362, loss=0.02978681866079569\n",
      "Surface training t=34363, loss=0.026450506411492825\n",
      "Surface training t=34364, loss=0.022037643007934093\n",
      "Surface training t=34365, loss=0.02091251965612173\n",
      "Surface training t=34366, loss=0.021592730656266212\n",
      "Surface training t=34367, loss=0.018785938620567322\n",
      "Surface training t=34368, loss=0.022326448000967503\n",
      "Surface training t=34369, loss=0.022186757996678352\n",
      "Surface training t=34370, loss=0.02983028255403042\n",
      "Surface training t=34371, loss=0.024103090167045593\n",
      "Surface training t=34372, loss=0.026009399443864822\n",
      "Surface training t=34373, loss=0.021230186335742474\n",
      "Surface training t=34374, loss=0.02341720275580883\n",
      "Surface training t=34375, loss=0.020288207568228245\n",
      "Surface training t=34376, loss=0.016849172301590443\n",
      "Surface training t=34377, loss=0.01860393863171339\n",
      "Surface training t=34378, loss=0.01316682854667306\n",
      "Surface training t=34379, loss=0.01778957899659872\n",
      "Surface training t=34380, loss=0.014273162465542555\n",
      "Surface training t=34381, loss=0.01333967037498951\n",
      "Surface training t=34382, loss=0.01440578093752265\n",
      "Surface training t=34383, loss=0.012758233118802309\n",
      "Surface training t=34384, loss=0.016010764054954052\n",
      "Surface training t=34385, loss=0.01936205243691802\n",
      "Surface training t=34386, loss=0.02236692700535059\n",
      "Surface training t=34387, loss=0.022305598482489586\n",
      "Surface training t=34388, loss=0.01812645234167576\n",
      "Surface training t=34389, loss=0.025906802155077457\n",
      "Surface training t=34390, loss=0.016966570168733597\n",
      "Surface training t=34391, loss=0.017766382545232773\n",
      "Surface training t=34392, loss=0.028560115955770016\n",
      "Surface training t=34393, loss=0.022431187331676483\n",
      "Surface training t=34394, loss=0.02247348614037037\n",
      "Surface training t=34395, loss=0.018059725873172283\n",
      "Surface training t=34396, loss=0.018803873099386692\n",
      "Surface training t=34397, loss=0.028069057501852512\n",
      "Surface training t=34398, loss=0.022782851941883564\n",
      "Surface training t=34399, loss=0.021566515788435936\n",
      "Surface training t=34400, loss=0.023832054808735847\n",
      "Surface training t=34401, loss=0.02032522391527891\n",
      "Surface training t=34402, loss=0.024153157137334347\n",
      "Surface training t=34403, loss=0.020540348254144192\n",
      "Surface training t=34404, loss=0.01838546246290207\n",
      "Surface training t=34405, loss=0.026608348824083805\n",
      "Surface training t=34406, loss=0.023429746739566326\n",
      "Surface training t=34407, loss=0.01618551230058074\n",
      "Surface training t=34408, loss=0.016331826336681843\n",
      "Surface training t=34409, loss=0.016666628420352936\n",
      "Surface training t=34410, loss=0.012018265202641487\n",
      "Surface training t=34411, loss=0.01584492437541485\n",
      "Surface training t=34412, loss=0.018773164600133896\n",
      "Surface training t=34413, loss=0.013995234854519367\n",
      "Surface training t=34414, loss=0.015838684514164925\n",
      "Surface training t=34415, loss=0.011914811097085476\n",
      "Surface training t=34416, loss=0.017713190987706184\n",
      "Surface training t=34417, loss=0.016813830938190222\n",
      "Surface training t=34418, loss=0.017310502473264933\n",
      "Surface training t=34419, loss=0.020492377690970898\n",
      "Surface training t=34420, loss=0.021012088283896446\n",
      "Surface training t=34421, loss=0.02940718550235033\n",
      "Surface training t=34422, loss=0.026696096174418926\n",
      "Surface training t=34423, loss=0.021332849748432636\n",
      "Surface training t=34424, loss=0.020924540236592293\n",
      "Surface training t=34425, loss=0.02023478876799345\n",
      "Surface training t=34426, loss=0.01747103128582239\n",
      "Surface training t=34427, loss=0.030129048973321915\n",
      "Surface training t=34428, loss=0.024493038654327393\n",
      "Surface training t=34429, loss=0.02767643705010414\n",
      "Surface training t=34430, loss=0.029870244674384594\n",
      "Surface training t=34431, loss=0.024554098956286907\n",
      "Surface training t=34432, loss=0.03322239965200424\n",
      "Surface training t=34433, loss=0.01706588175147772\n",
      "Surface training t=34434, loss=0.012599281035363674\n",
      "Surface training t=34435, loss=0.01607399992644787\n",
      "Surface training t=34436, loss=0.013733586762100458\n",
      "Surface training t=34437, loss=0.01474725129082799\n",
      "Surface training t=34438, loss=0.016665246337652206\n",
      "Surface training t=34439, loss=0.015920435078442097\n",
      "Surface training t=34440, loss=0.014360559172928333\n",
      "Surface training t=34441, loss=0.020411483943462372\n",
      "Surface training t=34442, loss=0.025276804342865944\n",
      "Surface training t=34443, loss=0.025160188786685467\n",
      "Surface training t=34444, loss=0.01805876661092043\n",
      "Surface training t=34445, loss=0.021229072473943233\n",
      "Surface training t=34446, loss=0.0187287125736475\n",
      "Surface training t=34447, loss=0.017049037851393223\n",
      "Surface training t=34448, loss=0.016976994462311268\n",
      "Surface training t=34449, loss=0.01562500884756446\n",
      "Surface training t=34450, loss=0.016707617789506912\n",
      "Surface training t=34451, loss=0.013319301884621382\n",
      "Surface training t=34452, loss=0.01834613550454378\n",
      "Surface training t=34453, loss=0.011457661632448435\n",
      "Surface training t=34454, loss=0.014927594922482967\n",
      "Surface training t=34455, loss=0.012915373779833317\n",
      "Surface training t=34456, loss=0.019123224541544914\n",
      "Surface training t=34457, loss=0.022386894561350346\n",
      "Surface training t=34458, loss=0.014942727982997894\n",
      "Surface training t=34459, loss=0.0174404950812459\n",
      "Surface training t=34460, loss=0.018611645326018333\n",
      "Surface training t=34461, loss=0.02232478093355894\n",
      "Surface training t=34462, loss=0.019118253141641617\n",
      "Surface training t=34463, loss=0.021047329530119896\n",
      "Surface training t=34464, loss=0.01998698292300105\n",
      "Surface training t=34465, loss=0.022627951577305794\n",
      "Surface training t=34466, loss=0.018321430310606956\n",
      "Surface training t=34467, loss=0.02161341067403555\n",
      "Surface training t=34468, loss=0.021527398377656937\n",
      "Surface training t=34469, loss=0.01986122550442815\n",
      "Surface training t=34470, loss=0.019587043672800064\n",
      "Surface training t=34471, loss=0.013553069438785315\n",
      "Surface training t=34472, loss=0.015402314718812704\n",
      "Surface training t=34473, loss=0.013725955970585346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=34474, loss=0.013869647402316332\n",
      "Surface training t=34475, loss=0.015569649171084166\n",
      "Surface training t=34476, loss=0.01749298255890608\n",
      "Surface training t=34477, loss=0.016140829771757126\n",
      "Surface training t=34478, loss=0.015289805829524994\n",
      "Surface training t=34479, loss=0.017187023535370827\n",
      "Surface training t=34480, loss=0.0178162707015872\n",
      "Surface training t=34481, loss=0.016607508063316345\n",
      "Surface training t=34482, loss=0.011926589999347925\n",
      "Surface training t=34483, loss=0.02001266460865736\n",
      "Surface training t=34484, loss=0.017184000462293625\n",
      "Surface training t=34485, loss=0.018349834717810154\n",
      "Surface training t=34486, loss=0.019424221478402615\n",
      "Surface training t=34487, loss=0.03145209886133671\n",
      "Surface training t=34488, loss=0.020672205835580826\n",
      "Surface training t=34489, loss=0.024265854619443417\n",
      "Surface training t=34490, loss=0.02115636970847845\n",
      "Surface training t=34491, loss=0.019695489667356014\n",
      "Surface training t=34492, loss=0.01715986616909504\n",
      "Surface training t=34493, loss=0.022927550598978996\n",
      "Surface training t=34494, loss=0.01825634203851223\n",
      "Surface training t=34495, loss=0.01921779103577137\n",
      "Surface training t=34496, loss=0.01620945706963539\n",
      "Surface training t=34497, loss=0.021142530255019665\n",
      "Surface training t=34498, loss=0.02274950034916401\n",
      "Surface training t=34499, loss=0.020092913880944252\n",
      "Surface training t=34500, loss=0.022704618982970715\n",
      "Surface training t=34501, loss=0.023053397424519062\n",
      "Surface training t=34502, loss=0.016953233163803816\n",
      "Surface training t=34503, loss=0.020408358424901962\n",
      "Surface training t=34504, loss=0.016718626022338867\n",
      "Surface training t=34505, loss=0.01840299926698208\n",
      "Surface training t=34506, loss=0.017193731386214495\n",
      "Surface training t=34507, loss=0.015884730499237776\n",
      "Surface training t=34508, loss=0.015434322878718376\n",
      "Surface training t=34509, loss=0.017592248506844044\n",
      "Surface training t=34510, loss=0.017248221673071384\n",
      "Surface training t=34511, loss=0.017872276715934277\n",
      "Surface training t=34512, loss=0.020725484006106853\n",
      "Surface training t=34513, loss=0.017216460313647985\n",
      "Surface training t=34514, loss=0.01752778934314847\n",
      "Surface training t=34515, loss=0.017110696993768215\n",
      "Surface training t=34516, loss=0.019267989322543144\n",
      "Surface training t=34517, loss=0.02121692430227995\n",
      "Surface training t=34518, loss=0.02073546266183257\n",
      "Surface training t=34519, loss=0.020216073840856552\n",
      "Surface training t=34520, loss=0.025977790355682373\n",
      "Surface training t=34521, loss=0.02217742893844843\n",
      "Surface training t=34522, loss=0.020486363675445318\n",
      "Surface training t=34523, loss=0.023980013094842434\n",
      "Surface training t=34524, loss=0.022818869911134243\n",
      "Surface training t=34525, loss=0.02365193236619234\n",
      "Surface training t=34526, loss=0.022166620939970016\n",
      "Surface training t=34527, loss=0.0250181145966053\n",
      "Surface training t=34528, loss=0.024020408280193806\n",
      "Surface training t=34529, loss=0.02483164705336094\n",
      "Surface training t=34530, loss=0.018135041929781437\n",
      "Surface training t=34531, loss=0.02087950985878706\n",
      "Surface training t=34532, loss=0.02977157663553953\n",
      "Surface training t=34533, loss=0.021826217882335186\n",
      "Surface training t=34534, loss=0.026972340419888496\n",
      "Surface training t=34535, loss=0.020347364246845245\n",
      "Surface training t=34536, loss=0.025944851338863373\n",
      "Surface training t=34537, loss=0.01829792745411396\n",
      "Surface training t=34538, loss=0.01885658409446478\n",
      "Surface training t=34539, loss=0.018624790012836456\n",
      "Surface training t=34540, loss=0.01458226004615426\n",
      "Surface training t=34541, loss=0.0194504177197814\n",
      "Surface training t=34542, loss=0.01798576395958662\n",
      "Surface training t=34543, loss=0.020371776074171066\n",
      "Surface training t=34544, loss=0.018592690583318472\n",
      "Surface training t=34545, loss=0.020735392346978188\n",
      "Surface training t=34546, loss=0.021665912121534348\n",
      "Surface training t=34547, loss=0.02257402241230011\n",
      "Surface training t=34548, loss=0.02815026417374611\n",
      "Surface training t=34549, loss=0.03181109577417374\n",
      "Surface training t=34550, loss=0.021619943901896477\n",
      "Surface training t=34551, loss=0.021401094272732735\n",
      "Surface training t=34552, loss=0.013452146202325821\n",
      "Surface training t=34553, loss=0.02033290732651949\n",
      "Surface training t=34554, loss=0.018943535163998604\n",
      "Surface training t=34555, loss=0.019224824383854866\n",
      "Surface training t=34556, loss=0.01650784071534872\n",
      "Surface training t=34557, loss=0.01441464526578784\n",
      "Surface training t=34558, loss=0.017026078887283802\n",
      "Surface training t=34559, loss=0.017627092078328133\n",
      "Surface training t=34560, loss=0.015507693402469158\n",
      "Surface training t=34561, loss=0.014271079562604427\n",
      "Surface training t=34562, loss=0.017917374148964882\n",
      "Surface training t=34563, loss=0.013978204224258661\n",
      "Surface training t=34564, loss=0.015004675835371017\n",
      "Surface training t=34565, loss=0.017282925080507994\n",
      "Surface training t=34566, loss=0.01333639770746231\n",
      "Surface training t=34567, loss=0.01281007332727313\n",
      "Surface training t=34568, loss=0.012120299972593784\n",
      "Surface training t=34569, loss=0.016445976682007313\n",
      "Surface training t=34570, loss=0.011679815594106913\n",
      "Surface training t=34571, loss=0.01604748610407114\n",
      "Surface training t=34572, loss=0.01296300534158945\n",
      "Surface training t=34573, loss=0.014550656080245972\n",
      "Surface training t=34574, loss=0.010948569979518652\n",
      "Surface training t=34575, loss=0.013971386011689901\n",
      "Surface training t=34576, loss=0.01643161801621318\n",
      "Surface training t=34577, loss=0.021798860281705856\n",
      "Surface training t=34578, loss=0.020126007962971926\n",
      "Surface training t=34579, loss=0.024844381026923656\n",
      "Surface training t=34580, loss=0.02367325872182846\n",
      "Surface training t=34581, loss=0.032067738473415375\n",
      "Surface training t=34582, loss=0.02677349280565977\n",
      "Surface training t=34583, loss=0.025586777366697788\n",
      "Surface training t=34584, loss=0.024798192083835602\n",
      "Surface training t=34585, loss=0.02673314232379198\n",
      "Surface training t=34586, loss=0.02792196534574032\n",
      "Surface training t=34587, loss=0.018019800540059805\n",
      "Surface training t=34588, loss=0.032240135595202446\n",
      "Surface training t=34589, loss=0.026313416659832\n",
      "Surface training t=34590, loss=0.027951800264418125\n",
      "Surface training t=34591, loss=0.01895781233906746\n",
      "Surface training t=34592, loss=0.013344605453312397\n",
      "Surface training t=34593, loss=0.016080768313258886\n",
      "Surface training t=34594, loss=0.01518542179837823\n",
      "Surface training t=34595, loss=0.022558841854333878\n",
      "Surface training t=34596, loss=0.02068316377699375\n",
      "Surface training t=34597, loss=0.024964408949017525\n",
      "Surface training t=34598, loss=0.024952154606580734\n",
      "Surface training t=34599, loss=0.018895119428634644\n",
      "Surface training t=34600, loss=0.01938935648649931\n",
      "Surface training t=34601, loss=0.017079665791243315\n",
      "Surface training t=34602, loss=0.01595912780612707\n",
      "Surface training t=34603, loss=0.012221353594213724\n",
      "Surface training t=34604, loss=0.015994082670658827\n",
      "Surface training t=34605, loss=0.017335542012006044\n",
      "Surface training t=34606, loss=0.018139651976525784\n",
      "Surface training t=34607, loss=0.013492506928741932\n",
      "Surface training t=34608, loss=0.016849488019943237\n",
      "Surface training t=34609, loss=0.016334827058017254\n",
      "Surface training t=34610, loss=0.015250862576067448\n",
      "Surface training t=34611, loss=0.015795189421623945\n",
      "Surface training t=34612, loss=0.0139733268879354\n",
      "Surface training t=34613, loss=0.014052562415599823\n",
      "Surface training t=34614, loss=0.01833695825189352\n",
      "Surface training t=34615, loss=0.020280794240534306\n",
      "Surface training t=34616, loss=0.016348015516996384\n",
      "Surface training t=34617, loss=0.019153531175106764\n",
      "Surface training t=34618, loss=0.021637135185301304\n",
      "Surface training t=34619, loss=0.025052612647414207\n",
      "Surface training t=34620, loss=0.02029071655124426\n",
      "Surface training t=34621, loss=0.026354688219726086\n",
      "Surface training t=34622, loss=0.028193820267915726\n",
      "Surface training t=34623, loss=0.01435898244380951\n",
      "Surface training t=34624, loss=0.01914292573928833\n",
      "Surface training t=34625, loss=0.021916847676038742\n",
      "Surface training t=34626, loss=0.02068578079342842\n",
      "Surface training t=34627, loss=0.018939722329378128\n",
      "Surface training t=34628, loss=0.02123998012393713\n",
      "Surface training t=34629, loss=0.017752202227711678\n",
      "Surface training t=34630, loss=0.01657275902107358\n",
      "Surface training t=34631, loss=0.016391875222325325\n",
      "Surface training t=34632, loss=0.01889688428491354\n",
      "Surface training t=34633, loss=0.01739432755857706\n",
      "Surface training t=34634, loss=0.020475552417337894\n",
      "Surface training t=34635, loss=0.013058794662356377\n",
      "Surface training t=34636, loss=0.01965885329991579\n",
      "Surface training t=34637, loss=0.021751313470304012\n",
      "Surface training t=34638, loss=0.016739755868911743\n",
      "Surface training t=34639, loss=0.022306805476546288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=34640, loss=0.02670437004417181\n",
      "Surface training t=34641, loss=0.02592182345688343\n",
      "Surface training t=34642, loss=0.01766217779368162\n",
      "Surface training t=34643, loss=0.01647672150284052\n",
      "Surface training t=34644, loss=0.017342657782137394\n",
      "Surface training t=34645, loss=0.016888443380594254\n",
      "Surface training t=34646, loss=0.01902626547962427\n",
      "Surface training t=34647, loss=0.021278519183397293\n",
      "Surface training t=34648, loss=0.020899550057947636\n",
      "Surface training t=34649, loss=0.02236087527126074\n",
      "Surface training t=34650, loss=0.023427557200193405\n",
      "Surface training t=34651, loss=0.024634527042508125\n",
      "Surface training t=34652, loss=0.02276471257209778\n",
      "Surface training t=34653, loss=0.025206537917256355\n",
      "Surface training t=34654, loss=0.022291592322289944\n",
      "Surface training t=34655, loss=0.022113224491477013\n",
      "Surface training t=34656, loss=0.02107603568583727\n",
      "Surface training t=34657, loss=0.023097416386008263\n",
      "Surface training t=34658, loss=0.017855998128652573\n",
      "Surface training t=34659, loss=0.021930625662207603\n",
      "Surface training t=34660, loss=0.01901340950280428\n",
      "Surface training t=34661, loss=0.02012214669957757\n",
      "Surface training t=34662, loss=0.01634560152888298\n",
      "Surface training t=34663, loss=0.025629130192101002\n",
      "Surface training t=34664, loss=0.019657340832054615\n",
      "Surface training t=34665, loss=0.01702371332794428\n",
      "Surface training t=34666, loss=0.019430329091846943\n",
      "Surface training t=34667, loss=0.020847653038799763\n",
      "Surface training t=34668, loss=0.014984112698584795\n",
      "Surface training t=34669, loss=0.01764107681810856\n",
      "Surface training t=34670, loss=0.018107435666024685\n",
      "Surface training t=34671, loss=0.0195637047290802\n",
      "Surface training t=34672, loss=0.020192111376672983\n",
      "Surface training t=34673, loss=0.022491078823804855\n",
      "Surface training t=34674, loss=0.023348144255578518\n",
      "Surface training t=34675, loss=0.020877785980701447\n",
      "Surface training t=34676, loss=0.018019639421254396\n",
      "Surface training t=34677, loss=0.015227561350911856\n",
      "Surface training t=34678, loss=0.02009679563343525\n",
      "Surface training t=34679, loss=0.018699809908866882\n",
      "Surface training t=34680, loss=0.017485685646533966\n",
      "Surface training t=34681, loss=0.01820471603423357\n",
      "Surface training t=34682, loss=0.02181190811097622\n",
      "Surface training t=34683, loss=0.01967803854495287\n",
      "Surface training t=34684, loss=0.03981282003223896\n",
      "Surface training t=34685, loss=0.03288835473358631\n",
      "Surface training t=34686, loss=0.039884088560938835\n",
      "Surface training t=34687, loss=0.033237216994166374\n",
      "Surface training t=34688, loss=0.036988476291298866\n",
      "Surface training t=34689, loss=0.024020216427743435\n",
      "Surface training t=34690, loss=0.027081750333309174\n",
      "Surface training t=34691, loss=0.03203068673610687\n",
      "Surface training t=34692, loss=0.026685486547648907\n",
      "Surface training t=34693, loss=0.03020607866346836\n",
      "Surface training t=34694, loss=0.02035706490278244\n",
      "Surface training t=34695, loss=0.023681730963289738\n",
      "Surface training t=34696, loss=0.025479079224169254\n",
      "Surface training t=34697, loss=0.021302073262631893\n",
      "Surface training t=34698, loss=0.02014964632689953\n",
      "Surface training t=34699, loss=0.024544970132410526\n",
      "Surface training t=34700, loss=0.02194041758775711\n",
      "Surface training t=34701, loss=0.019681585021317005\n",
      "Surface training t=34702, loss=0.01768049318343401\n",
      "Surface training t=34703, loss=0.016701129265129566\n",
      "Surface training t=34704, loss=0.020533524453639984\n",
      "Surface training t=34705, loss=0.02102417405694723\n",
      "Surface training t=34706, loss=0.016636420972645283\n",
      "Surface training t=34707, loss=0.014048694632947445\n",
      "Surface training t=34708, loss=0.0160637809894979\n",
      "Surface training t=34709, loss=0.0204179547727108\n",
      "Surface training t=34710, loss=0.01831031357869506\n",
      "Surface training t=34711, loss=0.026013446040451527\n",
      "Surface training t=34712, loss=0.028452463448047638\n",
      "Surface training t=34713, loss=0.02361073810607195\n",
      "Surface training t=34714, loss=0.03442319482564926\n",
      "Surface training t=34715, loss=0.02253655530512333\n",
      "Surface training t=34716, loss=0.017612903378903866\n",
      "Surface training t=34717, loss=0.021396014839410782\n",
      "Surface training t=34718, loss=0.016939330846071243\n",
      "Surface training t=34719, loss=0.017997180111706257\n",
      "Surface training t=34720, loss=0.02388626802712679\n",
      "Surface training t=34721, loss=0.01346151065081358\n",
      "Surface training t=34722, loss=0.021494065411388874\n",
      "Surface training t=34723, loss=0.019882116466760635\n",
      "Surface training t=34724, loss=0.016355253756046295\n",
      "Surface training t=34725, loss=0.01676286105066538\n",
      "Surface training t=34726, loss=0.0150191611610353\n",
      "Surface training t=34727, loss=0.012026547454297543\n",
      "Surface training t=34728, loss=0.0215064762160182\n",
      "Surface training t=34729, loss=0.019956136122345924\n",
      "Surface training t=34730, loss=0.017062275670468807\n",
      "Surface training t=34731, loss=0.018849891610443592\n",
      "Surface training t=34732, loss=0.024210317060351372\n",
      "Surface training t=34733, loss=0.021364516578614712\n",
      "Surface training t=34734, loss=0.022302682511508465\n",
      "Surface training t=34735, loss=0.021189013496041298\n",
      "Surface training t=34736, loss=0.029937097802758217\n",
      "Surface training t=34737, loss=0.026991155929863453\n",
      "Surface training t=34738, loss=0.027540245093405247\n",
      "Surface training t=34739, loss=0.0390955600887537\n",
      "Surface training t=34740, loss=0.032434580847620964\n",
      "Surface training t=34741, loss=0.027878996916115284\n",
      "Surface training t=34742, loss=0.02458254899829626\n",
      "Surface training t=34743, loss=0.022835384123027325\n",
      "Surface training t=34744, loss=0.028033350594341755\n",
      "Surface training t=34745, loss=0.022333715576678514\n",
      "Surface training t=34746, loss=0.033045087940990925\n",
      "Surface training t=34747, loss=0.033940703608095646\n",
      "Surface training t=34748, loss=0.024181711487472057\n",
      "Surface training t=34749, loss=0.020388627890497446\n",
      "Surface training t=34750, loss=0.02109778020530939\n",
      "Surface training t=34751, loss=0.025021099485456944\n",
      "Surface training t=34752, loss=0.02176574617624283\n",
      "Surface training t=34753, loss=0.02595026884227991\n",
      "Surface training t=34754, loss=0.018347712233662605\n",
      "Surface training t=34755, loss=0.017918448895215988\n",
      "Surface training t=34756, loss=0.019120446406304836\n",
      "Surface training t=34757, loss=0.017949257045984268\n",
      "Surface training t=34758, loss=0.015040923841297626\n",
      "Surface training t=34759, loss=0.02107108384370804\n",
      "Surface training t=34760, loss=0.01902313344180584\n",
      "Surface training t=34761, loss=0.020692845806479454\n",
      "Surface training t=34762, loss=0.01613552402704954\n",
      "Surface training t=34763, loss=0.01595690194517374\n",
      "Surface training t=34764, loss=0.014164620079100132\n",
      "Surface training t=34765, loss=0.019396232441067696\n",
      "Surface training t=34766, loss=0.014317998196929693\n",
      "Surface training t=34767, loss=0.018075804226100445\n",
      "Surface training t=34768, loss=0.02252427488565445\n",
      "Surface training t=34769, loss=0.019050419330596924\n",
      "Surface training t=34770, loss=0.0195014625787735\n",
      "Surface training t=34771, loss=0.03026948869228363\n",
      "Surface training t=34772, loss=0.02109114918857813\n",
      "Surface training t=34773, loss=0.031699489802122116\n",
      "Surface training t=34774, loss=0.02252760250121355\n",
      "Surface training t=34775, loss=0.01933224219828844\n",
      "Surface training t=34776, loss=0.021271858364343643\n",
      "Surface training t=34777, loss=0.021241863258183002\n",
      "Surface training t=34778, loss=0.02159243542701006\n",
      "Surface training t=34779, loss=0.018400960601866245\n",
      "Surface training t=34780, loss=0.02305808011442423\n",
      "Surface training t=34781, loss=0.027477937750518322\n",
      "Surface training t=34782, loss=0.021660836413502693\n",
      "Surface training t=34783, loss=0.02945487666875124\n",
      "Surface training t=34784, loss=0.0199508061632514\n",
      "Surface training t=34785, loss=0.018182051833719015\n",
      "Surface training t=34786, loss=0.018946579657495022\n",
      "Surface training t=34787, loss=0.023377404548227787\n",
      "Surface training t=34788, loss=0.029855611734092236\n",
      "Surface training t=34789, loss=0.024717964231967926\n",
      "Surface training t=34790, loss=0.024733704514801502\n",
      "Surface training t=34791, loss=0.021980736404657364\n",
      "Surface training t=34792, loss=0.02718358486890793\n",
      "Surface training t=34793, loss=0.02228925283998251\n",
      "Surface training t=34794, loss=0.024554332718253136\n",
      "Surface training t=34795, loss=0.030195255763828754\n",
      "Surface training t=34796, loss=0.0271902815438807\n",
      "Surface training t=34797, loss=0.03515850193798542\n",
      "Surface training t=34798, loss=0.028261876665055752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surface training t=34799, loss=0.026117272675037384\n",
      "Surface training t=34800, loss=0.02433091588318348\n",
      "Surface training t=34801, loss=0.019477914087474346\n",
      "Surface training t=34802, loss=0.02539461199194193\n",
      "Surface training t=34803, loss=0.024719344452023506\n",
      "Surface training t=34804, loss=0.029927401803433895\n",
      "Surface training t=34805, loss=0.02640475705265999\n",
      "Surface training t=34806, loss=0.0248259324580431\n",
      "Surface training t=34807, loss=0.0335962176322937\n",
      "Surface training t=34808, loss=0.027604208327829838\n",
      "Surface training t=34809, loss=0.020849178545176983\n",
      "Surface training t=34810, loss=0.017007664777338505\n",
      "Surface training t=34811, loss=0.016392712481319904\n",
      "Surface training t=34812, loss=0.019100003875792027\n",
      "Surface training t=34813, loss=0.01667583966627717\n",
      "Surface training t=34814, loss=0.013074218295514584\n",
      "Surface training t=34815, loss=0.017193845473229885\n",
      "Surface training t=34816, loss=0.020351399667561054\n",
      "Surface training t=34817, loss=0.026148061268031597\n",
      "Surface training t=34818, loss=0.020209407433867455\n",
      "Surface training t=34819, loss=0.020887858234345913\n",
      "Surface training t=34820, loss=0.024946855381131172\n",
      "Surface training t=34821, loss=0.01661391742527485\n",
      "Surface training t=34822, loss=0.022057742811739445\n",
      "Surface training t=34823, loss=0.0176599295809865\n",
      "Surface training t=34824, loss=0.020229910500347614\n",
      "Surface training t=34825, loss=0.018029040656983852\n",
      "Surface training t=34826, loss=0.020440176129341125\n",
      "Surface training t=34827, loss=0.021527480334043503\n",
      "Surface training t=34828, loss=0.01585905533283949\n",
      "Surface training t=34829, loss=0.018570474348962307\n",
      "Surface training t=34830, loss=0.02239773515611887\n",
      "Surface training t=34831, loss=0.01987755997106433\n",
      "Surface training t=34832, loss=0.020827862434089184\n",
      "Surface training t=34833, loss=0.018411780707538128\n",
      "Surface training t=34834, loss=0.02025254536420107\n",
      "Surface training t=34835, loss=0.023701020516455173\n",
      "Surface training t=34836, loss=0.02269093319773674\n",
      "Surface training t=34837, loss=0.01464746380224824\n",
      "Surface training t=34838, loss=0.016715017147362232\n",
      "Surface training t=34839, loss=0.01519046537578106\n",
      "Surface training t=34840, loss=0.011867452412843704\n",
      "Surface training t=34841, loss=0.011636168230324984\n",
      "Surface training t=34842, loss=0.016591941937804222\n",
      "Surface training t=34843, loss=0.017768999561667442\n",
      "Surface training t=34844, loss=0.019599519670009613\n",
      "Surface training t=34845, loss=0.022462508641183376\n",
      "Surface training t=34846, loss=0.021687778644263744\n",
      "Surface training t=34847, loss=0.021037409082055092\n",
      "Surface training t=34848, loss=0.02439987752586603\n",
      "Surface training t=34849, loss=0.020578342489898205\n",
      "Surface training t=34850, loss=0.021962212398648262\n",
      "Surface training t=34851, loss=0.02482516411691904\n",
      "Surface training t=34852, loss=0.022764362394809723\n",
      "Surface training t=34853, loss=0.020060501992702484\n",
      "Surface training t=34854, loss=0.021213039755821228\n",
      "Surface training t=34855, loss=0.02755255252122879\n",
      "Surface training t=34856, loss=0.025652037002146244\n",
      "Surface training t=34857, loss=0.022367196157574654\n",
      "Surface training t=34858, loss=0.02783518936485052\n",
      "Surface training t=34859, loss=0.026177968829870224\n",
      "Surface training t=34860, loss=0.025765015743672848\n",
      "Surface training t=34861, loss=0.01916061621159315\n",
      "Surface training t=34862, loss=0.02376366127282381\n",
      "Surface training t=34863, loss=0.023841118440032005\n",
      "Surface training t=34864, loss=0.02124106790870428\n",
      "Surface training t=34865, loss=0.022221134044229984\n",
      "Surface training t=34866, loss=0.023387539200484753\n",
      "Surface training t=34867, loss=0.022847881074994802\n",
      "Surface training t=34868, loss=0.035439975559711456\n",
      "Surface training t=34869, loss=0.030222492292523384\n",
      "Surface training t=34870, loss=0.02362515637651086\n",
      "Surface training t=34871, loss=0.04176817834377289\n",
      "Surface training t=34872, loss=0.029200782999396324\n",
      "Surface training t=34873, loss=0.03440520726144314\n",
      "Surface training t=34874, loss=0.03161246422678232\n",
      "Surface training t=34875, loss=0.029994556680321693\n",
      "Surface training t=34876, loss=0.027251560240983963\n",
      "Surface training t=34877, loss=0.026241346262395382\n",
      "Surface training t=34878, loss=0.024917080998420715\n",
      "Surface training t=34879, loss=0.01993947010487318\n",
      "Surface training t=34880, loss=0.0188433974981308\n",
      "Surface training t=34881, loss=0.017753089778125286\n",
      "Surface training t=34882, loss=0.020525083877146244\n",
      "Surface training t=34883, loss=0.015861323568969965\n",
      "Surface training t=34884, loss=0.017822980880737305\n",
      "Surface training t=34885, loss=0.016211694106459618\n",
      "Surface training t=34886, loss=0.016968607902526855\n",
      "Surface training t=34887, loss=0.015408390201628208\n",
      "Surface training t=34888, loss=0.019920732360333204\n",
      "Surface training t=34889, loss=0.02389075979590416\n",
      "Surface training t=34890, loss=0.01864552777260542\n",
      "Surface training t=34891, loss=0.022887198254466057\n",
      "Surface training t=34892, loss=0.026346709579229355\n",
      "Surface training t=34893, loss=0.01846855878829956\n",
      "Surface training t=34894, loss=0.017730477266013622\n",
      "Surface training t=34895, loss=0.02771679125726223\n",
      "Surface training t=34896, loss=0.02473179902881384\n",
      "Surface training t=34897, loss=0.0231174873188138\n",
      "Surface training t=34898, loss=0.026910274289548397\n",
      "Surface training t=34899, loss=0.023727894760668278\n",
      "Surface training t=34900, loss=0.018904478289186954\n",
      "Surface training t=34901, loss=0.024552345275878906\n",
      "Surface training t=34902, loss=0.019812763668596745\n",
      "Surface training t=34903, loss=0.018290837295353413\n",
      "Surface training t=34904, loss=0.024894464761018753\n",
      "Surface training t=34905, loss=0.02104884944856167\n",
      "Surface training t=34906, loss=0.02574630081653595\n",
      "Surface training t=34907, loss=0.038464490324258804\n",
      "Surface training t=34908, loss=0.023784141056239605\n",
      "Surface training t=34909, loss=0.02195094246417284\n",
      "Surface training t=34910, loss=0.020893988199532032\n",
      "Surface training t=34911, loss=0.020913940854370594\n",
      "Surface training t=34912, loss=0.026544984430074692\n",
      "Surface training t=34913, loss=0.021656086668372154\n",
      "Surface training t=34914, loss=0.020359653048217297\n",
      "Surface training t=34915, loss=0.03135811723768711\n",
      "Surface training t=34916, loss=0.020910675637423992\n",
      "Surface training t=34917, loss=0.017749878577888012\n",
      "Surface training t=34918, loss=0.020222026854753494\n",
      "Surface training t=34919, loss=0.02393217198550701\n",
      "Surface training t=34920, loss=0.016548813320696354\n",
      "Surface training t=34921, loss=0.019879535771906376\n",
      "Surface training t=34922, loss=0.01920226402580738\n",
      "Surface training t=34923, loss=0.018716405145823956\n",
      "Surface training t=34924, loss=0.023366253823041916\n",
      "Surface training t=34925, loss=0.019327973946928978\n",
      "Surface training t=34926, loss=0.017683050595223904\n",
      "Surface training t=34927, loss=0.020316983573138714\n",
      "Surface training t=34928, loss=0.020344726741313934\n",
      "Surface training t=34929, loss=0.026577656157314777\n",
      "Surface training t=34930, loss=0.02179804816842079\n",
      "Surface training t=34931, loss=0.020178446546196938\n",
      "Surface training t=34932, loss=0.02301362156867981\n",
      "Surface training t=34933, loss=0.019782649353146553\n",
      "Surface training t=34934, loss=0.013613355346024036\n",
      "Surface training t=34935, loss=0.0259089358150959\n",
      "Surface training t=34936, loss=0.030492007732391357\n",
      "Surface training t=34937, loss=0.02249159663915634\n",
      "Surface training t=34938, loss=0.030717474408447742\n",
      "Surface training t=34939, loss=0.027749376371502876\n",
      "Surface training t=34940, loss=0.03235188499093056\n",
      "Surface training t=34941, loss=0.0234224796295166\n",
      "Surface training t=34942, loss=0.018969848286360502\n",
      "Surface training t=34943, loss=0.018471150659024715\n",
      "Surface training t=34944, loss=0.028222582302987576\n",
      "Surface training t=34945, loss=0.025806882418692112\n",
      "Surface training t=34946, loss=0.019465764053165913\n",
      "Surface training t=34947, loss=0.01811573188751936\n",
      "Surface training t=34948, loss=0.014019811991602182\n",
      "Surface training t=34949, loss=0.017545283772051334\n",
      "Surface training t=34950, loss=0.021025624126195908\n",
      "Surface training t=34951, loss=0.0252420655451715\n",
      "Surface training t=34952, loss=0.02560752909630537\n",
      "Surface training t=34953, loss=0.027730890549719334\n",
      "Surface training t=34954, loss=0.03186838701367378\n",
      "Surface training t=34955, loss=0.018848947249352932\n",
      "Surface training t=34956, loss=0.02562570385634899\n",
      "Surface training t=34957, loss=0.02473611757159233\n",
      "Surface training t=34958, loss=0.02060551755130291\n",
      "Surface training t=34959, loss=0.019661255180835724\n",
      "Surface training t=34960, loss=0.020308485254645348\n",
      "Surface training t=34961, loss=0.023836958222091198\n",
      "Surface training t=34962, loss=0.022735705599188805\n",
      "Surface training t=34963, loss=0.01930199656635523\n",
      "Surface training t=34964, loss=0.020365570671856403\n",
      "Surface training t=34965, loss=0.016280648298561573\n",
      "Surface training t=34966, loss=0.017339137848466635\n",
      "Surface training t=34967, loss=0.024161885492503643\n",
      "Surface training t=34968, loss=0.02390156500041485\n",
      "Surface training t=34969, loss=0.021423902362585068\n",
      "Surface training t=34970, loss=0.02267182432115078\n",
      "Surface training t=34971, loss=0.027643512934446335\n",
      "Surface training t=34972, loss=0.022161363624036312\n",
      "Surface training t=34973, loss=0.02699727565050125\n",
      "Surface training t=34974, loss=0.026327064260840416\n",
      "Surface training t=34975, loss=0.025325514376163483\n",
      "Surface training t=34976, loss=0.037122342735528946\n",
      "Surface training t=34977, loss=0.02853887900710106\n",
      "Surface training t=34978, loss=0.023209617473185062\n",
      "Surface training t=34979, loss=0.01999070681631565\n",
      "Surface training t=34980, loss=0.025170637294650078\n",
      "Surface training t=34981, loss=0.023379364982247353\n",
      "Surface training t=34982, loss=0.0173968356102705\n",
      "Surface training t=34983, loss=0.01793838944286108\n",
      "Surface training t=34984, loss=0.03014253731817007\n",
      "Surface training t=34985, loss=0.022308599203824997\n",
      "Surface training t=34986, loss=0.023372254334390163\n",
      "Surface training t=34987, loss=0.02462085708975792\n",
      "Surface training t=34988, loss=0.022074414417147636\n",
      "Surface training t=34989, loss=0.01872312417253852\n",
      "Surface training t=34990, loss=0.021898345090448856\n",
      "Surface training t=34991, loss=0.015109047759324312\n",
      "Surface training t=34992, loss=0.01684784423559904\n",
      "Surface training t=34993, loss=0.01605638675391674\n",
      "Surface training t=34994, loss=0.018933079205453396\n",
      "Surface training t=34995, loss=0.017416946589946747\n",
      "Surface training t=34996, loss=0.013738160021603107\n",
      "Surface training t=34997, loss=0.019315749406814575\n",
      "Surface training t=34998, loss=0.019791975151747465\n",
      "Surface training t=34999, loss=0.023263991810381413\n",
      "initial_shape (320,) derivs_tensor.shape (320, 2)\n",
      "self.tokens is ['u', 'du/dx0', 'd^2u/dx0^2']\n",
      "Here, derivs order is {'u': [None], 'du/dx0': [0], 'd^2u/dx0^2': [0, 0]}\n",
      "The cardinality of defined token pool is [3]\n",
      "Among them, the pool contains [3]\n",
      "Creating new equation, sparsity value [2.29105278e-05]\n",
      "New solution accepted, confirmed 1/12 solutions.\n",
      "Creating new equation, sparsity value [1.63536929e-06]\n",
      "New solution accepted, confirmed 2/12 solutions.\n",
      "Creating new equation, sparsity value [8.9282741e-08]\n",
      "New solution accepted, confirmed 3/12 solutions.\n",
      "Creating new equation, sparsity value [4.48556493e-11]\n",
      "New solution accepted, confirmed 4/12 solutions.\n",
      "Creating new equation, sparsity value [4.24010872e-07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New solution accepted, confirmed 5/12 solutions.\n",
      "Creating new equation, sparsity value [0.00443635]\n",
      "New solution accepted, confirmed 6/12 solutions.\n",
      "Creating new equation, sparsity value [2.60449761e-05]\n",
      "New solution accepted, confirmed 7/12 solutions.\n",
      "Creating new equation, sparsity value [0.00421066]\n",
      "New solution accepted, confirmed 8/12 solutions.\n",
      "Creating new equation, sparsity value [0.4397165]\n",
      "New solution accepted, confirmed 9/12 solutions.\n",
      "Creating new equation, sparsity value [4.93953099e-12]\n",
      "New solution accepted, confirmed 10/12 solutions.\n",
      "Creating new equation, sparsity value [1.37251881e-09]\n",
      "New solution accepted, confirmed 11/12 solutions.\n",
      "Creating new equation, sparsity value [2.5158653e-05]\n",
      "New solution accepted, confirmed 12/12 solutions.\n",
      "[0.16, 0.84] [[0.42, 0.5800000000000001], [0.26, 0.74], [0.76, 0.24], [0.5, 0.5], [0.32, 0.6799999999999999], [0.54, 0.45999999999999996], [0.16, 0.84]]\n",
      "best_obj 2\n",
      "Multiobjective optimization : 0-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 1-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 2-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 3-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 4-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 5-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 6-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 7-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 8-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 9-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 10-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 11-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 12-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 13-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 14-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 15-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 16-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 17-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 18-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 19-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 20-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 21-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 22-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 23-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 24-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 25-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 26-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 27-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 28-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 29-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 30-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 31-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 32-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 33-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 34-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 35-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 36-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 37-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 38-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 39-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 40-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 41-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 42-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 43-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 44-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 45-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 46-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 47-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 48-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 49-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 50-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 51-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 52-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 53-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 54-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 55-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 56-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 57-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 58-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 59-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 60-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 61-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 62-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 63-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 64-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 65-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 66-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 67-th epoch.\n",
      "During MO : processing 0-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 68-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 69-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 70-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 71-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 72-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 73-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 74-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 75-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 76-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 77-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 78-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 79-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 80-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 81-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 82-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 83-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 84-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 85-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 86-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 87-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 88-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 89-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 90-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 91-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 92-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 93-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 94-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 95-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 96-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 97-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 98-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "Multiobjective optimization : 99-th epoch.\n",
      "During MO : processing 0-th weight.\n",
      "During MO : processing 1-th weight.\n",
      "During MO : processing 2-th weight.\n",
      "During MO : processing 3-th weight.\n",
      "During MO : processing 4-th weight.\n",
      "During MO : processing 5-th weight.\n",
      "During MO : processing 6-th weight.\n",
      "During MO : processing 7-th weight.\n",
      "During MO : processing 8-th weight.\n",
      "During MO : processing 9-th weight.\n",
      "During MO : processing 10-th weight.\n",
      "During MO : processing 11-th weight.\n",
      "The optimization has been conducted.\n",
      "\n",
      "\n",
      "0-th non-dominated level\n",
      "\n",
      "\n",
      "0.0 * u{power: 2.0} + -1.0110582770581997 * u{power: 1.0} + 0.0 * du/dx0{power: 1.0} * d^2u/dx0^2{power: 1.0} + 0.0 * du/dx0{power: 2.0} * u{power: 1.0} + 0.0 * u{power: 2.0} * du/dx0{power: 1.0} + 0.021952156101475665 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.05974208487282046}} , with objective function values of [2.097144 2.5     ] \n",
      "\n",
      "-1.0047079868134445 * u{power: 1.0} + 0.0 * d^2u/dx0^2{power: 1.0} * u{power: 1.0} + -0.18089255641524984 * u{power: 2.0} * du/dx0{power: 1.0} + 0.19230730762704124 * du/dx0{power: 1.0} + 0.0 * u{power: 2.0} + 0.011183282281263822 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.008613160064087704}} , with objective function values of [0.74193618 5.        ] \n",
      "\n",
      "0.0 * u{power: 1.0} + 0.0 * d^2u/dx0^2{power: 1.0} + 0.0 * du/dx0{power: 1.0} + -1.0197061933962375 * u{power: 2.0} + 0.0 * u{power: 2.0} * du/dx0{power: 1.0} + 0.010457764456203493 = d^2u/dx0^2{power: 1.0} * u{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.03667289193472477}} , with objective function values of [1.67064687 3.5       ] \n",
      "\n",
      "0.0 * u{power: 1.0} * du/dx0{power: 1.0} + 0.19230730762704118 * du/dx0{power: 1.0} + -1.0047079868134439 * u{power: 1.0} + -0.18089255641524948 * u{power: 2.0} * du/dx0{power: 1.0} + 0.0 * u{power: 2.0} + 0.011183282281263557 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.021488562446729875}} , with objective function values of [0.74193618 5.        ] \n",
      "\n",
      "0.0 * du/dx0{power: 2.0} + 0.0 * du/dx0{power: 1.0} * u{power: 1.0} + 0.0 * u{power: 2.0} + 0.0 * u{power: 1.0} * d^2u/dx0^2{power: 1.0} + -1.0110582770581997 * u{power: 1.0} + 0.021952156101475665 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.0283864611443392}} , with objective function values of [2.097144 2.5     ] \n",
      "\n",
      "0.0 * du/dx0{power: 2.0} + 0.0 * d^2u/dx0^2{power: 1.0} * u{power: 1.0} + -1.0003527793361071 * u{power: 1.0} + 0.187412393154041 * du/dx0{power: 1.0} + 0.17107497251970538 * du/dx0{power: 1.0} * u{power: 1.0} * d^2u/dx0^2{power: 1.0} + 0.010208938915226819 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.03748517114027669}} , with objective function values of [0.73759843 7.        ] \n",
      "\n",
      "-1.0047079868134445 * u{power: 1.0} + -0.18089255641524984 * u{power: 2.0} * du/dx0{power: 1.0} + 0.0 * du/dx0{power: 2.0} + 0.19230730762704124 * du/dx0{power: 1.0} + 0.0 * d^2u/dx0^2{power: 2.0} + 0.011183282281263822 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.015830397552355376}} , with objective function values of [0.74193618 5.        ] \n",
      "\n",
      "0.0 * du/dx0{power: 2.0} + -1.0110582770581997 * u{power: 1.0} + 0.0 * d^2u/dx0^2{power: 2.0} + 0.0 * u{power: 1.0} * du/dx0{power: 1.0} * d^2u/dx0^2{power: 1.0} + 0.0 * d^2u/dx0^2{power: 1.0} * du/dx0{power: 1.0} + 0.021952156101475665 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.05365862114145223}} , with objective function values of [2.097144 2.5     ] \n",
      "\n",
      "-1.0047079868134445 * u{power: 1.0} + 0.0 * du/dx0{power: 2.0} + 0.0 * d^2u/dx0^2{power: 1.0} * du/dx0{power: 1.0} + 0.19230730762704154 * du/dx0{power: 1.0} + -0.18089255641524976 * u{power: 2.0} * du/dx0{power: 1.0} + 0.011183282281263975 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.014178070427050249}} , with objective function values of [0.74193618 5.        ] \n",
      "\n",
      "0.0 * d^2u/dx0^2{power: 1.0} * du/dx0{power: 1.0} + 0.0 * du/dx0{power: 2.0} * u{power: 1.0} + -1.0110582770581997 * u{power: 1.0} + 0.0 * du/dx0{power: 2.0} + 0.0 * u{power: 1.0} * d^2u/dx0^2{power: 1.0} + 0.021952156101475665 = d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.03574021833661281}} , with objective function values of [2.097144 2.5     ] \n",
      "\n",
      "0.0 * u{power: 1.0} * du/dx0{power: 1.0} * d^2u/dx0^2{power: 1.0} + 0.0 * u{power: 1.0} * du/dx0{power: 2.0} + -1.0197061933962375 * u{power: 2.0} + 0.0 * du/dx0{power: 1.0} + 0.0 * d^2u/dx0^2{power: 1.0} * u{power: 2.0} + 0.010457764456203493 = u{power: 1.0} * d^2u/dx0^2{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.13917917271388758}} , with objective function values of [1.67064687 3.5       ] \n",
      "\n",
      "-1.0197061933962375 * u{power: 2.0} + 0.0 * d^2u/dx0^2{power: 2.0} * du/dx0{power: 1.0} + 0.0 * u{power: 1.0} + 0.0 * u{power: 2.0} * du/dx0{power: 1.0} + 0.0 * d^2u/dx0^2{power: 1.0} + 0.010457764456203493 = d^2u/dx0^2{power: 1.0} * u{power: 1.0}\n",
      "{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.05927480366474771}} , with objective function values of [1.67064687 3.5       ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epde_search_obj = epde_discovery_as_ode(t_train, x_train, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "347125f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0.0 * u{power: 1.0} * d^2u/dx0^2{power: 1.0} + 0.0 * u{power: 1.0} * du/dx0{power: 2.0} + -1.0072168825573178 * u{power: 1.0} + 0.0 * du/dx0{power: 1.0} * d^2u/dx0^2{power: 1.0} * u{power: 1.0} + 0.0 * u{power: 2.0} * d^2u/dx0^2{power: 1.0} + 0.010336798407375216 = d^2u/dx0^2{power: 1.0}\\n{'terms_number': {'optimizable': False, 'value': 6}, 'max_factors_in_term': {'optimizable': False, 'value': {'factors_num': [1, 2, 3], 'probas': [0.4, 0.3, 0.3]}}, ('sparsity', 'u'): {'optimizable': True, 'value': 0.24369256577585763}}\""
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.text_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ec960001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\\begin{eqnarray*} \\frac{\\partial ^2u}{\\partial x_0^2} = -1.011u + 2.195\\cdot 10^{-2}  \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial ^2u}{\\partial x_0^2} = -1.005u + -1.809\\cdot 10^{-1} \\left(u\\right)^{2.0} \\cdot \\frac{\\partial u}{\\partial x_0} + 1.923\\cdot 10^{-1} \\frac{\\partial u}{\\partial x_0} + 1.118\\cdot 10^{-2}  \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial ^2u}{\\partial x_0^2} \\cdot u = -1.02\\left(u\\right)^{2.0} + 1.046\\cdot 10^{-2}  \\end{eqnarray*}$\n",
      "$\\begin{eqnarray*} \\frac{\\partial ^2u}{\\partial x_0^2} = -1.0u + 1.874\\cdot 10^{-1} \\frac{\\partial u}{\\partial x_0} + 1.711\\cdot 10^{-1} \\frac{\\partial u}{\\partial x_0} \\cdot u \\cdot \\frac{\\partial ^2u}{\\partial x_0^2} + 1.021\\cdot 10^{-2}  \\end{eqnarray*}$\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epde_search_obj.visualize_solutions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c10029",
   "metadata": {},
   "source": [
    "Let's extract equations, that can be useful for system description: \n",
    "\n",
    "*Note*: during other example launches, the discovered equations may be different. To proceed, adopt getting equations by complexities, according to the Pareto frontier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "62135a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eq_compl_5 = epde_search_obj.get_equations_by_complexity(5)[0]\n",
    "eq_compl_3_5 = epde_search_obj.get_equations_by_complexity(3.5)[0]\n",
    "eq_compl_7 = epde_search_obj.get_equations_by_complexity(7)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bccfff",
   "metadata": {},
   "source": [
    "The Pareto frontier is concave, having the relatively simple yet very descriptive equation, matching the equation $u'' = -u + 0.2 u' - 0.2 u^2 u' + \\delta$, $\\delta << 1$ which we will consider as a useful equation-based model of the process.\n",
    "\n",
    "Even though we already know, that it is very similar to the equation, from which we generated the data, we should still examine its predictive properties. Let's solve the equation with solver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a556d63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using explicitly sent system of equations.\n",
      "dimensionality is 1\n",
      "grid.shape is (320,)\n",
      "Shape of the grid for solver torch.Size([320, 1])\n",
      "Grid is  <class 'torch.Tensor'> torch.Size([320, 1])\n",
      "torch.Size([1])\n",
      "[2023-10-27 18:55:25.204678] initial (min) loss is 114.25733184814453\n",
      "[2023-10-27 18:55:25.338582] Print every 1000 step\n",
      "Step = 0 loss = 114.257332 normalized loss line= -0.000000x+1.000000. There was 1 stop dings already.\n",
      "[2023-10-27 18:57:06.004703] Print every 1000 step\n",
      "Step = 1000 loss = 0.000560 normalized loss line= -0.002775x+1.563721. There was 1 stop dings already.\n",
      "[2023-10-27 18:57:40.499169] No improvement in 100 steps\n",
      "Step = 1356 loss = 0.000065 normalized loss line= 0.060959x+4.262471. There was 1 stop dings already.\n",
      "[2023-10-27 18:57:50.585647] No improvement in 100 steps\n",
      "Step = 1456 loss = 0.000622 normalized loss line= 0.017430x+-0.053238. There was 2 stop dings already.\n",
      "[2023-10-27 18:58:00.498611] No improvement in 100 steps\n",
      "Step = 1556 loss = 0.174487 normalized loss line= 0.000057x+-0.000060. There was 3 stop dings already.\n",
      "[2023-10-27 18:58:17.368791] No improvement in 100 steps\n",
      "Step = 1731 loss = 0.195137 normalized loss line= -0.000015x+0.002732. There was 4 stop dings already.\n",
      "[2023-10-27 18:58:36.462726] No improvement in 100 steps\n",
      "Step = 1930 loss = 0.000100 normalized loss line= 0.476587x+-7.774933. There was 5 stop dings already.\n",
      "[2023-10-27 18:58:43.472264] Print every 1000 step\n",
      "Step = 2000 loss = 0.000019 normalized loss line= 2.565460x+-41.852368. There was 6 stop dings already.\n",
      "[2023-10-27 18:58:51.098915] No improvement in 100 steps\n",
      "Step = 2078 loss = 0.001526 normalized loss line= -0.006493x+0.472462. There was 6 stop dings already.\n",
      "[2023-10-27 18:59:26.061702] No improvement in 100 steps\n",
      "Step = 2439 loss = 0.001744 normalized loss line= 0.161112x+-4.935905. There was 7 stop dings already.\n",
      "[2023-10-27 18:59:44.465505] No improvement in 100 steps\n",
      "Step = 2630 loss = 0.068704 normalized loss line= 0.000317x+-0.004612. There was 8 stop dings already.\n",
      "[2023-10-27 19:00:05.606421] No improvement in 100 steps\n",
      "Step = 2845 loss = 0.120123 normalized loss line= 0.000299x+-0.008078. There was 9 stop dings already.\n",
      "[2023-10-27 19:00:15.466881] No improvement in 100 steps\n",
      "Step = 2945 loss = 0.000015 normalized loss line= -1.213092x+1979.490185. There was 10 stop dings already.\n",
      "[2023-10-27 19:00:21.108867] Print every 1000 step\n",
      "Step = 3000 loss = 0.000005 normalized loss line= -4.076946x+6652.649545. There was 11 stop dings already.\n",
      "[2023-10-27 19:00:40.201630] No improvement in 100 steps\n",
      "Step = 3167 loss = 0.001589 normalized loss line= 0.012293x+-0.385413. There was 11 stop dings already.\n",
      "Using explicitly sent system of equations.\n",
      "dimensionality is 1\n",
      "grid.shape is (320,)\n",
      "Shape of the grid for solver torch.Size([320, 1])\n",
      "Grid is  <class 'torch.Tensor'> torch.Size([320, 1])\n",
      "torch.Size([1])\n",
      "[2023-10-27 19:00:40.696888] initial (min) loss is 196.3807830810547\n",
      "[2023-10-27 19:00:40.807497] Print every 1000 step\n",
      "Step = 0 loss = 196.380783 normalized loss line= 0.000000x+1.000000. There was 1 stop dings already.\n",
      "[2023-10-27 19:01:50.616373] Print every 1000 step\n",
      "Step = 1000 loss = 0.000165 normalized loss line= -0.473463x+34.612788. There was 1 stop dings already.\n",
      "[2023-10-27 19:02:09.853673] No improvement in 100 steps\n",
      "Step = 1280 loss = 0.000517 normalized loss line= 0.011158x+-0.099722. There was 1 stop dings already.\n",
      "[2023-10-27 19:02:42.467247] No improvement in 100 steps\n",
      "Step = 1771 loss = 0.000380 normalized loss line= 6.184229x+-198.458141. There was 2 stop dings already.\n",
      "[2023-10-27 19:02:57.458969] Print every 1000 step\n",
      "Step = 2000 loss = 0.000049 normalized loss line= -0.002891x+1.379557. There was 3 stop dings already.\n",
      "[2023-10-27 19:03:24.892572] No improvement in 100 steps\n",
      "Step = 2389 loss = 0.000172 normalized loss line= 0.000041x+0.206047. There was 3 stop dings already.\n",
      "[2023-10-27 19:04:07.555050] Print every 1000 step\n",
      "Step = 3000 loss = 0.000020 normalized loss line= -0.000393x+1.070815. There was 4 stop dings already.\n",
      "[2023-10-27 19:04:14.489689] No improvement in 100 steps\n",
      "Step = 3098 loss = 0.000295 normalized loss line= -0.000023x+0.069003. There was 4 stop dings already.\n",
      "[2023-10-27 19:04:46.576217] No improvement in 100 steps\n",
      "Step = 3576 loss = 0.000361 normalized loss line= 0.000791x+0.022618. There was 5 stop dings already.\n",
      "[2023-10-27 19:05:14.095773] Print every 1000 step\n",
      "Step = 4000 loss = 0.503594 normalized loss line= -0.000000x+0.000031. There was 6 stop dings already.\n",
      "[2023-10-27 19:05:18.452670] No improvement in 100 steps\n",
      "Step = 4062 loss = 0.000275 normalized loss line= 4.603574x+-146.850933. There was 6 stop dings already.\n",
      "[2023-10-27 19:05:53.898385] No improvement in 100 steps\n",
      "Step = 4599 loss = 0.000158 normalized loss line= -0.000026x+0.082744. There was 7 stop dings already.\n",
      "[2023-10-27 19:06:21.865832] Print every 1000 step\n",
      "Step = 5000 loss = 0.000011 normalized loss line= -0.000432x+1.073953. There was 8 stop dings already.\n",
      "[2023-10-27 19:06:28.546415] No improvement in 100 steps\n",
      "Step = 5092 loss = 0.000226 normalized loss line= -0.000020x+0.048638. There was 8 stop dings already.\n",
      "[2023-10-27 19:06:56.960076] No improvement in 100 steps\n",
      "Step = 5514 loss = 0.000100 normalized loss line= 1.780045x+116.174507. There was 9 stop dings already.\n",
      "[2023-10-27 19:07:25.613212] No improvement in 100 steps\n",
      "Step = 5957 loss = 0.000519 normalized loss line= 0.077216x+-2.463396. There was 10 stop dings already.\n",
      "[2023-10-27 19:07:28.443307] Print every 1000 step\n",
      "Step = 6000 loss = 0.000012 normalized loss line= 3.245062x+-103.526684. There was 11 stop dings already.\n",
      "[2023-10-27 19:07:49.922269] No improvement in 100 steps\n",
      "Step = 6351 loss = 0.025929 normalized loss line= 0.022275x+-0.682806. There was 11 stop dings already.\n",
      "Using explicitly sent system of equations.\n",
      "dimensionality is 1\n",
      "grid.shape is (320,)\n",
      "Shape of the grid for solver torch.Size([320, 1])\n",
      "Grid is  <class 'torch.Tensor'> torch.Size([320, 1])\n",
      "torch.Size([1])\n",
      "[2023-10-27 19:07:50.339617] initial (min) loss is 223.90512084960938\n",
      "[2023-10-27 19:07:50.525267] Print every 1000 step\n",
      "Step = 0 loss = 223.905121 normalized loss line= -0.000000x+1.000000. There was 1 stop dings already.\n",
      "[2023-10-27 19:10:12.435790] Print every 1000 step\n",
      "Step = 1000 loss = 0.000113 normalized loss line= -0.003690x+0.832294. There was 1 stop dings already.\n",
      "[2023-10-27 19:10:39.247605] No improvement in 100 steps\n",
      "Step = 1203 loss = 0.000278 normalized loss line= -4.719180x+555.937133. There was 1 stop dings already.\n",
      "[2023-10-27 19:11:19.715492] No improvement in 100 steps\n",
      "Step = 1509 loss = 0.000036 normalized loss line= -0.126788x+32.126369. There was 2 stop dings already.\n",
      "[2023-10-27 19:11:45.376907] No improvement in 100 steps\n",
      "Step = 1709 loss = 0.000870 normalized loss line= -0.264556x+112.970188. There was 3 stop dings already.\n",
      "[2023-10-27 19:11:57.343735] No improvement in 100 steps\n",
      "Step = 1809 loss = 0.000021 normalized loss line= -0.461683x+33.675682. There was 4 stop dings already.\n",
      "[2023-10-27 19:12:09.883817] No improvement in 100 steps\n",
      "Step = 1909 loss = 0.000964 normalized loss line= -0.000103x+0.224012. There was 5 stop dings already.\n",
      "[2023-10-27 19:12:21.090062] Print every 1000 step\n",
      "Step = 2000 loss = 0.000263 normalized loss line= -0.000378x+0.820198. There was 6 stop dings already.\n",
      "[2023-10-27 19:12:22.330148] No improvement in 100 steps\n",
      "Step = 2009 loss = 0.000816 normalized loss line= 0.024140x+-0.199091. There was 6 stop dings already.\n",
      "[2023-10-27 19:12:50.746214] No improvement in 100 steps\n",
      "Step = 2247 loss = 0.000360 normalized loss line= 7.732536x+-208.933284. There was 7 stop dings already.\n",
      "[2023-10-27 19:13:02.919363] No improvement in 100 steps\n",
      "Step = 2347 loss = 0.000847 normalized loss line= -0.382231x+26.921681. There was 8 stop dings already.\n",
      "[2023-10-27 19:13:27.492650] No improvement in 100 steps\n",
      "Step = 2537 loss = 0.000111 normalized loss line= 0.017029x+0.788984. There was 9 stop dings already.\n",
      "[2023-10-27 19:13:46.896315] No improvement in 100 steps\n",
      "Step = 2685 loss = 0.387056 normalized loss line= -0.000001x+0.000524. There was 10 stop dings already.\n",
      "[2023-10-27 19:13:59.221020] No improvement in 100 steps\n",
      "Step = 2785 loss = 0.000017 normalized loss line= 78.509820x+-1951.388910. There was 11 stop dings already.\n"
     ]
    }
   ],
   "source": [
    "bop_u = get_ode_bop('u', grid_loc = t_test[0], value = x_test[0], term = [None], var = 0)\n",
    "bop_dudt = get_ode_bop('dudt', grid_loc = t_test[0], value = y_test[0], term = [0], var = 0)\n",
    "\n",
    "#get solution for equation with complexity 5\n",
    "pred_u_compl_5 = epde_search_obj.predict(system=eq_compl_5, boundary_conditions=[bop_u(), bop_dudt()], \n",
    "                                   grid = [t_test,], strategy='autograd')\n",
    "pred_u_compl_5 = pred_u_compl_5.reshape(pred_u_compl_5.size)\n",
    "\n",
    "#get solution for equation with complexity 3.5\n",
    "pred_u_compl_3_5 = epde_search_obj.predict(system=eq_compl_3_5, boundary_conditions=[bop_u(), bop_dudt()], \n",
    "                                   grid = [t_test,], strategy='autograd')\n",
    "pred_u_compl_3_5 = pred_u_compl_3_5.reshape(pred_u_compl_3_5.size)\n",
    "\n",
    "#get solution for equation with complexity 7\n",
    "pred_u_compl_7 = epde_search_obj.predict(system=eq_compl_7, boundary_conditions=[bop_u(), bop_dudt()], \n",
    "                                   grid = [t_test,], strategy='autograd')\n",
    "pred_u_compl_7 = pred_u_compl_7.reshape(pred_u_compl_7.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "db10489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE on the test dataset for eq with $C = 3.5$ is 1.0291776644268698\n",
      "MAPE on the test dataset for eq with $C = 5$ is 0.1408898496016532\n",
      "MAPE on the test dataset for eq with $C = 7$ is 0.45236988834056024\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mae_compl_3_5 = np.mean(np.abs((pred_u_compl_3_5 - x_test)/x_test))\n",
    "mae_compl_5 = np.mean(np.abs((pred_u_compl_5 - x_test)/x_test))\n",
    "mae_compl_7 = np.mean(np.abs((pred_u_compl_7 - x_test)/x_test))\n",
    "\n",
    "print(f'MAPE on the test dataset for eq with $C = 3.5$ is {mae_compl_3_5}')\n",
    "print(f'MAPE on the test dataset for eq with $C = 5$ is {mae_compl_5}')\n",
    "print(f'MAPE on the test dataset for eq with $C = 7$ is {mae_compl_7}')\n",
    "\n",
    "plt.plot(t_test, pred_u_compl_5, color = 'b', label = 'Automatic solution of the equation, $C = 5$')\n",
    "plt.plot(t_test, pred_u_compl_3_5, color = 'magenta', label = 'Automatic solution of the equation, $C = 3.5$')\n",
    "plt.plot(t_test, pred_u_compl_7, color = 'seagreen', label = 'Automatic solution of the equation, $C = 7$')\n",
    "\n",
    "plt.plot(t_test[::3], x_test[::3], '*', color = 'r', label = 'Referential data')\n",
    "plt.grid()\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda446ab",
   "metadata": {},
   "source": [
    "Here, we can notice, that the \"correct\" governing equation, which closely matches the Van der Pol equation, has the lowest MAPE metric on the test dataset, even outperforming the equations with higher complexities. The equation with lower complexity can represent only the averaged values of the system. Thus, we can select the equation like $u'' = -u + 0.2 u' - 0.2 u^2 u' + \\delta$ for system representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d8d08",
   "metadata": {},
   "source": [
    "## Lotka-Volterra system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c0d69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
